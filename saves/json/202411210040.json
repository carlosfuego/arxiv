[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v1",
                "updated": "2024-11-18T11:12:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v4",
                "updated": "2024-11-13T16:33:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    33,
                    33,
                    2,
                    318,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.11844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11844v2",
                "updated": "2024-11-19T18:59:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    59,
                    42,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-18T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "title": "Generative World Explorer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative World Explorer"
                },
                "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jieneng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieneng Chen"
                },
                "author": "Jieneng Chen",
                "arxiv_comment": "Website: generative-world-explorer.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12741v1",
                "updated": "2024-11-19T18:59:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    59,
                    31,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    59,
                    31,
                    1,
                    324,
                    0
                ],
                "title": "RR Lyrae Stars in Intermediate-age Magellanic Clusters: Membership\n  Probabilities and Delay Time Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RR Lyrae Stars in Intermediate-age Magellanic Clusters: Membership\n  Probabilities and Delay Time Distribution"
                },
                "summary": "Recent works have challenged our canonical view of RR Lyrae (RRL) stars as\ntracers of exclusively old populations ($\\gtrsim10$~Gyr) by proposing a\nfraction of these stars to be of intermediate ages ($\\sim$2-5~Gyr). Since it is\ncurrently not possible to infer stellar ages directly for individual RRL stars,\nour goal in this work is to search for these in association to intermediate-age\nclusters whose reliable ages can then be safely be attributed to the RRL. We\nused the Gaia DR3 Specific Object Study and OGLE IV public catalogues to search\nfor RRL stars around stellar clusters older than 1~Gyr in the Large and Small\nMagellanic Clouds. Modelling membership probabilities based on proper motion\nand photometric distance we obtained a list of 302 RRL stars associated with\nMagellanic clusters. Of these, 23 RRL are likely members of 10 intermediate-age\nclusters: 3 and 7 in the Small and Large Magellanic Clouds, respectively. By\nmodelling the inferred expectation values of the number of RRL stars per\ncluster, we inferred the delay time distribution of the RRL in three age\nranges. For the old population ($>8$~Gyr) we find $2.6^{+0.4}_{-0.3}$ RRL$/10^5\nM_\\odot$. For the young (1-2 Gyr) and intermediate age (2-8 Gyr) populations we\nfind rates of $0.9^{+0.3}_{-0.2}$ and $0.27^{+0.1}_{-0.09}$ RRL$/10^5 M_\\odot$,\nrespectively. While radial velocities are necessary for definitively confirming\ncluster memberships, the high-probability list of intermediate-age RRL stars\npresented here offers a promising opportunity for the first direct confirmation\nof these enigmatic stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have challenged our canonical view of RR Lyrae (RRL) stars as\ntracers of exclusively old populations ($\\gtrsim10$~Gyr) by proposing a\nfraction of these stars to be of intermediate ages ($\\sim$2-5~Gyr). Since it is\ncurrently not possible to infer stellar ages directly for individual RRL stars,\nour goal in this work is to search for these in association to intermediate-age\nclusters whose reliable ages can then be safely be attributed to the RRL. We\nused the Gaia DR3 Specific Object Study and OGLE IV public catalogues to search\nfor RRL stars around stellar clusters older than 1~Gyr in the Large and Small\nMagellanic Clouds. Modelling membership probabilities based on proper motion\nand photometric distance we obtained a list of 302 RRL stars associated with\nMagellanic clusters. Of these, 23 RRL are likely members of 10 intermediate-age\nclusters: 3 and 7 in the Small and Large Magellanic Clouds, respectively. By\nmodelling the inferred expectation values of the number of RRL stars per\ncluster, we inferred the delay time distribution of the RRL in three age\nranges. For the old population ($>8$~Gyr) we find $2.6^{+0.4}_{-0.3}$ RRL$/10^5\nM_\\odot$. For the young (1-2 Gyr) and intermediate age (2-8 Gyr) populations we\nfind rates of $0.9^{+0.3}_{-0.2}$ and $0.27^{+0.1}_{-0.09}$ RRL$/10^5 M_\\odot$,\nrespectively. While radial velocities are necessary for definitively confirming\ncluster memberships, the high-probability list of intermediate-age RRL stars\npresented here offers a promising opportunity for the first direct confirmation\nof these enigmatic stars."
                },
                "authors": [
                    {
                        "name": "Bolivia Cuevas-Otahola"
                    },
                    {
                        "name": "Cecilia Mateu"
                    },
                    {
                        "name": "Ivan Cabrera-Ziri"
                    },
                    {
                        "name": "Gustavo Bruzual"
                    },
                    {
                        "name": "Fabiola Hernndez-Prez"
                    },
                    {
                        "name": "Gladis Magris"
                    },
                    {
                        "name": "Holger Baumgardt"
                    }
                ],
                "author_detail": {
                    "name": "Holger Baumgardt"
                },
                "author": "Holger Baumgardt",
                "arxiv_comment": "14 pages, 8 figures, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "85",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12736v1",
                "updated": "2024-11-19T18:58:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:58:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n  Models"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Salma Kharrat"
                    },
                    {
                        "name": "Fares Fourati"
                    },
                    {
                        "name": "Marco Canini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Canini"
                },
                "author": "Marco Canini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12726v1",
                "updated": "2024-11-19T18:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    48,
                    0,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    48,
                    0,
                    1,
                    324,
                    0
                ],
                "title": "LazyDINO: Fast, scalable, and efficiently amortized Bayesian inversion\n  via structure-exploiting and surrogate-driven measure transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDINO: Fast, scalable, and efficiently amortized Bayesian inversion\n  via structure-exploiting and surrogate-driven measure transport"
                },
                "summary": "We present LazyDINO, a transport map variational inference method for fast,\nscalable, and efficiently amortized solutions of high-dimensional nonlinear\nBayesian inverse problems with expensive parameter-to-observable (PtO) maps.\nOur method consists of an offline phase in which we construct a\nderivative-informed neural surrogate of the PtO map using joint samples of the\nPtO map and its Jacobian. During the online phase, when given observational\ndata, we seek rapid posterior approximation using surrogate-driven training of\na lazy map [Brennan et al., NeurIPS, (2020)], i.e., a structure-exploiting\ntransport map with low-dimensional nonlinearity. The trained lazy map then\nproduces approximate posterior samples or density evaluations. Our surrogate\nconstruction is optimized for amortized Bayesian inversion using lazy map\nvariational inference. We show that (i) the derivative-based reduced basis\narchitecture [O'Leary-Roseberry et al., Comput. Methods Appl. Mech. Eng., 388\n(2022)] minimizes the upper bound on the expected error in surrogate posterior\napproximation, and (ii) the derivative-informed training formulation\n[O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] minimizes the expected\nerror due to surrogate-driven transport map optimization. Our numerical results\ndemonstrate that LazyDINO is highly efficient in cost amortization for Bayesian\ninversion. We observe one to two orders of magnitude reduction of offline cost\nfor accurate posterior approximation, compared to simulation-based amortized\ninference via conditional transport and conventional surrogate-driven\ntransport. In particular, LazyDINO outperforms Laplace approximation\nconsistently using fewer than 1000 offline samples, while other amortized\ninference methods struggle and sometimes fail at 16,000 offline samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LazyDINO, a transport map variational inference method for fast,\nscalable, and efficiently amortized solutions of high-dimensional nonlinear\nBayesian inverse problems with expensive parameter-to-observable (PtO) maps.\nOur method consists of an offline phase in which we construct a\nderivative-informed neural surrogate of the PtO map using joint samples of the\nPtO map and its Jacobian. During the online phase, when given observational\ndata, we seek rapid posterior approximation using surrogate-driven training of\na lazy map [Brennan et al., NeurIPS, (2020)], i.e., a structure-exploiting\ntransport map with low-dimensional nonlinearity. The trained lazy map then\nproduces approximate posterior samples or density evaluations. Our surrogate\nconstruction is optimized for amortized Bayesian inversion using lazy map\nvariational inference. We show that (i) the derivative-based reduced basis\narchitecture [O'Leary-Roseberry et al., Comput. Methods Appl. Mech. Eng., 388\n(2022)] minimizes the upper bound on the expected error in surrogate posterior\napproximation, and (ii) the derivative-informed training formulation\n[O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] minimizes the expected\nerror due to surrogate-driven transport map optimization. Our numerical results\ndemonstrate that LazyDINO is highly efficient in cost amortization for Bayesian\ninversion. We observe one to two orders of magnitude reduction of offline cost\nfor accurate posterior approximation, compared to simulation-based amortized\ninference via conditional transport and conventional surrogate-driven\ntransport. In particular, LazyDINO outperforms Laplace approximation\nconsistently using fewer than 1000 offline samples, while other amortized\ninference methods struggle and sometimes fail at 16,000 offline samples."
                },
                "authors": [
                    {
                        "name": "Lianghao Cao"
                    },
                    {
                        "name": "Joshua Chen"
                    },
                    {
                        "name": "Michael Brennan"
                    },
                    {
                        "name": "Thomas O'Leary-Roseberry"
                    },
                    {
                        "name": "Youssef Marzouk"
                    },
                    {
                        "name": "Omar Ghattas"
                    }
                ],
                "author_detail": {
                    "name": "Omar Ghattas"
                },
                "author": "Omar Ghattas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14035v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14035v5",
                "updated": "2024-11-19T18:33:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    33,
                    12,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-17T21:17:17Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    21,
                    17,
                    17,
                    3,
                    291,
                    0
                ],
                "title": "Optimal Communication and Key Rate Region for Hierarchical Secure\n  Aggregation with User Collusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Communication and Key Rate Region for Hierarchical Secure\n  Aggregation with User Collusion"
                },
                "summary": "Secure aggregation is concerned with the task of securely uploading the\ninputs of multiple users to an aggregation server without letting the server\nknow the inputs beyond their summation. It finds broad applications in\ndistributed machine learning paradigms such as federated learning (FL) where\nmultiple clients, each having access to a proprietary dataset, periodically\nupload their locally trained models (abstracted as inputs) to a parameter\nserver which then generates an aggregate (e.g., averaged) model that is sent\nback to the clients as an initializing point for a new round of local training.\nTo enhance the data privacy of the clients, secure aggregation protocols are\ndeveloped using techniques from cryptography to ensure that the server infers\nno more information of the users' inputs beyond the desired aggregated input,\neven if the server can collude with some users. Although laying the ground for\nunderstanding the fundamental utility-security trade-off in secure aggregation,\nthe simple star client-server architecture cannot capture more complex network\narchitectures used in practical systems. Motivated by hierarchical federated\nlearning, we investigate the secure aggregation problem in a $3$-layer\nhierarchical network consisting of clustered users connecting to an aggregation\nserver through an intermediate layer of relays. Besides the conventional server\nsecurity which requires that the server learns nothing beyond the desired sum\nof inputs, relay security is also imposed so that the relays infer nothing\nabout the users' inputs and remain oblivious. For such a hierarchical secure\naggregation (HSA) problem, we characterize the optimal multifaceted trade-off\nbetween communication (in terms of user-to-relay and relay-to-server\ncommunication rates) and secret key generation efficiency (in terms of\nindividual key and source key rates).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure aggregation is concerned with the task of securely uploading the\ninputs of multiple users to an aggregation server without letting the server\nknow the inputs beyond their summation. It finds broad applications in\ndistributed machine learning paradigms such as federated learning (FL) where\nmultiple clients, each having access to a proprietary dataset, periodically\nupload their locally trained models (abstracted as inputs) to a parameter\nserver which then generates an aggregate (e.g., averaged) model that is sent\nback to the clients as an initializing point for a new round of local training.\nTo enhance the data privacy of the clients, secure aggregation protocols are\ndeveloped using techniques from cryptography to ensure that the server infers\nno more information of the users' inputs beyond the desired aggregated input,\neven if the server can collude with some users. Although laying the ground for\nunderstanding the fundamental utility-security trade-off in secure aggregation,\nthe simple star client-server architecture cannot capture more complex network\narchitectures used in practical systems. Motivated by hierarchical federated\nlearning, we investigate the secure aggregation problem in a $3$-layer\nhierarchical network consisting of clustered users connecting to an aggregation\nserver through an intermediate layer of relays. Besides the conventional server\nsecurity which requires that the server learns nothing beyond the desired sum\nof inputs, relay security is also imposed so that the relays infer nothing\nabout the users' inputs and remain oblivious. For such a hierarchical secure\naggregation (HSA) problem, we characterize the optimal multifaceted trade-off\nbetween communication (in terms of user-to-relay and relay-to-server\ncommunication rates) and secret key generation efficiency (in terms of\nindividual key and source key rates)."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Shiqiang Wang"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14035v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14035v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12712v1",
                "updated": "2024-11-19T18:27:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    27,
                    25,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:27:25Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    27,
                    25,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular,\n  Nervous System, and Digestive Disorders Using Advanced LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular,\n  Nervous System, and Digestive Disorders Using Advanced LLMs"
                },
                "summary": "In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks."
                },
                "authors": [
                    {
                        "name": "Ahmed Akib Jawad Karim"
                    },
                    {
                        "name": "Muhammad Zawad Mahmud"
                    },
                    {
                        "name": "Samiha Islam"
                    },
                    {
                        "name": "Aznur Azam"
                    }
                ],
                "author_detail": {
                    "name": "Aznur Azam"
                },
                "author": "Aznur Azam",
                "arxiv_comment": "7 Pages, 4 tables and 11 figures. Under review in a IEEE conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01306v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01306v4",
                "updated": "2024-11-19T18:12:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    12,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-02T10:53:36Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    10,
                    53,
                    36,
                    4,
                    33,
                    0
                ],
                "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KTO: Model Alignment as Prospect Theoretic Optimization"
                },
                "summary": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration."
                },
                "authors": [
                    {
                        "name": "Kawin Ethayarajh"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Douwe Kiela"
                    }
                ],
                "author_detail": {
                    "name": "Douwe Kiela"
                },
                "author": "Douwe Kiela",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01306v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01306v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12701v1",
                "updated": "2024-11-19T18:11:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    11,
                    36,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:11:36Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    11,
                    36,
                    1,
                    324,
                    0
                ],
                "title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations"
                },
                "summary": "Large Language Models (LLMs) are vulnerable to backdoor attacks, where hidden\ntriggers can maliciously manipulate model behavior. While several backdoor\nattack methods have been proposed, the mechanisms by which backdoor functions\noperate in LLMs remain underexplored. In this paper, we move beyond attacking\nLLMs and investigate backdoor functionality through the novel lens of natural\nlanguage explanations. Specifically, we leverage LLMs' generative capabilities\nto produce human-understandable explanations for their decisions, allowing us\nto compare explanations for clean and poisoned samples. We explore various\nbackdoor attacks and embed the backdoor into LLaMA models for multiple tasks.\nOur experiments show that backdoored models produce higher-quality explanations\nfor clean data compared to poisoned data, while generating significantly more\nconsistent explanations for poisoned data than for clean data. We further\nanalyze the explanation generation process, revealing that at the token level,\nthe explanation token of poisoned samples only appears in the final few\ntransformer layers of the LLM. At the sentence level, attention dynamics\nindicate that poisoned inputs shift attention from the input context when\ngenerating the explanation. These findings deepen our understanding of backdoor\nattack mechanisms in LLMs and offer a framework for detecting such\nvulnerabilities through explainability techniques, contributing to the\ndevelopment of more secure LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are vulnerable to backdoor attacks, where hidden\ntriggers can maliciously manipulate model behavior. While several backdoor\nattack methods have been proposed, the mechanisms by which backdoor functions\noperate in LLMs remain underexplored. In this paper, we move beyond attacking\nLLMs and investigate backdoor functionality through the novel lens of natural\nlanguage explanations. Specifically, we leverage LLMs' generative capabilities\nto produce human-understandable explanations for their decisions, allowing us\nto compare explanations for clean and poisoned samples. We explore various\nbackdoor attacks and embed the backdoor into LLaMA models for multiple tasks.\nOur experiments show that backdoored models produce higher-quality explanations\nfor clean data compared to poisoned data, while generating significantly more\nconsistent explanations for poisoned data than for clean data. We further\nanalyze the explanation generation process, revealing that at the token level,\nthe explanation token of poisoned samples only appears in the final few\ntransformer layers of the LLM. At the sentence level, attention dynamics\nindicate that poisoned inputs shift attention from the input context when\ngenerating the explanation. These findings deepen our understanding of backdoor\nattack mechanisms in LLMs and offer a framework for detecting such\nvulnerabilities through explainability techniques, contributing to the\ndevelopment of more secure LLMs."
                },
                "authors": [
                    {
                        "name": "Huaizhi Ge"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12697v1",
                "updated": "2024-11-19T18:06:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    6,
                    6,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:06:06Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    6,
                    6,
                    1,
                    324,
                    0
                ],
                "title": "Attribute Inference Attacks for Federated Regression Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute Inference Attacks for Federated Regression Tasks"
                },
                "summary": "Federated Learning (FL) enables multiple clients, such as mobile phones and\nIoT devices, to collaboratively train a global machine learning model while\nkeeping their data localized. However, recent studies have revealed that the\ntraining phase of FL is vulnerable to reconstruction attacks, such as attribute\ninference attacks (AIA), where adversaries exploit exchanged messages and\nauxiliary public information to uncover sensitive attributes of targeted\nclients. While these attacks have been extensively studied in the context of\nclassification tasks, their impact on regression tasks remains largely\nunexplored. In this paper, we address this gap by proposing novel model-based\nAIAs specifically designed for regression tasks in FL environments. Our\napproach considers scenarios where adversaries can either eavesdrop on\nexchanged messages or directly interfere with the training process. We\nbenchmark our proposed attacks against state-of-the-art methods using\nreal-world datasets. The results demonstrate a significant increase in\nreconstruction accuracy, particularly in heterogeneous client datasets, a\ncommon scenario in FL. The efficacy of our model-based AIAs makes them better\ncandidates for empirically quantifying privacy leakage for federated regression\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables multiple clients, such as mobile phones and\nIoT devices, to collaboratively train a global machine learning model while\nkeeping their data localized. However, recent studies have revealed that the\ntraining phase of FL is vulnerable to reconstruction attacks, such as attribute\ninference attacks (AIA), where adversaries exploit exchanged messages and\nauxiliary public information to uncover sensitive attributes of targeted\nclients. While these attacks have been extensively studied in the context of\nclassification tasks, their impact on regression tasks remains largely\nunexplored. In this paper, we address this gap by proposing novel model-based\nAIAs specifically designed for regression tasks in FL environments. Our\napproach considers scenarios where adversaries can either eavesdrop on\nexchanged messages or directly interfere with the training process. We\nbenchmark our proposed attacks against state-of-the-art methods using\nreal-world datasets. The results demonstrate a significant increase in\nreconstruction accuracy, particularly in heterogeneous client datasets, a\ncommon scenario in FL. The efficacy of our model-based AIAs makes them better\ncandidates for empirically quantifying privacy leakage for federated regression\ntasks."
                },
                "authors": [
                    {
                        "name": "Francesco Diana"
                    },
                    {
                        "name": "Othmane Marfoq"
                    },
                    {
                        "name": "Chuan Xu"
                    },
                    {
                        "name": "Giovanni Neglia"
                    },
                    {
                        "name": "Frdric Giroire"
                    },
                    {
                        "name": "Eoin Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Eoin Thomas"
                },
                "author": "Eoin Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12593v1",
                "updated": "2024-11-19T18:04:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    4,
                    13,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:04:13Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    4,
                    13,
                    1,
                    324,
                    0
                ],
                "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction"
                },
                "summary": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%."
                },
                "authors": [
                    {
                        "name": "Yuanbin Man"
                    },
                    {
                        "name": "Ying Huang"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Miao Yin"
                    }
                ],
                "author_detail": {
                    "name": "Miao Yin"
                },
                "author": "Miao Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12692v1",
                "updated": "2024-11-19T17:59:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    59,
                    12,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:59:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    59,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast\n  LLM Inference"
                },
                "summary": "Leveraging sparsity is crucial for optimizing large language model inference.\nhowever, modern LLMs employing SiLU as their activation function exhibit\nminimal activation sparsity. Recent research has proposed replacing SiLU with\nReLU to induce significant activation sparsity and showed no downstream task\naccuracy degradation through fine tuning. However, taking full advantage of it\nrequired training a predictor to estimate this sparsity. In this paper, we\nintroduce SparseInfer, a simple, light weight, and training free predictor for\nactivation sparsity of ReLU field LLMs, in which activation sparsity is\npredicted by comparing only the sign bits of inputs and weights. To compensate\nfor possible prediction inaccuracy, an adaptive tuning of the predictor's\nconservativeness is enabled, which can also serve as a control knob for\noptimizing LLM inference. The proposed method achieves approximately faster\ninference speed over the state of the art, with negligible accuracy loss of\nwithin 1%p.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging sparsity is crucial for optimizing large language model inference.\nhowever, modern LLMs employing SiLU as their activation function exhibit\nminimal activation sparsity. Recent research has proposed replacing SiLU with\nReLU to induce significant activation sparsity and showed no downstream task\naccuracy degradation through fine tuning. However, taking full advantage of it\nrequired training a predictor to estimate this sparsity. In this paper, we\nintroduce SparseInfer, a simple, light weight, and training free predictor for\nactivation sparsity of ReLU field LLMs, in which activation sparsity is\npredicted by comparing only the sign bits of inputs and weights. To compensate\nfor possible prediction inaccuracy, an adaptive tuning of the predictor's\nconservativeness is enabled, which can also serve as a control knob for\noptimizing LLM inference. The proposed method achieves approximately faster\ninference speed over the state of the art, with negligible accuracy loss of\nwithin 1%p."
                },
                "authors": [
                    {
                        "name": "Jiho Shin"
                    },
                    {
                        "name": "Hoeseok Yang"
                    },
                    {
                        "name": "Youngmin Yi"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Yi"
                },
                "author": "Youngmin Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08316v3",
                "updated": "2024-11-19T17:49:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    49,
                    27,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-12T15:16:40Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    15,
                    16,
                    40,
                    2,
                    164,
                    0
                ],
                "title": "Is Programming by Example solved by LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Programming by Example solved by LLMs?"
                },
                "summary": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n\"solved\" PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n\"solved\" PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short."
                },
                "authors": [
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00248v2",
                "updated": "2024-11-19T17:46:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    46,
                    48,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-31T22:58:08Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    22,
                    58,
                    8,
                    3,
                    305,
                    0
                ],
                "title": "A Demonstration of Adaptive Collaboration of Large Language Models for\n  Medical Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Demonstration of Adaptive Collaboration of Large Language Models for\n  Medical Decision-Making"
                },
                "summary": "Medical Decision-Making (MDM) is a multi-faceted process that requires\nclinicians to assess complex multi-modal patient data patient, often\ncollaboratively. Large Language Models (LLMs) promise to streamline this\nprocess by synthesizing vast medical knowledge and multi-modal health data.\nHowever, single-agent are often ill-suited for nuanced medical contexts\nrequiring adaptable, collaborative problem-solving. Our MDAgents addresses this\nneed by dynamically assigning collaboration structures to LLMs based on task\ncomplexity, mimicking real-world clinical collaboration and decision-making.\nThis framework improves diagnostic accuracy and supports adaptive responses in\ncomplex, real-world medical scenarios, making it a valuable tool for clinicians\nin various healthcare settings, and at the same time, being more efficient in\nterms of computing cost than static multi-agent decision making methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Decision-Making (MDM) is a multi-faceted process that requires\nclinicians to assess complex multi-modal patient data patient, often\ncollaboratively. Large Language Models (LLMs) promise to streamline this\nprocess by synthesizing vast medical knowledge and multi-modal health data.\nHowever, single-agent are often ill-suited for nuanced medical contexts\nrequiring adaptable, collaborative problem-solving. Our MDAgents addresses this\nneed by dynamically assigning collaboration structures to LLMs based on task\ncomplexity, mimicking real-world clinical collaboration and decision-making.\nThis framework improves diagnostic accuracy and supports adaptive responses in\ncomplex, real-world medical scenarios, making it a valuable tool for clinicians\nin various healthcare settings, and at the same time, being more efficient in\nterms of computing cost than static multi-agent decision making methods."
                },
                "authors": [
                    {
                        "name": "Yubin Kim"
                    },
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Hyewon Jeong"
                    },
                    {
                        "name": "Cristina Grau-Vilchez"
                    },
                    {
                        "name": "Yik Siu Chan"
                    },
                    {
                        "name": "Xuhai Xu"
                    },
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Hyeonhoon Lee"
                    },
                    {
                        "name": "Cynthia Breazeal"
                    },
                    {
                        "name": "Hae Won Park"
                    }
                ],
                "author_detail": {
                    "name": "Hae Won Park"
                },
                "author": "Hae Won Park",
                "arxiv_comment": "Under Review for ML4H 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12685v1",
                "updated": "2024-11-19T17:45:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    45,
                    12,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:45:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    45,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "Enhanced Sign Language Translation between American Sign Language (ASL)\n  and Indian Sign Language (ISL) Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Sign Language Translation between American Sign Language (ASL)\n  and Indian Sign Language (ISL) Using LLMs"
                },
                "summary": "We have come up with a research that hopes to provide a bridge between the\nusers of American Sign Language and the users of spoken language and Indian\nSign Language (ISL). The research enabled us to create a novel framework that\nwe have developed for Learner Systems. Leveraging art of Large models to create\nkey features including: - Real-time translation between these two sign\nlanguages in an efficient manner. Making LLM's capability available for\nseamless translations to ISL. Here is the full study showing its implementation\nin this paper. The core of the system is a sophisticated pipeline that begins\nwith reclassification and recognition of ASL gestures based on a strong Random\nForest Classifier. By recognizing the ASL, it is translated into text which can\nbe more easily processed. Highly evolved natural language NLP (Natural Language\nProcessing) techniques come in handy as they play a role in our LLM integration\nwhere you then use LLMs to be able to convert the ASL text to ISL which\nprovides you with the intent of sentence or phrase. The final step is to\nsynthesize the translated text back into ISL gestures, creating an end-to-end\ntranslation experience using RIFE-Net. This framework is tasked with key\nchallenges such as automatically dealing with gesture variability and\novercoming the linguistic differences between ASL and ISL. By automating the\ntranslation process, we hope to vastly improve accessibility for sign language\nusers. No longer will the communication gap between ASL and ISL create\nbarriers; this totally cool innovation aims to bring our communities closer\ntogether. And we believe, with full confidence in our framework, that we're\nable to apply the same principles across a wide variety of sign language\ndialects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have come up with a research that hopes to provide a bridge between the\nusers of American Sign Language and the users of spoken language and Indian\nSign Language (ISL). The research enabled us to create a novel framework that\nwe have developed for Learner Systems. Leveraging art of Large models to create\nkey features including: - Real-time translation between these two sign\nlanguages in an efficient manner. Making LLM's capability available for\nseamless translations to ISL. Here is the full study showing its implementation\nin this paper. The core of the system is a sophisticated pipeline that begins\nwith reclassification and recognition of ASL gestures based on a strong Random\nForest Classifier. By recognizing the ASL, it is translated into text which can\nbe more easily processed. Highly evolved natural language NLP (Natural Language\nProcessing) techniques come in handy as they play a role in our LLM integration\nwhere you then use LLMs to be able to convert the ASL text to ISL which\nprovides you with the intent of sentence or phrase. The final step is to\nsynthesize the translated text back into ISL gestures, creating an end-to-end\ntranslation experience using RIFE-Net. This framework is tasked with key\nchallenges such as automatically dealing with gesture variability and\novercoming the linguistic differences between ASL and ISL. By automating the\ntranslation process, we hope to vastly improve accessibility for sign language\nusers. No longer will the communication gap between ASL and ISL create\nbarriers; this totally cool innovation aims to bring our communities closer\ntogether. And we believe, with full confidence in our framework, that we're\nable to apply the same principles across a wide variety of sign language\ndialects."
                },
                "authors": [
                    {
                        "name": "Malay Kumar"
                    },
                    {
                        "name": "S. Sarvajit Visagan"
                    },
                    {
                        "name": "Tanish Sarang Mahajan"
                    },
                    {
                        "name": "Anisha Natarajan"
                    }
                ],
                "author_detail": {
                    "name": "Anisha Natarajan"
                },
                "author": "Anisha Natarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03530v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03530v4",
                "updated": "2024-11-19T17:41:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    41,
                    0,
                    1,
                    324,
                    0
                ],
                "published": "2023-06-06T09:26:43Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    9,
                    26,
                    43,
                    1,
                    157,
                    0
                ],
                "title": "RLtools: A Fast, Portable Deep Reinforcement Learning Library for\n  Continuous Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLtools: A Fast, Portable Deep Reinforcement Learning Library for\n  Continuous Control"
                },
                "summary": "Deep Reinforcement Learning (RL) can yield capable agents and control\npolicies in several domains but is commonly plagued by prohibitively long\ntraining times. Additionally, in the case of continuous control problems, the\napplicability of learned policies on real-world embedded devices is limited due\nto the lack of real-time guarantees and portability of existing libraries. To\naddress these challenges, we present RLtools, a dependency-free, header-only,\npure C++ library for deep supervised and reinforcement learning. Its novel\narchitecture allows RLtools to be used on a wide variety of platforms, from HPC\nclusters over workstations and laptops to smartphones, smartwatches, and\nmicrocontrollers. Specifically, due to the tight integration of the RL\nalgorithms with simulation environments, RLtools can solve popular RL problems\nup to 76 times faster than other popular RL frameworks. We also benchmark the\ninference on a diverse set of microcontrollers and show that in most cases our\noptimized implementation is by far the fastest. Finally, RLtools enables the\nfirst-ever demonstration of training a deep RL algorithm directly on a\nmicrocontroller, giving rise to the field of TinyRL. The source code as well as\ndocumentation and live demos are available through our project page at\nhttps://rl.tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (RL) can yield capable agents and control\npolicies in several domains but is commonly plagued by prohibitively long\ntraining times. Additionally, in the case of continuous control problems, the\napplicability of learned policies on real-world embedded devices is limited due\nto the lack of real-time guarantees and portability of existing libraries. To\naddress these challenges, we present RLtools, a dependency-free, header-only,\npure C++ library for deep supervised and reinforcement learning. Its novel\narchitecture allows RLtools to be used on a wide variety of platforms, from HPC\nclusters over workstations and laptops to smartphones, smartwatches, and\nmicrocontrollers. Specifically, due to the tight integration of the RL\nalgorithms with simulation environments, RLtools can solve popular RL problems\nup to 76 times faster than other popular RL frameworks. We also benchmark the\ninference on a diverse set of microcontrollers and show that in most cases our\noptimized implementation is by far the fastest. Finally, RLtools enables the\nfirst-ever demonstration of training a deep RL algorithm directly on a\nmicrocontroller, giving rise to the field of TinyRL. The source code as well as\ndocumentation and live demos are available through our project page at\nhttps://rl.tools."
                },
                "authors": [
                    {
                        "name": "Jonas Eschmann"
                    },
                    {
                        "name": "Dario Albani"
                    },
                    {
                        "name": "Giuseppe Loianno"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Loianno"
                },
                "author": "Giuseppe Loianno",
                "arxiv_comment": "Project page: https://rl.tools",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03530v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03530v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02272v3",
                "updated": "2024-11-19T17:29:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    29,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-04T17:03:55Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    3,
                    55,
                    0,
                    309,
                    0
                ],
                "title": "Combining Induction and Transduction for Abstract Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Induction and Transduction for Abstract Reasoning"
                },
                "summary": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture."
                },
                "authors": [
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Keya Hu"
                    },
                    {
                        "name": "Carter Larsen"
                    },
                    {
                        "name": "Yuqing Wu"
                    },
                    {
                        "name": "Simon Alford"
                    },
                    {
                        "name": "Caleb Woo"
                    },
                    {
                        "name": "Spencer M. Dunn"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Michelangelo Naim"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Wei-Long Zheng"
                    },
                    {
                        "name": "Zenna Tavares"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10556v2",
                "updated": "2024-11-19T17:26:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    26,
                    35,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-15T20:07:25Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    20,
                    7,
                    25,
                    4,
                    320,
                    0
                ],
                "title": "Star Log-extended eMulation: a method for efficient computation of the\n  Tolman-Oppenheimer-Volkoff equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Log-extended eMulation: a method for efficient computation of the\n  Tolman-Oppenheimer-Volkoff equations"
                },
                "summary": "We emulate the Tolman-Oppenheimer-Volkoff (TOV) equations, including tidal\ndeformability, for neutron stars using a novel hybrid method based upon the\nDynamic Mode Decomposition (DMD) for the first time. This new method, which we\ncall Star Log-extended eMulation (SLM), utilizes the underlying logarithmic\nbehavior of the differential equations to enable accurate emulation of the\nnonlinear system. We show predictions for well-known phenomenological equations\nof state (EOS) with fixed parameters and with a freely-varying parametric\nQuarkyonic EOS. Our results produce a significant computational speed-up of\n$\\approx 3 \\times 10^4$ compared to high fidelity (HF) calculations using\nstandard Runge-Kutta methods, providing an efficient emulator for the numerous\nTOV evaluations required by multi-messenger astrophysical frameworks that infer\nconstraints on the EOS from neutron star mergers. The ability of the SLM\nalgorithm to learn a mapping between parameters of the EOS and subsequent\nneutron star properties also opens up potential extensions for assisting in\ncomputationally prohibitive uncertainty quantification (UQ) for any type of\nEOS. The source code for the methods developed in this work will be openly\navailable in a public GitHub repository for community modification and use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We emulate the Tolman-Oppenheimer-Volkoff (TOV) equations, including tidal\ndeformability, for neutron stars using a novel hybrid method based upon the\nDynamic Mode Decomposition (DMD) for the first time. This new method, which we\ncall Star Log-extended eMulation (SLM), utilizes the underlying logarithmic\nbehavior of the differential equations to enable accurate emulation of the\nnonlinear system. We show predictions for well-known phenomenological equations\nof state (EOS) with fixed parameters and with a freely-varying parametric\nQuarkyonic EOS. Our results produce a significant computational speed-up of\n$\\approx 3 \\times 10^4$ compared to high fidelity (HF) calculations using\nstandard Runge-Kutta methods, providing an efficient emulator for the numerous\nTOV evaluations required by multi-messenger astrophysical frameworks that infer\nconstraints on the EOS from neutron star mergers. The ability of the SLM\nalgorithm to learn a mapping between parameters of the EOS and subsequent\nneutron star properties also opens up potential extensions for assisting in\ncomputationally prohibitive uncertainty quantification (UQ) for any type of\nEOS. The source code for the methods developed in this work will be openly\navailable in a public GitHub repository for community modification and use."
                },
                "authors": [
                    {
                        "name": "Sudhanva Lalit"
                    },
                    {
                        "name": "Alexandra C. Semposki"
                    },
                    {
                        "name": "Joshua M. Maldonado"
                    }
                ],
                "author_detail": {
                    "name": "Joshua M. Maldonado"
                },
                "author": "Joshua M. Maldonado",
                "arxiv_comment": "6 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12671v1",
                "updated": "2024-11-19T17:23:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    23,
                    55,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:23:55Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    23,
                    55,
                    1,
                    324,
                    0
                ],
                "title": "Neurosymbolic Graph Enrichment for Grounded World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic Graph Enrichment for Grounded World Models"
                },
                "summary": "The development of artificial intelligence systems capable of understanding\nand reasoning about complex real-world scenarios is a significant challenge. In\nthis work we present a novel approach to enhance and exploit LLM reactive\ncapability to address complex problems and interpret deeply contextual\nreal-world meaning. We introduce a method and a tool for creating a multimodal,\nknowledge-augmented formal representation of meaning that combines the\nstrengths of large language models with structured semantic representations.\nOur method begins with an image input, utilizing state-of-the-art large\nlanguage models to generate a natural language description. This description is\nthen transformed into an Abstract Meaning Representation (AMR) graph, which is\nformalized and enriched with logical design patterns, and layered semantics\nderived from linguistic and factual knowledge bases. The resulting graph is\nthen fed back into the LLM to be extended with implicit knowledge activated by\ncomplex heuristic learning, including semantic implicatures, moral values,\nembodied cognition, and metaphorical representations. By bridging the gap\nbetween unstructured language models and formal semantic structures, our method\nopens new avenues for tackling intricate problems in natural language\nunderstanding and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of artificial intelligence systems capable of understanding\nand reasoning about complex real-world scenarios is a significant challenge. In\nthis work we present a novel approach to enhance and exploit LLM reactive\ncapability to address complex problems and interpret deeply contextual\nreal-world meaning. We introduce a method and a tool for creating a multimodal,\nknowledge-augmented formal representation of meaning that combines the\nstrengths of large language models with structured semantic representations.\nOur method begins with an image input, utilizing state-of-the-art large\nlanguage models to generate a natural language description. This description is\nthen transformed into an Abstract Meaning Representation (AMR) graph, which is\nformalized and enriched with logical design patterns, and layered semantics\nderived from linguistic and factual knowledge bases. The resulting graph is\nthen fed back into the LLM to be extended with implicit knowledge activated by\ncomplex heuristic learning, including semantic implicatures, moral values,\nembodied cognition, and metaphorical representations. By bridging the gap\nbetween unstructured language models and formal semantic structures, our method\nopens new avenues for tackling intricate problems in natural language\nunderstanding and reasoning."
                },
                "authors": [
                    {
                        "name": "Stefano De Giorgis"
                    },
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Alessandro Russo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Russo"
                },
                "author": "Alessandro Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12665v1",
                "updated": "2024-11-19T17:17:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    17,
                    46,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    17,
                    46,
                    1,
                    324,
                    0
                ],
                "title": "Auto-Evaluation with Few Labels through Post-hoc Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Evaluation with Few Labels through Post-hoc Regression"
                },
                "summary": "Continually evaluating large generative models provides a unique challenge.\nOften, human annotations are necessary to evaluate high-level properties of\nthese models (e.g. in text or images). However, collecting human annotations of\nsamples can be resource intensive, and using other machine learning systems to\nprovide the annotations, or automatic evaluation, can introduce systematic\nerrors into the evaluation. The Prediction Powered Inference (PPI) framework\nprovides a way of leveraging both the statistical power of automatic evaluation\nand a small pool of labelled data to produce a low-variance, unbiased estimate\nof the quantity being evaluated for. However, most work on PPI considers a\nrelatively sizable set of labelled samples, which is not always practical to\nobtain. To this end, we present two new PPI-based techniques that leverage\nrobust regressors to produce even lower variance estimators in the few-label\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continually evaluating large generative models provides a unique challenge.\nOften, human annotations are necessary to evaluate high-level properties of\nthese models (e.g. in text or images). However, collecting human annotations of\nsamples can be resource intensive, and using other machine learning systems to\nprovide the annotations, or automatic evaluation, can introduce systematic\nerrors into the evaluation. The Prediction Powered Inference (PPI) framework\nprovides a way of leveraging both the statistical power of automatic evaluation\nand a small pool of labelled data to produce a low-variance, unbiased estimate\nof the quantity being evaluated for. However, most work on PPI considers a\nrelatively sizable set of labelled samples, which is not always practical to\nobtain. To this end, we present two new PPI-based techniques that leverage\nrobust regressors to produce even lower variance estimators in the few-label\nregime."
                },
                "authors": [
                    {
                        "name": "Benjamin Eyre"
                    },
                    {
                        "name": "David Madras"
                    }
                ],
                "author_detail": {
                    "name": "David Madras"
                },
                "author": "David Madras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05972v3",
                "updated": "2024-11-19T17:16:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    16,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-01-11T15:20:06Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    15,
                    20,
                    6,
                    3,
                    11,
                    0
                ],
                "title": "Scientific Machine Learning Based Reduced-Order Models for Plasma\n  Turbulence Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific Machine Learning Based Reduced-Order Models for Plasma\n  Turbulence Simulations"
                },
                "summary": "This paper investigates non-intrusive Scientific Machine Learning (SciML)\nReduced-Order Models (ROMs) for plasma turbulence simulations. In particular,\nwe focus on Operator Inference (OpInf) to build low-cost physics-based ROMs\nfrom data for such simulations. As a representative example, we consider the\n(classical) Hasegawa-Wakatani (HW) equations used for modeling two-dimensional\nelectrostatic drift-wave turbulence. For a comprehensive perspective of the\npotential of OpInf to construct predictive ROMs, we consider three setups for\nthe HW equations by varying a key parameter, namely the adiabaticity\ncoefficient. These setups lead to the formation of complex and nonlinear\ndynamics, which makes the construction of predictive ROMs of any kind\nchallenging. We generate the training datasets by performing direct numerical\nsimulations of the HW equations and recording the computed state data and\noutputs the over a time horizon of $100$ time units in the turbulent phase. We\nthen use these datasets to construct OpInf ROMs for predictions over $400$\nadditional time units, that is, $400\\%$ more than the training horizon. Our\nresults show that the OpInf ROMs capture important statistical features of the\nturbulent dynamics and generalize beyond the training time horizon while\nreducing the computational effort of the high-fidelity simulation by up to five\norders of magnitude. In the broader context of fusion research, this shows that\nnon-intrusive SciML ROMs have the potential to drastically accelerate numerical\nstudies, which can ultimately enable tasks such as the design of optimized\nfusion devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates non-intrusive Scientific Machine Learning (SciML)\nReduced-Order Models (ROMs) for plasma turbulence simulations. In particular,\nwe focus on Operator Inference (OpInf) to build low-cost physics-based ROMs\nfrom data for such simulations. As a representative example, we consider the\n(classical) Hasegawa-Wakatani (HW) equations used for modeling two-dimensional\nelectrostatic drift-wave turbulence. For a comprehensive perspective of the\npotential of OpInf to construct predictive ROMs, we consider three setups for\nthe HW equations by varying a key parameter, namely the adiabaticity\ncoefficient. These setups lead to the formation of complex and nonlinear\ndynamics, which makes the construction of predictive ROMs of any kind\nchallenging. We generate the training datasets by performing direct numerical\nsimulations of the HW equations and recording the computed state data and\noutputs the over a time horizon of $100$ time units in the turbulent phase. We\nthen use these datasets to construct OpInf ROMs for predictions over $400$\nadditional time units, that is, $400\\%$ more than the training horizon. Our\nresults show that the OpInf ROMs capture important statistical features of the\nturbulent dynamics and generalize beyond the training time horizon while\nreducing the computational effort of the high-fidelity simulation by up to five\norders of magnitude. In the broader context of fusion research, this shows that\nnon-intrusive SciML ROMs have the potential to drastically accelerate numerical\nstudies, which can ultimately enable tasks such as the design of optimized\nfusion devices."
                },
                "authors": [
                    {
                        "name": "Constantin Gahr"
                    },
                    {
                        "name": "Ionut-Gabriel Farcas"
                    },
                    {
                        "name": "Frank Jenko"
                    }
                ],
                "author_detail": {
                    "name": "Frank Jenko"
                },
                "author": "Frank Jenko",
                "arxiv_doi": "10.1063/5.0225584",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1063/5.0225584",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.05972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages in double column format, 9 figures, 8 tables",
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v3",
                "updated": "2024-11-19T17:14:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    14,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02380v2",
                "updated": "2024-11-19T17:07:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    7,
                    59,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-04T18:48:18Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    48,
                    18,
                    0,
                    309,
                    0
                ],
                "title": "Robust Bayesian regression in astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bayesian regression in astronomy"
                },
                "summary": "Model mis-specification (e.g. the presence of outliers) is commonly\nencountered in astronomical analyses, often requiring the use of ad hoc\nalgorithms (e.g. sigma-clipping). We develop and implement a generic Bayesian\napproach to linear regression, based on Student's t-distributions, that is\nrobust to outliers and mis-specification of the noise model. Our method is\nvalidated using simulated datasets with various degrees of model\nmis-specification; the derived constraints are shown to be systematically less\nbiased than those from a similar model using normal distributions. We\ndemonstrate that, for a dataset without outliers, a worst-case inference using\nt-distributions would give unbiased results with $\\lesssim\\!10$ per cent\nincrease in the reported parameter uncertainties. We also compare with existing\nanalyses of real-world datasets, finding qualitatively different results where\nnormal distributions have been used and agreement where more robust methods\nhave been applied. A Python implementation of this model, t-cup, is made\navailable for others to use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model mis-specification (e.g. the presence of outliers) is commonly\nencountered in astronomical analyses, often requiring the use of ad hoc\nalgorithms (e.g. sigma-clipping). We develop and implement a generic Bayesian\napproach to linear regression, based on Student's t-distributions, that is\nrobust to outliers and mis-specification of the noise model. Our method is\nvalidated using simulated datasets with various degrees of model\nmis-specification; the derived constraints are shown to be systematically less\nbiased than those from a similar model using normal distributions. We\ndemonstrate that, for a dataset without outliers, a worst-case inference using\nt-distributions would give unbiased results with $\\lesssim\\!10$ per cent\nincrease in the reported parameter uncertainties. We also compare with existing\nanalyses of real-world datasets, finding qualitatively different results where\nnormal distributions have been used and agreement where more robust methods\nhave been applied. A Python implementation of this model, t-cup, is made\navailable for others to use."
                },
                "authors": [
                    {
                        "name": "William Martin"
                    },
                    {
                        "name": "Daniel J. Mortlock"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Mortlock"
                },
                "author": "Daniel J. Mortlock",
                "arxiv_comment": "14 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12643v1",
                "updated": "2024-11-19T16:54:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    54,
                    30,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T16:54:30Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    54,
                    30,
                    1,
                    324,
                    0
                ],
                "title": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning\n  Models"
                },
                "summary": "The rapid advancement of artificial intelligence has led to increasingly\nsophisticated deep learning models, which frequently operate as opaque 'black\nboxes' with limited transparency in their decision-making processes. This lack\nof interpretability presents considerable challenges, especially in high-stakes\napplications where understanding the rationale behind a model's outputs is as\nessential as the outputs themselves. This study addresses the pressing need for\ninterpretability in AI systems, emphasizing its role in fostering trust,\nensuring accountability, and promoting responsible deployment in\nmission-critical fields. To address the interpretability challenge in deep\nlearning, we introduce DLBacktrace, an innovative technique developed by the\nAryaXAI team to illuminate model decisions across a wide array of domains,\nincluding simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks\n(CNNs), Large Language Models (LLMs), Computer Vision Models, and more.\n  We provide a comprehensive overview of the DLBacktrace algorithm and present\nbenchmarking results, comparing its performance against established\ninterpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients,\nSmoothGrad, and Attention Rollout, using diverse task-based metrics. The\nproposed DLBacktrace technique is compatible with various model architectures\nbuilt in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP\narchitectures such as BERT and LSTMs, computer vision models like ResNet and\nU-Net, as well as custom deep neural network (DNN) models for tabular data.\nThis flexibility underscores DLBacktrace's adaptability and effectiveness in\nenhancing model transparency across a broad spectrum of applications. The\nlibrary is open-sourced and available at https://github.com/AryaXAI/DLBacktrace .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence has led to increasingly\nsophisticated deep learning models, which frequently operate as opaque 'black\nboxes' with limited transparency in their decision-making processes. This lack\nof interpretability presents considerable challenges, especially in high-stakes\napplications where understanding the rationale behind a model's outputs is as\nessential as the outputs themselves. This study addresses the pressing need for\ninterpretability in AI systems, emphasizing its role in fostering trust,\nensuring accountability, and promoting responsible deployment in\nmission-critical fields. To address the interpretability challenge in deep\nlearning, we introduce DLBacktrace, an innovative technique developed by the\nAryaXAI team to illuminate model decisions across a wide array of domains,\nincluding simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks\n(CNNs), Large Language Models (LLMs), Computer Vision Models, and more.\n  We provide a comprehensive overview of the DLBacktrace algorithm and present\nbenchmarking results, comparing its performance against established\ninterpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients,\nSmoothGrad, and Attention Rollout, using diverse task-based metrics. The\nproposed DLBacktrace technique is compatible with various model architectures\nbuilt in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP\narchitectures such as BERT and LSTMs, computer vision models like ResNet and\nU-Net, as well as custom deep neural network (DNN) models for tabular data.\nThis flexibility underscores DLBacktrace's adaptability and effectiveness in\nenhancing model transparency across a broad spectrum of applications. The\nlibrary is open-sourced and available at https://github.com/AryaXAI/DLBacktrace ."
                },
                "authors": [
                    {
                        "name": "Vinay Kumar Sankarapu"
                    },
                    {
                        "name": "Chintan Chitroda"
                    },
                    {
                        "name": "Yashwardhan Rathore"
                    },
                    {
                        "name": "Neeraj Kumar Singh"
                    },
                    {
                        "name": "Pratinav Seth"
                    }
                ],
                "author_detail": {
                    "name": "Pratinav Seth"
                },
                "author": "Pratinav Seth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12641v1",
                "updated": "2024-11-19T16:52:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    52,
                    34,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T16:52:34Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    52,
                    34,
                    1,
                    324,
                    0
                ],
                "title": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models"
                },
                "summary": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ..."
                },
                "authors": [
                    {
                        "name": "Yixiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixiao Zhang"
                },
                "author": "Yixiao Zhang",
                "arxiv_comment": "PhD Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12629v1",
                "updated": "2024-11-19T16:40:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    40,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T16:40:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    40,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with\n  Graph Neural Networks"
                },
                "summary": "Galaxies grow and evolve in dark matter halos. Because dark matter is not\nvisible, galaxies' halo masses ($\\rm{M}_{\\rm{halo}}$) must be inferred\nindirectly. We present a graph neural network (GNN) model for predicting\n$\\rm{M}_{\\rm{halo}}$ from stellar mass ($\\rm{M}_{*}$) in simulated galaxy\nclusters using data from the IllustrisTNG simulation suite. Unlike traditional\nmachine learning models like random forests, our GNN captures the\ninformation-rich substructure of galaxy clusters by using spatial and kinematic\nrelationships between galaxy neighbour. A GNN model trained on the TNG-Cluster\ndataset and independently tested on the TNG300 simulation achieves superior\npredictive performance compared to other baseline models we tested. Future work\nwill extend this approach to different simulations and real observational\ndatasets to further validate the GNN model's ability to generalise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxies grow and evolve in dark matter halos. Because dark matter is not\nvisible, galaxies' halo masses ($\\rm{M}_{\\rm{halo}}$) must be inferred\nindirectly. We present a graph neural network (GNN) model for predicting\n$\\rm{M}_{\\rm{halo}}$ from stellar mass ($\\rm{M}_{*}$) in simulated galaxy\nclusters using data from the IllustrisTNG simulation suite. Unlike traditional\nmachine learning models like random forests, our GNN captures the\ninformation-rich substructure of galaxy clusters by using spatial and kinematic\nrelationships between galaxy neighbour. A GNN model trained on the TNG-Cluster\ndataset and independently tested on the TNG300 simulation achieves superior\npredictive performance compared to other baseline models we tested. Future work\nwill extend this approach to different simulations and real observational\ndatasets to further validate the GNN model's ability to generalise."
                },
                "authors": [
                    {
                        "name": "Nikhil Garuda"
                    },
                    {
                        "name": "John F. Wu"
                    },
                    {
                        "name": "Dylan Nelson"
                    },
                    {
                        "name": "Annalisa Pillepich"
                    }
                ],
                "author_detail": {
                    "name": "Annalisa Pillepich"
                },
                "author": "Annalisa Pillepich",
                "arxiv_comment": "9 pages, 4 figures, accepted at the NeurIPS ML4PS 2024 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11579v2",
                "updated": "2024-11-19T16:39:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    39,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-09-17T22:06:46Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    6,
                    46,
                    1,
                    261,
                    0
                ],
                "title": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection"
                },
                "summary": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs..."
                },
                "authors": [
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Accepted in NeurIPS 2024 SoLaR Workshop and Safety Gen AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19028v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19028v3",
                "updated": "2024-11-19T16:39:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    39,
                    50,
                    1,
                    324,
                    0
                ],
                "published": "2024-07-26T18:12:46Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    18,
                    12,
                    46,
                    4,
                    208,
                    0
                ],
                "title": "Axion signals from neutron star populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Axion signals from neutron star populations"
                },
                "summary": "Neutron stars provide a powerful probe of axion dark matter, especially in\nhigher frequency ranges where there remain fewer laboratory constraints.\nPopulations of neutron stars near the Galactic Centre have been proposed as a\nmeans to place strong constraints on axion dark matter. One downside of this\napproach is that there are very few direct observations of neutron stars in\nthis region, introducing uncertainties in the total number of neutron stars in\nthis ``invisible\" population at the Galactic Centre, whose size must be\ninferred through birth rate modelling. We suggest this number could also be\nreduced due to stellar dynamics carrying stars away from the Galactic Centre\nvia large kick velocities at birth. We attempt to circumvent the uncertainty on\nthe Galactic Centre population size by modelling the axion signal from better\nunderstood populations outside the Galactic Centre using {\\tt PsrPopPy} which\nis normalised against pulsar observations. We consider lower-frequency,\nwider-angle searches for this signal via a range of instruments including\nMeerKAT and SKA-low but find that the sensitivity is not competitive with\nexisting constraints. Finally, returning to the Galactic Centre, we compare\npopulations to single objects as targets for axion detection. Using the latest\nmodelling of axion-photon conversion in the Galactic Centre magnetar, we\nconclude that within astrophysical uncertainties, the Galactic Centre\npopulation and the magnetar could give comparable sensitivities to axion dark\nmatter, suggesting one should continue to search for both signals in future\nsurveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron stars provide a powerful probe of axion dark matter, especially in\nhigher frequency ranges where there remain fewer laboratory constraints.\nPopulations of neutron stars near the Galactic Centre have been proposed as a\nmeans to place strong constraints on axion dark matter. One downside of this\napproach is that there are very few direct observations of neutron stars in\nthis region, introducing uncertainties in the total number of neutron stars in\nthis ``invisible\" population at the Galactic Centre, whose size must be\ninferred through birth rate modelling. We suggest this number could also be\nreduced due to stellar dynamics carrying stars away from the Galactic Centre\nvia large kick velocities at birth. We attempt to circumvent the uncertainty on\nthe Galactic Centre population size by modelling the axion signal from better\nunderstood populations outside the Galactic Centre using {\\tt PsrPopPy} which\nis normalised against pulsar observations. We consider lower-frequency,\nwider-angle searches for this signal via a range of instruments including\nMeerKAT and SKA-low but find that the sensitivity is not competitive with\nexisting constraints. Finally, returning to the Galactic Centre, we compare\npopulations to single objects as targets for axion detection. Using the latest\nmodelling of axion-photon conversion in the Galactic Centre magnetar, we\nconclude that within astrophysical uncertainties, the Galactic Centre\npopulation and the magnetar could give comparable sensitivities to axion dark\nmatter, suggesting one should continue to search for both signals in future\nsurveys."
                },
                "authors": [
                    {
                        "name": "U. Bhura"
                    },
                    {
                        "name": "R. A. Battye"
                    },
                    {
                        "name": "J. I. McDonald"
                    },
                    {
                        "name": "S. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "S. Srinivasan"
                },
                "author": "S. Srinivasan",
                "arxiv_comment": "49 pages, 23 figures, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19028v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19028v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14533v2",
                "updated": "2024-11-19T16:34:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    34,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2023-05-23T21:33:43Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    21,
                    33,
                    43,
                    1,
                    143,
                    0
                ],
                "title": "How to Choose How to Choose Your Chatbot: A Massively Multi-System\n  MultiReference Data Set for Dialog Metric Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Choose How to Choose Your Chatbot: A Massively Multi-System\n  MultiReference Data Set for Dialog Metric Evaluation"
                },
                "summary": "We release MMSMR, a Massively Multi-System MultiReference dataset to enable\nfuture work on metrics and evaluation for dialog. Automatic metrics for\ndialogue evaluation should be robust proxies for human judgments; however, the\nverification of robustness is currently far from satisfactory. To quantify the\nrobustness correlation and understand what is necessary in a test set, we\ncreate and release an 8-reference dialog dataset by extending single-reference\nevaluation sets and introduce this new language learning conversation dataset.\nWe then train 1750 systems and evaluate them on our novel test set and the\nDailyDialog dataset. We release the novel test set, and model hyper parameters,\ninference outputs, and metric scores for each system on a variety of datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We release MMSMR, a Massively Multi-System MultiReference dataset to enable\nfuture work on metrics and evaluation for dialog. Automatic metrics for\ndialogue evaluation should be robust proxies for human judgments; however, the\nverification of robustness is currently far from satisfactory. To quantify the\nrobustness correlation and understand what is necessary in a test set, we\ncreate and release an 8-reference dialog dataset by extending single-reference\nevaluation sets and introduce this new language learning conversation dataset.\nWe then train 1750 systems and evaluate them on our novel test set and the\nDailyDialog dataset. We release the novel test set, and model hyper parameters,\ninference outputs, and metric scores for each system on a variety of datasets."
                },
                "authors": [
                    {
                        "name": "Huda Khayrallah"
                    },
                    {
                        "name": "Zuhaib Akhtar"
                    },
                    {
                        "name": "Edward Cohen"
                    },
                    {
                        "name": "Jyothir S V"
                    },
                    {
                        "name": "Joo Sedoc"
                    }
                ],
                "author_detail": {
                    "name": "Joo Sedoc"
                },
                "author": "Joo Sedoc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12603v1",
                "updated": "2024-11-19T16:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    6,
                    32,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T16:06:32Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    6,
                    32,
                    1,
                    324,
                    0
                ],
                "title": "STREAM: A Universal State-Space Model for Sparse Geometric Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STREAM: A Universal State-Space Model for Sparse Geometric Data"
                },
                "summary": "Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset."
                },
                "authors": [
                    {
                        "name": "Mark Schne"
                    },
                    {
                        "name": "Yash Bhisikar"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Khaleelulla Khan Nazeer"
                    },
                    {
                        "name": "Christian Mayr"
                    },
                    {
                        "name": "Anand Subramoney"
                    },
                    {
                        "name": "David Kappel"
                    }
                ],
                "author_detail": {
                    "name": "David Kappel"
                },
                "author": "David Kappel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19145v2",
                "updated": "2024-11-19T15:54:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    54,
                    14,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-29T13:29:10Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    13,
                    29,
                    10,
                    3,
                    60,
                    0
                ],
                "title": "A SAM-guided Two-stream Lightweight Model for Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A SAM-guided Two-stream Lightweight Model for Anomaly Detection"
                },
                "summary": "In industrial anomaly detection, model efficiency and mobile-friendliness\nbecome the primary concerns in real-world applications. Simultaneously, the\nimpressive generalization capabilities of Segment Anything (SAM) have garnered\nbroad academic attention, making it an ideal choice for localizing unseen\nanomalies and diverse real-world patterns. In this paper, considering these two\ncritical factors, we propose a SAM-guided Two-stream Lightweight Model for\nunsupervised anomaly detection (STLM) that not only aligns with the two\npractical application requirements but also harnesses the robust generalization\ncapabilities of SAM. We employ two lightweight image encoders, i.e., our\ntwo-stream lightweight module, guided by SAM's knowledge. To be specific, one\nstream is trained to generate discriminative and general feature\nrepresentations in both normal and anomalous regions, while the other stream\nreconstructs the same images without anomalies, which effectively enhances the\ndifferentiation of two-stream representations when facing anomalous regions.\nFurthermore, we employ a shared mask decoder and a feature aggregation module\nto generate anomaly maps. Our experiments conducted on MVTec AD benchmark show\nthat STLM, with about 16M parameters and achieving an inference time in 20ms,\ncompetes effectively with state-of-the-art methods in terms of performance,\n98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more\ndifficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and\ngeneralizability of STLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In industrial anomaly detection, model efficiency and mobile-friendliness\nbecome the primary concerns in real-world applications. Simultaneously, the\nimpressive generalization capabilities of Segment Anything (SAM) have garnered\nbroad academic attention, making it an ideal choice for localizing unseen\nanomalies and diverse real-world patterns. In this paper, considering these two\ncritical factors, we propose a SAM-guided Two-stream Lightweight Model for\nunsupervised anomaly detection (STLM) that not only aligns with the two\npractical application requirements but also harnesses the robust generalization\ncapabilities of SAM. We employ two lightweight image encoders, i.e., our\ntwo-stream lightweight module, guided by SAM's knowledge. To be specific, one\nstream is trained to generate discriminative and general feature\nrepresentations in both normal and anomalous regions, while the other stream\nreconstructs the same images without anomalies, which effectively enhances the\ndifferentiation of two-stream representations when facing anomalous regions.\nFurthermore, we employ a shared mask decoder and a feature aggregation module\nto generate anomaly maps. Our experiments conducted on MVTec AD benchmark show\nthat STLM, with about 16M parameters and achieving an inference time in 20ms,\ncompetes effectively with state-of-the-art methods in terms of performance,\n98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more\ndifficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and\ngeneralizability of STLM."
                },
                "authors": [
                    {
                        "name": "Chenghao Li"
                    },
                    {
                        "name": "Lei Qi"
                    },
                    {
                        "name": "Xin Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Geng"
                },
                "author": "Xin Geng",
                "arxiv_comment": "Accepted by ACM TOMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07372v2",
                "updated": "2024-11-19T15:53:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    53,
                    51,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-11T21:21:32Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    21,
                    21,
                    32,
                    0,
                    316,
                    0
                ],
                "title": "Identifying Differential Patient Care Through Inverse Intent Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Differential Patient Care Through Inverse Intent Inference"
                },
                "summary": "Sepsis is a life-threatening condition defined by end-organ dysfunction due\nto a dysregulated host response to infection. Although the Surviving Sepsis\nCampaign has launched and has been releasing sepsis treatment guidelines to\nunify and normalize the care for sepsis patients, it has been reported in\nnumerous studies that disparities in care exist across the trajectory of\npatient stay in the emergency department and intensive care unit. Here, we\napply a number of reinforcement learning techniques including behavioral\ncloning, imitation learning, and inverse reinforcement learning, to learn the\noptimal policy in the management of septic patient subgroups using expert\ndemonstrations. Then we estimate the counterfactual optimal policies by\napplying the model to another subset of unseen medical populations and identify\nthe difference in cure by comparing it to the real policy. Our data comes from\nthe sepsis cohort of MIMIC-IV and the clinical data warehouses of the Mass\nGeneral Brigham healthcare system. The ultimate objective of this work is to\nuse the optimal learned policy function to estimate the counterfactual\ntreatment policy and identify deviations across sub-populations of interest. We\nhope this approach would help us identify any disparities in care and also\nchanges in cure in response to the publication of national sepsis treatment\nguidelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sepsis is a life-threatening condition defined by end-organ dysfunction due\nto a dysregulated host response to infection. Although the Surviving Sepsis\nCampaign has launched and has been releasing sepsis treatment guidelines to\nunify and normalize the care for sepsis patients, it has been reported in\nnumerous studies that disparities in care exist across the trajectory of\npatient stay in the emergency department and intensive care unit. Here, we\napply a number of reinforcement learning techniques including behavioral\ncloning, imitation learning, and inverse reinforcement learning, to learn the\noptimal policy in the management of septic patient subgroups using expert\ndemonstrations. Then we estimate the counterfactual optimal policies by\napplying the model to another subset of unseen medical populations and identify\nthe difference in cure by comparing it to the real policy. Our data comes from\nthe sepsis cohort of MIMIC-IV and the clinical data warehouses of the Mass\nGeneral Brigham healthcare system. The ultimate objective of this work is to\nuse the optimal learned policy function to estimate the counterfactual\ntreatment policy and identify deviations across sub-populations of interest. We\nhope this approach would help us identify any disparities in care and also\nchanges in cure in response to the publication of national sepsis treatment\nguidelines."
                },
                "authors": [
                    {
                        "name": "Hyewon Jeong"
                    },
                    {
                        "name": "Siddharth Nayak"
                    },
                    {
                        "name": "Taylor Killian"
                    },
                    {
                        "name": "Sanjat Kanjilal"
                    }
                ],
                "author_detail": {
                    "name": "Sanjat Kanjilal"
                },
                "author": "Sanjat Kanjilal",
                "arxiv_journal_ref": "Reinforcement Learning for Real Life (RL4RealLife) Workshop,\n  NeurIPS 2022",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03625v2",
                "updated": "2024-11-19T15:48:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    48,
                    16,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-06T02:46:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    46,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "Identification and Inference in General Bunching Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Inference in General Bunching Designs"
                },
                "summary": "This paper develops a formal econometric framework and tools for the\nidentification and inference of a structural parameter in general bunching\ndesigns. We present both point and partial identification results, which\ngeneralize previous approaches in the literature. The key assumption for point\nidentification is the analyticity of the counterfactual density, which defines\na broader class of distributions than many well-known parametric families. In\nthe partial identification approach, the analyticity condition is relaxed and\nvarious shape restrictions can be incorporated, including those found in the\nliterature. Both of our identification results account for observable\nheterogeneity in the model, which has previously been permitted only in limited\nways. We provide a suite of counterfactual estimation and inference methods,\ntermed the generalized polynomial strategy. Our method restores the merits of\nthe original polynomial strategy proposed by Chetty et al. (2011) while\naddressing several weaknesses in the widespread practice. The efficacy of the\nproposed method is demonstrated compared to a version of the polynomial\nestimator in a series of Monte Carlo studies within the augmented isoelastic\nmodel. We revisit the data used in Saez (2010) and find substantially different\nresults relative to those from the polynomial strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a formal econometric framework and tools for the\nidentification and inference of a structural parameter in general bunching\ndesigns. We present both point and partial identification results, which\ngeneralize previous approaches in the literature. The key assumption for point\nidentification is the analyticity of the counterfactual density, which defines\na broader class of distributions than many well-known parametric families. In\nthe partial identification approach, the analyticity condition is relaxed and\nvarious shape restrictions can be incorporated, including those found in the\nliterature. Both of our identification results account for observable\nheterogeneity in the model, which has previously been permitted only in limited\nways. We provide a suite of counterfactual estimation and inference methods,\ntermed the generalized polynomial strategy. Our method restores the merits of\nthe original polynomial strategy proposed by Chetty et al. (2011) while\naddressing several weaknesses in the widespread practice. The efficacy of the\nproposed method is demonstrated compared to a version of the polynomial\nestimator in a series of Monte Carlo studies within the augmented isoelastic\nmodel. We revisit the data used in Saez (2010) and find substantially different\nresults relative to those from the polynomial strategy."
                },
                "authors": [
                    {
                        "name": "Myunghyun Song"
                    }
                ],
                "author_detail": {
                    "name": "Myunghyun Song"
                },
                "author": "Myunghyun Song",
                "arxiv_comment": "I have corrected errors in the exposition on the extension of Theorem\n  1 on p.19, improved the clarity of assumptions, and addressed other typos in\n  the text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12580v1",
                "updated": "2024-11-19T15:47:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    47,
                    12,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T15:47:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    47,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models"
                },
                "summary": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning."
                },
                "authors": [
                    {
                        "name": "Laura Ruis"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Siddhartha Rao Kamalakara"
                    },
                    {
                        "name": "Dwarak Talupuru"
                    },
                    {
                        "name": "Acyr Locatelli"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Tim Rocktschel"
                    },
                    {
                        "name": "Edward Grefenstette"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12578v1",
                "updated": "2024-11-19T15:43:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    43,
                    36,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T15:43:36Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    43,
                    36,
                    1,
                    324,
                    0
                ],
                "title": "Robust Inference for High-dimensional Linear Models with Heavy-tailed\n  Errors via Partial Gini Covariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Inference for High-dimensional Linear Models with Heavy-tailed\n  Errors via Partial Gini Covariance"
                },
                "summary": "This paper introduces the partial Gini covariance, a novel dependence measure\nthat addresses the challenges of high-dimensional inference with heavy-tailed\nerrors, often encountered in fields like finance, insurance, climate, and\nbiology. Conventional high-dimensional regression inference methods suffer from\ninaccurate type I errors and reduced power in heavy-tailed contexts, limiting\ntheir effectiveness. Our proposed approach leverages the partial Gini\ncovariance to construct a robust statistical inference framework that requires\nminimal tuning and does not impose restrictive moment conditions on error\ndistributions. Unlike traditional methods, it circumvents the need for\nestimating the density of random errors and enhances the computational\nfeasibility and robustness. Extensive simulations demonstrate the proposed\nmethod's superior power and robustness over standard high-dimensional inference\napproaches, such as those based on the debiased Lasso. The asymptotic relative\nefficiency analysis provides additional theoretical insight on the improved\nefficiency of the new approach in the heavy-tailed setting. Additionally, the\npartial Gini covariance extends to the multivariate setting, enabling\nchi-square testing for a group of coefficients. We illustrate the method's\npractical application with a real-world data example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the partial Gini covariance, a novel dependence measure\nthat addresses the challenges of high-dimensional inference with heavy-tailed\nerrors, often encountered in fields like finance, insurance, climate, and\nbiology. Conventional high-dimensional regression inference methods suffer from\ninaccurate type I errors and reduced power in heavy-tailed contexts, limiting\ntheir effectiveness. Our proposed approach leverages the partial Gini\ncovariance to construct a robust statistical inference framework that requires\nminimal tuning and does not impose restrictive moment conditions on error\ndistributions. Unlike traditional methods, it circumvents the need for\nestimating the density of random errors and enhances the computational\nfeasibility and robustness. Extensive simulations demonstrate the proposed\nmethod's superior power and robustness over standard high-dimensional inference\napproaches, such as those based on the debiased Lasso. The asymptotic relative\nefficiency analysis provides additional theoretical insight on the improved\nefficiency of the new approach in the heavy-tailed setting. Additionally, the\npartial Gini covariance extends to the multivariate setting, enabling\nchi-square testing for a group of coefficients. We illustrate the method's\npractical application with a real-world data example."
                },
                "authors": [
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Songshan Yang"
                    },
                    {
                        "name": "Yunan Wu"
                    },
                    {
                        "name": "Lan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lan Wang"
                },
                "author": "Lan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12571v1",
                "updated": "2024-11-19T15:39:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    39,
                    51,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T15:39:51Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    39,
                    51,
                    1,
                    324,
                    0
                ],
                "title": "Large Language Models for Combinatorial Optimization of Design Structure\n  Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Combinatorial Optimization of Design Structure\n  Matrix"
                },
                "summary": "Combinatorial optimization (CO) is essential for improving efficiency and\nperformance in engineering applications. As complexity increases with larger\nproblem sizes and more intricate dependencies, identifying the optimal solution\nbecome challenging. When it comes to real-world engineering problems,\nalgorithms based on pure mathematical reasoning are limited and incapable to\ncapture the contextual nuances necessary for optimization. This study explores\nthe potential of Large Language Models (LLMs) in solving engineering CO\nproblems by leveraging their reasoning power and contextual knowledge. We\npropose a novel LLM-based framework that integrates network topology and domain\nknowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed\nmethod achieves faster convergence and higher solution quality than benchmark\nmethods. Moreover, results show that incorporating contextual domain knowledge\nsignificantly improves performance despite the choice of LLMs. These findings\nhighlight the potential of LLMs in tackling complex real-world CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for\na new paradigm in in real-world combinatorial optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial optimization (CO) is essential for improving efficiency and\nperformance in engineering applications. As complexity increases with larger\nproblem sizes and more intricate dependencies, identifying the optimal solution\nbecome challenging. When it comes to real-world engineering problems,\nalgorithms based on pure mathematical reasoning are limited and incapable to\ncapture the contextual nuances necessary for optimization. This study explores\nthe potential of Large Language Models (LLMs) in solving engineering CO\nproblems by leveraging their reasoning power and contextual knowledge. We\npropose a novel LLM-based framework that integrates network topology and domain\nknowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed\nmethod achieves faster convergence and higher solution quality than benchmark\nmethods. Moreover, results show that incorporating contextual domain knowledge\nsignificantly improves performance despite the choice of LLMs. These findings\nhighlight the potential of LLMs in tackling complex real-world CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for\na new paradigm in in real-world combinatorial optimization."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Min Xie"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17213v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17213v5",
                "updated": "2024-11-19T15:37:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    37,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-09-25T17:38:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    38,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles"
                },
                "summary": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by deliberative democracy, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by deliberative democracy, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Emily Fry"
                    },
                    {
                        "name": "Narendra Edara"
                    },
                    {
                        "name": "Eric Gilbert"
                    },
                    {
                        "name": "Ceren Budak"
                    }
                ],
                "author_detail": {
                    "name": "Ceren Budak"
                },
                "author": "Ceren Budak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17213v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17213v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12537v1",
                "updated": "2024-11-19T14:35:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    14,
                    35,
                    38,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T14:35:38Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    14,
                    35,
                    38,
                    1,
                    324,
                    0
                ],
                "title": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues"
                },
                "summary": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers in large\nlanguage modeling, offering linear scaling with sequence length and improved\ntraining efficiency. However, LRNNs struggle to perform state-tracking which\nmay impair performance in tasks such as code evaluation or tracking a chess\ngame. Even parity, the simplest state-tracking task, which non-linear RNNs like\nLSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et\nal. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity\nstems from restricting the value range of their diagonal state-transition\nmatrices to $[0, 1]$ and that incorporating negative values can resolve this\nissue. We extend this result to non-diagonal LRNNs, which have recently shown\npromise in models such as DeltaNet. We prove that finite precision LRNNs with\nstate-transition matrices having only positive eigenvalues cannot solve parity,\nwhile complex eigenvalues are needed to count modulo $3$. Notably, we also\nprove that LRNNs can learn any regular language when their state-transition\nmatrices are products of identity minus vector outer product matrices, each\nwith eigenvalues in the range $[-1, 1]$. Our empirical results confirm that\nextending the eigenvalue range of models like Mamba and DeltaNet to include\nnegative values not only enables them to solve parity but consistently improves\ntheir performance on state-tracking tasks. Furthermore, pre-training LRNNs with\nan extended eigenvalue range for language modeling achieves comparable\nperformance and stability while showing promise on code and math data. Our work\nenhances the expressivity of modern LRNNs, broadening their applicability\nwithout changing the cost of training or inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers in large\nlanguage modeling, offering linear scaling with sequence length and improved\ntraining efficiency. However, LRNNs struggle to perform state-tracking which\nmay impair performance in tasks such as code evaluation or tracking a chess\ngame. Even parity, the simplest state-tracking task, which non-linear RNNs like\nLSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et\nal. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity\nstems from restricting the value range of their diagonal state-transition\nmatrices to $[0, 1]$ and that incorporating negative values can resolve this\nissue. We extend this result to non-diagonal LRNNs, which have recently shown\npromise in models such as DeltaNet. We prove that finite precision LRNNs with\nstate-transition matrices having only positive eigenvalues cannot solve parity,\nwhile complex eigenvalues are needed to count modulo $3$. Notably, we also\nprove that LRNNs can learn any regular language when their state-transition\nmatrices are products of identity minus vector outer product matrices, each\nwith eigenvalues in the range $[-1, 1]$. Our empirical results confirm that\nextending the eigenvalue range of models like Mamba and DeltaNet to include\nnegative values not only enables them to solve parity but consistently improves\ntheir performance on state-tracking tasks. Furthermore, pre-training LRNNs with\nan extended eigenvalue range for language modeling achieves comparable\nperformance and stability while showing promise on code and math data. Our work\nenhances the expressivity of modern LRNNs, broadening their applicability\nwithout changing the cost of training or inference."
                },
                "authors": [
                    {
                        "name": "Riccardo Grazzi"
                    },
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Jrg K. H. Franke"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Massimiliano Pontil"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Pontil"
                },
                "author": "Massimiliano Pontil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00028v2",
                "updated": "2024-11-19T14:29:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    14,
                    29,
                    32,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-29T04:03:15Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    3,
                    15,
                    1,
                    303,
                    0
                ],
                "title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction\n  in LBSN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction\n  in LBSN"
                },
                "summary": "The fast development of location-based social networks (LBSNs) has led to\nsignificant changes in society, resulting in popular studies of using LBSN data\nfor socioeconomic prediction, e.g., regional population and commercial activity\nestimation. Existing studies design various graphs to model heterogeneous LBSN\ndata, and further apply graph representation learning methods for socioeconomic\nprediction. However, these approaches heavily rely on heuristic ideas and\nexpertise to extract task-relevant knowledge from diverse data, which may not\nbe optimal for specific tasks. Additionally, they tend to overlook the inherent\nrelationships between different indicators, limiting the prediction accuracy.\nMotivated by the remarkable abilities of large language models (LLMs) in\ncommonsense reasoning, embedding, and multi-agent collaboration, in this work,\nwe synergize LLM agents and knowledge graph for socioeconomic prediction. We\nfirst construct a location-based knowledge graph (LBKG) to integrate\nmulti-sourced LBSN data. Then we leverage the reasoning power of LLM agent to\nidentify relevant meta-paths in the LBKG for each type of socioeconomic\nprediction task, and design a semantic-guided attention module for knowledge\nfusion with meta-paths. Moreover, we introduce a cross-task communication\nmechanism to further enhance performance by enabling knowledge sharing across\ntasks at both LLM agent and KG levels. On the one hand, the LLM agents for\ndifferent tasks collaborate to generate more diverse and comprehensive\nmeta-paths. On the other hand, the embeddings from different tasks are\nadaptively merged for better socioeconomic prediction. Experiments on two\ndatasets demonstrate the effectiveness of the synergistic design between LLM\nand KG, providing insights for information sharing across socioeconomic\nprediction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fast development of location-based social networks (LBSNs) has led to\nsignificant changes in society, resulting in popular studies of using LBSN data\nfor socioeconomic prediction, e.g., regional population and commercial activity\nestimation. Existing studies design various graphs to model heterogeneous LBSN\ndata, and further apply graph representation learning methods for socioeconomic\nprediction. However, these approaches heavily rely on heuristic ideas and\nexpertise to extract task-relevant knowledge from diverse data, which may not\nbe optimal for specific tasks. Additionally, they tend to overlook the inherent\nrelationships between different indicators, limiting the prediction accuracy.\nMotivated by the remarkable abilities of large language models (LLMs) in\ncommonsense reasoning, embedding, and multi-agent collaboration, in this work,\nwe synergize LLM agents and knowledge graph for socioeconomic prediction. We\nfirst construct a location-based knowledge graph (LBKG) to integrate\nmulti-sourced LBSN data. Then we leverage the reasoning power of LLM agent to\nidentify relevant meta-paths in the LBKG for each type of socioeconomic\nprediction task, and design a semantic-guided attention module for knowledge\nfusion with meta-paths. Moreover, we introduce a cross-task communication\nmechanism to further enhance performance by enabling knowledge sharing across\ntasks at both LLM agent and KG levels. On the one hand, the LLM agents for\ndifferent tasks collaborate to generate more diverse and comprehensive\nmeta-paths. On the other hand, the embeddings from different tasks are\nadaptively merged for better socioeconomic prediction. Experiments on two\ndatasets demonstrate the effectiveness of the synergistic design between LLM\nand KG, providing insights for information sharing across socioeconomic\nprediction tasks."
                },
                "authors": [
                    {
                        "name": "Zhilun Zhou"
                    },
                    {
                        "name": "Jingyang Fan"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Depeng Jin"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12520v1",
                "updated": "2024-11-19T14:07:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    14,
                    7,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T14:07:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    14,
                    7,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "VMGNet: A Low Computational Complexity Robotic Grasping Network Based on\n  VMamba with Multi-Scale Feature Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VMGNet: A Low Computational Complexity Robotic Grasping Network Based on\n  VMamba with Multi-Scale Feature Fusion"
                },
                "summary": "While deep learning-based robotic grasping technology has demonstrated strong\nadaptability, its computational complexity has also significantly increased,\nmaking it unsuitable for scenarios with high real-time requirements. Therefore,\nwe propose a low computational complexity and high accuracy model named VMGNet\nfor robotic grasping. For the first time, we introduce the Visual State Space\ninto the robotic grasping field to achieve linear computational complexity,\nthereby greatly reducing the model's computational cost. Meanwhile, to improve\nthe accuracy of the model, we propose an efficient and lightweight multi-scale\nfeature fusion module, named Fusion Bridge Module, to extract and fuse\ninformation at different scales. We also present a new loss function\ncalculation method to enhance the importance differences between subtasks,\nimproving the model's fitting ability. Experiments show that VMGNet has only\n8.7G Floating Point Operations and an inference time of 8.1 ms on our devices.\nVMGNet also achieved state-of-the-art performance on the Cornell and Jacquard\npublic datasets. To validate VMGNet's effectiveness in practical applications,\nwe conducted real grasping experiments in multi-object scenarios, and VMGNet\nachieved an excellent performance with a 94.4% success rate in real-world\ngrasping tasks. The video for the real-world robotic grasping experiments is\navailable at https://youtu.be/S-QHBtbmLc4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep learning-based robotic grasping technology has demonstrated strong\nadaptability, its computational complexity has also significantly increased,\nmaking it unsuitable for scenarios with high real-time requirements. Therefore,\nwe propose a low computational complexity and high accuracy model named VMGNet\nfor robotic grasping. For the first time, we introduce the Visual State Space\ninto the robotic grasping field to achieve linear computational complexity,\nthereby greatly reducing the model's computational cost. Meanwhile, to improve\nthe accuracy of the model, we propose an efficient and lightweight multi-scale\nfeature fusion module, named Fusion Bridge Module, to extract and fuse\ninformation at different scales. We also present a new loss function\ncalculation method to enhance the importance differences between subtasks,\nimproving the model's fitting ability. Experiments show that VMGNet has only\n8.7G Floating Point Operations and an inference time of 8.1 ms on our devices.\nVMGNet also achieved state-of-the-art performance on the Cornell and Jacquard\npublic datasets. To validate VMGNet's effectiveness in practical applications,\nwe conducted real grasping experiments in multi-object scenarios, and VMGNet\nachieved an excellent performance with a 94.4% success rate in real-world\ngrasping tasks. The video for the real-world robotic grasping experiments is\navailable at https://youtu.be/S-QHBtbmLc4."
                },
                "authors": [
                    {
                        "name": "Yuhao Jin"
                    },
                    {
                        "name": "Qizhong Gao"
                    },
                    {
                        "name": "Xiaohui Zhu"
                    },
                    {
                        "name": "Yong Yue"
                    },
                    {
                        "name": "Eng Gee Lim"
                    },
                    {
                        "name": "Yuqing Chen"
                    },
                    {
                        "name": "Prudence Wong"
                    },
                    {
                        "name": "Yijie Chu"
                    }
                ],
                "author_detail": {
                    "name": "Yijie Chu"
                },
                "author": "Yijie Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04566v3",
                "updated": "2024-11-19T13:57:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    57,
                    41,
                    1,
                    324,
                    0
                ],
                "published": "2024-04-06T09:27:04Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    9,
                    27,
                    4,
                    5,
                    97,
                    0
                ],
                "title": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering."
                },
                "authors": [
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12498v1",
                "updated": "2024-11-19T13:31:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    31,
                    53,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T13:31:53Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    31,
                    53,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus"
                },
                "summary": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$^{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$^{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$^{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$^{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH."
                },
                "authors": [
                    {
                        "name": "Terufumi Morishita"
                    },
                    {
                        "name": "Gaku Morio"
                    },
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Yasuhiro Sogawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhiro Sogawa"
                },
                "author": "Yasuhiro Sogawa",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11132v2",
                "updated": "2024-11-19T13:13:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    13,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-17T17:36:30Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    17,
                    36,
                    30,
                    6,
                    322,
                    0
                ],
                "title": "Variational Bayesian Bow tie Neural Networks with Shrinkage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayesian Bow tie Neural Networks with Shrinkage"
                },
                "summary": "Despite the dominant role of deep models in machine learning, limitations\npersist, including overconfident predictions, susceptibility to adversarial\nattacks, and underestimation of variability in predictions. The Bayesian\nparadigm provides a natural framework to overcome such issues and has become\nthe gold standard for uncertainty estimation with deep models, also providing\nimproved accuracy and a framework for tuning critical hyperparameters. However,\nexact Bayesian inference is challenging, typically involving variational\nalgorithms that impose strong independence and distributional assumptions.\nMoreover, existing methods are sensitive to the architectural choice of the\nnetwork. We address these issues by constructing a relaxed version of the\nstandard feed-forward rectified neural network, and employing Polya-Gamma data\naugmentation tricks to render a conditionally linear and Gaussian model.\nAdditionally, we use sparsity-promoting priors on the weights of the neural\nnetwork for data-driven architectural design. To approximate the posterior, we\nderive a variational inference algorithm that avoids distributional assumptions\nand independence across layers and is a faster alternative to the usual Markov\nChain Monte Carlo schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the dominant role of deep models in machine learning, limitations\npersist, including overconfident predictions, susceptibility to adversarial\nattacks, and underestimation of variability in predictions. The Bayesian\nparadigm provides a natural framework to overcome such issues and has become\nthe gold standard for uncertainty estimation with deep models, also providing\nimproved accuracy and a framework for tuning critical hyperparameters. However,\nexact Bayesian inference is challenging, typically involving variational\nalgorithms that impose strong independence and distributional assumptions.\nMoreover, existing methods are sensitive to the architectural choice of the\nnetwork. We address these issues by constructing a relaxed version of the\nstandard feed-forward rectified neural network, and employing Polya-Gamma data\naugmentation tricks to render a conditionally linear and Gaussian model.\nAdditionally, we use sparsity-promoting priors on the weights of the neural\nnetwork for data-driven architectural design. To approximate the posterior, we\nderive a variational inference algorithm that avoids distributional assumptions\nand independence across layers and is a faster alternative to the usual Markov\nChain Monte Carlo schemes."
                },
                "authors": [
                    {
                        "name": "Alisa Sheinkman"
                    },
                    {
                        "name": "Sara Wade"
                    }
                ],
                "author_detail": {
                    "name": "Sara Wade"
                },
                "author": "Sara Wade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12484v1",
                "updated": "2024-11-19T13:08:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    8,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T13:08:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    8,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "Regular-pattern-sensitive CRFs for Distant Label Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regular-pattern-sensitive CRFs for Distant Label Interactions"
                },
                "summary": "Linear-chain conditional random fields (CRFs) are a common model component\nfor sequence labeling tasks when modeling the interactions between different\nlabels is important. However, the Markov assumption limits linear-chain CRFs to\nonly directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs) are a related approach which can be made to\nmodel distant label-label interactions, but exact label inference is\nintractable for these models in the general case, and the task of selecting an\nappropriate automaton structure for the desired interaction types poses a\npractical challenge. In this work, we present regular-pattern-sensitive CRFs\n(RPCRFs), a method of enriching standard linear-chain CRFs with the ability to\nlearn long-distance label interactions which occur in user-specified patterns.\nThis approach allows users to write regular-expression label patterns concisely\nspecifying which types of interactions the model should take into account,\nallowing the model to learn from data whether and in which contexts these\npatterns occur. The result can be interpreted alternatively as a CRF augmented\nwith additional, non-local potentials, or as a finite-state transducer whose\nstructure is defined by a set of easily-interpretable patterns. Critically,\nunlike the general case for FSTs (and for non-chain CRFs), exact training and\ninference are tractable for many pattern sets. In this work, we detail how a\nRPCRF can be automatically constructed from a set of user-specified patterns,\nand demonstrate the model's effectiveness on synthetic data, showing how\ndifferent types of patterns can capture different nonlocal dependency\nstructures in label sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-chain conditional random fields (CRFs) are a common model component\nfor sequence labeling tasks when modeling the interactions between different\nlabels is important. However, the Markov assumption limits linear-chain CRFs to\nonly directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs) are a related approach which can be made to\nmodel distant label-label interactions, but exact label inference is\nintractable for these models in the general case, and the task of selecting an\nappropriate automaton structure for the desired interaction types poses a\npractical challenge. In this work, we present regular-pattern-sensitive CRFs\n(RPCRFs), a method of enriching standard linear-chain CRFs with the ability to\nlearn long-distance label interactions which occur in user-specified patterns.\nThis approach allows users to write regular-expression label patterns concisely\nspecifying which types of interactions the model should take into account,\nallowing the model to learn from data whether and in which contexts these\npatterns occur. The result can be interpreted alternatively as a CRF augmented\nwith additional, non-local potentials, or as a finite-state transducer whose\nstructure is defined by a set of easily-interpretable patterns. Critically,\nunlike the general case for FSTs (and for non-chain CRFs), exact training and\ninference are tractable for many pattern sets. In this work, we detail how a\nRPCRF can be automatically constructed from a set of user-specified patterns,\nand demonstrate the model's effectiveness on synthetic data, showing how\ndifferent types of patterns can capture different nonlocal dependency\nstructures in label sequences."
                },
                "authors": [
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Roman Klinger"
                    },
                    {
                        "name": "Sebastian Pado"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pado"
                },
                "author": "Sebastian Pado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12477v1",
                "updated": "2024-11-19T13:00:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    0,
                    41,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T13:00:41Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    0,
                    41,
                    1,
                    324,
                    0
                ],
                "title": "Robust Bayesian causal estimation for causal inference in medical\n  diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bayesian causal estimation for causal inference in medical\n  diagnosis"
                },
                "summary": "Causal effect estimation is a critical task in statistical learning that aims\nto find the causal effect on subjects by identifying causal links between a\nnumber of predictor (or, explanatory) variables and the outcome of a treatment.\nIn a regressional framework, we assign a treatment and outcome model to\nestimate the average causal effect. Additionally, for high dimensional\nregression problems, variable selection methods are also used to find a subset\nof predictor variables that maximises the predictive performance of the\nunderlying model for better estimation of the causal effect. In this paper, we\npropose a different approach. We focus on the variable selection aspects of\nhigh dimensional causal estimation problem. We suggest a cautious Bayesian\ngroup LASSO (least absolute shrinkage and selection operator) framework for\nvariable selection using prior sensitivity analysis. We argue that in some\ncases, abstaining from selecting (or, rejecting) a predictor is beneficial and\nwe should gather more information to obtain a more decisive result. We also\nshow that for problems with very limited information, expert elicited variable\nselection can give us a more stable causal effect estimation as it avoids\noverfitting. Lastly, we carry a comparative study with synthetic dataset and\nshow the applicability of our method in real-life situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal effect estimation is a critical task in statistical learning that aims\nto find the causal effect on subjects by identifying causal links between a\nnumber of predictor (or, explanatory) variables and the outcome of a treatment.\nIn a regressional framework, we assign a treatment and outcome model to\nestimate the average causal effect. Additionally, for high dimensional\nregression problems, variable selection methods are also used to find a subset\nof predictor variables that maximises the predictive performance of the\nunderlying model for better estimation of the causal effect. In this paper, we\npropose a different approach. We focus on the variable selection aspects of\nhigh dimensional causal estimation problem. We suggest a cautious Bayesian\ngroup LASSO (least absolute shrinkage and selection operator) framework for\nvariable selection using prior sensitivity analysis. We argue that in some\ncases, abstaining from selecting (or, rejecting) a predictor is beneficial and\nwe should gather more information to obtain a more decisive result. We also\nshow that for problems with very limited information, expert elicited variable\nselection can give us a more stable causal effect estimation as it avoids\noverfitting. Lastly, we carry a comparative study with synthetic dataset and\nshow the applicability of our method in real-life situations."
                },
                "authors": [
                    {
                        "name": "Tathagata Basu"
                    },
                    {
                        "name": "Matthias C. M. Troffaes"
                    }
                ],
                "author_detail": {
                    "name": "Matthias C. M. Troffaes"
                },
                "author": "Matthias C. M. Troffaes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12469v1",
                "updated": "2024-11-19T12:51:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    51,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:51:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    51,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "AI Flow at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Flow at the Network Edge"
                },
                "summary": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow."
                },
                "authors": [
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05232v2",
                "updated": "2024-11-19T12:42:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    42,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2023-11-09T09:25:37Z",
                "published_parsed": [
                    2023,
                    11,
                    9,
                    9,
                    25,
                    37,
                    3,
                    313,
                    0
                ],
                "title": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions"
                },
                "summary": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Weijiang Yu"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Zhangyin Feng"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Weihua Peng"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_doi": "10.1145/3703155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.05232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Information Systems (TOIS)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04625v3",
                "updated": "2024-11-19T12:41:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    41,
                    4,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-07T04:19:01Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    4,
                    19,
                    1,
                    4,
                    159,
                    0
                ],
                "title": "Key-Element-Informed sLLM Tuning for Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Element-Informed sLLM Tuning for Document Summarization"
                },
                "summary": "Remarkable advances in large language models (LLMs) have enabled high-quality\ntext summarization. However, this capability is currently accessible only\nthrough LLMs of substantial size or proprietary LLMs with usage fees. In\nresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have\nbeen extensively studied, yet they often suffer from missing key information\nand entities, i.e., low relevance, in particular, when input documents are\nlong. We hence propose a key-element-informed instruction tuning for\nsummarization, so-called KEITSum, which identifies key elements in documents\nand instructs sLLM to generate summaries capturing these key elements.\nExperimental results on dialogue and news datasets demonstrate that sLLM with\nKEITSum indeed provides high-quality summarization with higher relevance and\nless hallucinations, competitive to proprietary LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remarkable advances in large language models (LLMs) have enabled high-quality\ntext summarization. However, this capability is currently accessible only\nthrough LLMs of substantial size or proprietary LLMs with usage fees. In\nresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have\nbeen extensively studied, yet they often suffer from missing key information\nand entities, i.e., low relevance, in particular, when input documents are\nlong. We hence propose a key-element-informed instruction tuning for\nsummarization, so-called KEITSum, which identifies key elements in documents\nand instructs sLLM to generate summaries capturing these key elements.\nExperimental results on dialogue and news datasets demonstrate that sLLM with\nKEITSum indeed provides high-quality summarization with higher relevance and\nless hallucinations, competitive to proprietary LLM."
                },
                "authors": [
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Heejin Do"
                    },
                    {
                        "name": "Yunsu Kim"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "arxiv_comment": "Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12460v1",
                "updated": "2024-11-19T12:36:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    36,
                    2,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:36:02Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    36,
                    2,
                    1,
                    324,
                    0
                ],
                "title": "Guide-to-Explain for Controllable Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guide-to-Explain for Controllable Summarization"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nperformance in abstractive summarization tasks. However, controllable\nsummarization with LLMs remains underexplored, limiting their ability to\ngenerate summaries that align with specific user preferences. In this paper, we\nfirst investigate the capability of LLMs to control diverse attributes,\nrevealing that they encounter greater challenges with numerical attributes,\nsuch as length and extractiveness, compared to linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in explaining errors\nin the previous output. Based on this reflection, the model generates a\nwell-adjusted summary. As a result, by allowing the model to reflect on its\nmisalignment, we generate summaries that satisfy the desired attributes in\nsurprisingly fewer iterations than other iterative methods solely using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated remarkable\nperformance in abstractive summarization tasks. However, controllable\nsummarization with LLMs remains underexplored, limiting their ability to\ngenerate summaries that align with specific user preferences. In this paper, we\nfirst investigate the capability of LLMs to control diverse attributes,\nrevealing that they encounter greater challenges with numerical attributes,\nsuch as length and extractiveness, compared to linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in explaining errors\nin the previous output. Based on this reflection, the model generates a\nwell-adjusted summary. As a result, by allowing the model to reflect on its\nmisalignment, we generate summaries that satisfy the desired attributes in\nsurprisingly fewer iterations than other iterative methods solely using LLMs."
                },
                "authors": [
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Heejin Do"
                    },
                    {
                        "name": "Daehee Kim"
                    },
                    {
                        "name": "Yunsu Kim"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11410v2",
                "updated": "2024-11-19T12:25:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    25,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-18T09:30:14Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    30,
                    14,
                    0,
                    323,
                    0
                ],
                "title": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries"
                },
                "summary": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose MPDetector, for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detects\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose MPDetector, for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detects\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing."
                },
                "authors": [
                    {
                        "name": "Xiufeng Xu"
                    },
                    {
                        "name": "Fuman Xie"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Guangdong Bai"
                    },
                    {
                        "name": "Sarfraz Khurshid"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12449v1",
                "updated": "2024-11-19T12:17:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    17,
                    43,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:17:43Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    17,
                    43,
                    1,
                    324,
                    0
                ],
                "title": "\\textsc{Neon}: News Entity-Interaction Extraction for Enhanced Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textsc{Neon}: News Entity-Interaction Extraction for Enhanced Question\n  Answering"
                },
                "summary": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses."
                },
                "authors": [
                    {
                        "name": "Sneha Singhania"
                    },
                    {
                        "name": "Silviu Cucerzan"
                    },
                    {
                        "name": "Allen Herring"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12448v1",
                "updated": "2024-11-19T12:15:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    15,
                    40,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:15:40Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    15,
                    40,
                    1,
                    324,
                    0
                ],
                "title": "Large Language Models for Lossless Image Compression: Next-Pixel\n  Prediction in Language Space is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Lossless Image Compression: Next-Pixel\n  Prediction in Language Space is All You Need"
                },
                "summary": "We have recently witnessed that ``Intelligence\" and `` Compression\" are the\ntwo sides of the same coin, where the language large model (LLM) with\nunprecedented intelligence is a general-purpose lossless compressor for various\ndata modalities. This attribute particularly appeals to the lossless image\ncompression community, given the increasing need to compress high-resolution\nimages in the current streaming media era. Consequently, a spontaneous envision\nemerges: Can the compression performance of the LLM elevate lossless image\ncompression to new heights? However, our findings indicate that the naive\napplication of LLM-based lossless image compressors suffers from a considerable\nperformance gap compared with existing state-of-the-art (SOTA) codecs on common\nbenchmark datasets. In light of this, we are dedicated to fulfilling the\nunprecedented intelligence (compression) capacity of the LLM for lossless image\ncompression tasks, thereby bridging the gap between theoretical and practical\ncompression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel\nprediction-based LLM, which integrates various elaborated insights and\nmethodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of\nLLM, and a pixel-level semantic preservation strategy, to enhance the\nunderstanding capacity of pixel sequences for better next-pixel predictions.\nExtensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can\nbeat SOTA classical and learned codecs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have recently witnessed that ``Intelligence\" and `` Compression\" are the\ntwo sides of the same coin, where the language large model (LLM) with\nunprecedented intelligence is a general-purpose lossless compressor for various\ndata modalities. This attribute particularly appeals to the lossless image\ncompression community, given the increasing need to compress high-resolution\nimages in the current streaming media era. Consequently, a spontaneous envision\nemerges: Can the compression performance of the LLM elevate lossless image\ncompression to new heights? However, our findings indicate that the naive\napplication of LLM-based lossless image compressors suffers from a considerable\nperformance gap compared with existing state-of-the-art (SOTA) codecs on common\nbenchmark datasets. In light of this, we are dedicated to fulfilling the\nunprecedented intelligence (compression) capacity of the LLM for lossless image\ncompression tasks, thereby bridging the gap between theoretical and practical\ncompression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel\nprediction-based LLM, which integrates various elaborated insights and\nmethodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of\nLLM, and a pixel-level semantic preservation strategy, to enhance the\nunderstanding capacity of pixel sequences for better next-pixel predictions.\nExtensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can\nbeat SOTA classical and learned codecs."
                },
                "authors": [
                    {
                        "name": "Kecheng Chen"
                    },
                    {
                        "name": "Pingping Zhang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Yibing Liu"
                    },
                    {
                        "name": "Jixin Huang"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Hong Yan"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11194v2",
                "updated": "2024-11-19T11:26:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    26,
                    29,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-17T22:58:28Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    22,
                    58,
                    28,
                    6,
                    322,
                    0
                ],
                "title": "Careless Whisper: Exploiting Stealthy End-to-End Leakage in Mobile\n  Instant Messengers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Careless Whisper: Exploiting Stealthy End-to-End Leakage in Mobile\n  Instant Messengers"
                },
                "summary": "With over 3 billion users globally, mobile instant messaging apps have become\nindispensable for both personal and professional communication. Besides plain\nmessaging, many services implement additional features such as delivery and\nread receipts informing a user when a message has successfully reached its\ntarget. This paper highlights that delivery receipts can pose significant\nprivacy risks to users. We use specifically crafted messages that trigger\ndelivery receipts allowing any user to be pinged without their knowledge or\nconsent. By using this technique at high frequency, we demonstrate how an\nattacker could extract private information such as the online and activity\nstatus of a victim, e.g., screen on/off. Moreover, we can infer the number of\ncurrently active user devices and their operating system, as well as launch\nresource exhaustion attacks, such as draining a user's battery or data\nallowance, all without generating any notification on the target side. Due to\nthe widespread adoption of vulnerable messengers (WhatsApp and Signal) and the\nfact that any user can be targeted simply by knowing their phone number, we\nargue for a design change to address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With over 3 billion users globally, mobile instant messaging apps have become\nindispensable for both personal and professional communication. Besides plain\nmessaging, many services implement additional features such as delivery and\nread receipts informing a user when a message has successfully reached its\ntarget. This paper highlights that delivery receipts can pose significant\nprivacy risks to users. We use specifically crafted messages that trigger\ndelivery receipts allowing any user to be pinged without their knowledge or\nconsent. By using this technique at high frequency, we demonstrate how an\nattacker could extract private information such as the online and activity\nstatus of a victim, e.g., screen on/off. Moreover, we can infer the number of\ncurrently active user devices and their operating system, as well as launch\nresource exhaustion attacks, such as draining a user's battery or data\nallowance, all without generating any notification on the target side. Due to\nthe widespread adoption of vulnerable messengers (WhatsApp and Signal) and the\nfact that any user can be targeted simply by knowing their phone number, we\nargue for a design change to address this issue."
                },
                "authors": [
                    {
                        "name": "Gabriel K. Gegenhuber"
                    },
                    {
                        "name": "Maximilian Gnther"
                    },
                    {
                        "name": "Markus Maier"
                    },
                    {
                        "name": "Aljosha Judmayer"
                    },
                    {
                        "name": "Florian Holzbauer"
                    },
                    {
                        "name": "Philipp . Frenzel"
                    },
                    {
                        "name": "Johanna Ullrich"
                    }
                ],
                "author_detail": {
                    "name": "Johanna Ullrich"
                },
                "author": "Johanna Ullrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04793v2",
                "updated": "2024-11-19T10:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    59,
                    30,
                    1,
                    324,
                    0
                ],
                "published": "2024-05-08T03:57:45Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    3,
                    57,
                    45,
                    2,
                    129,
                    0
                ],
                "title": "Zero-shot LLM-guided Counterfactual Generation: A Case Study on NLP\n  Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot LLM-guided Counterfactual Generation: A Case Study on NLP\n  Model Evaluation"
                },
                "summary": "With the development and proliferation of large, complex, black-box models\nfor solving many natural language processing (NLP) tasks, there is also an\nincreasing necessity of methods to stress-test these models and provide some\ndegree of interpretability or explainability. While counterfactual examples are\nuseful in this regard, automated generation of counterfactuals is a data and\nresource intensive process. such methods depend on models such as pre-trained\nlanguage models that are then fine-tuned on auxiliary, often task-specific\ndatasets, that may be infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the possibility of\nleveraging large language models (LLMs) for zero-shot counterfactual generation\nin order to stress-test NLP models. We propose a structured pipeline to\nfacilitate this generation, and we hypothesize that the instruction-following\nand textual understanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-shot manner,\nwithout requiring any training or fine-tuning. Through comprehensive\nexperiments on a variety of propreitary and open-source LLMs, along with\nvarious downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development and proliferation of large, complex, black-box models\nfor solving many natural language processing (NLP) tasks, there is also an\nincreasing necessity of methods to stress-test these models and provide some\ndegree of interpretability or explainability. While counterfactual examples are\nuseful in this regard, automated generation of counterfactuals is a data and\nresource intensive process. such methods depend on models such as pre-trained\nlanguage models that are then fine-tuned on auxiliary, often task-specific\ndatasets, that may be infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the possibility of\nleveraging large language models (LLMs) for zero-shot counterfactual generation\nin order to stress-test NLP models. We propose a structured pipeline to\nfacilitate this generation, and we hypothesize that the instruction-following\nand textual understanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-shot manner,\nwithout requiring any training or fine-tuning. Through comprehensive\nexperiments on a variety of propreitary and open-source LLMs, along with\nvarious downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models."
                },
                "authors": [
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Raha Moraffah"
                    },
                    {
                        "name": "Joshua Garland"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Longer version of short paper accepted at IEEE BigData 2024 (Main\n  Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10987v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10987v4",
                "updated": "2024-11-19T10:55:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    55,
                    52,
                    1,
                    324,
                    0
                ],
                "published": "2023-09-20T01:04:57Z",
                "published_parsed": [
                    2023,
                    9,
                    20,
                    1,
                    4,
                    57,
                    2,
                    263,
                    0
                ],
                "title": "SpikingNeRF: Making Bio-inspired Neural Networks See through the Real\n  World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikingNeRF: Making Bio-inspired Neural Networks See through the Real\n  World"
                },
                "summary": "In this paper, we propose SpikingNeRF, which aligns the temporal dimension of\nspiking neural networks (SNNs) with the radiance rays, to seamlessly\naccommodate SNNs to the reconstruction of neural radiance fields (NeRF). Thus,\nthe computation turns into a spike-based, multiplication-free manner, reducing\nenergy consumption and making high-quality 3D rendering, for the first time,\naccessible to neuromorphic hardware. In SpikingNeRF, each sampled point on the\nray is matched to a particular time step and represented in a hybrid manner\nwhere the voxel grids are maintained as well. Based on the voxel grids, sampled\npoints are determined whether to be masked out for faster training and\ninference. However, this masking operation also incurs irregular temporal\nlength, making it intractable for hardware processors, e.g., GPUs, to conduct\nparallel training. To address this problem, we develop the temporal padding\nstrategy to tackle the masked samples to maintain regular temporal length,\ni.e., regular tensors, and further propose the temporal condensing strategy to\nform a denser data structure for hardware-friendly computation. Experiments on\nvarious datasets demonstrate that our method can reduce energy consumption by\nan average of 70.79\\% and obtain comparable synthesis quality with the ANN\nbaseline. Verification on the neuromorphic hardware accelerator also shows that\nSpikingNeRF can further benefit from neuromorphic computing over the ANN\nbaselines on energy efficiency. Codes and the appendix are in\n\\url{https://github.com/Ikarosy/SpikingNeRF-of-CASIA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose SpikingNeRF, which aligns the temporal dimension of\nspiking neural networks (SNNs) with the radiance rays, to seamlessly\naccommodate SNNs to the reconstruction of neural radiance fields (NeRF). Thus,\nthe computation turns into a spike-based, multiplication-free manner, reducing\nenergy consumption and making high-quality 3D rendering, for the first time,\naccessible to neuromorphic hardware. In SpikingNeRF, each sampled point on the\nray is matched to a particular time step and represented in a hybrid manner\nwhere the voxel grids are maintained as well. Based on the voxel grids, sampled\npoints are determined whether to be masked out for faster training and\ninference. However, this masking operation also incurs irregular temporal\nlength, making it intractable for hardware processors, e.g., GPUs, to conduct\nparallel training. To address this problem, we develop the temporal padding\nstrategy to tackle the masked samples to maintain regular temporal length,\ni.e., regular tensors, and further propose the temporal condensing strategy to\nform a denser data structure for hardware-friendly computation. Experiments on\nvarious datasets demonstrate that our method can reduce energy consumption by\nan average of 70.79\\% and obtain comparable synthesis quality with the ANN\nbaseline. Verification on the neuromorphic hardware accelerator also shows that\nSpikingNeRF can further benefit from neuromorphic computing over the ANN\nbaselines on energy efficiency. Codes and the appendix are in\n\\url{https://github.com/Ikarosy/SpikingNeRF-of-CASIA}."
                },
                "authors": [
                    {
                        "name": "Xingting Yao"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Fei Zhou"
                    },
                    {
                        "name": "Tielong Liu"
                    },
                    {
                        "name": "Zitao Mo"
                    },
                    {
                        "name": "Zeyu Zhu"
                    },
                    {
                        "name": "Zhengyang Zhuge"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10987v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10987v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12402v1",
                "updated": "2024-11-19T10:38:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    38,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T10:38:57Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    38,
                    57,
                    1,
                    324,
                    0
                ],
                "title": "A total-shear-stress-conserved wall model for large-eddy simulation of\n  high-Reynolds number wall turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A total-shear-stress-conserved wall model for large-eddy simulation of\n  high-Reynolds number wall turbulence"
                },
                "summary": "Wall-modeled large-eddy simulation (WMLES) is widely recognized as a useful\nmethod for simulation of turbulent flows at high Reynolds numbers.\nNevertheless, a continual issue in different wall models is the shift of the\nmean velocity profile from the wall-model/RANS (Reynolds-averaged\nNavier-Stokes) region to the LES region. This phenomenon, referred to as\nlogarithmic layer mismatch (LLM), occurs in both wall shear stress models and\nhybrid RANS/LES models. Many efforts have been made to explain and resolve this\nmismatch, including decreasing the high correlation between the wall shear\nstress and the velocity at the matching layer, modifying the subgrid-scale\n(SGS) eddy viscosity, and adding a stochastic forcing. It is widely believed\nthat the inclusion of the resolved Reynolds shear stress (or the convection\nterm) is essential to elliminate the LLM, as it prevents the overseimation of\nthe modeled Reynolds shear stress and promotes the generation of the\nsmall-scale flow structures in the near-wall region. In this work, by comparing\nthree different SGS eddy viscosity models, we demonstrate that ensuring the\ntotal shear stress conservation (TSSC) conservation is key to resolving the\nLLM. Under the TSSC framework, the effect of the convection term on LLM can be\nquantitatively assessed. Furthermore, a modified SGS eddy viscosity modfication\nmodel that adheres to the TSSC constraint is tested at different Reynolds\nnumbers ($Re_\\tau=1000, 2000, 4200$). Our results demonstrate the robust\nperformance of the present model in predicting skin friction and low-order\nturbulence statistics, even under a relatively low grid resolution ($\\Delta\nx^+, \\Delta z^+ \\lesssim 500$, $2\\leq \\Delta_x/\\Delta_{y,mat} \\leq 4$, where\n$\\Delta_{y,mat}$ is the wall-normal grid spacing in the wall-model region).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wall-modeled large-eddy simulation (WMLES) is widely recognized as a useful\nmethod for simulation of turbulent flows at high Reynolds numbers.\nNevertheless, a continual issue in different wall models is the shift of the\nmean velocity profile from the wall-model/RANS (Reynolds-averaged\nNavier-Stokes) region to the LES region. This phenomenon, referred to as\nlogarithmic layer mismatch (LLM), occurs in both wall shear stress models and\nhybrid RANS/LES models. Many efforts have been made to explain and resolve this\nmismatch, including decreasing the high correlation between the wall shear\nstress and the velocity at the matching layer, modifying the subgrid-scale\n(SGS) eddy viscosity, and adding a stochastic forcing. It is widely believed\nthat the inclusion of the resolved Reynolds shear stress (or the convection\nterm) is essential to elliminate the LLM, as it prevents the overseimation of\nthe modeled Reynolds shear stress and promotes the generation of the\nsmall-scale flow structures in the near-wall region. In this work, by comparing\nthree different SGS eddy viscosity models, we demonstrate that ensuring the\ntotal shear stress conservation (TSSC) conservation is key to resolving the\nLLM. Under the TSSC framework, the effect of the convection term on LLM can be\nquantitatively assessed. Furthermore, a modified SGS eddy viscosity modfication\nmodel that adheres to the TSSC constraint is tested at different Reynolds\nnumbers ($Re_\\tau=1000, 2000, 4200$). Our results demonstrate the robust\nperformance of the present model in predicting skin friction and low-order\nturbulence statistics, even under a relatively low grid resolution ($\\Delta\nx^+, \\Delta z^+ \\lesssim 500$, $2\\leq \\Delta_x/\\Delta_{y,mat} \\leq 4$, where\n$\\Delta_{y,mat}$ is the wall-normal grid spacing in the wall-model region)."
                },
                "authors": [
                    {
                        "name": "Huan-Cong Liu"
                    },
                    {
                        "name": "Chun-Xiao Xu"
                    },
                    {
                        "name": "Wei-Xi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Xi Huang"
                },
                "author": "Wei-Xi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12397v1",
                "updated": "2024-11-19T10:30:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    30,
                    10,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T10:30:10Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    30,
                    10,
                    1,
                    324,
                    0
                ],
                "title": "Non-Local Thermodynamic Equilibrium inversions of the Si I 10827 A\n  spectral line",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Local Thermodynamic Equilibrium inversions of the Si I 10827 A\n  spectral line"
                },
                "summary": "Inferring the coupling of different atmospheric layers requires observing\nspectral lines sensitive to the atmospheric parameters, particularly the\nmagnetic field vector, at various heights. The best way to tackle this goal is\nto perform multi-line observations simultaneously. For instance, the new\nversion of the Gregor Infrared Spectrograph instrument offers the possibility\nto observe the spectral lines at 8542 and 10830 A simultaneously for the first\ntime. The first spectral window contains the Ca II 8542 A spectral line, while\nthe Si I 10827 A transition and He I 10830 A triplet infrared lines can be\nfound in the second spectral window. As the sensitivity to the atmospheric\nparameters and the height of formation of those transitions is different,\ncombining them can help understand the properties of the solar photosphere and\nchromosphere and how they are magnetically coupled. Traditionally, the analysis\nof the Si I 10827 A transition assumes local thermodynamic equilibrium (LTE),\nwhich is not the best approximation to model this transition. Hence, in this\nwork, we examine the potential of performing non-LTE (NLTE) inversions of the\nfull Stokes vector of the Si I 10827 A spectral line. The results indicate that\nwe properly infer the atmospheric parameters through an extended range of\natmospheric layers in comparison with the LTE case (only valid for the spectral\nline wings, i.e., the low photosphere), with no impact on the robustness of the\nsolution and just a minor increase in computational time. Thus, the NLTE\nassumption will help to accurately constrain the photospheric physical\nparameters when performing combined inversions with, e.g., the Ca II 8542 A\nspectral line.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the coupling of different atmospheric layers requires observing\nspectral lines sensitive to the atmospheric parameters, particularly the\nmagnetic field vector, at various heights. The best way to tackle this goal is\nto perform multi-line observations simultaneously. For instance, the new\nversion of the Gregor Infrared Spectrograph instrument offers the possibility\nto observe the spectral lines at 8542 and 10830 A simultaneously for the first\ntime. The first spectral window contains the Ca II 8542 A spectral line, while\nthe Si I 10827 A transition and He I 10830 A triplet infrared lines can be\nfound in the second spectral window. As the sensitivity to the atmospheric\nparameters and the height of formation of those transitions is different,\ncombining them can help understand the properties of the solar photosphere and\nchromosphere and how they are magnetically coupled. Traditionally, the analysis\nof the Si I 10827 A transition assumes local thermodynamic equilibrium (LTE),\nwhich is not the best approximation to model this transition. Hence, in this\nwork, we examine the potential of performing non-LTE (NLTE) inversions of the\nfull Stokes vector of the Si I 10827 A spectral line. The results indicate that\nwe properly infer the atmospheric parameters through an extended range of\natmospheric layers in comparison with the LTE case (only valid for the spectral\nline wings, i.e., the low photosphere), with no impact on the robustness of the\nsolution and just a minor increase in computational time. Thus, the NLTE\nassumption will help to accurately constrain the photospheric physical\nparameters when performing combined inversions with, e.g., the Ca II 8542 A\nspectral line."
                },
                "authors": [
                    {
                        "name": "C. Quintero Noda"
                    },
                    {
                        "name": "N. G. Shchukina"
                    },
                    {
                        "name": "A. Asensio Ramos"
                    },
                    {
                        "name": "M. J. Martnez Gonzlez"
                    },
                    {
                        "name": "T. del Pino Alemn"
                    },
                    {
                        "name": "J. C. Trelles Arjona"
                    },
                    {
                        "name": "M. Collados"
                    }
                ],
                "author_detail": {
                    "name": "M. Collados"
                },
                "author": "M. Collados",
                "arxiv_comment": "7 pages, 8 figures, regular paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12395v1",
                "updated": "2024-11-19T10:27:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    27,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T10:27:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    27,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Do LLMs Understand Ambiguity in Text? A Case Study in Open-world\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Ambiguity in Text? A Case Study in Open-world\n  Question Answering"
                },
                "summary": "Ambiguity in natural language poses significant challenges to Large Language\nModels (LLMs) used for open-domain question answering. LLMs often struggle with\nthe inherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and biased responses.\nThis significantly weakens their ability to be used for tasks like\nfact-checking, question answering, feature extraction, and sentiment analysis.\nUsing open-domain question answering as a test case, we compare off-the-shelf\nand few-shot LLM performance, focusing on measuring the impact of explicit\ndisambiguation strategies. We demonstrate how simple, training-free,\ntoken-level disambiguation methods may be effectively used to improve LLM\nperformance for ambiguous question answering tasks. We empirically show our\nfindings and discuss best practices and broader impacts regarding ambiguity in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in natural language poses significant challenges to Large Language\nModels (LLMs) used for open-domain question answering. LLMs often struggle with\nthe inherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and biased responses.\nThis significantly weakens their ability to be used for tasks like\nfact-checking, question answering, feature extraction, and sentiment analysis.\nUsing open-domain question answering as a test case, we compare off-the-shelf\nand few-shot LLM performance, focusing on measuring the impact of explicit\ndisambiguation strategies. We demonstrate how simple, training-free,\ntoken-level disambiguation methods may be effectively used to improve LLM\nperformance for ambiguous question answering tasks. We empirically show our\nfindings and discuss best practices and broader impacts regarding ambiguity in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Aryan Keluskar"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Accepted at the REU Symposium at IEEE BigData 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.07631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.07631v2",
                "updated": "2024-11-19T10:23:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    23,
                    41,
                    1,
                    324,
                    0
                ],
                "published": "2023-03-14T05:06:06Z",
                "published_parsed": [
                    2023,
                    3,
                    14,
                    5,
                    6,
                    6,
                    1,
                    73,
                    0
                ],
                "title": "Multiple Testing under High-dimensional Dynamic Factor Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Testing under High-dimensional Dynamic Factor Model"
                },
                "summary": "Large-scale multiple testing under static factor models is commonly used to\nselect skilled funds in financial market. However, static factor models are\narguably too stringent as it ignores the serial correlation, which severely\ndistorts error rate control in large-scale inference. In this manuscript, we\npropose a new multiple testing procedure under dynamic factor models that is\nrobust to the nonlinear serial dependence (e.g., GARCH). The idea is to\nintegrate a new sample-splitting strategy based on chronological order and a\ntwo-pass Fama-Macbeth regression to form a series of statistics with marginal\nsymmetry properties and then to utilize the symmetry properties to obtain a\ndata-driven threshold. We show that our procedure is able to control the false\ndiscovery rate (FDR) asymptotically under high-dimensional dynamic factor\nmodels. As a byproduct that is of independent interest, we establish a new\nexponential-type deviation inequality for the sum of random variables on a\nvariety of functionals of linear and non-linear processes. Numerical results\nincluding a case study on hedge fund selection demonstrate the advantage of the\nproposed method over several state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale multiple testing under static factor models is commonly used to\nselect skilled funds in financial market. However, static factor models are\narguably too stringent as it ignores the serial correlation, which severely\ndistorts error rate control in large-scale inference. In this manuscript, we\npropose a new multiple testing procedure under dynamic factor models that is\nrobust to the nonlinear serial dependence (e.g., GARCH). The idea is to\nintegrate a new sample-splitting strategy based on chronological order and a\ntwo-pass Fama-Macbeth regression to form a series of statistics with marginal\nsymmetry properties and then to utilize the symmetry properties to obtain a\ndata-driven threshold. We show that our procedure is able to control the false\ndiscovery rate (FDR) asymptotically under high-dimensional dynamic factor\nmodels. As a byproduct that is of independent interest, we establish a new\nexponential-type deviation inequality for the sum of random variables on a\nvariety of functionals of linear and non-linear processes. Numerical results\nincluding a case study on hedge fund selection demonstrate the advantage of the\nproposed method over several state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xinxin Yang"
                    },
                    {
                        "name": "Lilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Lilun Du"
                },
                "author": "Lilun Du",
                "arxiv_comment": "30 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.07631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.07631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v4",
                "updated": "2024-11-19T10:00:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    0,
                    41,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ishneet Sukhvinder Singh"
                    },
                    {
                        "name": "Ritvik Aggarwal"
                    },
                    {
                        "name": "Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17151v3",
                "updated": "2024-11-19T09:48:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    48,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-05-27T13:26:34Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    13,
                    26,
                    34,
                    0,
                    148,
                    0
                ],
                "title": "Smoke and Mirrors in Causal Downstream Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smoke and Mirrors in Causal Downstream Tasks"
                },
                "summary": "Machine Learning and AI have the potential to transform data-driven\nscientific discovery, enabling accurate predictions for several scientific\nphenomena. As many scientific questions are inherently causal, this paper looks\nat the causal inference task of treatment effect estimation, where the outcome\nof interest is recorded in high-dimensional observations in a Randomized\nControlled Trial (RCT). Despite being the simplest possible causal setting and\na perfect fit for deep learning, we theoretically find that many common choices\nin the literature may lead to biased estimates. To test the practical impact of\nthese considerations, we recorded ISTAnt, the first real-world benchmark for\ncausal inference downstream tasks on high-dimensional observations as an RCT\nstudying how garden ants (Lasius neglectus) respond to microparticles applied\nonto their colony members by hygienic grooming. Comparing 6 480 models\nfine-tuned from state-of-the-art visual backbones, we find that the sampling\nand modeling choices significantly affect the accuracy of the causal estimate,\nand that classification accuracy is not a proxy thereof. We further validated\nthe analysis, repeating it on a synthetically generated visual data set\ncontrolling the causal model. Our results suggest that future benchmarks should\ncarefully consider real downstream scientific questions, especially causal\nones. Further, we highlight guidelines for representation learning methods to\nhelp answer causal questions in the sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning and AI have the potential to transform data-driven\nscientific discovery, enabling accurate predictions for several scientific\nphenomena. As many scientific questions are inherently causal, this paper looks\nat the causal inference task of treatment effect estimation, where the outcome\nof interest is recorded in high-dimensional observations in a Randomized\nControlled Trial (RCT). Despite being the simplest possible causal setting and\na perfect fit for deep learning, we theoretically find that many common choices\nin the literature may lead to biased estimates. To test the practical impact of\nthese considerations, we recorded ISTAnt, the first real-world benchmark for\ncausal inference downstream tasks on high-dimensional observations as an RCT\nstudying how garden ants (Lasius neglectus) respond to microparticles applied\nonto their colony members by hygienic grooming. Comparing 6 480 models\nfine-tuned from state-of-the-art visual backbones, we find that the sampling\nand modeling choices significantly affect the accuracy of the causal estimate,\nand that classification accuracy is not a proxy thereof. We further validated\nthe analysis, repeating it on a synthetically generated visual data set\ncontrolling the causal model. Our results suggest that future benchmarks should\ncarefully consider real downstream scientific questions, especially causal\nones. Further, we highlight guidelines for representation learning methods to\nhelp answer causal questions in the sciences."
                },
                "authors": [
                    {
                        "name": "Riccardo Cadei"
                    },
                    {
                        "name": "Lukas Lindorfer"
                    },
                    {
                        "name": "Sylvia Cremer"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Francesco Locatello"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Locatello"
                },
                "author": "Francesco Locatello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12364v1",
                "updated": "2024-11-19T09:24:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    24,
                    34,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T09:24:34Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    24,
                    34,
                    1,
                    324,
                    0
                ],
                "title": "Ultra-Sparse Memory Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-Sparse Memory Network"
                },
                "summary": "It is widely acknowledged that the performance of Transformer models is\nexponentially related to their number of parameters and computational\ncomplexity. While approaches like Mixture of Experts (MoE) decouple parameter\ncount from computational complexity, they still face challenges in inference\ndue to high memory access costs. This work introduces UltraMem, incorporating\nlarge-scale, ultra-sparse memory layer to address these limitations. Our\napproach significantly reduces inference latency while maintaining model\nperformance. We also investigate the scaling laws of this new architecture,\ndemonstrating that it not only exhibits favorable scaling properties but\noutperforms traditional models. In our experiments, we train networks with up\nto 20 million memory slots. The results show that our method achieves\nstate-of-the-art inference speed and model performance within a given\ncomputational budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is widely acknowledged that the performance of Transformer models is\nexponentially related to their number of parameters and computational\ncomplexity. While approaches like Mixture of Experts (MoE) decouple parameter\ncount from computational complexity, they still face challenges in inference\ndue to high memory access costs. This work introduces UltraMem, incorporating\nlarge-scale, ultra-sparse memory layer to address these limitations. Our\napproach significantly reduces inference latency while maintaining model\nperformance. We also investigate the scaling laws of this new architecture,\ndemonstrating that it not only exhibits favorable scaling properties but\noutperforms traditional models. In our experiments, we train networks with up\nto 20 million memory slots. The results show that our method achieves\nstate-of-the-art inference speed and model performance within a given\ncomputational budget."
                },
                "authors": [
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Qiyang Min"
                    },
                    {
                        "name": "Hongzhi Huang"
                    },
                    {
                        "name": "Defa Zhu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ran Guo"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12357v1",
                "updated": "2024-11-19T09:18:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    18,
                    20,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T09:18:20Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    18,
                    20,
                    1,
                    324,
                    0
                ],
                "title": "A Layered Architecture for Developing and Enhancing Capabilities in\n  Large Language Model-based Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Layered Architecture for Developing and Enhancing Capabilities in\n  Large Language Model-based Software Systems"
                },
                "summary": "Significant efforts has been made to expand the use of Large Language Models\n(LLMs) beyond basic language tasks. While the generalizability and versatility\nof LLMs have enabled widespread adoption, evolving demands in application\ndevelopment often exceed their native capabilities. Meeting these demands may\ninvolve a diverse set of methods, such as enhancing creativity through either\ninference temperature adjustments or creativity-provoking prompts. Selecting\nthe right approach is critical, as different methods lead to trade-offs in\nengineering complexity, scalability, and operational costs. This paper\nintroduces a layered architecture that organizes LLM software system\ndevelopment into distinct layers, each characterized by specific attributes. By\naligning capabilities with these layers, the framework encourages the\nsystematic implementation of capabilities in effective and efficient ways that\nultimately supports desired functionalities and qualities. Through practical\ncase studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based\nsoftware system development, promoting robustness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant efforts has been made to expand the use of Large Language Models\n(LLMs) beyond basic language tasks. While the generalizability and versatility\nof LLMs have enabled widespread adoption, evolving demands in application\ndevelopment often exceed their native capabilities. Meeting these demands may\ninvolve a diverse set of methods, such as enhancing creativity through either\ninference temperature adjustments or creativity-provoking prompts. Selecting\nthe right approach is critical, as different methods lead to trade-offs in\nengineering complexity, scalability, and operational costs. This paper\nintroduces a layered architecture that organizes LLM software system\ndevelopment into distinct layers, each characterized by specific attributes. By\naligning capabilities with these layers, the framework encourages the\nsystematic implementation of capabilities in effective and efficient ways that\nultimately supports desired functionalities and qualities. Through practical\ncase studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based\nsoftware system development, promoting robustness and scalability."
                },
                "authors": [
                    {
                        "name": "Dawen Zhang"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Robert Mao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mao"
                },
                "author": "Robert Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12355v1",
                "updated": "2024-11-19T09:16:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    16,
                    54,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T09:16:54Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    16,
                    54,
                    1,
                    324,
                    0
                ],
                "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding"
                },
                "summary": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance."
                },
                "authors": [
                    {
                        "name": "Yudong Han"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Liyuan Pan"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09328v2",
                "updated": "2024-11-19T09:13:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    13,
                    24,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-14T10:19:55Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    19,
                    55,
                    3,
                    319,
                    0
                ],
                "title": "A Flexible Framework for Grant-Free Random Access in Cell-Free Massive\n  MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Framework for Grant-Free Random Access in Cell-Free Massive\n  MIMO Systems"
                },
                "summary": "We propose a novel generalized framework for grant-free random-access (GFRA)\nin cell-free massive multiple input multiple-output systems where multiple\ngeographically separated access points (APs) or base stations (BSs) aim to\ndetect sporadically active user-equipment (UEs). Unlike a conventional\narchitecture in which all the active UEs transmit their signature or pilot\nsequences of equal length, we admit a flexible pilot length for each UE, which\nalso enables a seamless integration into conventional grant-based wireless\nsystems. We formulate the joint UE activity detection and the distributed\nchannel estimation as a sparse support and signal recovery problem, and\ndescribe a Bayesian learning procedure to solve it. We develop a scheme to fuse\nthe posterior statistics of the latent variables inferred by each AP to jointly\ndetect the UEs' activities, and utilize them to further refine the channel\nestimates. In addition, we allude to an interesting point which enables this\nflexible GFRA framework to encode the information bits from the active UEs. We\nnumerically evaluate the normalized mean square error and the probability of\nmiss-detection performances obtained by the Bayesian algorithm and show that\nthe latent-variable fusion enhances the detection and the channel estimation\nperformances by a large margin. We also benchmark against a genie-aided\nalgorithm which has a prior knowledge of the UEs' activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel generalized framework for grant-free random-access (GFRA)\nin cell-free massive multiple input multiple-output systems where multiple\ngeographically separated access points (APs) or base stations (BSs) aim to\ndetect sporadically active user-equipment (UEs). Unlike a conventional\narchitecture in which all the active UEs transmit their signature or pilot\nsequences of equal length, we admit a flexible pilot length for each UE, which\nalso enables a seamless integration into conventional grant-based wireless\nsystems. We formulate the joint UE activity detection and the distributed\nchannel estimation as a sparse support and signal recovery problem, and\ndescribe a Bayesian learning procedure to solve it. We develop a scheme to fuse\nthe posterior statistics of the latent variables inferred by each AP to jointly\ndetect the UEs' activities, and utilize them to further refine the channel\nestimates. In addition, we allude to an interesting point which enables this\nflexible GFRA framework to encode the information bits from the active UEs. We\nnumerically evaluate the normalized mean square error and the probability of\nmiss-detection performances obtained by the Bayesian algorithm and show that\nthe latent-variable fusion enhances the detection and the channel estimation\nperformances by a large margin. We also benchmark against a genie-aided\nalgorithm which has a prior knowledge of the UEs' activities."
                },
                "authors": [
                    {
                        "name": "Sai Subramanyam Thoota"
                    },
                    {
                        "name": "Erik G. Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Erik G. Larsson"
                },
                "author": "Erik G. Larsson",
                "arxiv_comment": "Published in the Proceedings of SPAWC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10715v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10715v2",
                "updated": "2024-11-19T09:07:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    7,
                    18,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-16T06:11:10Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    6,
                    11,
                    10,
                    5,
                    321,
                    0
                ],
                "title": "EVT: Efficient View Transformation for Multi-Modal 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVT: Efficient View Transformation for Multi-Modal 3D Object Detection"
                },
                "summary": "Multi-modal sensor fusion in bird's-eye-view (BEV) representation has become\nthe leading approach in 3D object detection. However, existing methods often\nrely on depth estimators or transformer encoders for view transformation,\nincurring substantial computational overhead. Furthermore, the lack of precise\ngeometric correspondence between 2D and 3D spaces leads to spatial and\nray-directional misalignments, restricting the effectiveness of BEV\nrepresentations. To address these challenges, we propose a novel 3D object\ndetector via efficient view transformation (EVT), which leverages a\nwell-structured BEV representation to enhance accuracy and efficiency. EVT\nfocuses on two main areas. First, it employs Adaptive Sampling and Adaptive\nProjection (ASAP), using LiDAR guidance to generate 3D sampling points and\nadaptive kernels. The generated points and kernels are then used to facilitate\nthe transformation of image features into BEV space and refine the BEV\nfeatures. Second, EVT includes an improved transformer-based detection\nframework, which contains a group-wise query initialization method and an\nenhanced query update framework. It is designed to effectively utilize the\nobtained multi-modal BEV features within the transformer decoder. By leveraging\nthe geometric properties of object queries, this framework significantly\nenhances detection performance, especially in a multi-layer transformer decoder\nstructure. EVT achieves state-of-the-art performance on the nuScenes test set\nwith real-time inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal sensor fusion in bird's-eye-view (BEV) representation has become\nthe leading approach in 3D object detection. However, existing methods often\nrely on depth estimators or transformer encoders for view transformation,\nincurring substantial computational overhead. Furthermore, the lack of precise\ngeometric correspondence between 2D and 3D spaces leads to spatial and\nray-directional misalignments, restricting the effectiveness of BEV\nrepresentations. To address these challenges, we propose a novel 3D object\ndetector via efficient view transformation (EVT), which leverages a\nwell-structured BEV representation to enhance accuracy and efficiency. EVT\nfocuses on two main areas. First, it employs Adaptive Sampling and Adaptive\nProjection (ASAP), using LiDAR guidance to generate 3D sampling points and\nadaptive kernels. The generated points and kernels are then used to facilitate\nthe transformation of image features into BEV space and refine the BEV\nfeatures. Second, EVT includes an improved transformer-based detection\nframework, which contains a group-wise query initialization method and an\nenhanced query update framework. It is designed to effectively utilize the\nobtained multi-modal BEV features within the transformer decoder. By leveraging\nthe geometric properties of object queries, this framework significantly\nenhances detection performance, especially in a multi-layer transformer decoder\nstructure. EVT achieves state-of-the-art performance on the nuScenes test set\nwith real-time inference speed."
                },
                "authors": [
                    {
                        "name": "Yongjin Lee"
                    },
                    {
                        "name": "Hyeon-Mun Jeong"
                    },
                    {
                        "name": "Yurim Jeon"
                    },
                    {
                        "name": "Sanghyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Kim"
                },
                "author": "Sanghyun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10715v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10715v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06275v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06275v4",
                "updated": "2024-11-19T09:06:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    6,
                    33,
                    1,
                    324,
                    0
                ],
                "published": "2023-09-12T14:36:23Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    14,
                    36,
                    23,
                    1,
                    255,
                    0
                ],
                "title": "Re-Reading Improves Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-Reading Improves Reasoning in Large Language Models"
                },
                "summary": "To enhance the reasoning capabilities of off-the-shelf Large Language Models\n(LLMs), we introduce a simple, yet general and effective prompting method, Re2,\ni.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most\nthought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim\nto elicit the reasoning process in the output, Re2 shifts the focus to the\ninput by processing questions twice, thereby enhancing the understanding\nprocess. Consequently, Re2 demonstrates strong generality and compatibility\nwith most thought-eliciting prompting methods, including CoT. Crucially, Re2\nfacilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs\nbecause the first pass could provide global information for the second pass. We\nbegin with a preliminary empirical study as the foundation of Re2, illustrating\nits potential to enable \"bidirectional\" attention mechanisms. We then evaluate\nRe2 on extensive reasoning benchmarks across 14 datasets, spanning 112\nexperiments, to validate its effectiveness and generality. Our findings\nindicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2\nconsistently enhances the reasoning performance of LLMs through a simple\nre-reading strategy. Further analyses reveal Re2's adaptability, showing how it\ncan be effectively integrated with different LLMs, thought-eliciting prompting,\nand ensemble strategies. Our code is available at\n\\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the reasoning capabilities of off-the-shelf Large Language Models\n(LLMs), we introduce a simple, yet general and effective prompting method, Re2,\ni.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most\nthought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim\nto elicit the reasoning process in the output, Re2 shifts the focus to the\ninput by processing questions twice, thereby enhancing the understanding\nprocess. Consequently, Re2 demonstrates strong generality and compatibility\nwith most thought-eliciting prompting methods, including CoT. Crucially, Re2\nfacilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs\nbecause the first pass could provide global information for the second pass. We\nbegin with a preliminary empirical study as the foundation of Re2, illustrating\nits potential to enable \"bidirectional\" attention mechanisms. We then evaluate\nRe2 on extensive reasoning benchmarks across 14 datasets, spanning 112\nexperiments, to validate its effectiveness and generality. Our findings\nindicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2\nconsistently enhances the reasoning performance of LLMs through a simple\nre-reading strategy. Further analyses reveal Re2's adaptability, showing how it\ncan be effectively integrated with different LLMs, thought-eliciting prompting,\nand ensemble strategies. Our code is available at\n\\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}"
                },
                "authors": [
                    {
                        "name": "Xiaohan Xu"
                    },
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Hongbo Xu"
                    },
                    {
                        "name": "Guodong Long"
                    },
                    {
                        "name": "Jian-guang Lou"
                    },
                    {
                        "name": "Shuai Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Ma"
                },
                "author": "Shuai Ma",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.06275v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06275v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12343v1",
                "updated": "2024-11-19T08:51:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    51,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T08:51:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    51,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "The age of the Methuselah star in light of stellar evolution models with\n  tailored abundances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The age of the Methuselah star in light of stellar evolution models with\n  tailored abundances"
                },
                "summary": "Context. HD140283, or the Methuselah star, is a well-known reference object\nin stellar evolution. Its peculiar chemical composition, proximity and absence\nof reddening makes it an interesting case-study of Pop II stars. Thanks to\nrecent observational efforts, we now have precise interferometric and\nspectroscopic constraints, as well as revised astrometric parallaxes from the\nGaia mission. Aims. We aim at determining the age of HD140283 with these\nlastest constraints, as well as quantifying the impact of systematics from\nphysical inaccuracies in the stellar evolution models. Methods. Using recent\nspectroscopic abundances from the literature, including 3D non-LTE values for\nC, O, and Fe, we compute opacity tables specific to HD140283. We then use them\nin grids of stellar evolution models coupled to a Markov Chain Monte Carlo tool\nto determine the age of HD140283. Results. With our tailored models we find an\nage of 12.3Gy. Using a solar-scaled mixture instead results in an age value of\n14Gy, in tension with the age of the universe ($13.77\\pm0.06$Gy). We also find\nthat reducing the mixing length parameter from its solar calibrated value will\nlead to an even lower age, in agreement with other recent studies. However, we\nfind no direct evidence to favour a lower mixing length parameter value from\nour modelling. Conclusions. Taking into account the specific elemental\nabundances is crucial for the modelling of HD140283, as it leads to significant\ndifferences in the inferred age. However, this effect is degenerate with a\nlowering of the mixing length parameter. In this respect, asteroseismic\nconstraints might play a key role in accurately deriving the mass of HD140283,\ntherefore strongly constraining its age.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. HD140283, or the Methuselah star, is a well-known reference object\nin stellar evolution. Its peculiar chemical composition, proximity and absence\nof reddening makes it an interesting case-study of Pop II stars. Thanks to\nrecent observational efforts, we now have precise interferometric and\nspectroscopic constraints, as well as revised astrometric parallaxes from the\nGaia mission. Aims. We aim at determining the age of HD140283 with these\nlastest constraints, as well as quantifying the impact of systematics from\nphysical inaccuracies in the stellar evolution models. Methods. Using recent\nspectroscopic abundances from the literature, including 3D non-LTE values for\nC, O, and Fe, we compute opacity tables specific to HD140283. We then use them\nin grids of stellar evolution models coupled to a Markov Chain Monte Carlo tool\nto determine the age of HD140283. Results. With our tailored models we find an\nage of 12.3Gy. Using a solar-scaled mixture instead results in an age value of\n14Gy, in tension with the age of the universe ($13.77\\pm0.06$Gy). We also find\nthat reducing the mixing length parameter from its solar calibrated value will\nlead to an even lower age, in agreement with other recent studies. However, we\nfind no direct evidence to favour a lower mixing length parameter value from\nour modelling. Conclusions. Taking into account the specific elemental\nabundances is crucial for the modelling of HD140283, as it leads to significant\ndifferences in the inferred age. However, this effect is degenerate with a\nlowering of the mixing length parameter. In this respect, asteroseismic\nconstraints might play a key role in accurately deriving the mass of HD140283,\ntherefore strongly constraining its age."
                },
                "authors": [
                    {
                        "name": "C. Guillaume"
                    },
                    {
                        "name": "G. Buldgen"
                    },
                    {
                        "name": "A. M. Amarsi"
                    },
                    {
                        "name": "M. A. Dupret"
                    },
                    {
                        "name": "M. S. Lundkvist"
                    },
                    {
                        "name": "J. R. Larsen"
                    },
                    {
                        "name": "R. Scuflaire"
                    },
                    {
                        "name": "A. Noels"
                    }
                ],
                "author_detail": {
                    "name": "A. Noels"
                },
                "author": "A. Noels",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05085v2",
                "updated": "2024-11-19T08:46:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    46,
                    34,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-07T16:59:38Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    59,
                    38,
                    4,
                    159,
                    0
                ],
                "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs"
                },
                "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets that we release\nonline, and real-world use cases to demonstrate MRAG's effectiveness, showing\nimprovements of up to 20% in relevance over standard RAG baselines. MRAG can be\nseamlessly integrated with existing RAG frameworks and benchmarking tools like\nRAGAS as well as different classes of data stores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets that we release\nonline, and real-world use cases to demonstrate MRAG's effectiveness, showing\nimprovements of up to 20% in relevance over standard RAG baselines. MRAG can be\nseamlessly integrated with existing RAG frameworks and benchmarking tools like\nRAGAS as well as different classes of data stores."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Roman Niggli"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Lucas Weitzendorf"
                    },
                    {
                        "name": "Mingyuan Chi"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Joanna Gajda"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Micha Podstawski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01285v2",
                "updated": "2024-11-19T08:08:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    8,
                    38,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-02T07:14:26Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    7,
                    14,
                    26,
                    2,
                    276,
                    0
                ],
                "title": "Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration"
                },
                "summary": "The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral."
                },
                "authors": [
                    {
                        "name": "Kangxi Wu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to the EMNLP 2024 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17805v2",
                "updated": "2024-11-19T08:04:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    4,
                    25,
                    1,
                    324,
                    0
                ],
                "published": "2024-05-28T04:08:01Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    4,
                    8,
                    1,
                    1,
                    149,
                    0
                ],
                "title": "Strong-lensing cosmography using third-generation gravitational-wave\n  detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong-lensing cosmography using third-generation gravitational-wave\n  detectors"
                },
                "summary": "We present a detailed exposition of a statistical method for estimating\ncosmological parameters from the observation of a large number of strongly\nlensed binary-black-hole (BBH) mergers observable by next (third) generation\n(XG) gravitational-wave (GW) detectors. This method, first presented in Jana\n(2023 Phys. Rev. Lett. 130 261401), compares the observed number of strongly\nlensed GW events and their time delay distribution (between lensed images) with\nobserved events to infer cosmological parameters. We show that the precision of\nthe estimation of the cosmological parameters does not have a strong dependance\non the assumed BBH redshift distribution model. Using the large number of\nunlensed mergers, XG detectors are expected to measure the BBH redshift\ndistribution with sufficient precision for the cosmological inference. However,\na biased inference of the BBH redshift distribution will bias the estimation of\ncosmological parameters. An incorrect model for the distribution of lens\nproperties can also lead to a biased cosmological inference. However, Bayesian\nmodel selection can assist in selecting the right model from a set of available\nparametric models for the lens distribution. We also present a way to\nincorporate the effect of contamination in the data due to the limited\nefficiency of lensing identification methods, so that it will not bias the\ncosmological inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a detailed exposition of a statistical method for estimating\ncosmological parameters from the observation of a large number of strongly\nlensed binary-black-hole (BBH) mergers observable by next (third) generation\n(XG) gravitational-wave (GW) detectors. This method, first presented in Jana\n(2023 Phys. Rev. Lett. 130 261401), compares the observed number of strongly\nlensed GW events and their time delay distribution (between lensed images) with\nobserved events to infer cosmological parameters. We show that the precision of\nthe estimation of the cosmological parameters does not have a strong dependance\non the assumed BBH redshift distribution model. Using the large number of\nunlensed mergers, XG detectors are expected to measure the BBH redshift\ndistribution with sufficient precision for the cosmological inference. However,\na biased inference of the BBH redshift distribution will bias the estimation of\ncosmological parameters. An incorrect model for the distribution of lens\nproperties can also lead to a biased cosmological inference. However, Bayesian\nmodel selection can assist in selecting the right model from a set of available\nparametric models for the lens distribution. We also present a way to\nincorporate the effect of contamination in the data due to the limited\nefficiency of lensing identification methods, so that it will not bias the\ncosmological inference."
                },
                "authors": [
                    {
                        "name": "Souvik Jana"
                    },
                    {
                        "name": "Shasvath J Kapadia"
                    },
                    {
                        "name": "Tejaswi Venumadhav"
                    },
                    {
                        "name": "Surhud More"
                    },
                    {
                        "name": "Parameswaran Ajith"
                    }
                ],
                "author_detail": {
                    "name": "Parameswaran Ajith"
                },
                "author": "Parameswaran Ajith",
                "arxiv_doi": "10.1088/1361-6382/ad8d2e",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1361-6382/ad8d2e",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.17805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12313v1",
                "updated": "2024-11-19T08:01:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    1,
                    20,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T08:01:20Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    1,
                    20,
                    1,
                    324,
                    0
                ],
                "title": "C$^{2}$INet: Realizing Incremental Trajectory Prediction with\n  Prior-Aware Continual Causal Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^{2}$INet: Realizing Incremental Trajectory Prediction with\n  Prior-Aware Continual Causal Intervention"
                },
                "summary": "Trajectory prediction for multi-agents in complex scenarios is crucial for\napplications like autonomous driving. However, existing methods often overlook\nenvironmental biases, which leads to poor generalization. Additionally,\nhardware constraints limit the use of large-scale data across environments, and\ncontinual learning settings exacerbate the challenge of catastrophic\nforgetting. To address these issues, we propose the Continual Causal\nIntervention (C$^{2}$INet) method for generalizable multi-agent trajectory\nprediction within a continual learning framework. Using variational inference,\nwe align environment-related prior with posterior estimator of confounding\nfactors in the latent space, thereby intervening in causal correlations that\naffect trajectory representation. Furthermore, we store optimal variational\npriors across various scenarios using a memory queue, ensuring continuous\ndebiasing during incremental task training. The proposed C$^{2}$INet enhances\nadaptability to diverse tasks while preserving previous task information to\nprevent catastrophic forgetting. It also incorporates pruning strategies to\nmitigate overfitting. Comparative evaluations on three real and synthetic\ncomplex datasets against state-of-the-art methods demonstrate that our proposed\nmethod consistently achieves reliable prediction performance, effectively\nmitigating confounding factors unique to different scenarios. This highlights\nthe practical value of our method for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction for multi-agents in complex scenarios is crucial for\napplications like autonomous driving. However, existing methods often overlook\nenvironmental biases, which leads to poor generalization. Additionally,\nhardware constraints limit the use of large-scale data across environments, and\ncontinual learning settings exacerbate the challenge of catastrophic\nforgetting. To address these issues, we propose the Continual Causal\nIntervention (C$^{2}$INet) method for generalizable multi-agent trajectory\nprediction within a continual learning framework. Using variational inference,\nwe align environment-related prior with posterior estimator of confounding\nfactors in the latent space, thereby intervening in causal correlations that\naffect trajectory representation. Furthermore, we store optimal variational\npriors across various scenarios using a memory queue, ensuring continuous\ndebiasing during incremental task training. The proposed C$^{2}$INet enhances\nadaptability to diverse tasks while preserving previous task information to\nprevent catastrophic forgetting. It also incorporates pruning strategies to\nmitigate overfitting. Comparative evaluations on three real and synthetic\ncomplex datasets against state-of-the-art methods demonstrate that our proposed\nmethod consistently achieves reliable prediction performance, effectively\nmitigating confounding factors unique to different scenarios. This highlights\nthe practical value of our method for real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaohe Li"
                    },
                    {
                        "name": "Feilong Huang"
                    },
                    {
                        "name": "Zide Fan"
                    },
                    {
                        "name": "Fangli Mou"
                    },
                    {
                        "name": "Leilei Lin"
                    },
                    {
                        "name": "Yingyan Hou"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00476v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00476v3",
                "updated": "2024-11-19T07:53:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    53,
                    24,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-01T08:06:45Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    6,
                    45,
                    1,
                    275,
                    0
                ],
                "title": "Importance sampling-based gradient method for dimension reduction in\n  Poisson log-normal model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance sampling-based gradient method for dimension reduction in\n  Poisson log-normal model"
                },
                "summary": "High-dimensional count data poses significant challenges for statistical\nanalysis, necessitating effective methods that also preserve explainability. We\nfocus on a low rank constrained variant of the Poisson log-normal model, which\nrelates the observed data to a latent low-dimensional multivariate Gaussian\nvariable via a Poisson distribution. Variational inference methods have become\na golden standard solution to infer such a model. While computationally\nefficient, they usually lack theoretical statistical properties with respect to\nthe model. To address this issue we propose a projected stochastic gradient\nscheme that directly maximizes the log-likelihood. We prove the convergence of\nthe proposed method when using importance sampling for estimating the gradient.\nSpecifically, we obtain a rate of convergence of $O(T^{\\nicefrac{-1}{2}} +\nN^{-1})$ with $T$ the number of iterations and $N$ the number of Monte Carlo\ndraws. The latter follows from a novel descent lemma for non convex $L$-smooth\nobjective functions, and random biased gradient estimate. We also demonstrate\nnumerically the efficiency of our solution compared to its variational\ncompetitor. Our method not only scales with respect to the number of observed\nsamples but also provides access to the desirable properties of the maximum\nlikelihood estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional count data poses significant challenges for statistical\nanalysis, necessitating effective methods that also preserve explainability. We\nfocus on a low rank constrained variant of the Poisson log-normal model, which\nrelates the observed data to a latent low-dimensional multivariate Gaussian\nvariable via a Poisson distribution. Variational inference methods have become\na golden standard solution to infer such a model. While computationally\nefficient, they usually lack theoretical statistical properties with respect to\nthe model. To address this issue we propose a projected stochastic gradient\nscheme that directly maximizes the log-likelihood. We prove the convergence of\nthe proposed method when using importance sampling for estimating the gradient.\nSpecifically, we obtain a rate of convergence of $O(T^{\\nicefrac{-1}{2}} +\nN^{-1})$ with $T$ the number of iterations and $N$ the number of Monte Carlo\ndraws. The latter follows from a novel descent lemma for non convex $L$-smooth\nobjective functions, and random biased gradient estimate. We also demonstrate\nnumerically the efficiency of our solution compared to its variational\ncompetitor. Our method not only scales with respect to the number of observed\nsamples but also provides access to the desirable properties of the maximum\nlikelihood estimator."
                },
                "authors": [
                    {
                        "name": "Bastien Batardire"
                    },
                    {
                        "name": "Julien Chiquet"
                    },
                    {
                        "name": "Joon Kwon"
                    },
                    {
                        "name": "Julien Stoehr"
                    }
                ],
                "author_detail": {
                    "name": "Julien Stoehr"
                },
                "arxiv_affiliation": "CEREMADE",
                "author": "Julien Stoehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00476v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00476v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12307v1",
                "updated": "2024-11-19T07:48:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    48,
                    35,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:48:35Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    48,
                    35,
                    1,
                    324,
                    0
                ],
                "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification\n  for LLM-Powered Dialog Systems in Production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification\n  for LLM-Powered Dialog Systems in Production"
                },
                "summary": "Accurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity of\ncomprehensive datasets and the complexity of contextual dependencies across\ndialogue turns hinder progress. This paper presents two novel approaches\nleveraging Large Language Models (LLMs) to enhance scalability and reduce\nlatency in production dialogue systems. First, we introduce Symbol Tuning,\nwhich simplifies intent labels to reduce task complexity and improve\nperformance in multi-turn dialogues. Second, we propose C-LARA\n(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework\nthat employs LLMs for data augmentation and pseudo-labeling to generate\nsynthetic multi-turn dialogues. These enriched datasets are used to fine-tune a\nsmall, efficient model suitable for deployment. Experiments conducted on\nmultilingual dialogue datasets demonstrate significant improvements in\nclassification accuracy and resource efficiency. Our methods enhance multi-turn\nintent classification accuracy by 5.09%, reduce annotation costs by 40%, and\nenable scalable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity of\ncomprehensive datasets and the complexity of contextual dependencies across\ndialogue turns hinder progress. This paper presents two novel approaches\nleveraging Large Language Models (LLMs) to enhance scalability and reduce\nlatency in production dialogue systems. First, we introduce Symbol Tuning,\nwhich simplifies intent labels to reduce task complexity and improve\nperformance in multi-turn dialogues. Second, we propose C-LARA\n(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework\nthat employs LLMs for data augmentation and pseudo-labeling to generate\nsynthetic multi-turn dialogues. These enriched datasets are used to fine-tune a\nsmall, efficient model suitable for deployment. Experiments conducted on\nmultilingual dialogue datasets demonstrate significant improvements in\nclassification accuracy and resource efficiency. Our methods enhance multi-turn\nintent classification accuracy by 5.09%, reduce annotation costs by 40%, and\nenable scalable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15000v3",
                "updated": "2024-11-19T07:46:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    46,
                    16,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-22T22:28:46Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    22,
                    28,
                    46,
                    3,
                    53,
                    0
                ],
                "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide-or-Conquer? Which Part Should You Distill Your LLM?"
                },
                "summary": "Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation."
                },
                "authors": [
                    {
                        "name": "Zhuofeng Wu"
                    },
                    {
                        "name": "He Bai"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "VG Vinod Vydiswaran"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
                "arxiv_journal_ref": "2024.findings-emnlp.145",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02136v2",
                "updated": "2024-11-19T07:42:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    42,
                    15,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-03T12:53:17Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    12,
                    53,
                    17,
                    5,
                    34,
                    0
                ],
                "title": "User Intent Recognition and Satisfaction with Large Language Models: A\n  User Study with ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Intent Recognition and Satisfaction with Large Language Models: A\n  User Study with ChatGPT"
                },
                "summary": "The rapid evolution of LLMs represents an impactful paradigm shift in digital\ninteraction and content engagement. While they encode vast amounts of\nhuman-generated knowledge and excel in processing diverse data types, they\noften face the challenge of accurately responding to specific user intents,\nleading to user dissatisfaction. Based on a fine-grained intent taxonomy and\nintent-based prompt reformulations, we analyze the quality of intent\nrecognition and user satisfaction with answers from intent-based prompt\nreformulations of GPT-3.5 Turbo and GPT-4 Turbo models. Our study highlights\nthe importance of human-AI interaction and underscores the need for\ninterdisciplinary approaches to improve conversational AI systems. We show that\nGPT-4 outperforms GPT-3.5 in recognizing common intents but is often\noutperformed by GPT-3.5 in recognizing less frequent intents. Moreover,\nwhenever the user intent is correctly recognized, while users are more\nsatisfied with the intent-based reformulations of GPT-4 compared to GPT-3.5,\nthey tend to be more satisfied with the models' answers to their original\nprompts compared to the reformulated ones. The collected data from our study\nhas been made publicly available on GitHub\n(https://github.com/ConcealedIDentity/UserIntentStudy) for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of LLMs represents an impactful paradigm shift in digital\ninteraction and content engagement. While they encode vast amounts of\nhuman-generated knowledge and excel in processing diverse data types, they\noften face the challenge of accurately responding to specific user intents,\nleading to user dissatisfaction. Based on a fine-grained intent taxonomy and\nintent-based prompt reformulations, we analyze the quality of intent\nrecognition and user satisfaction with answers from intent-based prompt\nreformulations of GPT-3.5 Turbo and GPT-4 Turbo models. Our study highlights\nthe importance of human-AI interaction and underscores the need for\ninterdisciplinary approaches to improve conversational AI systems. We show that\nGPT-4 outperforms GPT-3.5 in recognizing common intents but is often\noutperformed by GPT-3.5 in recognizing less frequent intents. Moreover,\nwhenever the user intent is correctly recognized, while users are more\nsatisfied with the intent-based reformulations of GPT-4 compared to GPT-3.5,\nthey tend to be more satisfied with the models' answers to their original\nprompts compared to the reformulated ones. The collected data from our study\nhas been made publicly available on GitHub\n(https://github.com/ConcealedIDentity/UserIntentStudy) for further research."
                },
                "authors": [
                    {
                        "name": "Anna Bodonhelyi"
                    },
                    {
                        "name": "Efe Bozkir"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12299v1",
                "updated": "2024-11-19T07:40:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    40,
                    7,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:40:07Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    40,
                    7,
                    1,
                    324,
                    0
                ],
                "title": "Could Humans Outshine AI in Visual Data Analysis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Could Humans Outshine AI in Visual Data Analysis?"
                },
                "summary": "People often use visualizations not only to explore a dataset but also to\ndraw generalizable conclusions about underlying models or phenomena. While\nprevious research has viewed deviations from rational analysis as problematic,\nwe hypothesize that human reliance on non-normative heuristics may be\nadvantageous in certain situations. In this study, we investigate scenarios\nwhere human intuition might outperform idealized statistical rationality. Our\nexperiment assesses participants' accuracy in characterizing the parameters of\nknown data-generating models from bivariate visualizations. Our findings show\nthat, while participants generally demonstrated lower accuracy than statistical\nmodels, they often outperformed Bayesian agents, particularly when dealing with\nextreme samples. These results suggest that, even when deviating from\nrationality, human gut reactions to visualizations can provide an advantage.\nOur findings offer insights into how analyst intuition and statistical models\ncan be integrated to improve inference and decision-making, with important\nimplications for the design of visual analytics tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People often use visualizations not only to explore a dataset but also to\ndraw generalizable conclusions about underlying models or phenomena. While\nprevious research has viewed deviations from rational analysis as problematic,\nwe hypothesize that human reliance on non-normative heuristics may be\nadvantageous in certain situations. In this study, we investigate scenarios\nwhere human intuition might outperform idealized statistical rationality. Our\nexperiment assesses participants' accuracy in characterizing the parameters of\nknown data-generating models from bivariate visualizations. Our findings show\nthat, while participants generally demonstrated lower accuracy than statistical\nmodels, they often outperformed Bayesian agents, particularly when dealing with\nextreme samples. These results suggest that, even when deviating from\nrationality, human gut reactions to visualizations can provide an advantage.\nOur findings offer insights into how analyst intuition and statistical models\ncan be integrated to improve inference and decision-making, with important\nimplications for the design of visual analytics tools."
                },
                "authors": [
                    {
                        "name": "Ratanond Koonchanok"
                    },
                    {
                        "name": "Khairi Reda"
                    }
                ],
                "author_detail": {
                    "name": "Khairi Reda"
                },
                "author": "Khairi Reda",
                "arxiv_comment": "Accepted to the Workshop on Trust and Reliance in Evolving Human-AI\n  Workflows (TREW) at CHI 2024. arXiv admin note: text overlap with\n  arXiv:2407.16871",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06145v2",
                "updated": "2024-11-19T07:19:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    19,
                    27,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-09T11:13:14Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    11,
                    13,
                    14,
                    5,
                    314,
                    0
                ],
                "title": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era"
                },
                "summary": "In recent years, Large Language Models (LLMs) have significantly improved\nautomated code translation, often achieving over 80% accuracy on existing\nbenchmarks. However, most of these benchmarks consist of short, standalone,\nalgorithmic samples that do not reflect practical coding tasks. To address this\ngap, we introduce ClassEval-T, a class-level code translation benchmark\ndesigned to assess LLM performance on real-world coding scenarios. Built upon\nClassEval, a class-level Python code generation benchmark covering topics such\nas database operations and game design, ClassEval-T extends into Java and C++\nwith complete code samples and test suites, requiring 360 person-hours for\nmanual migration. We propose three translation strategies (holistic,\nmin-dependency, and standalone) and evaluate six recent LLMs across various\nfamilies and sizes on ClassEval-T. Results reveal a significant performance\ndrop compared to method-level benchmarks, highlighting discrepancies among LLMs\nand demonstrating ClassEval-T's effectiveness. We further analyze LLMs'\ndependency awareness in translating class samples and categorize 1,397 failure\ncases by the best-performing LLM for practical insights and future improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have significantly improved\nautomated code translation, often achieving over 80% accuracy on existing\nbenchmarks. However, most of these benchmarks consist of short, standalone,\nalgorithmic samples that do not reflect practical coding tasks. To address this\ngap, we introduce ClassEval-T, a class-level code translation benchmark\ndesigned to assess LLM performance on real-world coding scenarios. Built upon\nClassEval, a class-level Python code generation benchmark covering topics such\nas database operations and game design, ClassEval-T extends into Java and C++\nwith complete code samples and test suites, requiring 360 person-hours for\nmanual migration. We propose three translation strategies (holistic,\nmin-dependency, and standalone) and evaluate six recent LLMs across various\nfamilies and sizes on ClassEval-T. Results reveal a significant performance\ndrop compared to method-level benchmarks, highlighting discrepancies among LLMs\nand demonstrating ClassEval-T's effectiveness. We further analyze LLMs'\ndependency awareness in translating class samples and categorize 1,397 failure\ncases by the best-performing LLM for practical insights and future improvement."
                },
                "authors": [
                    {
                        "name": "Pengyu Xue"
                    },
                    {
                        "name": "Linhao Wu"
                    },
                    {
                        "name": "Chengyi Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Ruikai Jin"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Yifei Pei"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Xiran Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Xiran Lyu"
                },
                "author": "Xiran Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12289v1",
                "updated": "2024-11-19T07:18:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    18,
                    51,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:18:51Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    18,
                    51,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing UX Research Activities Using GenAI -- Potential Applications\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing UX Research Activities Using GenAI -- Potential Applications\n  and Challenges"
                },
                "summary": "User Experience (UX) Research covers various methods for gathering the users'\nsubjective impressions of a product. For this, practitioners face different\nactivities and tasks related to the research process. This includes processing\na large amount of data based on qualitative and quantitative data. However,\nthis can be very laborious in practice. Thus, the application of GenAI can\nsupport UX research activities. This paper provides a practical perspective on\nthis topic. Based on previous studies, we present different use cases\nindicating the potential of GenAI in UX research. Moreover, we provide insights\ninto an exploratory study using GenAI along an entire UX research process.\nResults show that Large Language Models (LLMs) are useful for various tasks.\nThus, the research activities can be carried out more efficiently. However, the\nresearcher should always review results to ensure quality. In summary, we want\nto express the potential of GenAI enhancing UX research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Experience (UX) Research covers various methods for gathering the users'\nsubjective impressions of a product. For this, practitioners face different\nactivities and tasks related to the research process. This includes processing\na large amount of data based on qualitative and quantitative data. However,\nthis can be very laborious in practice. Thus, the application of GenAI can\nsupport UX research activities. This paper provides a practical perspective on\nthis topic. Based on previous studies, we present different use cases\nindicating the potential of GenAI in UX research. Moreover, we provide insights\ninto an exploratory study using GenAI along an entire UX research process.\nResults show that Large Language Models (LLMs) are useful for various tasks.\nThus, the research activities can be carried out more efficiently. However, the\nresearcher should always review results to ensure quality. In summary, we want\nto express the potential of GenAI enhancing UX research"
                },
                "authors": [
                    {
                        "name": "Stefan Graser"
                    },
                    {
                        "name": "Anastasia Snimshchikova"
                    },
                    {
                        "name": "Martin Schrepp"
                    },
                    {
                        "name": "Stephan Bhm"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Bhm"
                },
                "author": "Stephan Bhm",
                "arxiv_comment": "13 pages, 4 figures, CENTRIC 2024 : The Seventeenth International\n  Conference on Advances in Human-oriented and Personalized Mechanisms,\n  Technologies, and Services",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12286v1",
                "updated": "2024-11-19T07:12:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    12,
                    48,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:12:48Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    12,
                    48,
                    1,
                    324,
                    0
                ],
                "title": "GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for\n  Task-Oriented Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for\n  Task-Oriented Grasping"
                },
                "summary": "Inferring affordable (i.e., graspable) parts of arbitrary objects based on\nhuman specifications is essential for robots advancing toward open-vocabulary\nmanipulation. Current grasp planners, however, are hindered by limited\nvision-language comprehension and time-consuming 3D radiance modeling,\nrestricting real-time, open-vocabulary interactions with objects. To address\nthese limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary\nAffordance Reasoning framework, which fine-tunes the Large Language Models\n(LLMs) to predict visual affordance of graspable object parts within RGB\nfeature space. We compile a dataset of over 10,000 images from human-object\ninteractions, annotated with unified visual and linguistic affordance labels,\nto enable multi-modal fine-tuning. GLOVER inherits world knowledge and\ncommon-sense reasoning from LLMs, facilitating more fine-grained object\nunderstanding and sophisticated tool-use reasoning. To enable effective\nreal-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a\nnon-parametric grasp planner that aligns the gripper pose with a superquadric\nsurface derived from affordance data. In evaluations across 30 real-world\nscenes, GLOVER achieves success rates of 86.0% in part identification and 76.3%\nin grasping, with speeds approximately 330 times faster in affordance reasoning\nand 40 times faster in grasping pose estimation than the previous\nstate-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring affordable (i.e., graspable) parts of arbitrary objects based on\nhuman specifications is essential for robots advancing toward open-vocabulary\nmanipulation. Current grasp planners, however, are hindered by limited\nvision-language comprehension and time-consuming 3D radiance modeling,\nrestricting real-time, open-vocabulary interactions with objects. To address\nthese limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary\nAffordance Reasoning framework, which fine-tunes the Large Language Models\n(LLMs) to predict visual affordance of graspable object parts within RGB\nfeature space. We compile a dataset of over 10,000 images from human-object\ninteractions, annotated with unified visual and linguistic affordance labels,\nto enable multi-modal fine-tuning. GLOVER inherits world knowledge and\ncommon-sense reasoning from LLMs, facilitating more fine-grained object\nunderstanding and sophisticated tool-use reasoning. To enable effective\nreal-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a\nnon-parametric grasp planner that aligns the gripper pose with a superquadric\nsurface derived from affordance data. In evaluations across 30 real-world\nscenes, GLOVER achieves success rates of 86.0% in part identification and 76.3%\nin grasping, with speeds approximately 330 times faster in affordance reasoning\nand 40 times faster in grasping pose estimation than the previous\nstate-of-the-art."
                },
                "authors": [
                    {
                        "name": "Teli Ma"
                    },
                    {
                        "name": "Zifan Wang"
                    },
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Junwei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liang"
                },
                "author": "Junwei Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08901v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08901v3",
                "updated": "2024-11-19T07:04:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    4,
                    6,
                    1,
                    324,
                    0
                ],
                "published": "2024-04-13T05:01:54Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    5,
                    1,
                    54,
                    5,
                    104,
                    0
                ],
                "title": "Bullion: A Column Store for Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bullion: A Column Store for Machine Learning"
                },
                "summary": "The past two decades have witnessed significant success in applying columnar\nstorage to data warehousing and analytics. However, the rapid growth of machine\nlearning poses new challenges. This paper presents Bullion, a columnar storage\nsystem tailored for machine learning workloads. Bullion addresses the\ncomplexities of data compliance, optimizes the encoding of long sequence sparse\nfeatures, efficiently manages wide-table projections, introduces feature\nquantization in storage, enables quality-aware sequential reads for multimodal\ntraining data, and provides a comprehensive cascading encoding framework that\nunifies diverse encoding schemes through modular, composable interfaces. By\naligning with the evolving requirements of ML applications, Bullion facilitates\nthe application of columnar storage and processing to modern application\nscenarios such as those within advertising, recommendation systems, and\nGenerative AI.\n  Preliminary experimental results and theoretical analysis demonstrate\nBullion's improved ability to deliver strong performance in the face of the\nunique demands of machine learning workloads compared to existing columnar\nstorage solutions. Bullion significantly reduces I/O costs for deletion\ncompliance, achieves substantial storage savings with its optimized encoding\nscheme for sparse features, and improves metadata parsing speed for wide-table\nprojections. These advancements enable Bullion to become an important component\nin the future of machine learning infrastructure, enabling organizations to\nefficiently manage and process the massive volumes of data required for\ntraining and inference in modern AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past two decades have witnessed significant success in applying columnar\nstorage to data warehousing and analytics. However, the rapid growth of machine\nlearning poses new challenges. This paper presents Bullion, a columnar storage\nsystem tailored for machine learning workloads. Bullion addresses the\ncomplexities of data compliance, optimizes the encoding of long sequence sparse\nfeatures, efficiently manages wide-table projections, introduces feature\nquantization in storage, enables quality-aware sequential reads for multimodal\ntraining data, and provides a comprehensive cascading encoding framework that\nunifies diverse encoding schemes through modular, composable interfaces. By\naligning with the evolving requirements of ML applications, Bullion facilitates\nthe application of columnar storage and processing to modern application\nscenarios such as those within advertising, recommendation systems, and\nGenerative AI.\n  Preliminary experimental results and theoretical analysis demonstrate\nBullion's improved ability to deliver strong performance in the face of the\nunique demands of machine learning workloads compared to existing columnar\nstorage solutions. Bullion significantly reduces I/O costs for deletion\ncompliance, achieves substantial storage savings with its optimized encoding\nscheme for sparse features, and improves metadata parsing speed for wide-table\nprojections. These advancements enable Bullion to become an important component\nin the future of machine learning infrastructure, enabling organizations to\nefficiently manage and process the massive volumes of data required for\ntraining and inference in modern AI applications."
                },
                "authors": [
                    {
                        "name": "Gang Liao"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Daniel J. Abadi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Abadi"
                },
                "author": "Daniel J. Abadi",
                "arxiv_journal_ref": "The Conference on Innovative Data Systems Research (CIDR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08901v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08901v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12280v1",
                "updated": "2024-11-19T07:03:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    3,
                    19,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:03:19Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    3,
                    19,
                    1,
                    324,
                    0
                ],
                "title": "Large Language Models for Material Property Predictions: elastic\n  constant tensor prediction and materials design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Material Property Predictions: elastic\n  constant tensor prediction and materials design"
                },
                "summary": "Efficient and accurate prediction of material properties is critical for\nadvancing materials design and applications. The rapid-evolution of large\nlanguage models (LLMs) presents a new opportunity for material property\npredictions, complementing experimental measurements and multi-scale\ncomputational methods. We focus on predicting the elastic constant tensor, as a\ncase study, and develop domain-specific LLMs for predicting elastic constants\nand for materials discovery. The proposed ElaTBot LLM enables simultaneous\nprediction of elastic constant tensors, bulk modulus at finite temperatures,\nand the generation of new materials with targeted properties. Moreover, the\ncapabilities of ElaTBot are further enhanced by integrating with general LLMs\n(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized\nvariant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,\nreduces the prediction errors by 33.1% compared with domain-specific, material\nscience LLMs (Darwin) trained on the same dataset. This natural language-based\napproach lowers the barriers to computational materials science and highlights\nthe broader potential of LLMs for material property predictions and inverse\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and accurate prediction of material properties is critical for\nadvancing materials design and applications. The rapid-evolution of large\nlanguage models (LLMs) presents a new opportunity for material property\npredictions, complementing experimental measurements and multi-scale\ncomputational methods. We focus on predicting the elastic constant tensor, as a\ncase study, and develop domain-specific LLMs for predicting elastic constants\nand for materials discovery. The proposed ElaTBot LLM enables simultaneous\nprediction of elastic constant tensors, bulk modulus at finite temperatures,\nand the generation of new materials with targeted properties. Moreover, the\ncapabilities of ElaTBot are further enhanced by integrating with general LLMs\n(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized\nvariant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,\nreduces the prediction errors by 33.1% compared with domain-specific, material\nscience LLMs (Darwin) trained on the same dataset. This natural language-based\napproach lowers the barriers to computational materials science and highlights\nthe broader potential of LLMs for material property predictions and inverse\ndesign."
                },
                "authors": [
                    {
                        "name": "Siyu Liu"
                    },
                    {
                        "name": "Tongqi Wen"
                    },
                    {
                        "name": "Beilin Ye"
                    },
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "David J. Srolovitz"
                    }
                ],
                "author_detail": {
                    "name": "David J. Srolovitz"
                },
                "author": "David J. Srolovitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v1",
                "updated": "2024-11-19T06:57:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation"
                },
                "summary": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12277v1",
                "updated": "2024-11-19T06:56:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    56,
                    40,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T06:56:40Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    56,
                    40,
                    1,
                    324,
                    0
                ],
                "title": "O-MAGIC: Online Change-Point Detection for Dynamic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O-MAGIC: Online Change-Point Detection for Dynamic Systems"
                },
                "summary": "The capture of changes in dynamic systems, especially ordinary differential\nequations (ODEs), is an important and challenging task, with multiple\napplications in biomedical research and other scientific areas. This article\nproposes a fast and mathematically rigorous online method, called ODE-informed\nMAnifold-constrained Gaussian process Inference for Change point\ndetection(O-MAGIC), to detect changes of parameters in the ODE system using\nnoisy and sparse observation data. O-MAGIC imposes a Gaussian process prior to\nthe time series of system components with a latent manifold constraint, induced\nby restricting the derivative process to satisfy ODE conditions. To detect the\nparameter changes from the observation, we propose a procedure based on a\ntwo-sample generalized likelihood ratio (GLR) test that can detect multiple\nchange points in the dynamic system automatically. O-MAGIC bypasses\nconventional numerical integration and achieves substantial savings in\ncomputation time. By incorporating the ODE structures through manifold\nconstraints, O-MAGIC enjoys a significant advantage in detection delay, while\nfollowing principled statistical construction under the Bayesian paradigm,\nwhich further enables it to handle systems with missing data or unobserved\ncomponents. O-MAGIC can also be applied to general nonlinear systems.\nSimulation studies on three challenging examples: SEIRD model, Lotka-Volterra\nmodel and Lorenz model are provided to illustrate the robustness and efficiency\nof O-MAGIC, compared with numerical integration and other popular\ntime-series-based change point detection benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capture of changes in dynamic systems, especially ordinary differential\nequations (ODEs), is an important and challenging task, with multiple\napplications in biomedical research and other scientific areas. This article\nproposes a fast and mathematically rigorous online method, called ODE-informed\nMAnifold-constrained Gaussian process Inference for Change point\ndetection(O-MAGIC), to detect changes of parameters in the ODE system using\nnoisy and sparse observation data. O-MAGIC imposes a Gaussian process prior to\nthe time series of system components with a latent manifold constraint, induced\nby restricting the derivative process to satisfy ODE conditions. To detect the\nparameter changes from the observation, we propose a procedure based on a\ntwo-sample generalized likelihood ratio (GLR) test that can detect multiple\nchange points in the dynamic system automatically. O-MAGIC bypasses\nconventional numerical integration and achieves substantial savings in\ncomputation time. By incorporating the ODE structures through manifold\nconstraints, O-MAGIC enjoys a significant advantage in detection delay, while\nfollowing principled statistical construction under the Bayesian paradigm,\nwhich further enables it to handle systems with missing data or unobserved\ncomponents. O-MAGIC can also be applied to general nonlinear systems.\nSimulation studies on three challenging examples: SEIRD model, Lotka-Volterra\nmodel and Lorenz model are provided to illustrate the robustness and efficiency\nof O-MAGIC, compared with numerical integration and other popular\ntime-series-based change point detection benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Yeping Wang"
                    },
                    {
                        "name": "Zhaohui Li"
                    },
                    {
                        "name": "Shihao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shihao Yang"
                },
                "author": "Shihao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11072v2",
                "updated": "2024-11-19T06:45:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    45,
                    13,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-17T13:21:26Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    13,
                    21,
                    26,
                    6,
                    322,
                    0
                ],
                "title": "Multilingual Large Language Models: A Systematic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models: A Systematic Survey"
                },
                "summary": "This paper provides a comprehensive survey of the latest research on\nmultilingual large language models (MLLMs). MLLMs not only are able to\nunderstand and generate language across linguistic boundaries, but also\nrepresent an important advancement in artificial intelligence. We first discuss\nthe architecture and pre-training objectives of MLLMs, highlighting the key\ncomponents and methodologies that contribute to their multilingual\ncapabilities. We then discuss the construction of multilingual pre-training and\nalignment datasets, underscoring the importance of data quality and diversity\nin enhancing MLLM performance. An important focus of this survey is on the\nevaluation of MLLMs. We present a detailed taxonomy and roadmap covering the\nassessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human\nvalues, safety, interpretability and specialized applications. Specifically, we\nextensively discuss multilingual evaluation benchmarks and datasets, and\nexplore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs\nfrom black to white boxes, we also address the interpretability of multilingual\ncapabilities, cross-lingual transfer and language bias within these models.\nFinally, we provide a comprehensive review of real-world applications of MLLMs\nacross diverse domains, including biology, medicine, computer science,\nmathematics and law. We showcase how these models have driven innovation and\nimprovements in these specialized fields while also highlighting the challenges\nand opportunities in deploying MLLMs within diverse language communities and\napplication scenarios. We listed the paper related in this survey and publicly\navailable at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive survey of the latest research on\nmultilingual large language models (MLLMs). MLLMs not only are able to\nunderstand and generate language across linguistic boundaries, but also\nrepresent an important advancement in artificial intelligence. We first discuss\nthe architecture and pre-training objectives of MLLMs, highlighting the key\ncomponents and methodologies that contribute to their multilingual\ncapabilities. We then discuss the construction of multilingual pre-training and\nalignment datasets, underscoring the importance of data quality and diversity\nin enhancing MLLM performance. An important focus of this survey is on the\nevaluation of MLLMs. We present a detailed taxonomy and roadmap covering the\nassessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human\nvalues, safety, interpretability and specialized applications. Specifically, we\nextensively discuss multilingual evaluation benchmarks and datasets, and\nexplore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs\nfrom black to white boxes, we also address the interpretability of multilingual\ncapabilities, cross-lingual transfer and language bias within these models.\nFinally, we provide a comprehensive review of real-world applications of MLLMs\nacross diverse domains, including biology, medicine, computer science,\nmathematics and law. We showcase how these models have driven innovation and\nimprovements in these specialized fields while also highlighting the challenges\nand opportunities in deploying MLLMs within diverse language communities and\napplication scenarios. We listed the paper related in this survey and publicly\navailable at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers."
                },
                "authors": [
                    {
                        "name": "Shaolin Zhu"
                    },
                    {
                        "name": "Supryadi"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Jiangcun Du"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Antnio Branco"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01812v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01812v4",
                "updated": "2024-11-19T06:14:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    14,
                    31,
                    1,
                    324,
                    0
                ],
                "published": "2024-09-14T02:35:29Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    35,
                    29,
                    5,
                    258,
                    0
                ],
                "title": "From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice"
                },
                "summary": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Lawrence KQ Yan"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yunze Wang"
                    },
                    {
                        "name": "Silin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Silin Chen"
                },
                "author": "Silin Chen",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01812v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01812v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12256v1",
                "updated": "2024-11-19T06:10:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    10,
                    22,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T06:10:22Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    10,
                    22,
                    1,
                    324,
                    0
                ],
                "title": "Restructuring Tractable Probabilistic Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Restructuring Tractable Probabilistic Circuits"
                },
                "summary": "Probabilistic circuits (PCs) is a unifying representation for probabilistic\nmodels that support tractable inference. Numerous applications of PCs like\ncontrollable text generation depend on the ability to efficiently multiply two\ncircuits. Existing multiplication algorithms require that the circuits respect\nthe same structure, i.e. variable scopes decomposes according to the same\nvtree. In this work, we propose and study the task of restructuring\nstructured(-decomposable) PCs, that is, transforming a structured PC such that\nit conforms to a target vtree. We propose a generic approach for this problem\nand show that it leads to novel polynomial-time algorithms for multiplying\ncircuits respecting different vtrees, as well as a practical depth-reduction\nalgorithm that preserves structured decomposibility. Our work opens up new\navenues for tractable PC inference, suggesting the possibility of training with\nless restrictive PC structures while enabling efficient inference by changing\ntheir structures at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic circuits (PCs) is a unifying representation for probabilistic\nmodels that support tractable inference. Numerous applications of PCs like\ncontrollable text generation depend on the ability to efficiently multiply two\ncircuits. Existing multiplication algorithms require that the circuits respect\nthe same structure, i.e. variable scopes decomposes according to the same\nvtree. In this work, we propose and study the task of restructuring\nstructured(-decomposable) PCs, that is, transforming a structured PC such that\nit conforms to a target vtree. We propose a generic approach for this problem\nand show that it leads to novel polynomial-time algorithms for multiplying\ncircuits respecting different vtrees, as well as a practical depth-reduction\nalgorithm that preserves structured decomposibility. Our work opens up new\navenues for tractable PC inference, suggesting the possibility of training with\nless restrictive PC structures while enabling efficient inference by changing\ntheir structures at inference time."
                },
                "authors": [
                    {
                        "name": "Honghua Zhang"
                    },
                    {
                        "name": "Benjie Wang"
                    },
                    {
                        "name": "Marcelo Arenas"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03182v2",
                "updated": "2024-11-19T05:57:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    57,
                    28,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-04T06:45:48Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    45,
                    48,
                    4,
                    278,
                    0
                ],
                "title": "Generating bilingual example sentences with large language models as\n  lexicography assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating bilingual example sentences with large language models as\n  lexicography assistants"
                },
                "summary": "We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Raphael Merx"
                    },
                    {
                        "name": "Ekaterina Vylomova"
                    },
                    {
                        "name": "Kemal Kurniawan"
                    }
                ],
                "author_detail": {
                    "name": "Kemal Kurniawan"
                },
                "author": "Kemal Kurniawan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10494v2",
                "updated": "2024-11-19T05:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    43,
                    28,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-15T02:30:13Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    30,
                    13,
                    4,
                    320,
                    0
                ],
                "title": "Efficient inference for differential equation models without numerical\n  solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference for differential equation models without numerical\n  solvers"
                },
                "summary": "Parameter inference is essential when interpreting observational data using\nmathematical models. Standard inference methods for differential equation\nmodels typically rely on obtaining repeated numerical solutions of the\ndifferential equation(s). Recent results have explored how numerical truncation\nerror can have major, detrimental, and sometimes hidden impacts on\nlikelihood-based inference by introducing false local maxima into the\nlog-likelihood function. We present a straightforward approach for inference\nthat eliminates the need for solving the underlying differential equations,\nthereby completely avoiding the impact of truncation error. Open-access Jupyter\nnotebooks, available on GitHub, allow others to implement this method for a\nbroad class of widely-used models to interpret biological data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter inference is essential when interpreting observational data using\nmathematical models. Standard inference methods for differential equation\nmodels typically rely on obtaining repeated numerical solutions of the\ndifferential equation(s). Recent results have explored how numerical truncation\nerror can have major, detrimental, and sometimes hidden impacts on\nlikelihood-based inference by introducing false local maxima into the\nlog-likelihood function. We present a straightforward approach for inference\nthat eliminates the need for solving the underlying differential equations,\nthereby completely avoiding the impact of truncation error. Open-access Jupyter\nnotebooks, available on GitHub, allow others to implement this method for a\nbroad class of widely-used models to interpret biological data."
                },
                "authors": [
                    {
                        "name": "Alexander Johnston"
                    },
                    {
                        "name": "Oliver J. Maclaren"
                    },
                    {
                        "name": "Ruth E. Baker"
                    },
                    {
                        "name": "Matthew J. Simpson"
                    }
                ],
                "author_detail": {
                    "name": "Matthew J. Simpson"
                },
                "author": "Matthew J. Simpson",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A71",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12240v1",
                "updated": "2024-11-19T05:37:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    37,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T05:37:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    37,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages"
                },
                "summary": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency."
                },
                "authors": [
                    {
                        "name": "S. Tamang"
                    },
                    {
                        "name": "D. J. Bora"
                    }
                ],
                "author_detail": {
                    "name": "D. J. Bora"
                },
                "author": "D. J. Bora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.11019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.11019v3",
                "updated": "2024-11-19T05:35:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    35,
                    2,
                    1,
                    324,
                    0
                ],
                "published": "2023-07-20T16:46:10Z",
                "published_parsed": [
                    2023,
                    7,
                    20,
                    16,
                    46,
                    10,
                    3,
                    201,
                    0
                ],
                "title": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation"
                },
                "summary": "Large language models (LLMs) have shown impressive prowess in solving a wide\nrange of tasks with world knowledge. However, it remains unclear how well LLMs\nare able to perceive their factual knowledge boundaries, particularly under\nretrieval augmentation settings. In this study, we present the first analysis\non the factual knowledge boundaries of LLMs and how retrieval augmentation\naffects LLMs on open-domain question answering (QA), with a bunch of important\nfindings. Specifically, we focus on three research questions and analyze them\nby examining QA, priori judgement and posteriori judgement capabilities of\nLLMs. We show evidence that LLMs possess unwavering confidence in their\nknowledge and cannot handle the conflict between internal and external\nknowledge well. Furthermore, retrieval augmentation proves to be an effective\napproach in enhancing LLMs' awareness of knowledge boundaries. We further\nconduct thorough experiments to examine how different factors affect LLMs and\npropose a simple method to dynamically utilize supporting documents with our\njudgement strategy. Additionally, we find that the relevance between the\nsupporting documents and the questions significantly impacts LLMs' QA and\njudgemental capabilities. The code to reproduce this work is available at\nhttps://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive prowess in solving a wide\nrange of tasks with world knowledge. However, it remains unclear how well LLMs\nare able to perceive their factual knowledge boundaries, particularly under\nretrieval augmentation settings. In this study, we present the first analysis\non the factual knowledge boundaries of LLMs and how retrieval augmentation\naffects LLMs on open-domain question answering (QA), with a bunch of important\nfindings. Specifically, we focus on three research questions and analyze them\nby examining QA, priori judgement and posteriori judgement capabilities of\nLLMs. We show evidence that LLMs possess unwavering confidence in their\nknowledge and cannot handle the conflict between internal and external\nknowledge well. Furthermore, retrieval augmentation proves to be an effective\napproach in enhancing LLMs' awareness of knowledge boundaries. We further\nconduct thorough experiments to examine how different factors affect LLMs and\npropose a simple method to dynamically utilize supporting documents with our\njudgement strategy. Additionally, we find that the relevance between the\nsupporting documents and the questions significantly impacts LLMs' QA and\njudgemental capabilities. The code to reproduce this work is available at\nhttps://github.com/RUCAIBox/LLM-Knowledge-Boundary."
                },
                "authors": [
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yingqi Qu"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Hao Tian"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.11019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.11019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12238v1",
                "updated": "2024-11-19T05:29:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    29,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T05:29:58Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    29,
                    58,
                    1,
                    324,
                    0
                ],
                "title": "Performance of Large Language Models in Technical MRI Question\n  Answering: A Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of Large Language Models in Technical MRI Question\n  Answering: A Comparative Study"
                },
                "summary": "Background: Advances in artificial intelligence, particularly large language\nmodels (LLMs), have the potential to enhance technical expertise in magnetic\nresonance imaging (MRI), regardless of operator skill or geographic location.\n  Methods: We assessed the accuracy of several LLMs in answering 570 technical\nMRI questions derived from a standardized review book. The questions spanned\nnine MRI topics, including Basic Principles, Image Production, and Safety.\nClosed-source models (e.g., OpenAI's o1 Preview, GPT-4o, GPT-4 Turbo, and\nClaude 3.5 Haiku) and open-source models (e.g., Phi 3.5 Mini, Llama 3.1,\nsmolLM2) were tested. Models were queried using standardized prompts via the\nLangChain framework, and responses were graded against correct answers using an\nautomated scoring protocol. Accuracy, defined as the proportion of correct\nanswers, was the primary outcome.\n  Results: The closed-source o1 Preview model achieved the highest accuracy\n(94%), exceeding the random-guess baseline (26.5%). GPT-4o and o1 Mini scored\n88%, and GPT-4 Turbo and Claude 3.5 Haiku each scored 84%. Among open-source\nmodels, Phi 3.5 Mini performed well, achieving 78% accuracy, comparable to\nseveral closed-source models. Accuracy was highest in Basic Principles and\nInstrumentation categories but lower in Image Weighting and Contrast, History,\nand Artifacts and Corrections.\n  Conclusions: LLMs exhibit high accuracy in addressing technical MRI\nquestions, suggesting their potential to standardize and enhance MRI practice.\nThese models may improve image quality and consistency across varied clinical\nenvironments. Further studies are needed to refine LLMs for clinical use and\nintegrate them into MRI workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Advances in artificial intelligence, particularly large language\nmodels (LLMs), have the potential to enhance technical expertise in magnetic\nresonance imaging (MRI), regardless of operator skill or geographic location.\n  Methods: We assessed the accuracy of several LLMs in answering 570 technical\nMRI questions derived from a standardized review book. The questions spanned\nnine MRI topics, including Basic Principles, Image Production, and Safety.\nClosed-source models (e.g., OpenAI's o1 Preview, GPT-4o, GPT-4 Turbo, and\nClaude 3.5 Haiku) and open-source models (e.g., Phi 3.5 Mini, Llama 3.1,\nsmolLM2) were tested. Models were queried using standardized prompts via the\nLangChain framework, and responses were graded against correct answers using an\nautomated scoring protocol. Accuracy, defined as the proportion of correct\nanswers, was the primary outcome.\n  Results: The closed-source o1 Preview model achieved the highest accuracy\n(94%), exceeding the random-guess baseline (26.5%). GPT-4o and o1 Mini scored\n88%, and GPT-4 Turbo and Claude 3.5 Haiku each scored 84%. Among open-source\nmodels, Phi 3.5 Mini performed well, achieving 78% accuracy, comparable to\nseveral closed-source models. Accuracy was highest in Basic Principles and\nInstrumentation categories but lower in Image Weighting and Contrast, History,\nand Artifacts and Corrections.\n  Conclusions: LLMs exhibit high accuracy in addressing technical MRI\nquestions, suggesting their potential to standardize and enhance MRI practice.\nThese models may improve image quality and consistency across varied clinical\nenvironments. Further studies are needed to refine LLMs for clinical use and\nintegrate them into MRI workflows."
                },
                "authors": [
                    {
                        "name": "Alan B McMillan"
                    }
                ],
                "author_detail": {
                    "name": "Alan B McMillan"
                },
                "author": "Alan B McMillan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00068v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00068v3",
                "updated": "2024-11-19T05:08:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    8,
                    44,
                    1,
                    324,
                    0
                ],
                "published": "2024-01-30T14:47:15Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    14,
                    47,
                    15,
                    1,
                    30,
                    0
                ],
                "title": "Adapting Amidst Degradation: Cross Domain Li-ion Battery Health\n  Estimation via Physics-Guided Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Amidst Degradation: Cross Domain Li-ion Battery Health\n  Estimation via Physics-Guided Test-Time Training"
                },
                "summary": "Health modeling of lithium-ion batteries (LIBs) is crucial for safe and\nefficient energy management and carries significant socio-economic\nimplications. Although Machine Learning (ML)-based State of Health (SOH)\nestimation methods have made significant progress in accuracy, the scarcity of\nhigh-quality LIB data remains a major obstacle. Existing transfer learning\nmethods for cross-domain LIB SOH estimation have significantly alleviated the\nlabeling burden of target LIB data, however, they still require sufficient\nunlabeled target data (UTD) for effective adaptation to the target domain.\nCollecting this UTD is challenging due to the time-consuming nature of\ndegradation experiments. To address this issue, we introduce a practical\nTest-Time Training framework, BatteryTTT, which adapts the model continually\nusing each UTD collected amidst degradation, thereby significantly reducing\ndata collection time. To fully utilize each UTD, BatteryTTT integrates the\ninherent physical laws of modern LIBs into self-supervised learning, termed\nPhyscics-Guided Test-Time Training. Additionally, we explore the potential of\nlarge language models (LLMs) in battery sequence modeling by evaluating their\nperformance in SOH estimation through model reprogramming and prefix prompt\nadaptation. The combination of BatteryTTT and LLM modeling, termed GPT4Battery,\nachieves state-of-the-art generalization results across current LIB benchmarks.\nFurthermore, we demonstrate the practical value and scalability of our approach\nby deploying it in our real-world battery management system (BMS) for 300Ah\nlarge-scale energy storage LIBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health modeling of lithium-ion batteries (LIBs) is crucial for safe and\nefficient energy management and carries significant socio-economic\nimplications. Although Machine Learning (ML)-based State of Health (SOH)\nestimation methods have made significant progress in accuracy, the scarcity of\nhigh-quality LIB data remains a major obstacle. Existing transfer learning\nmethods for cross-domain LIB SOH estimation have significantly alleviated the\nlabeling burden of target LIB data, however, they still require sufficient\nunlabeled target data (UTD) for effective adaptation to the target domain.\nCollecting this UTD is challenging due to the time-consuming nature of\ndegradation experiments. To address this issue, we introduce a practical\nTest-Time Training framework, BatteryTTT, which adapts the model continually\nusing each UTD collected amidst degradation, thereby significantly reducing\ndata collection time. To fully utilize each UTD, BatteryTTT integrates the\ninherent physical laws of modern LIBs into self-supervised learning, termed\nPhyscics-Guided Test-Time Training. Additionally, we explore the potential of\nlarge language models (LLMs) in battery sequence modeling by evaluating their\nperformance in SOH estimation through model reprogramming and prefix prompt\nadaptation. The combination of BatteryTTT and LLM modeling, termed GPT4Battery,\nachieves state-of-the-art generalization results across current LIB benchmarks.\nFurthermore, we demonstrate the practical value and scalability of our approach\nby deploying it in our real-world battery management system (BMS) for 300Ah\nlarge-scale energy storage LIBs."
                },
                "authors": [
                    {
                        "name": "Yuyuan Feng"
                    },
                    {
                        "name": "Guosheng Hu"
                    },
                    {
                        "name": "Xiaodong Li"
                    },
                    {
                        "name": "Zhihong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhihong Zhang"
                },
                "author": "Zhihong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00068v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00068v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09003v2",
                "updated": "2024-11-19T04:53:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    4,
                    53,
                    47,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-13T20:12:55Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    12,
                    55,
                    2,
                    318,
                    0
                ],
                "title": "Refusal in LLMs is an Affine Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal in LLMs is an Affine Function"
                },
                "summary": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 ."
                },
                "authors": [
                    {
                        "name": "Thomas Marshall"
                    },
                    {
                        "name": "Adam Scherlis"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "arxiv_comment": "added plots for results from additional models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10487v2",
                "updated": "2024-11-19T04:11:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    4,
                    11,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-14T05:09:07Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    9,
                    7,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Patterns for Designing Quantum Artificial Intelligence\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Patterns for Designing Quantum Artificial Intelligence\n  Systems"
                },
                "summary": "Utilising quantum computing technology to enhance artificial intelligence\nsystems is expected to improve training and inference times, increase\nrobustness against noise and adversarial attacks, and reduce the number of\nparameters without compromising accuracy. However, moving beyond\nproof-of-concept or simulations to develop practical applications of these\nsystems while ensuring high software quality faces significant challenges due\nto the limitations of quantum hardware and the underdeveloped knowledge base in\nsoftware engineering for such systems. In this work, we have conducted a\nsystematic mapping study to identify the challenges and solutions associated\nwith the software architecture of quantum-enhanced artificial intelligence\nsystems. Our review uncovered several architectural patterns that describe how\nquantum components can be integrated into inference engines, as well as\nmiddleware patterns that facilitate communication between classical and quantum\ncomponents. These insights have been compiled into a catalog of architectural\npatterns. Each pattern realises a trade-off between efficiency, scalability,\ntrainability, simplicity, portability and deployability, and other software\nquality attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilising quantum computing technology to enhance artificial intelligence\nsystems is expected to improve training and inference times, increase\nrobustness against noise and adversarial attacks, and reduce the number of\nparameters without compromising accuracy. However, moving beyond\nproof-of-concept or simulations to develop practical applications of these\nsystems while ensuring high software quality faces significant challenges due\nto the limitations of quantum hardware and the underdeveloped knowledge base in\nsoftware engineering for such systems. In this work, we have conducted a\nsystematic mapping study to identify the challenges and solutions associated\nwith the software architecture of quantum-enhanced artificial intelligence\nsystems. Our review uncovered several architectural patterns that describe how\nquantum components can be integrated into inference engines, as well as\nmiddleware patterns that facilitate communication between classical and quantum\ncomponents. These insights have been compiled into a catalog of architectural\npatterns. Each pattern realises a trade-off between efficiency, scalability,\ntrainability, simplicity, portability and deployability, and other software\nquality attributes."
                },
                "authors": [
                    {
                        "name": "Mykhailo Klymenko"
                    },
                    {
                        "name": "Thong Hoang"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; D.2.m; I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12205v1",
                "updated": "2024-11-19T03:48:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    48,
                    48,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T03:48:48Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    48,
                    48,
                    1,
                    324,
                    0
                ],
                "title": "Sparser Training for On-Device Recommendation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparser Training for On-Device Recommendation Systems"
                },
                "summary": "Recommender systems often rely on large embedding tables that map users and\nitems to dense vectors of uniform size, leading to substantial memory\nconsumption and inefficiencies. This is particularly problematic in\nmemory-constrained environments like mobile and Web of Things (WoT)\napplications, where scalability and real-time performance are critical. Various\nresearch efforts have sought to address these issues. Although embedding\npruning methods utilizing Dynamic Sparse Training (DST) stand out due to their\nlow training and inference costs, consistent sparsity, and end-to-end\ndifferentiability, they face key challenges. Firstly, they typically\ninitializes the mask matrix, which is used to prune redundant parameters, with\nrandom uniform sparse initialization. This strategy often results in suboptimal\nperformance as it creates unstructured and inefficient connections. Secondly,\nthey tend to favor the users/items sampled in the single batch immediately\nbefore weight exploration when they reactivate pruned parameters with large\ngradient magnitudes, which does not necessarily improve the overall\nperformance. Thirdly, while they use sparse weights during forward passes, they\nstill need to compute dense gradients during backward passes. In this paper, we\npropose SparseRec, an lightweight embedding method based on DST, to address\nthese issues. Specifically, SparseRec initializes the mask matrix using\nNonnegative Matrix Factorization. It accumulates gradients to identify the\ninactive parameters that can better improve the model performance after\nactivation. Furthermore, it avoids dense gradients during backpropagation by\nsampling a subset of important vectors. Gradients are calculated only for\nparameters in this subset, thus maintaining sparsity during training in both\nforward and backward passes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems often rely on large embedding tables that map users and\nitems to dense vectors of uniform size, leading to substantial memory\nconsumption and inefficiencies. This is particularly problematic in\nmemory-constrained environments like mobile and Web of Things (WoT)\napplications, where scalability and real-time performance are critical. Various\nresearch efforts have sought to address these issues. Although embedding\npruning methods utilizing Dynamic Sparse Training (DST) stand out due to their\nlow training and inference costs, consistent sparsity, and end-to-end\ndifferentiability, they face key challenges. Firstly, they typically\ninitializes the mask matrix, which is used to prune redundant parameters, with\nrandom uniform sparse initialization. This strategy often results in suboptimal\nperformance as it creates unstructured and inefficient connections. Secondly,\nthey tend to favor the users/items sampled in the single batch immediately\nbefore weight exploration when they reactivate pruned parameters with large\ngradient magnitudes, which does not necessarily improve the overall\nperformance. Thirdly, while they use sparse weights during forward passes, they\nstill need to compute dense gradients during backward passes. In this paper, we\npropose SparseRec, an lightweight embedding method based on DST, to address\nthese issues. Specifically, SparseRec initializes the mask matrix using\nNonnegative Matrix Factorization. It accumulates gradients to identify the\ninactive parameters that can better improve the model performance after\nactivation. Furthermore, it avoids dense gradients during backpropagation by\nsampling a subset of important vectors. Gradients are calculated only for\nparameters in this subset, thus maintaining sparsity during training in both\nforward and backward passes."
                },
                "authors": [
                    {
                        "name": "Yunke Qu"
                    },
                    {
                        "name": "Liang Qu"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Jianxin Li"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12199v1",
                "updated": "2024-11-19T03:30:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    30,
                    44,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T03:30:44Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    30,
                    44,
                    1,
                    324,
                    0
                ],
                "title": "RoSIS: Robust Framework for Text-Promptable Surgical Instrument\n  Segmentation Using Vision-Language Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSIS: Robust Framework for Text-Promptable Surgical Instrument\n  Segmentation Using Vision-Language Fusion"
                },
                "summary": "Surgical instrument segmentation (SIS) is an essential task in\ncomputer-assisted surgeries, with deep learning-based research improving\naccuracy in complex environments. Recently, text-promptable segmentation\nmethods have been introduced to generate masks based on text prompts describing\ntarget objects. However, these methods assume that the object described by a\ngiven text prompt exists in the scene. This results in mask generation whenever\na related text prompt is provided, even if the object is absent from the image.\nExisting methods handle this by using prompts only for objects known to be\npresent in the image, which introduces inaccessible information in a\nvision-based method setting and results in unfair comparisons. For fair\ncomparison, we redefine existing text-promptable SIS settings to robust\nconditions, called Robust text-promptable SIS (R-SIS), designed to forward\nprompts of all classes and determine the existence of an object from a given\ntext prompt for the fair comparison. Furthermore, we propose a novel framework,\nRobust Surgical Instrument Segmentation (RoSIS), which combines visual and\nlanguage features for promptable segmentation in the R-SIS setting. RoSIS\nemploys an encoder-decoder architecture with a Multi-Modal Fusion Block (MMFB)\nand a Selective Gate Block (SGB) to achieve balanced integration of vision and\nlanguage features. Additionally, we introduce an iterative inference strategy\nthat refines segmentation masks in two steps: an initial pass using name-based\nprompts, followed by a refinement step using location prompts. Experiments on\nvarious datasets and settings demonstrate that RoSIS outperforms existing\nvision-based and promptable methods under robust conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical instrument segmentation (SIS) is an essential task in\ncomputer-assisted surgeries, with deep learning-based research improving\naccuracy in complex environments. Recently, text-promptable segmentation\nmethods have been introduced to generate masks based on text prompts describing\ntarget objects. However, these methods assume that the object described by a\ngiven text prompt exists in the scene. This results in mask generation whenever\na related text prompt is provided, even if the object is absent from the image.\nExisting methods handle this by using prompts only for objects known to be\npresent in the image, which introduces inaccessible information in a\nvision-based method setting and results in unfair comparisons. For fair\ncomparison, we redefine existing text-promptable SIS settings to robust\nconditions, called Robust text-promptable SIS (R-SIS), designed to forward\nprompts of all classes and determine the existence of an object from a given\ntext prompt for the fair comparison. Furthermore, we propose a novel framework,\nRobust Surgical Instrument Segmentation (RoSIS), which combines visual and\nlanguage features for promptable segmentation in the R-SIS setting. RoSIS\nemploys an encoder-decoder architecture with a Multi-Modal Fusion Block (MMFB)\nand a Selective Gate Block (SGB) to achieve balanced integration of vision and\nlanguage features. Additionally, we introduce an iterative inference strategy\nthat refines segmentation masks in two steps: an initial pass using name-based\nprompts, followed by a refinement step using location prompts. Experiments on\nvarious datasets and settings demonstrate that RoSIS outperforms existing\nvision-based and promptable methods under robust conditions."
                },
                "authors": [
                    {
                        "name": "Tae-Min Choi"
                    },
                    {
                        "name": "Juyoun Park"
                    }
                ],
                "author_detail": {
                    "name": "Juyoun Park"
                },
                "author": "Juyoun Park",
                "arxiv_comment": "10 pages, 6 figures, submitted to IEEE transactions on Medical\n  Imaging",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12196v1",
                "updated": "2024-11-19T03:29:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    29,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T03:29:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    29,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs"
                },
                "summary": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability."
                },
                "authors": [
                    {
                        "name": "Zixin Liu"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiran Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Ding"
                },
                "author": "Yiran Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14148v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14148v3",
                "updated": "2024-11-19T03:08:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    8,
                    34,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-18T03:34:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    34,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment"
                },
                "summary": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models."
                },
                "authors": [
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14148v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14148v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08000v3",
                "updated": "2024-11-19T02:31:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    2,
                    31,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2024-08-15T07:57:28Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    57,
                    28,
                    3,
                    228,
                    0
                ],
                "title": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and\n  3D Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and\n  3D Editing"
                },
                "summary": "Novel View Synthesis (NVS) and 3D generation have recently achieved prominent\nimprovements. However, these works mainly focus on confined categories or\nsynthetic 3D assets, which are discouraged from generalizing to challenging\nin-the-wild scenes and fail to be employed with 2D synthesis directly.\nMoreover, these methods heavily depended on camera poses, limiting their\nreal-world applications. To overcome these issues, we propose MVInpainter,\nre-formulating the 3D editing as a multi-view 2D inpainting task. Specifically,\nMVInpainter partially inpaints multi-view images with the reference guidance\nrather than intractably generating an entirely novel view from scratch, which\nlargely simplifies the difficulty of in-the-wild NVS and leverages unmasked\nclues instead of explicit pose conditions. To ensure cross-view consistency,\nMVInpainter is enhanced by video priors from motion components and appearance\nguidance from concatenated reference key&value attention. Furthermore,\nMVInpainter incorporates slot attention to aggregate high-level optical flow\nfeatures from unmasked regions to control the camera movement with pose-free\ntraining and inference. Sufficient scene-level experiments on both\nobject-centric and forward-facing datasets verify the effectiveness of\nMVInpainter, including diverse tasks, such as multi-view object removal,\nsynthesis, insertion, and replacement. The project page is\nhttps://ewrfcas.github.io/MVInpainter/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel View Synthesis (NVS) and 3D generation have recently achieved prominent\nimprovements. However, these works mainly focus on confined categories or\nsynthetic 3D assets, which are discouraged from generalizing to challenging\nin-the-wild scenes and fail to be employed with 2D synthesis directly.\nMoreover, these methods heavily depended on camera poses, limiting their\nreal-world applications. To overcome these issues, we propose MVInpainter,\nre-formulating the 3D editing as a multi-view 2D inpainting task. Specifically,\nMVInpainter partially inpaints multi-view images with the reference guidance\nrather than intractably generating an entirely novel view from scratch, which\nlargely simplifies the difficulty of in-the-wild NVS and leverages unmasked\nclues instead of explicit pose conditions. To ensure cross-view consistency,\nMVInpainter is enhanced by video priors from motion components and appearance\nguidance from concatenated reference key&value attention. Furthermore,\nMVInpainter incorporates slot attention to aggregate high-level optical flow\nfeatures from unmasked regions to control the camera movement with pose-free\ntraining and inference. Sufficient scene-level experiments on both\nobject-centric and forward-facing datasets verify the effectiveness of\nMVInpainter, including diverse tasks, such as multi-view object removal,\nsynthesis, insertion, and replacement. The project page is\nhttps://ewrfcas.github.io/MVInpainter/."
                },
                "authors": [
                    {
                        "name": "Chenjie Cao"
                    },
                    {
                        "name": "Chaohui Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "arxiv_comment": "Project page: https://ewrfcas.github.io/MVInpainter/. Accepted at\n  NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09209v2",
                "updated": "2024-11-19T02:09:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    2,
                    9,
                    23,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-14T06:13:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    13,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "JoyVASA: Portrait and Animal Image Animation with Diffusion-Based\n  Audio-Driven Facial Dynamics and Head Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyVASA: Portrait and Animal Image Animation with Diffusion-Based\n  Audio-Driven Facial Dynamics and Head Motion Generation"
                },
                "summary": "Audio-driven portrait animation has made significant advances with\ndiffusion-based models, improving video quality and lipsync accuracy. However,\nthe increasing complexity of these models has led to inefficiencies in training\nand inference, as well as constraints on video length and inter-frame\ncontinuity. In this paper, we propose JoyVASA, a diffusion-based method for\ngenerating facial dynamics and head motion in audio-driven facial animation.\nSpecifically, in the first stage, we introduce a decoupled facial\nrepresentation framework that separates dynamic facial expressions from static\n3D facial representations. This decoupling allows the system to generate longer\nvideos by combining any static 3D facial representation with dynamic motion\nsequences. Then, in the second stage, a diffusion transformer is trained to\ngenerate motion sequences directly from audio cues, independent of character\nidentity. Finally, a generator trained in the first stage uses the 3D facial\nrepresentation and the generated motion sequences as inputs to render\nhigh-quality animations. With the decoupled facial representation and the\nidentity-independent motion generation process, JoyVASA extends beyond human\nportraits to animate animal faces seamlessly. The model is trained on a hybrid\ndataset of private Chinese and public English data, enabling multilingual\nsupport. Experimental results validate the effectiveness of our approach.\nFuture work will focus on improving real-time performance and refining\nexpression control, further expanding the applications in portrait animation.\nThe code is available at: https://github.com/jdh-algo/JoyVASA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven portrait animation has made significant advances with\ndiffusion-based models, improving video quality and lipsync accuracy. However,\nthe increasing complexity of these models has led to inefficiencies in training\nand inference, as well as constraints on video length and inter-frame\ncontinuity. In this paper, we propose JoyVASA, a diffusion-based method for\ngenerating facial dynamics and head motion in audio-driven facial animation.\nSpecifically, in the first stage, we introduce a decoupled facial\nrepresentation framework that separates dynamic facial expressions from static\n3D facial representations. This decoupling allows the system to generate longer\nvideos by combining any static 3D facial representation with dynamic motion\nsequences. Then, in the second stage, a diffusion transformer is trained to\ngenerate motion sequences directly from audio cues, independent of character\nidentity. Finally, a generator trained in the first stage uses the 3D facial\nrepresentation and the generated motion sequences as inputs to render\nhigh-quality animations. With the decoupled facial representation and the\nidentity-independent motion generation process, JoyVASA extends beyond human\nportraits to animate animal faces seamlessly. The model is trained on a hybrid\ndataset of private Chinese and public English data, enabling multilingual\nsupport. Experimental results validate the effectiveness of our approach.\nFuture work will focus on improving real-time performance and refining\nexpression control, further expanding the applications in portrait animation.\nThe code is available at: https://github.com/jdh-algo/JoyVASA."
                },
                "authors": [
                    {
                        "name": "Xuyang Cao"
                    },
                    {
                        "name": "Guoxin Wang"
                    },
                    {
                        "name": "Sheng Shi"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Jintao Fei"
                    },
                    {
                        "name": "Minyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Minyu Gao"
                },
                "author": "Minyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.11844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11844v2",
                "updated": "2024-11-19T18:59:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    59,
                    42,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-18T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "title": "Generative World Explorer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative World Explorer"
                },
                "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jieneng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieneng Chen"
                },
                "author": "Jieneng Chen",
                "arxiv_comment": "Website: generative-world-explorer.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12736v1",
                "updated": "2024-11-19T18:58:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:58:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n  Models"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Salma Kharrat"
                    },
                    {
                        "name": "Fares Fourati"
                    },
                    {
                        "name": "Marco Canini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Canini"
                },
                "author": "Marco Canini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12712v1",
                "updated": "2024-11-19T18:27:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    27,
                    25,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:27:25Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    27,
                    25,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular,\n  Nervous System, and Digestive Disorders Using Advanced LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular,\n  Nervous System, and Digestive Disorders Using Advanced LLMs"
                },
                "summary": "In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks."
                },
                "authors": [
                    {
                        "name": "Ahmed Akib Jawad Karim"
                    },
                    {
                        "name": "Muhammad Zawad Mahmud"
                    },
                    {
                        "name": "Samiha Islam"
                    },
                    {
                        "name": "Aznur Azam"
                    }
                ],
                "author_detail": {
                    "name": "Aznur Azam"
                },
                "author": "Aznur Azam",
                "arxiv_comment": "7 Pages, 4 tables and 11 figures. Under review in a IEEE conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01306v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01306v4",
                "updated": "2024-11-19T18:12:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    12,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-02T10:53:36Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    10,
                    53,
                    36,
                    4,
                    33,
                    0
                ],
                "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KTO: Model Alignment as Prospect Theoretic Optimization"
                },
                "summary": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration."
                },
                "authors": [
                    {
                        "name": "Kawin Ethayarajh"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Douwe Kiela"
                    }
                ],
                "author_detail": {
                    "name": "Douwe Kiela"
                },
                "author": "Douwe Kiela",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01306v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01306v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12701v1",
                "updated": "2024-11-19T18:11:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    11,
                    36,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:11:36Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    11,
                    36,
                    1,
                    324,
                    0
                ],
                "title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations"
                },
                "summary": "Large Language Models (LLMs) are vulnerable to backdoor attacks, where hidden\ntriggers can maliciously manipulate model behavior. While several backdoor\nattack methods have been proposed, the mechanisms by which backdoor functions\noperate in LLMs remain underexplored. In this paper, we move beyond attacking\nLLMs and investigate backdoor functionality through the novel lens of natural\nlanguage explanations. Specifically, we leverage LLMs' generative capabilities\nto produce human-understandable explanations for their decisions, allowing us\nto compare explanations for clean and poisoned samples. We explore various\nbackdoor attacks and embed the backdoor into LLaMA models for multiple tasks.\nOur experiments show that backdoored models produce higher-quality explanations\nfor clean data compared to poisoned data, while generating significantly more\nconsistent explanations for poisoned data than for clean data. We further\nanalyze the explanation generation process, revealing that at the token level,\nthe explanation token of poisoned samples only appears in the final few\ntransformer layers of the LLM. At the sentence level, attention dynamics\nindicate that poisoned inputs shift attention from the input context when\ngenerating the explanation. These findings deepen our understanding of backdoor\nattack mechanisms in LLMs and offer a framework for detecting such\nvulnerabilities through explainability techniques, contributing to the\ndevelopment of more secure LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are vulnerable to backdoor attacks, where hidden\ntriggers can maliciously manipulate model behavior. While several backdoor\nattack methods have been proposed, the mechanisms by which backdoor functions\noperate in LLMs remain underexplored. In this paper, we move beyond attacking\nLLMs and investigate backdoor functionality through the novel lens of natural\nlanguage explanations. Specifically, we leverage LLMs' generative capabilities\nto produce human-understandable explanations for their decisions, allowing us\nto compare explanations for clean and poisoned samples. We explore various\nbackdoor attacks and embed the backdoor into LLaMA models for multiple tasks.\nOur experiments show that backdoored models produce higher-quality explanations\nfor clean data compared to poisoned data, while generating significantly more\nconsistent explanations for poisoned data than for clean data. We further\nanalyze the explanation generation process, revealing that at the token level,\nthe explanation token of poisoned samples only appears in the final few\ntransformer layers of the LLM. At the sentence level, attention dynamics\nindicate that poisoned inputs shift attention from the input context when\ngenerating the explanation. These findings deepen our understanding of backdoor\nattack mechanisms in LLMs and offer a framework for detecting such\nvulnerabilities through explainability techniques, contributing to the\ndevelopment of more secure LLMs."
                },
                "authors": [
                    {
                        "name": "Huaizhi Ge"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12593v1",
                "updated": "2024-11-19T18:04:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    4,
                    13,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:04:13Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    4,
                    13,
                    1,
                    324,
                    0
                ],
                "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction"
                },
                "summary": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%."
                },
                "authors": [
                    {
                        "name": "Yuanbin Man"
                    },
                    {
                        "name": "Ying Huang"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Miao Yin"
                    }
                ],
                "author_detail": {
                    "name": "Miao Yin"
                },
                "author": "Miao Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12692v1",
                "updated": "2024-11-19T17:59:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    59,
                    12,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:59:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    59,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast\n  LLM Inference"
                },
                "summary": "Leveraging sparsity is crucial for optimizing large language model inference.\nhowever, modern LLMs employing SiLU as their activation function exhibit\nminimal activation sparsity. Recent research has proposed replacing SiLU with\nReLU to induce significant activation sparsity and showed no downstream task\naccuracy degradation through fine tuning. However, taking full advantage of it\nrequired training a predictor to estimate this sparsity. In this paper, we\nintroduce SparseInfer, a simple, light weight, and training free predictor for\nactivation sparsity of ReLU field LLMs, in which activation sparsity is\npredicted by comparing only the sign bits of inputs and weights. To compensate\nfor possible prediction inaccuracy, an adaptive tuning of the predictor's\nconservativeness is enabled, which can also serve as a control knob for\noptimizing LLM inference. The proposed method achieves approximately faster\ninference speed over the state of the art, with negligible accuracy loss of\nwithin 1%p.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging sparsity is crucial for optimizing large language model inference.\nhowever, modern LLMs employing SiLU as their activation function exhibit\nminimal activation sparsity. Recent research has proposed replacing SiLU with\nReLU to induce significant activation sparsity and showed no downstream task\naccuracy degradation through fine tuning. However, taking full advantage of it\nrequired training a predictor to estimate this sparsity. In this paper, we\nintroduce SparseInfer, a simple, light weight, and training free predictor for\nactivation sparsity of ReLU field LLMs, in which activation sparsity is\npredicted by comparing only the sign bits of inputs and weights. To compensate\nfor possible prediction inaccuracy, an adaptive tuning of the predictor's\nconservativeness is enabled, which can also serve as a control knob for\noptimizing LLM inference. The proposed method achieves approximately faster\ninference speed over the state of the art, with negligible accuracy loss of\nwithin 1%p."
                },
                "authors": [
                    {
                        "name": "Jiho Shin"
                    },
                    {
                        "name": "Hoeseok Yang"
                    },
                    {
                        "name": "Youngmin Yi"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Yi"
                },
                "author": "Youngmin Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08316v3",
                "updated": "2024-11-19T17:49:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    49,
                    27,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-12T15:16:40Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    15,
                    16,
                    40,
                    2,
                    164,
                    0
                ],
                "title": "Is Programming by Example solved by LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Programming by Example solved by LLMs?"
                },
                "summary": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n\"solved\" PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n\"solved\" PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short."
                },
                "authors": [
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00248v2",
                "updated": "2024-11-19T17:46:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    46,
                    48,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-31T22:58:08Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    22,
                    58,
                    8,
                    3,
                    305,
                    0
                ],
                "title": "A Demonstration of Adaptive Collaboration of Large Language Models for\n  Medical Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Demonstration of Adaptive Collaboration of Large Language Models for\n  Medical Decision-Making"
                },
                "summary": "Medical Decision-Making (MDM) is a multi-faceted process that requires\nclinicians to assess complex multi-modal patient data patient, often\ncollaboratively. Large Language Models (LLMs) promise to streamline this\nprocess by synthesizing vast medical knowledge and multi-modal health data.\nHowever, single-agent are often ill-suited for nuanced medical contexts\nrequiring adaptable, collaborative problem-solving. Our MDAgents addresses this\nneed by dynamically assigning collaboration structures to LLMs based on task\ncomplexity, mimicking real-world clinical collaboration and decision-making.\nThis framework improves diagnostic accuracy and supports adaptive responses in\ncomplex, real-world medical scenarios, making it a valuable tool for clinicians\nin various healthcare settings, and at the same time, being more efficient in\nterms of computing cost than static multi-agent decision making methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Decision-Making (MDM) is a multi-faceted process that requires\nclinicians to assess complex multi-modal patient data patient, often\ncollaboratively. Large Language Models (LLMs) promise to streamline this\nprocess by synthesizing vast medical knowledge and multi-modal health data.\nHowever, single-agent are often ill-suited for nuanced medical contexts\nrequiring adaptable, collaborative problem-solving. Our MDAgents addresses this\nneed by dynamically assigning collaboration structures to LLMs based on task\ncomplexity, mimicking real-world clinical collaboration and decision-making.\nThis framework improves diagnostic accuracy and supports adaptive responses in\ncomplex, real-world medical scenarios, making it a valuable tool for clinicians\nin various healthcare settings, and at the same time, being more efficient in\nterms of computing cost than static multi-agent decision making methods."
                },
                "authors": [
                    {
                        "name": "Yubin Kim"
                    },
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Hyewon Jeong"
                    },
                    {
                        "name": "Cristina Grau-Vilchez"
                    },
                    {
                        "name": "Yik Siu Chan"
                    },
                    {
                        "name": "Xuhai Xu"
                    },
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Hyeonhoon Lee"
                    },
                    {
                        "name": "Cynthia Breazeal"
                    },
                    {
                        "name": "Hae Won Park"
                    }
                ],
                "author_detail": {
                    "name": "Hae Won Park"
                },
                "author": "Hae Won Park",
                "arxiv_comment": "Under Review for ML4H 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12685v1",
                "updated": "2024-11-19T17:45:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    45,
                    12,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:45:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    45,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "Enhanced Sign Language Translation between American Sign Language (ASL)\n  and Indian Sign Language (ISL) Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Sign Language Translation between American Sign Language (ASL)\n  and Indian Sign Language (ISL) Using LLMs"
                },
                "summary": "We have come up with a research that hopes to provide a bridge between the\nusers of American Sign Language and the users of spoken language and Indian\nSign Language (ISL). The research enabled us to create a novel framework that\nwe have developed for Learner Systems. Leveraging art of Large models to create\nkey features including: - Real-time translation between these two sign\nlanguages in an efficient manner. Making LLM's capability available for\nseamless translations to ISL. Here is the full study showing its implementation\nin this paper. The core of the system is a sophisticated pipeline that begins\nwith reclassification and recognition of ASL gestures based on a strong Random\nForest Classifier. By recognizing the ASL, it is translated into text which can\nbe more easily processed. Highly evolved natural language NLP (Natural Language\nProcessing) techniques come in handy as they play a role in our LLM integration\nwhere you then use LLMs to be able to convert the ASL text to ISL which\nprovides you with the intent of sentence or phrase. The final step is to\nsynthesize the translated text back into ISL gestures, creating an end-to-end\ntranslation experience using RIFE-Net. This framework is tasked with key\nchallenges such as automatically dealing with gesture variability and\novercoming the linguistic differences between ASL and ISL. By automating the\ntranslation process, we hope to vastly improve accessibility for sign language\nusers. No longer will the communication gap between ASL and ISL create\nbarriers; this totally cool innovation aims to bring our communities closer\ntogether. And we believe, with full confidence in our framework, that we're\nable to apply the same principles across a wide variety of sign language\ndialects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have come up with a research that hopes to provide a bridge between the\nusers of American Sign Language and the users of spoken language and Indian\nSign Language (ISL). The research enabled us to create a novel framework that\nwe have developed for Learner Systems. Leveraging art of Large models to create\nkey features including: - Real-time translation between these two sign\nlanguages in an efficient manner. Making LLM's capability available for\nseamless translations to ISL. Here is the full study showing its implementation\nin this paper. The core of the system is a sophisticated pipeline that begins\nwith reclassification and recognition of ASL gestures based on a strong Random\nForest Classifier. By recognizing the ASL, it is translated into text which can\nbe more easily processed. Highly evolved natural language NLP (Natural Language\nProcessing) techniques come in handy as they play a role in our LLM integration\nwhere you then use LLMs to be able to convert the ASL text to ISL which\nprovides you with the intent of sentence or phrase. The final step is to\nsynthesize the translated text back into ISL gestures, creating an end-to-end\ntranslation experience using RIFE-Net. This framework is tasked with key\nchallenges such as automatically dealing with gesture variability and\novercoming the linguistic differences between ASL and ISL. By automating the\ntranslation process, we hope to vastly improve accessibility for sign language\nusers. No longer will the communication gap between ASL and ISL create\nbarriers; this totally cool innovation aims to bring our communities closer\ntogether. And we believe, with full confidence in our framework, that we're\nable to apply the same principles across a wide variety of sign language\ndialects."
                },
                "authors": [
                    {
                        "name": "Malay Kumar"
                    },
                    {
                        "name": "S. Sarvajit Visagan"
                    },
                    {
                        "name": "Tanish Sarang Mahajan"
                    },
                    {
                        "name": "Anisha Natarajan"
                    }
                ],
                "author_detail": {
                    "name": "Anisha Natarajan"
                },
                "author": "Anisha Natarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02272v3",
                "updated": "2024-11-19T17:29:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    29,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-04T17:03:55Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    3,
                    55,
                    0,
                    309,
                    0
                ],
                "title": "Combining Induction and Transduction for Abstract Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Induction and Transduction for Abstract Reasoning"
                },
                "summary": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture."
                },
                "authors": [
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Keya Hu"
                    },
                    {
                        "name": "Carter Larsen"
                    },
                    {
                        "name": "Yuqing Wu"
                    },
                    {
                        "name": "Simon Alford"
                    },
                    {
                        "name": "Caleb Woo"
                    },
                    {
                        "name": "Spencer M. Dunn"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Michelangelo Naim"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Wei-Long Zheng"
                    },
                    {
                        "name": "Zenna Tavares"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12671v1",
                "updated": "2024-11-19T17:23:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    23,
                    55,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:23:55Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    23,
                    55,
                    1,
                    324,
                    0
                ],
                "title": "Neurosymbolic Graph Enrichment for Grounded World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic Graph Enrichment for Grounded World Models"
                },
                "summary": "The development of artificial intelligence systems capable of understanding\nand reasoning about complex real-world scenarios is a significant challenge. In\nthis work we present a novel approach to enhance and exploit LLM reactive\ncapability to address complex problems and interpret deeply contextual\nreal-world meaning. We introduce a method and a tool for creating a multimodal,\nknowledge-augmented formal representation of meaning that combines the\nstrengths of large language models with structured semantic representations.\nOur method begins with an image input, utilizing state-of-the-art large\nlanguage models to generate a natural language description. This description is\nthen transformed into an Abstract Meaning Representation (AMR) graph, which is\nformalized and enriched with logical design patterns, and layered semantics\nderived from linguistic and factual knowledge bases. The resulting graph is\nthen fed back into the LLM to be extended with implicit knowledge activated by\ncomplex heuristic learning, including semantic implicatures, moral values,\nembodied cognition, and metaphorical representations. By bridging the gap\nbetween unstructured language models and formal semantic structures, our method\nopens new avenues for tackling intricate problems in natural language\nunderstanding and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of artificial intelligence systems capable of understanding\nand reasoning about complex real-world scenarios is a significant challenge. In\nthis work we present a novel approach to enhance and exploit LLM reactive\ncapability to address complex problems and interpret deeply contextual\nreal-world meaning. We introduce a method and a tool for creating a multimodal,\nknowledge-augmented formal representation of meaning that combines the\nstrengths of large language models with structured semantic representations.\nOur method begins with an image input, utilizing state-of-the-art large\nlanguage models to generate a natural language description. This description is\nthen transformed into an Abstract Meaning Representation (AMR) graph, which is\nformalized and enriched with logical design patterns, and layered semantics\nderived from linguistic and factual knowledge bases. The resulting graph is\nthen fed back into the LLM to be extended with implicit knowledge activated by\ncomplex heuristic learning, including semantic implicatures, moral values,\nembodied cognition, and metaphorical representations. By bridging the gap\nbetween unstructured language models and formal semantic structures, our method\nopens new avenues for tackling intricate problems in natural language\nunderstanding and reasoning."
                },
                "authors": [
                    {
                        "name": "Stefano De Giorgis"
                    },
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Alessandro Russo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Russo"
                },
                "author": "Alessandro Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v3",
                "updated": "2024-11-19T17:14:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    14,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12662v1",
                "updated": "2024-11-19T17:12:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    12,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T17:12:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    12,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "bia-binder: A web-native cloud compute service for the bioimage analysis\n  community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "bia-binder: A web-native cloud compute service for the bioimage analysis\n  community"
                },
                "summary": "We introduce bia-binder (BioImage Archive Binder), an open-source,\ncloud-architectured, and web-based coding environment tailored to bioimage\nanalysis that is freely accessible to all researchers. The service generates\neasy-to-use Jupyter Notebook coding environments hosted on EMBL-EBI's Embassy\nCloud, which provides significant computational resources. The bia-binder\narchitecture is free, open-source and publicly available for deployment. It\nfeatures fast and direct access to images in the BioImage Archive, the Image\nData Resource, and the BioStudies databases. We believe that this service can\nplay a role in mitigating the current inequalities in access to scientific\nresources across academia. As bia-binder produces permanent links to compiled\ncoding environments, we foresee the service to become widely-used within the\ncommunity and enable exploratory research. bia-binder is built and deployed\nusing helmsman and helm and released under the MIT licence. It can be accessed\nat binder.bioimagearchive.org and runs on any standard web browser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce bia-binder (BioImage Archive Binder), an open-source,\ncloud-architectured, and web-based coding environment tailored to bioimage\nanalysis that is freely accessible to all researchers. The service generates\neasy-to-use Jupyter Notebook coding environments hosted on EMBL-EBI's Embassy\nCloud, which provides significant computational resources. The bia-binder\narchitecture is free, open-source and publicly available for deployment. It\nfeatures fast and direct access to images in the BioImage Archive, the Image\nData Resource, and the BioStudies databases. We believe that this service can\nplay a role in mitigating the current inequalities in access to scientific\nresources across academia. As bia-binder produces permanent links to compiled\ncoding environments, we foresee the service to become widely-used within the\ncommunity and enable exploratory research. bia-binder is built and deployed\nusing helmsman and helm and released under the MIT licence. It can be accessed\nat binder.bioimagearchive.org and runs on any standard web browser."
                },
                "authors": [
                    {
                        "name": "Craig T. Russell"
                    },
                    {
                        "name": "Jean-Marie Burel"
                    },
                    {
                        "name": "Awais Athar"
                    },
                    {
                        "name": "Simon Li"
                    },
                    {
                        "name": "Ugis Sarkans"
                    },
                    {
                        "name": "Jason Swedlow"
                    },
                    {
                        "name": "Alvis Brazma"
                    },
                    {
                        "name": "Matthew Hartley"
                    },
                    {
                        "name": "Virginie Uhlmann"
                    }
                ],
                "author_detail": {
                    "name": "Virginie Uhlmann"
                },
                "author": "Virginie Uhlmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12643v1",
                "updated": "2024-11-19T16:54:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    54,
                    30,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T16:54:30Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    54,
                    30,
                    1,
                    324,
                    0
                ],
                "title": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning\n  Models"
                },
                "summary": "The rapid advancement of artificial intelligence has led to increasingly\nsophisticated deep learning models, which frequently operate as opaque 'black\nboxes' with limited transparency in their decision-making processes. This lack\nof interpretability presents considerable challenges, especially in high-stakes\napplications where understanding the rationale behind a model's outputs is as\nessential as the outputs themselves. This study addresses the pressing need for\ninterpretability in AI systems, emphasizing its role in fostering trust,\nensuring accountability, and promoting responsible deployment in\nmission-critical fields. To address the interpretability challenge in deep\nlearning, we introduce DLBacktrace, an innovative technique developed by the\nAryaXAI team to illuminate model decisions across a wide array of domains,\nincluding simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks\n(CNNs), Large Language Models (LLMs), Computer Vision Models, and more.\n  We provide a comprehensive overview of the DLBacktrace algorithm and present\nbenchmarking results, comparing its performance against established\ninterpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients,\nSmoothGrad, and Attention Rollout, using diverse task-based metrics. The\nproposed DLBacktrace technique is compatible with various model architectures\nbuilt in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP\narchitectures such as BERT and LSTMs, computer vision models like ResNet and\nU-Net, as well as custom deep neural network (DNN) models for tabular data.\nThis flexibility underscores DLBacktrace's adaptability and effectiveness in\nenhancing model transparency across a broad spectrum of applications. The\nlibrary is open-sourced and available at https://github.com/AryaXAI/DLBacktrace .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence has led to increasingly\nsophisticated deep learning models, which frequently operate as opaque 'black\nboxes' with limited transparency in their decision-making processes. This lack\nof interpretability presents considerable challenges, especially in high-stakes\napplications where understanding the rationale behind a model's outputs is as\nessential as the outputs themselves. This study addresses the pressing need for\ninterpretability in AI systems, emphasizing its role in fostering trust,\nensuring accountability, and promoting responsible deployment in\nmission-critical fields. To address the interpretability challenge in deep\nlearning, we introduce DLBacktrace, an innovative technique developed by the\nAryaXAI team to illuminate model decisions across a wide array of domains,\nincluding simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks\n(CNNs), Large Language Models (LLMs), Computer Vision Models, and more.\n  We provide a comprehensive overview of the DLBacktrace algorithm and present\nbenchmarking results, comparing its performance against established\ninterpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients,\nSmoothGrad, and Attention Rollout, using diverse task-based metrics. The\nproposed DLBacktrace technique is compatible with various model architectures\nbuilt in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP\narchitectures such as BERT and LSTMs, computer vision models like ResNet and\nU-Net, as well as custom deep neural network (DNN) models for tabular data.\nThis flexibility underscores DLBacktrace's adaptability and effectiveness in\nenhancing model transparency across a broad spectrum of applications. The\nlibrary is open-sourced and available at https://github.com/AryaXAI/DLBacktrace ."
                },
                "authors": [
                    {
                        "name": "Vinay Kumar Sankarapu"
                    },
                    {
                        "name": "Chintan Chitroda"
                    },
                    {
                        "name": "Yashwardhan Rathore"
                    },
                    {
                        "name": "Neeraj Kumar Singh"
                    },
                    {
                        "name": "Pratinav Seth"
                    }
                ],
                "author_detail": {
                    "name": "Pratinav Seth"
                },
                "author": "Pratinav Seth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12641v1",
                "updated": "2024-11-19T16:52:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    52,
                    34,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T16:52:34Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    52,
                    34,
                    1,
                    324,
                    0
                ],
                "title": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models"
                },
                "summary": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ..."
                },
                "authors": [
                    {
                        "name": "Yixiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixiao Zhang"
                },
                "author": "Yixiao Zhang",
                "arxiv_comment": "PhD Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11579v2",
                "updated": "2024-11-19T16:39:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    39,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-09-17T22:06:46Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    6,
                    46,
                    1,
                    261,
                    0
                ],
                "title": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection"
                },
                "summary": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs..."
                },
                "authors": [
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Accepted in NeurIPS 2024 SoLaR Workshop and Safety Gen AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12580v1",
                "updated": "2024-11-19T15:47:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    47,
                    12,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T15:47:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    47,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models"
                },
                "summary": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning."
                },
                "authors": [
                    {
                        "name": "Laura Ruis"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Siddhartha Rao Kamalakara"
                    },
                    {
                        "name": "Dwarak Talupuru"
                    },
                    {
                        "name": "Acyr Locatelli"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Tim Rocktschel"
                    },
                    {
                        "name": "Edward Grefenstette"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12571v1",
                "updated": "2024-11-19T15:39:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    39,
                    51,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T15:39:51Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    39,
                    51,
                    1,
                    324,
                    0
                ],
                "title": "Large Language Models for Combinatorial Optimization of Design Structure\n  Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Combinatorial Optimization of Design Structure\n  Matrix"
                },
                "summary": "Combinatorial optimization (CO) is essential for improving efficiency and\nperformance in engineering applications. As complexity increases with larger\nproblem sizes and more intricate dependencies, identifying the optimal solution\nbecome challenging. When it comes to real-world engineering problems,\nalgorithms based on pure mathematical reasoning are limited and incapable to\ncapture the contextual nuances necessary for optimization. This study explores\nthe potential of Large Language Models (LLMs) in solving engineering CO\nproblems by leveraging their reasoning power and contextual knowledge. We\npropose a novel LLM-based framework that integrates network topology and domain\nknowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed\nmethod achieves faster convergence and higher solution quality than benchmark\nmethods. Moreover, results show that incorporating contextual domain knowledge\nsignificantly improves performance despite the choice of LLMs. These findings\nhighlight the potential of LLMs in tackling complex real-world CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for\na new paradigm in in real-world combinatorial optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial optimization (CO) is essential for improving efficiency and\nperformance in engineering applications. As complexity increases with larger\nproblem sizes and more intricate dependencies, identifying the optimal solution\nbecome challenging. When it comes to real-world engineering problems,\nalgorithms based on pure mathematical reasoning are limited and incapable to\ncapture the contextual nuances necessary for optimization. This study explores\nthe potential of Large Language Models (LLMs) in solving engineering CO\nproblems by leveraging their reasoning power and contextual knowledge. We\npropose a novel LLM-based framework that integrates network topology and domain\nknowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed\nmethod achieves faster convergence and higher solution quality than benchmark\nmethods. Moreover, results show that incorporating contextual domain knowledge\nsignificantly improves performance despite the choice of LLMs. These findings\nhighlight the potential of LLMs in tackling complex real-world CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for\na new paradigm in in real-world combinatorial optimization."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Min Xie"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17213v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17213v5",
                "updated": "2024-11-19T15:37:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    37,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-09-25T17:38:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    38,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles"
                },
                "summary": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by deliberative democracy, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by deliberative democracy, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Emily Fry"
                    },
                    {
                        "name": "Narendra Edara"
                    },
                    {
                        "name": "Eric Gilbert"
                    },
                    {
                        "name": "Ceren Budak"
                    }
                ],
                "author_detail": {
                    "name": "Ceren Budak"
                },
                "author": "Ceren Budak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17213v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17213v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00028v2",
                "updated": "2024-11-19T14:29:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    14,
                    29,
                    32,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-29T04:03:15Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    3,
                    15,
                    1,
                    303,
                    0
                ],
                "title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction\n  in LBSN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction\n  in LBSN"
                },
                "summary": "The fast development of location-based social networks (LBSNs) has led to\nsignificant changes in society, resulting in popular studies of using LBSN data\nfor socioeconomic prediction, e.g., regional population and commercial activity\nestimation. Existing studies design various graphs to model heterogeneous LBSN\ndata, and further apply graph representation learning methods for socioeconomic\nprediction. However, these approaches heavily rely on heuristic ideas and\nexpertise to extract task-relevant knowledge from diverse data, which may not\nbe optimal for specific tasks. Additionally, they tend to overlook the inherent\nrelationships between different indicators, limiting the prediction accuracy.\nMotivated by the remarkable abilities of large language models (LLMs) in\ncommonsense reasoning, embedding, and multi-agent collaboration, in this work,\nwe synergize LLM agents and knowledge graph for socioeconomic prediction. We\nfirst construct a location-based knowledge graph (LBKG) to integrate\nmulti-sourced LBSN data. Then we leverage the reasoning power of LLM agent to\nidentify relevant meta-paths in the LBKG for each type of socioeconomic\nprediction task, and design a semantic-guided attention module for knowledge\nfusion with meta-paths. Moreover, we introduce a cross-task communication\nmechanism to further enhance performance by enabling knowledge sharing across\ntasks at both LLM agent and KG levels. On the one hand, the LLM agents for\ndifferent tasks collaborate to generate more diverse and comprehensive\nmeta-paths. On the other hand, the embeddings from different tasks are\nadaptively merged for better socioeconomic prediction. Experiments on two\ndatasets demonstrate the effectiveness of the synergistic design between LLM\nand KG, providing insights for information sharing across socioeconomic\nprediction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fast development of location-based social networks (LBSNs) has led to\nsignificant changes in society, resulting in popular studies of using LBSN data\nfor socioeconomic prediction, e.g., regional population and commercial activity\nestimation. Existing studies design various graphs to model heterogeneous LBSN\ndata, and further apply graph representation learning methods for socioeconomic\nprediction. However, these approaches heavily rely on heuristic ideas and\nexpertise to extract task-relevant knowledge from diverse data, which may not\nbe optimal for specific tasks. Additionally, they tend to overlook the inherent\nrelationships between different indicators, limiting the prediction accuracy.\nMotivated by the remarkable abilities of large language models (LLMs) in\ncommonsense reasoning, embedding, and multi-agent collaboration, in this work,\nwe synergize LLM agents and knowledge graph for socioeconomic prediction. We\nfirst construct a location-based knowledge graph (LBKG) to integrate\nmulti-sourced LBSN data. Then we leverage the reasoning power of LLM agent to\nidentify relevant meta-paths in the LBKG for each type of socioeconomic\nprediction task, and design a semantic-guided attention module for knowledge\nfusion with meta-paths. Moreover, we introduce a cross-task communication\nmechanism to further enhance performance by enabling knowledge sharing across\ntasks at both LLM agent and KG levels. On the one hand, the LLM agents for\ndifferent tasks collaborate to generate more diverse and comprehensive\nmeta-paths. On the other hand, the embeddings from different tasks are\nadaptively merged for better socioeconomic prediction. Experiments on two\ndatasets demonstrate the effectiveness of the synergistic design between LLM\nand KG, providing insights for information sharing across socioeconomic\nprediction tasks."
                },
                "authors": [
                    {
                        "name": "Zhilun Zhou"
                    },
                    {
                        "name": "Jingyang Fan"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Depeng Jin"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04566v3",
                "updated": "2024-11-19T13:57:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    57,
                    41,
                    1,
                    324,
                    0
                ],
                "published": "2024-04-06T09:27:04Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    9,
                    27,
                    4,
                    5,
                    97,
                    0
                ],
                "title": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering."
                },
                "authors": [
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12498v1",
                "updated": "2024-11-19T13:31:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    31,
                    53,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T13:31:53Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    31,
                    53,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus"
                },
                "summary": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$^{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$^{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$^{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$^{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH."
                },
                "authors": [
                    {
                        "name": "Terufumi Morishita"
                    },
                    {
                        "name": "Gaku Morio"
                    },
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Yasuhiro Sogawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhiro Sogawa"
                },
                "author": "Yasuhiro Sogawa",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12492v1",
                "updated": "2024-11-19T13:23:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    23,
                    52,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T13:23:52Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    23,
                    52,
                    1,
                    324,
                    0
                ],
                "title": "Recent advances in space sailing missions and technology: review of the\n  6th International Symposium on Space Sailing (ISSS 2023)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in space sailing missions and technology: review of the\n  6th International Symposium on Space Sailing (ISSS 2023)"
                },
                "summary": "The 6th International Symposium on Space Sailing (ISSS 2023) took place on\nJune 5-9, 2023 at the New York City College of Technology, the City University\nof New York. Since its inauguration in Herrsching (Germany, 2007), the ISSS has\nbeen held in New York (USA, 2010), Glasgow (UK, 2013), Kyoto (Japan, 2017) and\nAachen (Germany, 2019). During the five-day symposium, participants from 14\ncountries gathered to discuss recent advances in space sailing, investigating\nnew concepts and designs, describing innovative hardware and enabling\ntechnologies, strategies for dynamics and control, and providing updates on\ntesting results for systems under development and future mission applications.\nAs part of the 18 sessions, almost 50 oral presentations were held and,\nsubsequently, 17 papers were submitted for review and publication. This paper\naims to give an overview of all the cutting-edge technologies, detailed\nanalysis and promising results shared with the scientific community as part of\nthe event. Following the noteworthy deployment of the world's first solar sail\nIKAROS in 2010, missions like NanoSail-D2 (2011) and LightSail-2 (2019) have\nshowcased the potential of solar sailing technology through successful\ndemonstrations. Besides highlighting advancements in present and future\nprograms, the symposium was an opportunity to reflect on objectives, design and\ntest results from research centers and universities, as well as illustrate\napplications for interstellar travel, evaluate degrading performance and\nsuggest alternative solutions for known limitations. The following Symposium is\nscheduled for early summer 2025 and will be hosted by TU Delft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 6th International Symposium on Space Sailing (ISSS 2023) took place on\nJune 5-9, 2023 at the New York City College of Technology, the City University\nof New York. Since its inauguration in Herrsching (Germany, 2007), the ISSS has\nbeen held in New York (USA, 2010), Glasgow (UK, 2013), Kyoto (Japan, 2017) and\nAachen (Germany, 2019). During the five-day symposium, participants from 14\ncountries gathered to discuss recent advances in space sailing, investigating\nnew concepts and designs, describing innovative hardware and enabling\ntechnologies, strategies for dynamics and control, and providing updates on\ntesting results for systems under development and future mission applications.\nAs part of the 18 sessions, almost 50 oral presentations were held and,\nsubsequently, 17 papers were submitted for review and publication. This paper\naims to give an overview of all the cutting-edge technologies, detailed\nanalysis and promising results shared with the scientific community as part of\nthe event. Following the noteworthy deployment of the world's first solar sail\nIKAROS in 2010, missions like NanoSail-D2 (2011) and LightSail-2 (2019) have\nshowcased the potential of solar sailing technology through successful\ndemonstrations. Besides highlighting advancements in present and future\nprograms, the symposium was an opportunity to reflect on objectives, design and\ntest results from research centers and universities, as well as illustrate\napplications for interstellar travel, evaluate degrading performance and\nsuggest alternative solutions for known limitations. The following Symposium is\nscheduled for early summer 2025 and will be hosted by TU Delft."
                },
                "authors": [
                    {
                        "name": "Elena Ancona"
                    },
                    {
                        "name": "Roman Ya. Kezerashvili"
                    }
                ],
                "author_detail": {
                    "name": "Roman Ya. Kezerashvili"
                },
                "author": "Roman Ya. Kezerashvili",
                "arxiv_comment": "27 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12469v1",
                "updated": "2024-11-19T12:51:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    51,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:51:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    51,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "AI Flow at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Flow at the Network Edge"
                },
                "summary": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow."
                },
                "authors": [
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05232v2",
                "updated": "2024-11-19T12:42:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    42,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2023-11-09T09:25:37Z",
                "published_parsed": [
                    2023,
                    11,
                    9,
                    9,
                    25,
                    37,
                    3,
                    313,
                    0
                ],
                "title": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions"
                },
                "summary": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Weijiang Yu"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Zhangyin Feng"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Weihua Peng"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_doi": "10.1145/3703155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.05232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Information Systems (TOIS)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04625v3",
                "updated": "2024-11-19T12:41:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    41,
                    4,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-07T04:19:01Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    4,
                    19,
                    1,
                    4,
                    159,
                    0
                ],
                "title": "Key-Element-Informed sLLM Tuning for Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Element-Informed sLLM Tuning for Document Summarization"
                },
                "summary": "Remarkable advances in large language models (LLMs) have enabled high-quality\ntext summarization. However, this capability is currently accessible only\nthrough LLMs of substantial size or proprietary LLMs with usage fees. In\nresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have\nbeen extensively studied, yet they often suffer from missing key information\nand entities, i.e., low relevance, in particular, when input documents are\nlong. We hence propose a key-element-informed instruction tuning for\nsummarization, so-called KEITSum, which identifies key elements in documents\nand instructs sLLM to generate summaries capturing these key elements.\nExperimental results on dialogue and news datasets demonstrate that sLLM with\nKEITSum indeed provides high-quality summarization with higher relevance and\nless hallucinations, competitive to proprietary LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remarkable advances in large language models (LLMs) have enabled high-quality\ntext summarization. However, this capability is currently accessible only\nthrough LLMs of substantial size or proprietary LLMs with usage fees. In\nresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have\nbeen extensively studied, yet they often suffer from missing key information\nand entities, i.e., low relevance, in particular, when input documents are\nlong. We hence propose a key-element-informed instruction tuning for\nsummarization, so-called KEITSum, which identifies key elements in documents\nand instructs sLLM to generate summaries capturing these key elements.\nExperimental results on dialogue and news datasets demonstrate that sLLM with\nKEITSum indeed provides high-quality summarization with higher relevance and\nless hallucinations, competitive to proprietary LLM."
                },
                "authors": [
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Heejin Do"
                    },
                    {
                        "name": "Yunsu Kim"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "arxiv_comment": "Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12460v1",
                "updated": "2024-11-19T12:36:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    36,
                    2,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:36:02Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    36,
                    2,
                    1,
                    324,
                    0
                ],
                "title": "Guide-to-Explain for Controllable Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guide-to-Explain for Controllable Summarization"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nperformance in abstractive summarization tasks. However, controllable\nsummarization with LLMs remains underexplored, limiting their ability to\ngenerate summaries that align with specific user preferences. In this paper, we\nfirst investigate the capability of LLMs to control diverse attributes,\nrevealing that they encounter greater challenges with numerical attributes,\nsuch as length and extractiveness, compared to linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in explaining errors\nin the previous output. Based on this reflection, the model generates a\nwell-adjusted summary. As a result, by allowing the model to reflect on its\nmisalignment, we generate summaries that satisfy the desired attributes in\nsurprisingly fewer iterations than other iterative methods solely using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated remarkable\nperformance in abstractive summarization tasks. However, controllable\nsummarization with LLMs remains underexplored, limiting their ability to\ngenerate summaries that align with specific user preferences. In this paper, we\nfirst investigate the capability of LLMs to control diverse attributes,\nrevealing that they encounter greater challenges with numerical attributes,\nsuch as length and extractiveness, compared to linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in explaining errors\nin the previous output. Based on this reflection, the model generates a\nwell-adjusted summary. As a result, by allowing the model to reflect on its\nmisalignment, we generate summaries that satisfy the desired attributes in\nsurprisingly fewer iterations than other iterative methods solely using LLMs."
                },
                "authors": [
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Heejin Do"
                    },
                    {
                        "name": "Daehee Kim"
                    },
                    {
                        "name": "Yunsu Kim"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11410v2",
                "updated": "2024-11-19T12:25:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    25,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-18T09:30:14Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    30,
                    14,
                    0,
                    323,
                    0
                ],
                "title": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries"
                },
                "summary": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose MPDetector, for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detects\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose MPDetector, for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detects\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing."
                },
                "authors": [
                    {
                        "name": "Xiufeng Xu"
                    },
                    {
                        "name": "Fuman Xie"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Guangdong Bai"
                    },
                    {
                        "name": "Sarfraz Khurshid"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12449v1",
                "updated": "2024-11-19T12:17:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    17,
                    43,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:17:43Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    17,
                    43,
                    1,
                    324,
                    0
                ],
                "title": "\\textsc{Neon}: News Entity-Interaction Extraction for Enhanced Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textsc{Neon}: News Entity-Interaction Extraction for Enhanced Question\n  Answering"
                },
                "summary": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses."
                },
                "authors": [
                    {
                        "name": "Sneha Singhania"
                    },
                    {
                        "name": "Silviu Cucerzan"
                    },
                    {
                        "name": "Allen Herring"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12448v1",
                "updated": "2024-11-19T12:15:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    15,
                    40,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T12:15:40Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    15,
                    40,
                    1,
                    324,
                    0
                ],
                "title": "Large Language Models for Lossless Image Compression: Next-Pixel\n  Prediction in Language Space is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Lossless Image Compression: Next-Pixel\n  Prediction in Language Space is All You Need"
                },
                "summary": "We have recently witnessed that ``Intelligence\" and `` Compression\" are the\ntwo sides of the same coin, where the language large model (LLM) with\nunprecedented intelligence is a general-purpose lossless compressor for various\ndata modalities. This attribute particularly appeals to the lossless image\ncompression community, given the increasing need to compress high-resolution\nimages in the current streaming media era. Consequently, a spontaneous envision\nemerges: Can the compression performance of the LLM elevate lossless image\ncompression to new heights? However, our findings indicate that the naive\napplication of LLM-based lossless image compressors suffers from a considerable\nperformance gap compared with existing state-of-the-art (SOTA) codecs on common\nbenchmark datasets. In light of this, we are dedicated to fulfilling the\nunprecedented intelligence (compression) capacity of the LLM for lossless image\ncompression tasks, thereby bridging the gap between theoretical and practical\ncompression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel\nprediction-based LLM, which integrates various elaborated insights and\nmethodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of\nLLM, and a pixel-level semantic preservation strategy, to enhance the\nunderstanding capacity of pixel sequences for better next-pixel predictions.\nExtensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can\nbeat SOTA classical and learned codecs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have recently witnessed that ``Intelligence\" and `` Compression\" are the\ntwo sides of the same coin, where the language large model (LLM) with\nunprecedented intelligence is a general-purpose lossless compressor for various\ndata modalities. This attribute particularly appeals to the lossless image\ncompression community, given the increasing need to compress high-resolution\nimages in the current streaming media era. Consequently, a spontaneous envision\nemerges: Can the compression performance of the LLM elevate lossless image\ncompression to new heights? However, our findings indicate that the naive\napplication of LLM-based lossless image compressors suffers from a considerable\nperformance gap compared with existing state-of-the-art (SOTA) codecs on common\nbenchmark datasets. In light of this, we are dedicated to fulfilling the\nunprecedented intelligence (compression) capacity of the LLM for lossless image\ncompression tasks, thereby bridging the gap between theoretical and practical\ncompression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel\nprediction-based LLM, which integrates various elaborated insights and\nmethodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of\nLLM, and a pixel-level semantic preservation strategy, to enhance the\nunderstanding capacity of pixel sequences for better next-pixel predictions.\nExtensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can\nbeat SOTA classical and learned codecs."
                },
                "authors": [
                    {
                        "name": "Kecheng Chen"
                    },
                    {
                        "name": "Pingping Zhang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Yibing Liu"
                    },
                    {
                        "name": "Jixin Huang"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Hong Yan"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04793v2",
                "updated": "2024-11-19T10:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    59,
                    30,
                    1,
                    324,
                    0
                ],
                "published": "2024-05-08T03:57:45Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    3,
                    57,
                    45,
                    2,
                    129,
                    0
                ],
                "title": "Zero-shot LLM-guided Counterfactual Generation: A Case Study on NLP\n  Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot LLM-guided Counterfactual Generation: A Case Study on NLP\n  Model Evaluation"
                },
                "summary": "With the development and proliferation of large, complex, black-box models\nfor solving many natural language processing (NLP) tasks, there is also an\nincreasing necessity of methods to stress-test these models and provide some\ndegree of interpretability or explainability. While counterfactual examples are\nuseful in this regard, automated generation of counterfactuals is a data and\nresource intensive process. such methods depend on models such as pre-trained\nlanguage models that are then fine-tuned on auxiliary, often task-specific\ndatasets, that may be infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the possibility of\nleveraging large language models (LLMs) for zero-shot counterfactual generation\nin order to stress-test NLP models. We propose a structured pipeline to\nfacilitate this generation, and we hypothesize that the instruction-following\nand textual understanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-shot manner,\nwithout requiring any training or fine-tuning. Through comprehensive\nexperiments on a variety of propreitary and open-source LLMs, along with\nvarious downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development and proliferation of large, complex, black-box models\nfor solving many natural language processing (NLP) tasks, there is also an\nincreasing necessity of methods to stress-test these models and provide some\ndegree of interpretability or explainability. While counterfactual examples are\nuseful in this regard, automated generation of counterfactuals is a data and\nresource intensive process. such methods depend on models such as pre-trained\nlanguage models that are then fine-tuned on auxiliary, often task-specific\ndatasets, that may be infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the possibility of\nleveraging large language models (LLMs) for zero-shot counterfactual generation\nin order to stress-test NLP models. We propose a structured pipeline to\nfacilitate this generation, and we hypothesize that the instruction-following\nand textual understanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-shot manner,\nwithout requiring any training or fine-tuning. Through comprehensive\nexperiments on a variety of propreitary and open-source LLMs, along with\nvarious downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models."
                },
                "authors": [
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Raha Moraffah"
                    },
                    {
                        "name": "Joshua Garland"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Longer version of short paper accepted at IEEE BigData 2024 (Main\n  Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12402v1",
                "updated": "2024-11-19T10:38:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    38,
                    57,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T10:38:57Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    38,
                    57,
                    1,
                    324,
                    0
                ],
                "title": "A total-shear-stress-conserved wall model for large-eddy simulation of\n  high-Reynolds number wall turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A total-shear-stress-conserved wall model for large-eddy simulation of\n  high-Reynolds number wall turbulence"
                },
                "summary": "Wall-modeled large-eddy simulation (WMLES) is widely recognized as a useful\nmethod for simulation of turbulent flows at high Reynolds numbers.\nNevertheless, a continual issue in different wall models is the shift of the\nmean velocity profile from the wall-model/RANS (Reynolds-averaged\nNavier-Stokes) region to the LES region. This phenomenon, referred to as\nlogarithmic layer mismatch (LLM), occurs in both wall shear stress models and\nhybrid RANS/LES models. Many efforts have been made to explain and resolve this\nmismatch, including decreasing the high correlation between the wall shear\nstress and the velocity at the matching layer, modifying the subgrid-scale\n(SGS) eddy viscosity, and adding a stochastic forcing. It is widely believed\nthat the inclusion of the resolved Reynolds shear stress (or the convection\nterm) is essential to elliminate the LLM, as it prevents the overseimation of\nthe modeled Reynolds shear stress and promotes the generation of the\nsmall-scale flow structures in the near-wall region. In this work, by comparing\nthree different SGS eddy viscosity models, we demonstrate that ensuring the\ntotal shear stress conservation (TSSC) conservation is key to resolving the\nLLM. Under the TSSC framework, the effect of the convection term on LLM can be\nquantitatively assessed. Furthermore, a modified SGS eddy viscosity modfication\nmodel that adheres to the TSSC constraint is tested at different Reynolds\nnumbers ($Re_\\tau=1000, 2000, 4200$). Our results demonstrate the robust\nperformance of the present model in predicting skin friction and low-order\nturbulence statistics, even under a relatively low grid resolution ($\\Delta\nx^+, \\Delta z^+ \\lesssim 500$, $2\\leq \\Delta_x/\\Delta_{y,mat} \\leq 4$, where\n$\\Delta_{y,mat}$ is the wall-normal grid spacing in the wall-model region).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wall-modeled large-eddy simulation (WMLES) is widely recognized as a useful\nmethod for simulation of turbulent flows at high Reynolds numbers.\nNevertheless, a continual issue in different wall models is the shift of the\nmean velocity profile from the wall-model/RANS (Reynolds-averaged\nNavier-Stokes) region to the LES region. This phenomenon, referred to as\nlogarithmic layer mismatch (LLM), occurs in both wall shear stress models and\nhybrid RANS/LES models. Many efforts have been made to explain and resolve this\nmismatch, including decreasing the high correlation between the wall shear\nstress and the velocity at the matching layer, modifying the subgrid-scale\n(SGS) eddy viscosity, and adding a stochastic forcing. It is widely believed\nthat the inclusion of the resolved Reynolds shear stress (or the convection\nterm) is essential to elliminate the LLM, as it prevents the overseimation of\nthe modeled Reynolds shear stress and promotes the generation of the\nsmall-scale flow structures in the near-wall region. In this work, by comparing\nthree different SGS eddy viscosity models, we demonstrate that ensuring the\ntotal shear stress conservation (TSSC) conservation is key to resolving the\nLLM. Under the TSSC framework, the effect of the convection term on LLM can be\nquantitatively assessed. Furthermore, a modified SGS eddy viscosity modfication\nmodel that adheres to the TSSC constraint is tested at different Reynolds\nnumbers ($Re_\\tau=1000, 2000, 4200$). Our results demonstrate the robust\nperformance of the present model in predicting skin friction and low-order\nturbulence statistics, even under a relatively low grid resolution ($\\Delta\nx^+, \\Delta z^+ \\lesssim 500$, $2\\leq \\Delta_x/\\Delta_{y,mat} \\leq 4$, where\n$\\Delta_{y,mat}$ is the wall-normal grid spacing in the wall-model region)."
                },
                "authors": [
                    {
                        "name": "Huan-Cong Liu"
                    },
                    {
                        "name": "Chun-Xiao Xu"
                    },
                    {
                        "name": "Wei-Xi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Xi Huang"
                },
                "author": "Wei-Xi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12395v1",
                "updated": "2024-11-19T10:27:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    27,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T10:27:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    27,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Do LLMs Understand Ambiguity in Text? A Case Study in Open-world\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Ambiguity in Text? A Case Study in Open-world\n  Question Answering"
                },
                "summary": "Ambiguity in natural language poses significant challenges to Large Language\nModels (LLMs) used for open-domain question answering. LLMs often struggle with\nthe inherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and biased responses.\nThis significantly weakens their ability to be used for tasks like\nfact-checking, question answering, feature extraction, and sentiment analysis.\nUsing open-domain question answering as a test case, we compare off-the-shelf\nand few-shot LLM performance, focusing on measuring the impact of explicit\ndisambiguation strategies. We demonstrate how simple, training-free,\ntoken-level disambiguation methods may be effectively used to improve LLM\nperformance for ambiguous question answering tasks. We empirically show our\nfindings and discuss best practices and broader impacts regarding ambiguity in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in natural language poses significant challenges to Large Language\nModels (LLMs) used for open-domain question answering. LLMs often struggle with\nthe inherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and biased responses.\nThis significantly weakens their ability to be used for tasks like\nfact-checking, question answering, feature extraction, and sentiment analysis.\nUsing open-domain question answering as a test case, we compare off-the-shelf\nand few-shot LLM performance, focusing on measuring the impact of explicit\ndisambiguation strategies. We demonstrate how simple, training-free,\ntoken-level disambiguation methods may be effectively used to improve LLM\nperformance for ambiguous question answering tasks. We empirically show our\nfindings and discuss best practices and broader impacts regarding ambiguity in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Aryan Keluskar"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Accepted at the REU Symposium at IEEE BigData 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12594v3",
                "updated": "2024-11-19T10:05:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    5,
                    7,
                    1,
                    324,
                    0
                ],
                "published": "2024-01-23T09:48:15Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    9,
                    48,
                    15,
                    1,
                    23,
                    0
                ],
                "title": "SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification,\n  and Learning Analytics to Train Cybersecurity Competencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification,\n  and Learning Analytics to Train Cybersecurity Competencies"
                },
                "summary": "It is undeniable that we are witnessing an unprecedented digital revolution.\nHowever, recent years have been characterized by the explosion of cyberattacks,\nmaking cybercrime one of the most profitable businesses on the planet. That is\nwhy training in cybersecurity is increasingly essential to protect the assets\nof cyberspace. One of the most vital tools to train cybersecurity competencies\nis the Cyber Range, a virtualized environment that simulates realistic\nnetworks. The paper at hand introduces SCORPION, a fully functional and\nvirtualized Cyber Range, which manages the authoring and automated deployment\nof scenarios. In addition, SCORPION includes several elements to improve\nstudent motivation, such as a gamification system with medals, points, or\nrankings, among other elements. Such a gamification system includes an adaptive\nlearning module that is able to adapt the cyberexercise based on the users'\nperformance. Moreover, SCORPION leverages learning analytics that collects and\nprocesses telemetric and biometric user data, including heart rate through a\nsmartwatch, which is available through a dashboard for instructors. Finally, we\ndeveloped a case study where SCORPION obtained 82.10% in usability and 4.57 out\nof 5 in usefulness from the viewpoint of a student and an instructor. The\npositive evaluation results are promising, indicating that SCORPION can become\nan effective, motivating, and advanced cybersecurity training tool to help fill\ncurrent gaps in this context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is undeniable that we are witnessing an unprecedented digital revolution.\nHowever, recent years have been characterized by the explosion of cyberattacks,\nmaking cybercrime one of the most profitable businesses on the planet. That is\nwhy training in cybersecurity is increasingly essential to protect the assets\nof cyberspace. One of the most vital tools to train cybersecurity competencies\nis the Cyber Range, a virtualized environment that simulates realistic\nnetworks. The paper at hand introduces SCORPION, a fully functional and\nvirtualized Cyber Range, which manages the authoring and automated deployment\nof scenarios. In addition, SCORPION includes several elements to improve\nstudent motivation, such as a gamification system with medals, points, or\nrankings, among other elements. Such a gamification system includes an adaptive\nlearning module that is able to adapt the cyberexercise based on the users'\nperformance. Moreover, SCORPION leverages learning analytics that collects and\nprocesses telemetric and biometric user data, including heart rate through a\nsmartwatch, which is available through a dashboard for instructors. Finally, we\ndeveloped a case study where SCORPION obtained 82.10% in usability and 4.57 out\nof 5 in usefulness from the viewpoint of a student and an instructor. The\npositive evaluation results are promising, indicating that SCORPION can become\nan effective, motivating, and advanced cybersecurity training tool to help fill\ncurrent gaps in this context."
                },
                "authors": [
                    {
                        "name": "Pantaleone Nespoli"
                    },
                    {
                        "name": "Mariano Albaladejo-Gonzlez"
                    },
                    {
                        "name": "Jos A. Ruiprez-Valiente"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v4",
                "updated": "2024-11-19T10:00:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    10,
                    0,
                    41,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ishneet Sukhvinder Singh"
                    },
                    {
                        "name": "Ritvik Aggarwal"
                    },
                    {
                        "name": "Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12357v1",
                "updated": "2024-11-19T09:18:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    18,
                    20,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T09:18:20Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    18,
                    20,
                    1,
                    324,
                    0
                ],
                "title": "A Layered Architecture for Developing and Enhancing Capabilities in\n  Large Language Model-based Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Layered Architecture for Developing and Enhancing Capabilities in\n  Large Language Model-based Software Systems"
                },
                "summary": "Significant efforts has been made to expand the use of Large Language Models\n(LLMs) beyond basic language tasks. While the generalizability and versatility\nof LLMs have enabled widespread adoption, evolving demands in application\ndevelopment often exceed their native capabilities. Meeting these demands may\ninvolve a diverse set of methods, such as enhancing creativity through either\ninference temperature adjustments or creativity-provoking prompts. Selecting\nthe right approach is critical, as different methods lead to trade-offs in\nengineering complexity, scalability, and operational costs. This paper\nintroduces a layered architecture that organizes LLM software system\ndevelopment into distinct layers, each characterized by specific attributes. By\naligning capabilities with these layers, the framework encourages the\nsystematic implementation of capabilities in effective and efficient ways that\nultimately supports desired functionalities and qualities. Through practical\ncase studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based\nsoftware system development, promoting robustness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant efforts has been made to expand the use of Large Language Models\n(LLMs) beyond basic language tasks. While the generalizability and versatility\nof LLMs have enabled widespread adoption, evolving demands in application\ndevelopment often exceed their native capabilities. Meeting these demands may\ninvolve a diverse set of methods, such as enhancing creativity through either\ninference temperature adjustments or creativity-provoking prompts. Selecting\nthe right approach is critical, as different methods lead to trade-offs in\nengineering complexity, scalability, and operational costs. This paper\nintroduces a layered architecture that organizes LLM software system\ndevelopment into distinct layers, each characterized by specific attributes. By\naligning capabilities with these layers, the framework encourages the\nsystematic implementation of capabilities in effective and efficient ways that\nultimately supports desired functionalities and qualities. Through practical\ncase studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based\nsoftware system development, promoting robustness and scalability."
                },
                "authors": [
                    {
                        "name": "Dawen Zhang"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Robert Mao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mao"
                },
                "author": "Robert Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12355v1",
                "updated": "2024-11-19T09:16:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    16,
                    54,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T09:16:54Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    16,
                    54,
                    1,
                    324,
                    0
                ],
                "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video\n  Understanding"
                },
                "summary": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge in LLM-based video understanding lies in preserving visual and\nsemantic information in long videos while maintaining a memory-affordable token\ncount. However, redundancy and correspondence in videos have hindered the\nperformance potential of existing methods. Through statistical learning on\ncurrent datasets, we observe that redundancy occurs in both repeated and\nanswer-irrelevant frames, and the corresponding frames vary with different\nquestions. This suggests the possibility of adopting dynamic encoding to\nbalance detailed video information preservation with token budget reduction. To\nthis end, we propose a dynamic cooperative network, DynFocus, for\nmemory-efficient video encoding in this paper. Specifically, i) a Dynamic Event\nPrototype Estimation (DPE) module to dynamically select meaningful frames for\nquestion answering; (ii) a Compact Cooperative Encoding (CCE) module that\nencodes meaningful frames with detailed visual appearance and the remaining\nframes with sketchy perception separately. We evaluate our method on five\npublicly available benchmarks, and experimental results consistently\ndemonstrate that our method achieves competitive performance."
                },
                "authors": [
                    {
                        "name": "Yudong Han"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Liyuan Pan"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12352v1",
                "updated": "2024-11-19T09:15:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    15,
                    8,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T09:15:08Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    15,
                    8,
                    1,
                    324,
                    0
                ],
                "title": "Perfecting Imperfect Physical Neural Networks with Transferable\n  Robustness using Sharpness-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perfecting Imperfect Physical Neural Networks with Transferable\n  Robustness using Sharpness-Aware Training"
                },
                "summary": "AI models are essential in science and engineering, but recent advances are\npushing the limits of traditional digital hardware. To address these\nlimitations, physical neural networks (PNNs), which use physical substrates for\ncomputation, have gained increasing attention. However, developing effective\ntraining methods for PNNs remains a significant challenge. Current approaches,\nregardless of offline and online training, suffer from significant accuracy\nloss. Offline training is hindered by imprecise modeling, while online training\nyields device-specific models that can't be transferred to other devices due to\nmanufacturing variances. Both methods face challenges from perturbations after\ndeployment, such as thermal drift or alignment errors, which make trained\nmodels invalid and require retraining. Here, we address the challenges with\nboth offline and online training through a novel technique called\nSharpness-Aware Training (SAT), where we innovatively leverage the geometry of\nthe loss landscape to tackle the problems in training physical systems. SAT\nenables accurate training using efficient backpropagation algorithms, even with\nimprecise models. PNNs trained by SAT offline even outperform those trained\nonline, despite modeling and fabrication errors. SAT also overcomes online\ntraining limitations by enabling reliable transfer of models between devices.\nFinally, SAT is highly resilient to perturbations after deployment, allowing\nPNNs to continuously operate accurately under perturbations without retraining.\nWe demonstrate SAT across three types of PNNs, showing it is universally\napplicable, regardless of whether the models are explicitly known. This work\noffers a transformative, efficient approach to training PNNs, addressing\ncritical challenges in analog computing and enabling real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models are essential in science and engineering, but recent advances are\npushing the limits of traditional digital hardware. To address these\nlimitations, physical neural networks (PNNs), which use physical substrates for\ncomputation, have gained increasing attention. However, developing effective\ntraining methods for PNNs remains a significant challenge. Current approaches,\nregardless of offline and online training, suffer from significant accuracy\nloss. Offline training is hindered by imprecise modeling, while online training\nyields device-specific models that can't be transferred to other devices due to\nmanufacturing variances. Both methods face challenges from perturbations after\ndeployment, such as thermal drift or alignment errors, which make trained\nmodels invalid and require retraining. Here, we address the challenges with\nboth offline and online training through a novel technique called\nSharpness-Aware Training (SAT), where we innovatively leverage the geometry of\nthe loss landscape to tackle the problems in training physical systems. SAT\nenables accurate training using efficient backpropagation algorithms, even with\nimprecise models. PNNs trained by SAT offline even outperform those trained\nonline, despite modeling and fabrication errors. SAT also overcomes online\ntraining limitations by enabling reliable transfer of models between devices.\nFinally, SAT is highly resilient to perturbations after deployment, allowing\nPNNs to continuously operate accurately under perturbations without retraining.\nWe demonstrate SAT across three types of PNNs, showing it is universally\napplicable, regardless of whether the models are explicitly known. This work\noffers a transformative, efficient approach to training PNNs, addressing\ncritical challenges in analog computing and enabling real-world deployment."
                },
                "authors": [
                    {
                        "name": "Tengji Xu"
                    },
                    {
                        "name": "Zeyu Luo"
                    },
                    {
                        "name": "Shaojie Liu"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Qiarong Xiao"
                    },
                    {
                        "name": "Benshan Wang"
                    },
                    {
                        "name": "Dongliang Wang"
                    },
                    {
                        "name": "Chaoran Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chaoran Huang"
                },
                "author": "Chaoran Huang",
                "arxiv_comment": "24 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06275v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06275v4",
                "updated": "2024-11-19T09:06:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    9,
                    6,
                    33,
                    1,
                    324,
                    0
                ],
                "published": "2023-09-12T14:36:23Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    14,
                    36,
                    23,
                    1,
                    255,
                    0
                ],
                "title": "Re-Reading Improves Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-Reading Improves Reasoning in Large Language Models"
                },
                "summary": "To enhance the reasoning capabilities of off-the-shelf Large Language Models\n(LLMs), we introduce a simple, yet general and effective prompting method, Re2,\ni.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most\nthought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim\nto elicit the reasoning process in the output, Re2 shifts the focus to the\ninput by processing questions twice, thereby enhancing the understanding\nprocess. Consequently, Re2 demonstrates strong generality and compatibility\nwith most thought-eliciting prompting methods, including CoT. Crucially, Re2\nfacilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs\nbecause the first pass could provide global information for the second pass. We\nbegin with a preliminary empirical study as the foundation of Re2, illustrating\nits potential to enable \"bidirectional\" attention mechanisms. We then evaluate\nRe2 on extensive reasoning benchmarks across 14 datasets, spanning 112\nexperiments, to validate its effectiveness and generality. Our findings\nindicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2\nconsistently enhances the reasoning performance of LLMs through a simple\nre-reading strategy. Further analyses reveal Re2's adaptability, showing how it\ncan be effectively integrated with different LLMs, thought-eliciting prompting,\nand ensemble strategies. Our code is available at\n\\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the reasoning capabilities of off-the-shelf Large Language Models\n(LLMs), we introduce a simple, yet general and effective prompting method, Re2,\ni.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most\nthought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim\nto elicit the reasoning process in the output, Re2 shifts the focus to the\ninput by processing questions twice, thereby enhancing the understanding\nprocess. Consequently, Re2 demonstrates strong generality and compatibility\nwith most thought-eliciting prompting methods, including CoT. Crucially, Re2\nfacilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs\nbecause the first pass could provide global information for the second pass. We\nbegin with a preliminary empirical study as the foundation of Re2, illustrating\nits potential to enable \"bidirectional\" attention mechanisms. We then evaluate\nRe2 on extensive reasoning benchmarks across 14 datasets, spanning 112\nexperiments, to validate its effectiveness and generality. Our findings\nindicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2\nconsistently enhances the reasoning performance of LLMs through a simple\nre-reading strategy. Further analyses reveal Re2's adaptability, showing how it\ncan be effectively integrated with different LLMs, thought-eliciting prompting,\nand ensemble strategies. Our code is available at\n\\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}"
                },
                "authors": [
                    {
                        "name": "Xiaohan Xu"
                    },
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Hongbo Xu"
                    },
                    {
                        "name": "Guodong Long"
                    },
                    {
                        "name": "Jian-guang Lou"
                    },
                    {
                        "name": "Shuai Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Ma"
                },
                "author": "Shuai Ma",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.06275v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06275v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05085v2",
                "updated": "2024-11-19T08:46:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    46,
                    34,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-07T16:59:38Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    59,
                    38,
                    4,
                    159,
                    0
                ],
                "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs"
                },
                "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets that we release\nonline, and real-world use cases to demonstrate MRAG's effectiveness, showing\nimprovements of up to 20% in relevance over standard RAG baselines. MRAG can be\nseamlessly integrated with existing RAG frameworks and benchmarking tools like\nRAGAS as well as different classes of data stores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets that we release\nonline, and real-world use cases to demonstrate MRAG's effectiveness, showing\nimprovements of up to 20% in relevance over standard RAG baselines. MRAG can be\nseamlessly integrated with existing RAG frameworks and benchmarking tools like\nRAGAS as well as different classes of data stores."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Roman Niggli"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Lucas Weitzendorf"
                    },
                    {
                        "name": "Mingyuan Chi"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Joanna Gajda"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Micha Podstawski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12319v1",
                "updated": "2024-11-19T08:23:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    23,
                    52,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T08:23:52Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    23,
                    52,
                    1,
                    324,
                    0
                ],
                "title": "CLIP Unreasonable Potential in Single-Shot Face Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP Unreasonable Potential in Single-Shot Face Recognition"
                },
                "summary": "Face recognition is a core task in computer vision designed to identify and\nauthenticate individuals by analyzing facial patterns and features. This field\nintersects with artificial intelligence image processing and machine learning\nwith applications in security authentication and personalization. Traditional\napproaches in facial recognition focus on capturing facial features like the\neyes, nose and mouth and matching these against a database to verify identities\nHowever challenges such as high false positive rates have persisted often due\nto the similarity among individuals facial features. Recently Contrastive\nLanguage Image Pretraining (CLIP) a model developed by OpenAI has shown\npromising advancements by linking natural language processing with vision tasks\nallowing it to generalize across modalities. Using CLIP's vision language\ncorrespondence and single-shot finetuning the model can achieve lower false\npositive rates upon deployment without the need of mass facial features\nextraction. This integration demonstrating CLIP's potential to address\npersistent issues in face recognition model performance without complicating\nour training paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face recognition is a core task in computer vision designed to identify and\nauthenticate individuals by analyzing facial patterns and features. This field\nintersects with artificial intelligence image processing and machine learning\nwith applications in security authentication and personalization. Traditional\napproaches in facial recognition focus on capturing facial features like the\neyes, nose and mouth and matching these against a database to verify identities\nHowever challenges such as high false positive rates have persisted often due\nto the similarity among individuals facial features. Recently Contrastive\nLanguage Image Pretraining (CLIP) a model developed by OpenAI has shown\npromising advancements by linking natural language processing with vision tasks\nallowing it to generalize across modalities. Using CLIP's vision language\ncorrespondence and single-shot finetuning the model can achieve lower false\npositive rates upon deployment without the need of mass facial features\nextraction. This integration demonstrating CLIP's potential to address\npersistent issues in face recognition model performance without complicating\nour training paradigm."
                },
                "authors": [
                    {
                        "name": "Nhan T. Luu"
                    }
                ],
                "author_detail": {
                    "name": "Nhan T. Luu"
                },
                "author": "Nhan T. Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01285v2",
                "updated": "2024-11-19T08:08:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    8,
                    38,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-02T07:14:26Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    7,
                    14,
                    26,
                    2,
                    276,
                    0
                ],
                "title": "Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration"
                },
                "summary": "The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral."
                },
                "authors": [
                    {
                        "name": "Kangxi Wu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to the EMNLP 2024 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12307v1",
                "updated": "2024-11-19T07:48:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    48,
                    35,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:48:35Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    48,
                    35,
                    1,
                    324,
                    0
                ],
                "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification\n  for LLM-Powered Dialog Systems in Production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification\n  for LLM-Powered Dialog Systems in Production"
                },
                "summary": "Accurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity of\ncomprehensive datasets and the complexity of contextual dependencies across\ndialogue turns hinder progress. This paper presents two novel approaches\nleveraging Large Language Models (LLMs) to enhance scalability and reduce\nlatency in production dialogue systems. First, we introduce Symbol Tuning,\nwhich simplifies intent labels to reduce task complexity and improve\nperformance in multi-turn dialogues. Second, we propose C-LARA\n(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework\nthat employs LLMs for data augmentation and pseudo-labeling to generate\nsynthetic multi-turn dialogues. These enriched datasets are used to fine-tune a\nsmall, efficient model suitable for deployment. Experiments conducted on\nmultilingual dialogue datasets demonstrate significant improvements in\nclassification accuracy and resource efficiency. Our methods enhance multi-turn\nintent classification accuracy by 5.09%, reduce annotation costs by 40%, and\nenable scalable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity of\ncomprehensive datasets and the complexity of contextual dependencies across\ndialogue turns hinder progress. This paper presents two novel approaches\nleveraging Large Language Models (LLMs) to enhance scalability and reduce\nlatency in production dialogue systems. First, we introduce Symbol Tuning,\nwhich simplifies intent labels to reduce task complexity and improve\nperformance in multi-turn dialogues. Second, we propose C-LARA\n(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework\nthat employs LLMs for data augmentation and pseudo-labeling to generate\nsynthetic multi-turn dialogues. These enriched datasets are used to fine-tune a\nsmall, efficient model suitable for deployment. Experiments conducted on\nmultilingual dialogue datasets demonstrate significant improvements in\nclassification accuracy and resource efficiency. Our methods enhance multi-turn\nintent classification accuracy by 5.09%, reduce annotation costs by 40%, and\nenable scalable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15000v3",
                "updated": "2024-11-19T07:46:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    46,
                    16,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-22T22:28:46Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    22,
                    28,
                    46,
                    3,
                    53,
                    0
                ],
                "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide-or-Conquer? Which Part Should You Distill Your LLM?"
                },
                "summary": "Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation."
                },
                "authors": [
                    {
                        "name": "Zhuofeng Wu"
                    },
                    {
                        "name": "He Bai"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "VG Vinod Vydiswaran"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
                "arxiv_journal_ref": "2024.findings-emnlp.145",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02136v2",
                "updated": "2024-11-19T07:42:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    42,
                    15,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-03T12:53:17Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    12,
                    53,
                    17,
                    5,
                    34,
                    0
                ],
                "title": "User Intent Recognition and Satisfaction with Large Language Models: A\n  User Study with ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Intent Recognition and Satisfaction with Large Language Models: A\n  User Study with ChatGPT"
                },
                "summary": "The rapid evolution of LLMs represents an impactful paradigm shift in digital\ninteraction and content engagement. While they encode vast amounts of\nhuman-generated knowledge and excel in processing diverse data types, they\noften face the challenge of accurately responding to specific user intents,\nleading to user dissatisfaction. Based on a fine-grained intent taxonomy and\nintent-based prompt reformulations, we analyze the quality of intent\nrecognition and user satisfaction with answers from intent-based prompt\nreformulations of GPT-3.5 Turbo and GPT-4 Turbo models. Our study highlights\nthe importance of human-AI interaction and underscores the need for\ninterdisciplinary approaches to improve conversational AI systems. We show that\nGPT-4 outperforms GPT-3.5 in recognizing common intents but is often\noutperformed by GPT-3.5 in recognizing less frequent intents. Moreover,\nwhenever the user intent is correctly recognized, while users are more\nsatisfied with the intent-based reformulations of GPT-4 compared to GPT-3.5,\nthey tend to be more satisfied with the models' answers to their original\nprompts compared to the reformulated ones. The collected data from our study\nhas been made publicly available on GitHub\n(https://github.com/ConcealedIDentity/UserIntentStudy) for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of LLMs represents an impactful paradigm shift in digital\ninteraction and content engagement. While they encode vast amounts of\nhuman-generated knowledge and excel in processing diverse data types, they\noften face the challenge of accurately responding to specific user intents,\nleading to user dissatisfaction. Based on a fine-grained intent taxonomy and\nintent-based prompt reformulations, we analyze the quality of intent\nrecognition and user satisfaction with answers from intent-based prompt\nreformulations of GPT-3.5 Turbo and GPT-4 Turbo models. Our study highlights\nthe importance of human-AI interaction and underscores the need for\ninterdisciplinary approaches to improve conversational AI systems. We show that\nGPT-4 outperforms GPT-3.5 in recognizing common intents but is often\noutperformed by GPT-3.5 in recognizing less frequent intents. Moreover,\nwhenever the user intent is correctly recognized, while users are more\nsatisfied with the intent-based reformulations of GPT-4 compared to GPT-3.5,\nthey tend to be more satisfied with the models' answers to their original\nprompts compared to the reformulated ones. The collected data from our study\nhas been made publicly available on GitHub\n(https://github.com/ConcealedIDentity/UserIntentStudy) for further research."
                },
                "authors": [
                    {
                        "name": "Anna Bodonhelyi"
                    },
                    {
                        "name": "Efe Bozkir"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06145v2",
                "updated": "2024-11-19T07:19:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    19,
                    27,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-09T11:13:14Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    11,
                    13,
                    14,
                    5,
                    314,
                    0
                ],
                "title": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era"
                },
                "summary": "In recent years, Large Language Models (LLMs) have significantly improved\nautomated code translation, often achieving over 80% accuracy on existing\nbenchmarks. However, most of these benchmarks consist of short, standalone,\nalgorithmic samples that do not reflect practical coding tasks. To address this\ngap, we introduce ClassEval-T, a class-level code translation benchmark\ndesigned to assess LLM performance on real-world coding scenarios. Built upon\nClassEval, a class-level Python code generation benchmark covering topics such\nas database operations and game design, ClassEval-T extends into Java and C++\nwith complete code samples and test suites, requiring 360 person-hours for\nmanual migration. We propose three translation strategies (holistic,\nmin-dependency, and standalone) and evaluate six recent LLMs across various\nfamilies and sizes on ClassEval-T. Results reveal a significant performance\ndrop compared to method-level benchmarks, highlighting discrepancies among LLMs\nand demonstrating ClassEval-T's effectiveness. We further analyze LLMs'\ndependency awareness in translating class samples and categorize 1,397 failure\ncases by the best-performing LLM for practical insights and future improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have significantly improved\nautomated code translation, often achieving over 80% accuracy on existing\nbenchmarks. However, most of these benchmarks consist of short, standalone,\nalgorithmic samples that do not reflect practical coding tasks. To address this\ngap, we introduce ClassEval-T, a class-level code translation benchmark\ndesigned to assess LLM performance on real-world coding scenarios. Built upon\nClassEval, a class-level Python code generation benchmark covering topics such\nas database operations and game design, ClassEval-T extends into Java and C++\nwith complete code samples and test suites, requiring 360 person-hours for\nmanual migration. We propose three translation strategies (holistic,\nmin-dependency, and standalone) and evaluate six recent LLMs across various\nfamilies and sizes on ClassEval-T. Results reveal a significant performance\ndrop compared to method-level benchmarks, highlighting discrepancies among LLMs\nand demonstrating ClassEval-T's effectiveness. We further analyze LLMs'\ndependency awareness in translating class samples and categorize 1,397 failure\ncases by the best-performing LLM for practical insights and future improvement."
                },
                "authors": [
                    {
                        "name": "Pengyu Xue"
                    },
                    {
                        "name": "Linhao Wu"
                    },
                    {
                        "name": "Chengyi Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Ruikai Jin"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Yifei Pei"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Xiran Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Xiran Lyu"
                },
                "author": "Xiran Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12289v1",
                "updated": "2024-11-19T07:18:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    18,
                    51,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:18:51Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    18,
                    51,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing UX Research Activities Using GenAI -- Potential Applications\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing UX Research Activities Using GenAI -- Potential Applications\n  and Challenges"
                },
                "summary": "User Experience (UX) Research covers various methods for gathering the users'\nsubjective impressions of a product. For this, practitioners face different\nactivities and tasks related to the research process. This includes processing\na large amount of data based on qualitative and quantitative data. However,\nthis can be very laborious in practice. Thus, the application of GenAI can\nsupport UX research activities. This paper provides a practical perspective on\nthis topic. Based on previous studies, we present different use cases\nindicating the potential of GenAI in UX research. Moreover, we provide insights\ninto an exploratory study using GenAI along an entire UX research process.\nResults show that Large Language Models (LLMs) are useful for various tasks.\nThus, the research activities can be carried out more efficiently. However, the\nresearcher should always review results to ensure quality. In summary, we want\nto express the potential of GenAI enhancing UX research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Experience (UX) Research covers various methods for gathering the users'\nsubjective impressions of a product. For this, practitioners face different\nactivities and tasks related to the research process. This includes processing\na large amount of data based on qualitative and quantitative data. However,\nthis can be very laborious in practice. Thus, the application of GenAI can\nsupport UX research activities. This paper provides a practical perspective on\nthis topic. Based on previous studies, we present different use cases\nindicating the potential of GenAI in UX research. Moreover, we provide insights\ninto an exploratory study using GenAI along an entire UX research process.\nResults show that Large Language Models (LLMs) are useful for various tasks.\nThus, the research activities can be carried out more efficiently. However, the\nresearcher should always review results to ensure quality. In summary, we want\nto express the potential of GenAI enhancing UX research"
                },
                "authors": [
                    {
                        "name": "Stefan Graser"
                    },
                    {
                        "name": "Anastasia Snimshchikova"
                    },
                    {
                        "name": "Martin Schrepp"
                    },
                    {
                        "name": "Stephan Bhm"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Bhm"
                },
                "author": "Stephan Bhm",
                "arxiv_comment": "13 pages, 4 figures, CENTRIC 2024 : The Seventeenth International\n  Conference on Advances in Human-oriented and Personalized Mechanisms,\n  Technologies, and Services",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12286v1",
                "updated": "2024-11-19T07:12:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    12,
                    48,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:12:48Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    12,
                    48,
                    1,
                    324,
                    0
                ],
                "title": "GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for\n  Task-Oriented Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for\n  Task-Oriented Grasping"
                },
                "summary": "Inferring affordable (i.e., graspable) parts of arbitrary objects based on\nhuman specifications is essential for robots advancing toward open-vocabulary\nmanipulation. Current grasp planners, however, are hindered by limited\nvision-language comprehension and time-consuming 3D radiance modeling,\nrestricting real-time, open-vocabulary interactions with objects. To address\nthese limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary\nAffordance Reasoning framework, which fine-tunes the Large Language Models\n(LLMs) to predict visual affordance of graspable object parts within RGB\nfeature space. We compile a dataset of over 10,000 images from human-object\ninteractions, annotated with unified visual and linguistic affordance labels,\nto enable multi-modal fine-tuning. GLOVER inherits world knowledge and\ncommon-sense reasoning from LLMs, facilitating more fine-grained object\nunderstanding and sophisticated tool-use reasoning. To enable effective\nreal-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a\nnon-parametric grasp planner that aligns the gripper pose with a superquadric\nsurface derived from affordance data. In evaluations across 30 real-world\nscenes, GLOVER achieves success rates of 86.0% in part identification and 76.3%\nin grasping, with speeds approximately 330 times faster in affordance reasoning\nand 40 times faster in grasping pose estimation than the previous\nstate-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring affordable (i.e., graspable) parts of arbitrary objects based on\nhuman specifications is essential for robots advancing toward open-vocabulary\nmanipulation. Current grasp planners, however, are hindered by limited\nvision-language comprehension and time-consuming 3D radiance modeling,\nrestricting real-time, open-vocabulary interactions with objects. To address\nthese limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary\nAffordance Reasoning framework, which fine-tunes the Large Language Models\n(LLMs) to predict visual affordance of graspable object parts within RGB\nfeature space. We compile a dataset of over 10,000 images from human-object\ninteractions, annotated with unified visual and linguistic affordance labels,\nto enable multi-modal fine-tuning. GLOVER inherits world knowledge and\ncommon-sense reasoning from LLMs, facilitating more fine-grained object\nunderstanding and sophisticated tool-use reasoning. To enable effective\nreal-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a\nnon-parametric grasp planner that aligns the gripper pose with a superquadric\nsurface derived from affordance data. In evaluations across 30 real-world\nscenes, GLOVER achieves success rates of 86.0% in part identification and 76.3%\nin grasping, with speeds approximately 330 times faster in affordance reasoning\nand 40 times faster in grasping pose estimation than the previous\nstate-of-the-art."
                },
                "authors": [
                    {
                        "name": "Teli Ma"
                    },
                    {
                        "name": "Zifan Wang"
                    },
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Junwei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liang"
                },
                "author": "Junwei Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12280v1",
                "updated": "2024-11-19T07:03:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    3,
                    19,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T07:03:19Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    3,
                    19,
                    1,
                    324,
                    0
                ],
                "title": "Large Language Models for Material Property Predictions: elastic\n  constant tensor prediction and materials design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Material Property Predictions: elastic\n  constant tensor prediction and materials design"
                },
                "summary": "Efficient and accurate prediction of material properties is critical for\nadvancing materials design and applications. The rapid-evolution of large\nlanguage models (LLMs) presents a new opportunity for material property\npredictions, complementing experimental measurements and multi-scale\ncomputational methods. We focus on predicting the elastic constant tensor, as a\ncase study, and develop domain-specific LLMs for predicting elastic constants\nand for materials discovery. The proposed ElaTBot LLM enables simultaneous\nprediction of elastic constant tensors, bulk modulus at finite temperatures,\nand the generation of new materials with targeted properties. Moreover, the\ncapabilities of ElaTBot are further enhanced by integrating with general LLMs\n(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized\nvariant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,\nreduces the prediction errors by 33.1% compared with domain-specific, material\nscience LLMs (Darwin) trained on the same dataset. This natural language-based\napproach lowers the barriers to computational materials science and highlights\nthe broader potential of LLMs for material property predictions and inverse\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and accurate prediction of material properties is critical for\nadvancing materials design and applications. The rapid-evolution of large\nlanguage models (LLMs) presents a new opportunity for material property\npredictions, complementing experimental measurements and multi-scale\ncomputational methods. We focus on predicting the elastic constant tensor, as a\ncase study, and develop domain-specific LLMs for predicting elastic constants\nand for materials discovery. The proposed ElaTBot LLM enables simultaneous\nprediction of elastic constant tensors, bulk modulus at finite temperatures,\nand the generation of new materials with targeted properties. Moreover, the\ncapabilities of ElaTBot are further enhanced by integrating with general LLMs\n(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized\nvariant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,\nreduces the prediction errors by 33.1% compared with domain-specific, material\nscience LLMs (Darwin) trained on the same dataset. This natural language-based\napproach lowers the barriers to computational materials science and highlights\nthe broader potential of LLMs for material property predictions and inverse\ndesign."
                },
                "authors": [
                    {
                        "name": "Siyu Liu"
                    },
                    {
                        "name": "Tongqi Wen"
                    },
                    {
                        "name": "Beilin Ye"
                    },
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "David J. Srolovitz"
                    }
                ],
                "author_detail": {
                    "name": "David J. Srolovitz"
                },
                "author": "David J. Srolovitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v1",
                "updated": "2024-11-19T06:57:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation"
                },
                "summary": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12273v1",
                "updated": "2024-11-19T06:52:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    52,
                    28,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T06:52:28Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    52,
                    28,
                    1,
                    324,
                    0
                ],
                "title": "Acquire Precise and Comparable Fundus Image Quality Score: FTHNet and\n  FQS Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquire Precise and Comparable Fundus Image Quality Score: FTHNet and\n  FQS Dataset"
                },
                "summary": "The retinal fundus images are utilized extensively in the diagnosis, and\ntheir quality can directly affect the diagnosis results. However, due to the\ninsufficient dataset and algorithm application, current fundus image quality\nassessment (FIQA) methods are not powerful enough to meet ophthalmologists`\ndemands. In this paper, we address the limitations of datasets and algorithms\nin FIQA. First, we establish a new FIQA dataset, Fundus Quality Score(FQS),\nwhich includes 2246 fundus images with two labels: a continuous Mean Opinion\nScore varying from 0 to 100 and a three-level quality label. Then, we propose a\nFIQA Transformer-based Hypernetwork (FTHNet) to solve these tasks with\nregression results rather than classification results in conventional FIQA\nworks. The FTHNet is optimized for the FIQA tasks with extensive experiments.\nResults on our FQS dataset show that the FTHNet can give quality scores for\nfundus images with PLCC of 0.9423 and SRCC of 0.9488, significantly\noutperforming other methods with fewer parameters and less computation\ncomplexity.We successfully build a dataset and model addressing the problems of\ncurrent FIQA methods. Furthermore, the model deployment experiments demonstrate\nits potential in automatic medical image quality control. All experiments are\ncarried out with 10-fold cross-validation to ensure the significance of the\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The retinal fundus images are utilized extensively in the diagnosis, and\ntheir quality can directly affect the diagnosis results. However, due to the\ninsufficient dataset and algorithm application, current fundus image quality\nassessment (FIQA) methods are not powerful enough to meet ophthalmologists`\ndemands. In this paper, we address the limitations of datasets and algorithms\nin FIQA. First, we establish a new FIQA dataset, Fundus Quality Score(FQS),\nwhich includes 2246 fundus images with two labels: a continuous Mean Opinion\nScore varying from 0 to 100 and a three-level quality label. Then, we propose a\nFIQA Transformer-based Hypernetwork (FTHNet) to solve these tasks with\nregression results rather than classification results in conventional FIQA\nworks. The FTHNet is optimized for the FIQA tasks with extensive experiments.\nResults on our FQS dataset show that the FTHNet can give quality scores for\nfundus images with PLCC of 0.9423 and SRCC of 0.9488, significantly\noutperforming other methods with fewer parameters and less computation\ncomplexity.We successfully build a dataset and model addressing the problems of\ncurrent FIQA methods. Furthermore, the model deployment experiments demonstrate\nits potential in automatic medical image quality control. All experiments are\ncarried out with 10-fold cross-validation to ensure the significance of the\nresults."
                },
                "authors": [
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhuo Deng"
                    },
                    {
                        "name": "Run Gan"
                    },
                    {
                        "name": "Zhiyuan Niu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Canfeng Huang"
                    },
                    {
                        "name": "Jia Liang"
                    },
                    {
                        "name": "Weihao Gao"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Shaochong Zhang"
                    },
                    {
                        "name": "Lan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lan Ma"
                },
                "author": "Lan Ma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11072v2",
                "updated": "2024-11-19T06:45:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    45,
                    13,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-17T13:21:26Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    13,
                    21,
                    26,
                    6,
                    322,
                    0
                ],
                "title": "Multilingual Large Language Models: A Systematic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models: A Systematic Survey"
                },
                "summary": "This paper provides a comprehensive survey of the latest research on\nmultilingual large language models (MLLMs). MLLMs not only are able to\nunderstand and generate language across linguistic boundaries, but also\nrepresent an important advancement in artificial intelligence. We first discuss\nthe architecture and pre-training objectives of MLLMs, highlighting the key\ncomponents and methodologies that contribute to their multilingual\ncapabilities. We then discuss the construction of multilingual pre-training and\nalignment datasets, underscoring the importance of data quality and diversity\nin enhancing MLLM performance. An important focus of this survey is on the\nevaluation of MLLMs. We present a detailed taxonomy and roadmap covering the\nassessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human\nvalues, safety, interpretability and specialized applications. Specifically, we\nextensively discuss multilingual evaluation benchmarks and datasets, and\nexplore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs\nfrom black to white boxes, we also address the interpretability of multilingual\ncapabilities, cross-lingual transfer and language bias within these models.\nFinally, we provide a comprehensive review of real-world applications of MLLMs\nacross diverse domains, including biology, medicine, computer science,\nmathematics and law. We showcase how these models have driven innovation and\nimprovements in these specialized fields while also highlighting the challenges\nand opportunities in deploying MLLMs within diverse language communities and\napplication scenarios. We listed the paper related in this survey and publicly\navailable at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive survey of the latest research on\nmultilingual large language models (MLLMs). MLLMs not only are able to\nunderstand and generate language across linguistic boundaries, but also\nrepresent an important advancement in artificial intelligence. We first discuss\nthe architecture and pre-training objectives of MLLMs, highlighting the key\ncomponents and methodologies that contribute to their multilingual\ncapabilities. We then discuss the construction of multilingual pre-training and\nalignment datasets, underscoring the importance of data quality and diversity\nin enhancing MLLM performance. An important focus of this survey is on the\nevaluation of MLLMs. We present a detailed taxonomy and roadmap covering the\nassessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human\nvalues, safety, interpretability and specialized applications. Specifically, we\nextensively discuss multilingual evaluation benchmarks and datasets, and\nexplore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs\nfrom black to white boxes, we also address the interpretability of multilingual\ncapabilities, cross-lingual transfer and language bias within these models.\nFinally, we provide a comprehensive review of real-world applications of MLLMs\nacross diverse domains, including biology, medicine, computer science,\nmathematics and law. We showcase how these models have driven innovation and\nimprovements in these specialized fields while also highlighting the challenges\nand opportunities in deploying MLLMs within diverse language communities and\napplication scenarios. We listed the paper related in this survey and publicly\navailable at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers."
                },
                "authors": [
                    {
                        "name": "Shaolin Zhu"
                    },
                    {
                        "name": "Supryadi"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Jiangcun Du"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Antnio Branco"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01812v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01812v4",
                "updated": "2024-11-19T06:14:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    14,
                    31,
                    1,
                    324,
                    0
                ],
                "published": "2024-09-14T02:35:29Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    35,
                    29,
                    5,
                    258,
                    0
                ],
                "title": "From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice"
                },
                "summary": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Lawrence KQ Yan"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yunze Wang"
                    },
                    {
                        "name": "Silin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Silin Chen"
                },
                "author": "Silin Chen",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01812v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01812v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03182v2",
                "updated": "2024-11-19T05:57:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    57,
                    28,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-04T06:45:48Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    45,
                    48,
                    4,
                    278,
                    0
                ],
                "title": "Generating bilingual example sentences with large language models as\n  lexicography assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating bilingual example sentences with large language models as\n  lexicography assistants"
                },
                "summary": "We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Raphael Merx"
                    },
                    {
                        "name": "Ekaterina Vylomova"
                    },
                    {
                        "name": "Kemal Kurniawan"
                    }
                ],
                "author_detail": {
                    "name": "Kemal Kurniawan"
                },
                "author": "Kemal Kurniawan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12244v1",
                "updated": "2024-11-19T05:49:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    49,
                    0,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T05:49:00Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    49,
                    0,
                    1,
                    324,
                    0
                ],
                "title": "Hyper-parameter Optimization for Federated Learning with Step-wise\n  Adaptive Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-parameter Optimization for Federated Learning with Step-wise\n  Adaptive Mechanism"
                },
                "summary": "Federated Learning (FL) is a decentralized learning approach that protects\nsensitive information by utilizing local model parameters rather than sharing\nclients' raw datasets. While this privacy-preserving method is widely employed\nacross various applications, it still requires significant development and\noptimization. Automated Machine Learning (Auto-ML) has been adapted for\nreducing the need for manual adjustments. Previous studies have explored the\nintegration of AutoML with different FL algorithms to evaluate their\neffectiveness in enhancing FL settings. However, Automated FL (Auto-FL) faces\nadditional challenges due to the involvement of a large cohort of clients and\nglobal training rounds between clients and the server, rendering the tuning\nprocess time-consuming and nearly impossible on resource-constrained edge\ndevices (e.g., IoT devices). This paper investigates the deployment and\nintegration of two lightweight Hyper-Parameter Optimization (HPO) tools,\nRaytune and Optuna, within the context of FL settings. A step-wise feedback\nmechanism has also been designed to accelerate the hyper-parameter tuning\nprocess and coordinate AutoML toolkits with the FL server. To this end, both\nlocal and global feedback mechanisms are integrated to limit the search space\nand expedite the HPO process. Further, a novel client selection technique is\nintroduced to mitigate the straggler effect in Auto-FL. The selected\nhyper-parameter tuning tools are evaluated using two benchmark datasets,\nFEMNIST, and CIFAR10. Further, the paper discusses the essential properties of\nsuccessful HPO tools, the integration mechanism with the FL pipeline, and the\nchallenges posed by the distributed and heterogeneous nature of FL\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a decentralized learning approach that protects\nsensitive information by utilizing local model parameters rather than sharing\nclients' raw datasets. While this privacy-preserving method is widely employed\nacross various applications, it still requires significant development and\noptimization. Automated Machine Learning (Auto-ML) has been adapted for\nreducing the need for manual adjustments. Previous studies have explored the\nintegration of AutoML with different FL algorithms to evaluate their\neffectiveness in enhancing FL settings. However, Automated FL (Auto-FL) faces\nadditional challenges due to the involvement of a large cohort of clients and\nglobal training rounds between clients and the server, rendering the tuning\nprocess time-consuming and nearly impossible on resource-constrained edge\ndevices (e.g., IoT devices). This paper investigates the deployment and\nintegration of two lightweight Hyper-Parameter Optimization (HPO) tools,\nRaytune and Optuna, within the context of FL settings. A step-wise feedback\nmechanism has also been designed to accelerate the hyper-parameter tuning\nprocess and coordinate AutoML toolkits with the FL server. To this end, both\nlocal and global feedback mechanisms are integrated to limit the search space\nand expedite the HPO process. Further, a novel client selection technique is\nintroduced to mitigate the straggler effect in Auto-FL. The selected\nhyper-parameter tuning tools are evaluated using two benchmark datasets,\nFEMNIST, and CIFAR10. Further, the paper discusses the essential properties of\nsuccessful HPO tools, the integration mechanism with the FL pipeline, and the\nchallenges posed by the distributed and heterogeneous nature of FL\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yasaman Saadati"
                    },
                    {
                        "name": "M. Hadi Amini"
                    }
                ],
                "author_detail": {
                    "name": "M. Hadi Amini"
                },
                "author": "M. Hadi Amini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12240v1",
                "updated": "2024-11-19T05:37:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    37,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T05:37:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    37,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages"
                },
                "summary": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency."
                },
                "authors": [
                    {
                        "name": "S. Tamang"
                    },
                    {
                        "name": "D. J. Bora"
                    }
                ],
                "author_detail": {
                    "name": "D. J. Bora"
                },
                "author": "D. J. Bora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.11019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.11019v3",
                "updated": "2024-11-19T05:35:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    35,
                    2,
                    1,
                    324,
                    0
                ],
                "published": "2023-07-20T16:46:10Z",
                "published_parsed": [
                    2023,
                    7,
                    20,
                    16,
                    46,
                    10,
                    3,
                    201,
                    0
                ],
                "title": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation"
                },
                "summary": "Large language models (LLMs) have shown impressive prowess in solving a wide\nrange of tasks with world knowledge. However, it remains unclear how well LLMs\nare able to perceive their factual knowledge boundaries, particularly under\nretrieval augmentation settings. In this study, we present the first analysis\non the factual knowledge boundaries of LLMs and how retrieval augmentation\naffects LLMs on open-domain question answering (QA), with a bunch of important\nfindings. Specifically, we focus on three research questions and analyze them\nby examining QA, priori judgement and posteriori judgement capabilities of\nLLMs. We show evidence that LLMs possess unwavering confidence in their\nknowledge and cannot handle the conflict between internal and external\nknowledge well. Furthermore, retrieval augmentation proves to be an effective\napproach in enhancing LLMs' awareness of knowledge boundaries. We further\nconduct thorough experiments to examine how different factors affect LLMs and\npropose a simple method to dynamically utilize supporting documents with our\njudgement strategy. Additionally, we find that the relevance between the\nsupporting documents and the questions significantly impacts LLMs' QA and\njudgemental capabilities. The code to reproduce this work is available at\nhttps://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive prowess in solving a wide\nrange of tasks with world knowledge. However, it remains unclear how well LLMs\nare able to perceive their factual knowledge boundaries, particularly under\nretrieval augmentation settings. In this study, we present the first analysis\non the factual knowledge boundaries of LLMs and how retrieval augmentation\naffects LLMs on open-domain question answering (QA), with a bunch of important\nfindings. Specifically, we focus on three research questions and analyze them\nby examining QA, priori judgement and posteriori judgement capabilities of\nLLMs. We show evidence that LLMs possess unwavering confidence in their\nknowledge and cannot handle the conflict between internal and external\nknowledge well. Furthermore, retrieval augmentation proves to be an effective\napproach in enhancing LLMs' awareness of knowledge boundaries. We further\nconduct thorough experiments to examine how different factors affect LLMs and\npropose a simple method to dynamically utilize supporting documents with our\njudgement strategy. Additionally, we find that the relevance between the\nsupporting documents and the questions significantly impacts LLMs' QA and\njudgemental capabilities. The code to reproduce this work is available at\nhttps://github.com/RUCAIBox/LLM-Knowledge-Boundary."
                },
                "authors": [
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yingqi Qu"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Hao Tian"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.11019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.11019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12238v1",
                "updated": "2024-11-19T05:29:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    29,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T05:29:58Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    29,
                    58,
                    1,
                    324,
                    0
                ],
                "title": "Performance of Large Language Models in Technical MRI Question\n  Answering: A Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of Large Language Models in Technical MRI Question\n  Answering: A Comparative Study"
                },
                "summary": "Background: Advances in artificial intelligence, particularly large language\nmodels (LLMs), have the potential to enhance technical expertise in magnetic\nresonance imaging (MRI), regardless of operator skill or geographic location.\n  Methods: We assessed the accuracy of several LLMs in answering 570 technical\nMRI questions derived from a standardized review book. The questions spanned\nnine MRI topics, including Basic Principles, Image Production, and Safety.\nClosed-source models (e.g., OpenAI's o1 Preview, GPT-4o, GPT-4 Turbo, and\nClaude 3.5 Haiku) and open-source models (e.g., Phi 3.5 Mini, Llama 3.1,\nsmolLM2) were tested. Models were queried using standardized prompts via the\nLangChain framework, and responses were graded against correct answers using an\nautomated scoring protocol. Accuracy, defined as the proportion of correct\nanswers, was the primary outcome.\n  Results: The closed-source o1 Preview model achieved the highest accuracy\n(94%), exceeding the random-guess baseline (26.5%). GPT-4o and o1 Mini scored\n88%, and GPT-4 Turbo and Claude 3.5 Haiku each scored 84%. Among open-source\nmodels, Phi 3.5 Mini performed well, achieving 78% accuracy, comparable to\nseveral closed-source models. Accuracy was highest in Basic Principles and\nInstrumentation categories but lower in Image Weighting and Contrast, History,\nand Artifacts and Corrections.\n  Conclusions: LLMs exhibit high accuracy in addressing technical MRI\nquestions, suggesting their potential to standardize and enhance MRI practice.\nThese models may improve image quality and consistency across varied clinical\nenvironments. Further studies are needed to refine LLMs for clinical use and\nintegrate them into MRI workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Advances in artificial intelligence, particularly large language\nmodels (LLMs), have the potential to enhance technical expertise in magnetic\nresonance imaging (MRI), regardless of operator skill or geographic location.\n  Methods: We assessed the accuracy of several LLMs in answering 570 technical\nMRI questions derived from a standardized review book. The questions spanned\nnine MRI topics, including Basic Principles, Image Production, and Safety.\nClosed-source models (e.g., OpenAI's o1 Preview, GPT-4o, GPT-4 Turbo, and\nClaude 3.5 Haiku) and open-source models (e.g., Phi 3.5 Mini, Llama 3.1,\nsmolLM2) were tested. Models were queried using standardized prompts via the\nLangChain framework, and responses were graded against correct answers using an\nautomated scoring protocol. Accuracy, defined as the proportion of correct\nanswers, was the primary outcome.\n  Results: The closed-source o1 Preview model achieved the highest accuracy\n(94%), exceeding the random-guess baseline (26.5%). GPT-4o and o1 Mini scored\n88%, and GPT-4 Turbo and Claude 3.5 Haiku each scored 84%. Among open-source\nmodels, Phi 3.5 Mini performed well, achieving 78% accuracy, comparable to\nseveral closed-source models. Accuracy was highest in Basic Principles and\nInstrumentation categories but lower in Image Weighting and Contrast, History,\nand Artifacts and Corrections.\n  Conclusions: LLMs exhibit high accuracy in addressing technical MRI\nquestions, suggesting their potential to standardize and enhance MRI practice.\nThese models may improve image quality and consistency across varied clinical\nenvironments. Further studies are needed to refine LLMs for clinical use and\nintegrate them into MRI workflows."
                },
                "authors": [
                    {
                        "name": "Alan B McMillan"
                    }
                ],
                "author_detail": {
                    "name": "Alan B McMillan"
                },
                "author": "Alan B McMillan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00068v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00068v3",
                "updated": "2024-11-19T05:08:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    8,
                    44,
                    1,
                    324,
                    0
                ],
                "published": "2024-01-30T14:47:15Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    14,
                    47,
                    15,
                    1,
                    30,
                    0
                ],
                "title": "Adapting Amidst Degradation: Cross Domain Li-ion Battery Health\n  Estimation via Physics-Guided Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Amidst Degradation: Cross Domain Li-ion Battery Health\n  Estimation via Physics-Guided Test-Time Training"
                },
                "summary": "Health modeling of lithium-ion batteries (LIBs) is crucial for safe and\nefficient energy management and carries significant socio-economic\nimplications. Although Machine Learning (ML)-based State of Health (SOH)\nestimation methods have made significant progress in accuracy, the scarcity of\nhigh-quality LIB data remains a major obstacle. Existing transfer learning\nmethods for cross-domain LIB SOH estimation have significantly alleviated the\nlabeling burden of target LIB data, however, they still require sufficient\nunlabeled target data (UTD) for effective adaptation to the target domain.\nCollecting this UTD is challenging due to the time-consuming nature of\ndegradation experiments. To address this issue, we introduce a practical\nTest-Time Training framework, BatteryTTT, which adapts the model continually\nusing each UTD collected amidst degradation, thereby significantly reducing\ndata collection time. To fully utilize each UTD, BatteryTTT integrates the\ninherent physical laws of modern LIBs into self-supervised learning, termed\nPhyscics-Guided Test-Time Training. Additionally, we explore the potential of\nlarge language models (LLMs) in battery sequence modeling by evaluating their\nperformance in SOH estimation through model reprogramming and prefix prompt\nadaptation. The combination of BatteryTTT and LLM modeling, termed GPT4Battery,\nachieves state-of-the-art generalization results across current LIB benchmarks.\nFurthermore, we demonstrate the practical value and scalability of our approach\nby deploying it in our real-world battery management system (BMS) for 300Ah\nlarge-scale energy storage LIBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health modeling of lithium-ion batteries (LIBs) is crucial for safe and\nefficient energy management and carries significant socio-economic\nimplications. Although Machine Learning (ML)-based State of Health (SOH)\nestimation methods have made significant progress in accuracy, the scarcity of\nhigh-quality LIB data remains a major obstacle. Existing transfer learning\nmethods for cross-domain LIB SOH estimation have significantly alleviated the\nlabeling burden of target LIB data, however, they still require sufficient\nunlabeled target data (UTD) for effective adaptation to the target domain.\nCollecting this UTD is challenging due to the time-consuming nature of\ndegradation experiments. To address this issue, we introduce a practical\nTest-Time Training framework, BatteryTTT, which adapts the model continually\nusing each UTD collected amidst degradation, thereby significantly reducing\ndata collection time. To fully utilize each UTD, BatteryTTT integrates the\ninherent physical laws of modern LIBs into self-supervised learning, termed\nPhyscics-Guided Test-Time Training. Additionally, we explore the potential of\nlarge language models (LLMs) in battery sequence modeling by evaluating their\nperformance in SOH estimation through model reprogramming and prefix prompt\nadaptation. The combination of BatteryTTT and LLM modeling, termed GPT4Battery,\nachieves state-of-the-art generalization results across current LIB benchmarks.\nFurthermore, we demonstrate the practical value and scalability of our approach\nby deploying it in our real-world battery management system (BMS) for 300Ah\nlarge-scale energy storage LIBs."
                },
                "authors": [
                    {
                        "name": "Yuyuan Feng"
                    },
                    {
                        "name": "Guosheng Hu"
                    },
                    {
                        "name": "Xiaodong Li"
                    },
                    {
                        "name": "Zhihong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhihong Zhang"
                },
                "author": "Zhihong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00068v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00068v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09003v2",
                "updated": "2024-11-19T04:53:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    4,
                    53,
                    47,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-13T20:12:55Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    12,
                    55,
                    2,
                    318,
                    0
                ],
                "title": "Refusal in LLMs is an Affine Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal in LLMs is an Affine Function"
                },
                "summary": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 ."
                },
                "authors": [
                    {
                        "name": "Thomas Marshall"
                    },
                    {
                        "name": "Adam Scherlis"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "arxiv_comment": "added plots for results from additional models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10487v2",
                "updated": "2024-11-19T04:11:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    4,
                    11,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-14T05:09:07Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    9,
                    7,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Patterns for Designing Quantum Artificial Intelligence\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Patterns for Designing Quantum Artificial Intelligence\n  Systems"
                },
                "summary": "Utilising quantum computing technology to enhance artificial intelligence\nsystems is expected to improve training and inference times, increase\nrobustness against noise and adversarial attacks, and reduce the number of\nparameters without compromising accuracy. However, moving beyond\nproof-of-concept or simulations to develop practical applications of these\nsystems while ensuring high software quality faces significant challenges due\nto the limitations of quantum hardware and the underdeveloped knowledge base in\nsoftware engineering for such systems. In this work, we have conducted a\nsystematic mapping study to identify the challenges and solutions associated\nwith the software architecture of quantum-enhanced artificial intelligence\nsystems. Our review uncovered several architectural patterns that describe how\nquantum components can be integrated into inference engines, as well as\nmiddleware patterns that facilitate communication between classical and quantum\ncomponents. These insights have been compiled into a catalog of architectural\npatterns. Each pattern realises a trade-off between efficiency, scalability,\ntrainability, simplicity, portability and deployability, and other software\nquality attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilising quantum computing technology to enhance artificial intelligence\nsystems is expected to improve training and inference times, increase\nrobustness against noise and adversarial attacks, and reduce the number of\nparameters without compromising accuracy. However, moving beyond\nproof-of-concept or simulations to develop practical applications of these\nsystems while ensuring high software quality faces significant challenges due\nto the limitations of quantum hardware and the underdeveloped knowledge base in\nsoftware engineering for such systems. In this work, we have conducted a\nsystematic mapping study to identify the challenges and solutions associated\nwith the software architecture of quantum-enhanced artificial intelligence\nsystems. Our review uncovered several architectural patterns that describe how\nquantum components can be integrated into inference engines, as well as\nmiddleware patterns that facilitate communication between classical and quantum\ncomponents. These insights have been compiled into a catalog of architectural\npatterns. Each pattern realises a trade-off between efficiency, scalability,\ntrainability, simplicity, portability and deployability, and other software\nquality attributes."
                },
                "authors": [
                    {
                        "name": "Mykhailo Klymenko"
                    },
                    {
                        "name": "Thong Hoang"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; D.2.m; I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12196v1",
                "updated": "2024-11-19T03:29:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    29,
                    17,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T03:29:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    29,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs"
                },
                "summary": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability."
                },
                "authors": [
                    {
                        "name": "Zixin Liu"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiran Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Ding"
                },
                "author": "Yiran Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12193v1",
                "updated": "2024-11-19T03:18:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    18,
                    31,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T03:18:31Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    18,
                    31,
                    1,
                    324,
                    0
                ],
                "title": "Hierarchical Spatio-Temporal Uncertainty Quantification for Distributed\n  Energy Adoption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Spatio-Temporal Uncertainty Quantification for Distributed\n  Energy Adoption"
                },
                "summary": "The rapid deployment of distributed energy resources (DER) has introduced\nsignificant spatio-temporal uncertainties in power grid management,\nnecessitating accurate multilevel forecasting methods. However, existing\napproaches often produce overly conservative uncertainty intervals at\nindividual spatial units and fail to properly capture uncertainties when\naggregating predictions across different spatial scales. This paper presents a\nnovel hierarchical spatio-temporal model based on the conformal prediction\nframework to address these challenges. Our approach generates circuit-level DER\ngrowth predictions and efficiently aggregates them to the substation level\nwhile maintaining statistical validity through a tailored non-conformity score.\nApplied to a decade of DER installation data from a local utility network, our\nmethod demonstrates superior performance over existing approaches, particularly\nin reducing prediction interval widths while maintaining coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of distributed energy resources (DER) has introduced\nsignificant spatio-temporal uncertainties in power grid management,\nnecessitating accurate multilevel forecasting methods. However, existing\napproaches often produce overly conservative uncertainty intervals at\nindividual spatial units and fail to properly capture uncertainties when\naggregating predictions across different spatial scales. This paper presents a\nnovel hierarchical spatio-temporal model based on the conformal prediction\nframework to address these challenges. Our approach generates circuit-level DER\ngrowth predictions and efficiently aggregates them to the substation level\nwhile maintaining statistical validity through a tailored non-conformity score.\nApplied to a decade of DER installation data from a local utility network, our\nmethod demonstrates superior performance over existing approaches, particularly\nin reducing prediction interval widths while maintaining coverage."
                },
                "authors": [
                    {
                        "name": "Wenbin Zhou"
                    },
                    {
                        "name": "Shixiang Zhu"
                    },
                    {
                        "name": "Feng Qiu"
                    },
                    {
                        "name": "Xuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Wu"
                },
                "author": "Xuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14148v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14148v3",
                "updated": "2024-11-19T03:08:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    8,
                    34,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-18T03:34:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    34,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment"
                },
                "summary": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models."
                },
                "authors": [
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14148v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14148v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10825v2",
                "updated": "2024-11-19T01:51:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    51,
                    37,
                    1,
                    324,
                    0
                ],
                "published": "2024-09-17T01:37:57Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    57,
                    1,
                    261,
                    0
                ],
                "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness"
                },
                "summary": "Large Language Model (LLM)-based recommendation systems provide more\ncomprehensive recommendations than traditional systems by deeply analyzing\ncontent and user behavior. However, these systems often exhibit biases,\nfavoring mainstream content while marginalizing non-traditional options due to\nskewed training data. This study investigates the intricate relationship\nbetween bias and LLM-based recommendation systems, with a focus on music, song,\nand book recommendations across diverse demographic and cultural groups.\nThrough a comprehensive analysis conducted over different LLM-models, this\npaper evaluates the impact of bias on recommendation outcomes. Our findings\nhighlight that biases are not only deeply embedded but also widely pervasive\nacross these systems, emphasizing the substantial and widespread nature of the\nissue. Moreover, contextual information, such as socioeconomic status, further\namplify these biases, demonstrating the complexity and depth of the challenges\nfaced in creating fair recommendations across different groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based recommendation systems provide more\ncomprehensive recommendations than traditional systems by deeply analyzing\ncontent and user behavior. However, these systems often exhibit biases,\nfavoring mainstream content while marginalizing non-traditional options due to\nskewed training data. This study investigates the intricate relationship\nbetween bias and LLM-based recommendation systems, with a focus on music, song,\nand book recommendations across diverse demographic and cultural groups.\nThrough a comprehensive analysis conducted over different LLM-models, this\npaper evaluates the impact of bias on recommendation outcomes. Our findings\nhighlight that biases are not only deeply embedded but also widely pervasive\nacross these systems, emphasizing the substantial and widespread nature of the\nissue. Moreover, contextual information, such as socioeconomic status, further\namplify these biases, demonstrating the complexity and depth of the challenges\nfaced in creating fair recommendations across different groups."
                },
                "authors": [
                    {
                        "name": "Shahnewaz Karim Sakib"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    }
                ],
                "author_detail": {
                    "name": "Anindya Bijoy Das"
                },
                "author": "Anindya Bijoy Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12121v1",
                "updated": "2024-11-18T23:18:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    23,
                    18,
                    55,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T23:18:55Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    23,
                    18,
                    55,
                    0,
                    323,
                    0
                ],
                "title": "Metamorphic Evaluation of ChatGPT as a Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metamorphic Evaluation of ChatGPT as a Recommender System"
                },
                "summary": "With the rise of Large Language Models (LLMs) such as ChatGPT, researchers\nhave been working on how to utilize the LLMs for better recommendations.\nHowever, although LLMs exhibit black-box and probabilistic characteristics\n(meaning their internal working is not visible), the evaluation framework used\nfor assessing these LLM-based recommender systems (RS) are the same as those\nused for traditional recommender systems. To address this gap, we introduce the\nmetamorphic testing for the evaluation of GPT-based RS. This testing technique\ninvolves defining of metamorphic relations (MRs) between the inputs and\nchecking if the relationship has been satisfied in the outputs. Specifically,\nwe examined the MRs from both RS and LLMs perspectives, including rating\nmultiplication/shifting in RS and adding spaces/randomness in the LLMs prompt\nvia prompt perturbation. Similarity metrics (e.g. Kendall tau and Ranking\nBiased Overlap(RBO)) are deployed to measure whether the relationship has been\nsatisfied in the outputs of MRs. The experiment results on MovieLens dataset\nwith GPT3.5 show that lower similarity are obtained in terms of Kendall $\\tau$\nand RBO, which concludes that there is a need of a comprehensive evaluation of\nthe LLM-based RS in addition to the existing evaluation metrics used for\ntraditional recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of Large Language Models (LLMs) such as ChatGPT, researchers\nhave been working on how to utilize the LLMs for better recommendations.\nHowever, although LLMs exhibit black-box and probabilistic characteristics\n(meaning their internal working is not visible), the evaluation framework used\nfor assessing these LLM-based recommender systems (RS) are the same as those\nused for traditional recommender systems. To address this gap, we introduce the\nmetamorphic testing for the evaluation of GPT-based RS. This testing technique\ninvolves defining of metamorphic relations (MRs) between the inputs and\nchecking if the relationship has been satisfied in the outputs. Specifically,\nwe examined the MRs from both RS and LLMs perspectives, including rating\nmultiplication/shifting in RS and adding spaces/randomness in the LLMs prompt\nvia prompt perturbation. Similarity metrics (e.g. Kendall tau and Ranking\nBiased Overlap(RBO)) are deployed to measure whether the relationship has been\nsatisfied in the outputs of MRs. The experiment results on MovieLens dataset\nwith GPT3.5 show that lower similarity are obtained in terms of Kendall $\\tau$\nand RBO, which concludes that there is a need of a comprehensive evaluation of\nthe LLM-based RS in addition to the existing evaluation metrics used for\ntraditional recommender systems."
                },
                "authors": [
                    {
                        "name": "Madhurima Khirbat"
                    },
                    {
                        "name": "Yongli Ren"
                    },
                    {
                        "name": "Pablo Castells"
                    },
                    {
                        "name": "Mark Sanderson"
                    }
                ],
                "author_detail": {
                    "name": "Mark Sanderson"
                },
                "author": "Mark Sanderson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12103v1",
                "updated": "2024-11-18T22:31:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    22,
                    31,
                    17,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T22:31:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    22,
                    31,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods"
                },
                "summary": "Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at\n$\\href{https://github.com/JaiDoshi/Knowledge-Erasure}{this\\, https\\, URL}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at\n$\\href{https://github.com/JaiDoshi/Knowledge-Erasure}{this\\, https\\, URL}$."
                },
                "authors": [
                    {
                        "name": "Jai Doshi"
                    },
                    {
                        "name": "Asa Cooper Stickland"
                    }
                ],
                "author_detail": {
                    "name": "Asa Cooper Stickland"
                },
                "author": "Asa Cooper Stickland",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00132v3",
                "updated": "2024-11-18T22:24:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    22,
                    24,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-31T18:47:30Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    18,
                    47,
                    30,
                    4,
                    152,
                    0
                ],
                "title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation"
                },
                "summary": "We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing."
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Rumen Dangovski"
                    },
                    {
                        "name": "Charlotte Loh"
                    },
                    {
                        "name": "Owen Dugan"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Marin Soljai"
                    }
                ],
                "author_detail": {
                    "name": "Marin Soljai"
                },
                "author": "Marin Soljai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.08850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.08850v2",
                "updated": "2024-11-18T21:51:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    21,
                    51,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2023-07-17T21:22:17Z",
                "published_parsed": [
                    2023,
                    7,
                    17,
                    21,
                    22,
                    17,
                    0,
                    198,
                    0
                ],
                "title": "LiDAR-BEVMTN: Real-Time LiDAR Bird's-Eye View Multi-Task Perception\n  Network for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-BEVMTN: Real-Time LiDAR Bird's-Eye View Multi-Task Perception\n  Network for Autonomous Driving"
                },
                "summary": "LiDAR is crucial for robust 3D scene perception in autonomous driving. LiDAR\nperception has the largest body of literature after camera perception. However,\nmulti-task learning across tasks like detection, segmentation, and motion\nestimation using LiDAR remains relatively unexplored, especially on\nautomotive-grade embedded platforms. We present a real-time multi-task\nconvolutional neural network for LiDAR-based object detection, semantics, and\nmotion segmentation. The unified architecture comprises a shared encoder and\ntask-specific decoders, enabling joint representation learning. We propose a\nnovel Semantic Weighting and Guidance (SWAG) module to transfer semantic\nfeatures for improved object detection selectively. Our heterogeneous training\nscheme combines diverse datasets and exploits complementary cues between tasks.\nThe work provides the first embedded implementation unifying these key\nperception tasks from LiDAR point clouds achieving 3ms latency on the embedded\nNVIDIA Xavier platform. We achieve state-of-the-art results for two tasks,\nsemantic and motion segmentation, and close to state-of-the-art performance for\n3D object detection. By maximizing hardware efficiency and leveraging\nmulti-task synergies, our method delivers an accurate and efficient solution\ntailored for real-world automated driving deployment. Qualitative results can\nbe seen at https://youtu.be/H-hWRzv2lIY.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR is crucial for robust 3D scene perception in autonomous driving. LiDAR\nperception has the largest body of literature after camera perception. However,\nmulti-task learning across tasks like detection, segmentation, and motion\nestimation using LiDAR remains relatively unexplored, especially on\nautomotive-grade embedded platforms. We present a real-time multi-task\nconvolutional neural network for LiDAR-based object detection, semantics, and\nmotion segmentation. The unified architecture comprises a shared encoder and\ntask-specific decoders, enabling joint representation learning. We propose a\nnovel Semantic Weighting and Guidance (SWAG) module to transfer semantic\nfeatures for improved object detection selectively. Our heterogeneous training\nscheme combines diverse datasets and exploits complementary cues between tasks.\nThe work provides the first embedded implementation unifying these key\nperception tasks from LiDAR point clouds achieving 3ms latency on the embedded\nNVIDIA Xavier platform. We achieve state-of-the-art results for two tasks,\nsemantic and motion segmentation, and close to state-of-the-art performance for\n3D object detection. By maximizing hardware efficiency and leveraging\nmulti-task synergies, our method delivers an accurate and efficient solution\ntailored for real-world automated driving deployment. Qualitative results can\nbe seen at https://youtu.be/H-hWRzv2lIY."
                },
                "authors": [
                    {
                        "name": "Sambit Mohapatra"
                    },
                    {
                        "name": "Senthil Yogamani"
                    },
                    {
                        "name": "Varun Ravi Kumar"
                    },
                    {
                        "name": "Stefan Milz"
                    },
                    {
                        "name": "Heinrich Gotzig"
                    },
                    {
                        "name": "Patrick Mder"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Mder"
                },
                "author": "Patrick Mder",
                "arxiv_comment": "Accepted for publication at IEEE Transactions on Intelligent\n  Transportation Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.08850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.08850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12044v1",
                "updated": "2024-11-18T20:31:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    31,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T20:31:38Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    31,
                    38,
                    0,
                    323,
                    0
                ],
                "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements"
                },
                "summary": "Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP."
                },
                "authors": [
                    {
                        "name": "M. Arda Aydn"
                    },
                    {
                        "name": "Efe Mert rpar"
                    },
                    {
                        "name": "Elvin Abdinli"
                    },
                    {
                        "name": "Gozde Unal"
                    },
                    {
                        "name": "Yusuf H. Sahin"
                    }
                ],
                "author_detail": {
                    "name": "Yusuf H. Sahin"
                },
                "author": "Yusuf H. Sahin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03080v2",
                "updated": "2024-11-18T20:27:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    27,
                    39,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-04T21:08:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    8,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Explainable AI for computational pathology identifies model limitations\n  and tissue biomarkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI for computational pathology identifies model limitations\n  and tissue biomarkers"
                },
                "summary": "Introduction: Deep learning models hold great promise for digital pathology,\nbut their opaque decision-making processes undermine trust and hinder clinical\nadoption. Explainable AI methods are essential to enhance model transparency\nand reliability. Methods: We developed HIPPO, an explainable AI framework that\nsystematically modifies tissue regions in whole slide images to generate image\ncounterfactuals, enabling quantitative hypothesis testing, bias detection, and\nmodel evaluation beyond traditional performance metrics. HIPPO was applied to a\nvariety of clinically important tasks, including breast metastasis detection in\naxillary lymph nodes, prognostication in breast cancer and melanoma, and IDH\nmutation classification in gliomas. In computational experiments, HIPPO was\ncompared against traditional metrics and attention-based approaches to assess\nits ability to identify key tissue elements driving model predictions. Results:\nIn metastasis detection, HIPPO uncovered critical model limitations that were\nundetectable by standard performance metrics or attention-based methods. For\nprognostic prediction, HIPPO outperformed attention by providing more nuanced\ninsights into tissue elements influencing outcomes. In a proof-of-concept\nstudy, HIPPO facilitated hypothesis generation for identifying melanoma\npatients who may benefit from immunotherapy. In IDH mutation classification,\nHIPPO more robustly identified the pathology regions responsible for false\nnegatives compared to attention, suggesting its potential to outperform\nattention in explaining model decisions. Conclusions: HIPPO expands the\nexplainable AI toolkit for computational pathology by enabling deeper insights\ninto model behavior. This framework supports the trustworthy development,\ndeployment, and regulation of weakly-supervised models in clinical and research\nsettings, promoting their broader adoption in digital pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Deep learning models hold great promise for digital pathology,\nbut their opaque decision-making processes undermine trust and hinder clinical\nadoption. Explainable AI methods are essential to enhance model transparency\nand reliability. Methods: We developed HIPPO, an explainable AI framework that\nsystematically modifies tissue regions in whole slide images to generate image\ncounterfactuals, enabling quantitative hypothesis testing, bias detection, and\nmodel evaluation beyond traditional performance metrics. HIPPO was applied to a\nvariety of clinically important tasks, including breast metastasis detection in\naxillary lymph nodes, prognostication in breast cancer and melanoma, and IDH\nmutation classification in gliomas. In computational experiments, HIPPO was\ncompared against traditional metrics and attention-based approaches to assess\nits ability to identify key tissue elements driving model predictions. Results:\nIn metastasis detection, HIPPO uncovered critical model limitations that were\nundetectable by standard performance metrics or attention-based methods. For\nprognostic prediction, HIPPO outperformed attention by providing more nuanced\ninsights into tissue elements influencing outcomes. In a proof-of-concept\nstudy, HIPPO facilitated hypothesis generation for identifying melanoma\npatients who may benefit from immunotherapy. In IDH mutation classification,\nHIPPO more robustly identified the pathology regions responsible for false\nnegatives compared to attention, suggesting its potential to outperform\nattention in explaining model decisions. Conclusions: HIPPO expands the\nexplainable AI toolkit for computational pathology by enabling deeper insights\ninto model behavior. This framework supports the trustworthy development,\ndeployment, and regulation of weakly-supervised models in clinical and research\nsettings, promoting their broader adoption in digital pathology."
                },
                "authors": [
                    {
                        "name": "Jakub R. Kaczmarzyk"
                    },
                    {
                        "name": "Joel H. Saltz"
                    },
                    {
                        "name": "Peter K. Koo"
                    }
                ],
                "author_detail": {
                    "name": "Peter K. Koo"
                },
                "author": "Peter K. Koo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.TO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14634v2",
                "updated": "2024-11-18T20:25:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    25,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-23T00:09:34Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    0,
                    9,
                    34,
                    0,
                    267,
                    0
                ],
                "title": "Scideator: Human-LLM Scientific Idea Generation Grounded in\n  Research-Paper Facet Recombination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scideator: Human-LLM Scientific Idea Generation Grounded in\n  Research-Paper Facet Recombination"
                },
                "summary": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas. To see if large language models (LLMs) can\nassist this process, we contribute Scideator, a novel mixed-initiative tool for\nscientific ideation. Starting from a user-provided set of papers, Scideator\nextracts key facets (purposes, mechanisms, and evaluations) from these and\nrelevant papers, allowing users to explore the idea space by interactively\nrecombining facets to synthesize inventive ideas. Scideator also helps users to\ngauge idea novelty by searching the literature for potential overlaps and\nshowing automated novelty assessments and explanations. To support these tasks,\nScideator introduces four LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty\nChecker, and Idea Novelty Iterator. In a within-subjects user study, 19\ncomputer-science researchers identified significantly more interesting ideas\nusing Scideator compared to a strong baseline combining a scientific search\nengine with LLM interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas. To see if large language models (LLMs) can\nassist this process, we contribute Scideator, a novel mixed-initiative tool for\nscientific ideation. Starting from a user-provided set of papers, Scideator\nextracts key facets (purposes, mechanisms, and evaluations) from these and\nrelevant papers, allowing users to explore the idea space by interactively\nrecombining facets to synthesize inventive ideas. Scideator also helps users to\ngauge idea novelty by searching the literature for potential overlaps and\nshowing automated novelty assessments and explanations. To support these tasks,\nScideator introduces four LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty\nChecker, and Idea Novelty Iterator. In a within-subjects user study, 19\ncomputer-science researchers identified significantly more interesting ideas\nusing Scideator compared to a strong baseline combining a scientific search\nengine with LLM interaction."
                },
                "authors": [
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Simra Shahid"
                    },
                    {
                        "name": "Raymond Fok"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Daniel S. Weld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel S. Weld"
                },
                "author": "Daniel S. Weld",
                "arxiv_comment": "Revised TextGRAD results after noting inaccuracies in their reporting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2, I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05981v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05981v4",
                "updated": "2024-11-18T20:18:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    18,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-10T02:47:55Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    2,
                    47,
                    55,
                    0,
                    162,
                    0
                ],
                "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization"
                },
                "summary": "Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM."
                },
                "authors": [
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Yipin Guo"
                    },
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Huihong Shi"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Yingyan Celine Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yingyan Celine Lin"
                },
                "author": "Yingyan Celine Lin",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05981v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05981v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12000v1",
                "updated": "2024-11-18T19:36:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    19,
                    36,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T19:36:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    19,
                    36,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "ByteScience: Bridging Unstructured Scientific Literature and Structured\n  Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteScience: Bridging Unstructured Scientific Literature and Structured\n  Data with Auto Fine-tuned Large Language Model in Token Granularity"
                },
                "summary": "Natural Language Processing (NLP) is widely used to supply summarization\nability from long context to structured information. However, extracting\nstructured knowledge from scientific text by NLP models remains a challenge\nbecause of its domain-specific nature to complex data preprocessing and the\ngranularity of multi-layered device-level information. To address this, we\nintroduce ByteScience, a non-profit cloud-based auto fine-tuned Large Language\nModel (LLM) platform, which is designed to extract structured scientific data\nand synthesize new scientific knowledge from vast scientific corpora. The\nplatform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to\nnatural science. The platform was built on Amazon Web Services (AWS) and\nprovides an automated, user-friendly workflow for custom model development and\ndata extraction. The platform achieves remarkable accuracy with only a small\namount of well-annotated articles. This innovative tool streamlines the\ntransition from the science literature to structured knowledge and data and\nbenefits the advancements in natural informatics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing (NLP) is widely used to supply summarization\nability from long context to structured information. However, extracting\nstructured knowledge from scientific text by NLP models remains a challenge\nbecause of its domain-specific nature to complex data preprocessing and the\ngranularity of multi-layered device-level information. To address this, we\nintroduce ByteScience, a non-profit cloud-based auto fine-tuned Large Language\nModel (LLM) platform, which is designed to extract structured scientific data\nand synthesize new scientific knowledge from vast scientific corpora. The\nplatform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to\nnatural science. The platform was built on Amazon Web Services (AWS) and\nprovides an automated, user-friendly workflow for custom model development and\ndata extraction. The platform achieves remarkable accuracy with only a small\namount of well-annotated articles. This innovative tool streamlines the\ntransition from the science literature to structured knowledge and data and\nbenefits the advancements in natural informatics."
                },
                "authors": [
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Hanzhi Zhang"
                    },
                    {
                        "name": "Shaozhou Wang"
                    },
                    {
                        "name": "Yuwei Wan"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Chunyu Kit"
                    },
                    {
                        "name": "Wenjie Zhangand Bram Hoex"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhangand Bram Hoex"
                },
                "author": "Wenjie Zhangand Bram Hoex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11984v1",
                "updated": "2024-11-18T19:14:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    19,
                    14,
                    36,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T19:14:36Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    19,
                    14,
                    36,
                    0,
                    323,
                    0
                ],
                "title": "Understanding Chain-of-Thought in LLMs through Information Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Chain-of-Thought in LLMs through Information Theory"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in complex\nreasoning tasks through Chain-of-Thought (CoT) reasoning, allowing models to\nbreak down problems into manageable sub-tasks. However, existing CoT evaluation\ntechniques either require annotated CoT data or fall short in accurately\nassessing intermediate reasoning steps, leading to high rates of false\npositives. In this paper, we formalize CoT reasoning in LLMs through an\ninformation-theoretic lens. Specifically, our framework quantifies the\n`information gain' at each reasoning step, enabling the identification of\nfailure modes in LLMs without the need for expensive annotated datasets. We\ndemonstrate the efficacy of our approach through extensive experiments on toy\nand GSM-8K data, where it significantly outperforms existing outcome-based\nmethods by providing more accurate insights into model performance on\nindividual tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in complex\nreasoning tasks through Chain-of-Thought (CoT) reasoning, allowing models to\nbreak down problems into manageable sub-tasks. However, existing CoT evaluation\ntechniques either require annotated CoT data or fall short in accurately\nassessing intermediate reasoning steps, leading to high rates of false\npositives. In this paper, we formalize CoT reasoning in LLMs through an\ninformation-theoretic lens. Specifically, our framework quantifies the\n`information gain' at each reasoning step, enabling the identification of\nfailure modes in LLMs without the need for expensive annotated datasets. We\ndemonstrate the efficacy of our approach through extensive experiments on toy\nand GSM-8K data, where it significantly outperforms existing outcome-based\nmethods by providing more accurate insights into model performance on\nindividual tasks."
                },
                "authors": [
                    {
                        "name": "Jean-Francois Ton"
                    },
                    {
                        "name": "Muhammad Faaiz Taufiq"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11982v1",
                "updated": "2024-11-18T19:13:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    19,
                    13,
                    6,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T19:13:06Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    19,
                    13,
                    6,
                    0,
                    323,
                    0
                ],
                "title": "HPA-MPC: Hybrid Perception-Aware Nonlinear Model Predictive Control for\n  Quadrotors with Suspended Loads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPA-MPC: Hybrid Perception-Aware Nonlinear Model Predictive Control for\n  Quadrotors with Suspended Loads"
                },
                "summary": "Quadrotors equipped with cable-suspended loads represent a versatile,\nlow-cost, and energy efficient solution for aerial transportation,\nconstruction, and manipulation tasks. However, their real-world deployment is\nhindered by several challenges. The system is difficult to control because it\nis nonlinear, underactuated, involves hybrid dynamics due to slack-taut cable\nmodes, and evolves on complex configuration spaces. Additionally, it is crucial\nto estimate the full state and the cable's mode transitions in real-time using\non-board sensors and computation. To address these challenges, we present a\nnovel Hybrid Perception-Aware Nonlinear Model Predictive Control (HPA-MPC)\ncontrol approach for quadrotors with suspended loads. Our method considers the\ncomplete hybrid system dynamics and includes a perception-aware cost to ensure\nthe payload remains visible in the robot's camera during navigation.\nFurthermore, the full state and hybrid dynamics' transitions are estimated\nusing onboard sensors. Experimental results demonstrate that our approach\nenables stable load tracking control, even during slack-taut transitions, and\noperates entirely onboard. The experiments also show that the perception-aware\nterm effectively keeps the payload in the robot's camera field of view when a\nhuman operator interacts with the load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadrotors equipped with cable-suspended loads represent a versatile,\nlow-cost, and energy efficient solution for aerial transportation,\nconstruction, and manipulation tasks. However, their real-world deployment is\nhindered by several challenges. The system is difficult to control because it\nis nonlinear, underactuated, involves hybrid dynamics due to slack-taut cable\nmodes, and evolves on complex configuration spaces. Additionally, it is crucial\nto estimate the full state and the cable's mode transitions in real-time using\non-board sensors and computation. To address these challenges, we present a\nnovel Hybrid Perception-Aware Nonlinear Model Predictive Control (HPA-MPC)\ncontrol approach for quadrotors with suspended loads. Our method considers the\ncomplete hybrid system dynamics and includes a perception-aware cost to ensure\nthe payload remains visible in the robot's camera during navigation.\nFurthermore, the full state and hybrid dynamics' transitions are estimated\nusing onboard sensors. Experimental results demonstrate that our approach\nenables stable load tracking control, even during slack-taut transitions, and\noperates entirely onboard. The experiments also show that the perception-aware\nterm effectively keeps the payload in the robot's camera field of view when a\nhuman operator interacts with the load."
                },
                "authors": [
                    {
                        "name": "Mrunal Sarvaiya"
                    },
                    {
                        "name": "Guanrui Li"
                    },
                    {
                        "name": "Giuseppe Loianno"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Loianno"
                },
                "author": "Giuseppe Loianno",
                "arxiv_comment": "Accepted to IEEE Robotics and Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07681v2",
                "updated": "2024-11-18T18:49:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    49,
                    59,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-12T09:52:40Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    52,
                    40,
                    1,
                    317,
                    0
                ],
                "title": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?"
                },
                "summary": "Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques."
                },
                "authors": [
                    {
                        "name": "Katie Kang"
                    },
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Dibya Ghosh"
                    },
                    {
                        "name": "Jacob Steinhardt"
                    },
                    {
                        "name": "Claire Tomlin"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11829v1",
                "updated": "2024-11-18T18:48:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:48:13Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    13,
                    0,
                    323,
                    0
                ],
                "title": "Tackling prediction tasks in relational databases with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tackling prediction tasks in relational databases with LLMs"
                },
                "summary": "Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction."
                },
                "authors": [
                    {
                        "name": "Marek Wydmuch"
                    },
                    {
                        "name": "ukasz Borchmann"
                    },
                    {
                        "name": "Filip Graliski"
                    }
                ],
                "author_detail": {
                    "name": "Filip Graliski"
                },
                "author": "Filip Graliski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00024v2",
                "updated": "2024-11-18T18:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    41,
                    8,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-28T22:30:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    22,
                    30,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges"
                },
                "summary": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements."
                },
                "authors": [
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Benjamin Danek"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Christina Mack"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Yajuan Wang"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11943v1",
                "updated": "2024-11-18T18:37:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    37,
                    9,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:37:09Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    37,
                    9,
                    0,
                    323,
                    0
                ],
                "title": "Medical Video Generation for Disease Progression Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Video Generation for Disease Progression Simulation"
                },
                "summary": "Modeling disease progression is crucial for improving the quality and\nefficacy of clinical diagnosis and prognosis, but it is often hindered by a\nlack of longitudinal medical image monitoring for individual patients. To\naddress this challenge, we propose the first Medical Video Generation (MVG)\nframework that enables controlled manipulation of disease-related image and\nvideo features, allowing precise, realistic, and personalized simulations of\ndisease progression. Our approach begins by leveraging large language models\n(LLMs) to recaption prompt for disease trajectory. Next, a controllable\nmulti-round diffusion model simulates the disease progression state for each\npatient, creating realistic intermediate disease state sequence. Finally, a\ndiffusion-based video transition generation model interpolates disease\nprogression between these states. We validate our framework across three\nmedical imaging domains: chest X-ray, fundus photography, and skin image. Our\nresults demonstrate that MVG significantly outperforms baseline models in\ngenerating coherent and clinically plausible disease trajectories. Two user\nstudies by veteran physicians, provide further validation and insights into the\nclinical utility of the generated sequences. MVG has the potential to assist\nhealthcare providers in modeling disease trajectories, interpolating missing\nmedical image data, and enhancing medical education through realistic, dynamic\nvisualizations of disease progression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling disease progression is crucial for improving the quality and\nefficacy of clinical diagnosis and prognosis, but it is often hindered by a\nlack of longitudinal medical image monitoring for individual patients. To\naddress this challenge, we propose the first Medical Video Generation (MVG)\nframework that enables controlled manipulation of disease-related image and\nvideo features, allowing precise, realistic, and personalized simulations of\ndisease progression. Our approach begins by leveraging large language models\n(LLMs) to recaption prompt for disease trajectory. Next, a controllable\nmulti-round diffusion model simulates the disease progression state for each\npatient, creating realistic intermediate disease state sequence. Finally, a\ndiffusion-based video transition generation model interpolates disease\nprogression between these states. We validate our framework across three\nmedical imaging domains: chest X-ray, fundus photography, and skin image. Our\nresults demonstrate that MVG significantly outperforms baseline models in\ngenerating coherent and clinically plausible disease trajectories. Two user\nstudies by veteran physicians, provide further validation and insights into the\nclinical utility of the generated sequences. MVG has the potential to assist\nhealthcare providers in modeling disease trajectories, interpolating missing\nmedical image data, and enhancing medical education through realistic, dynamic\nvisualizations of disease progression."
                },
                "authors": [
                    {
                        "name": "Xu Cao"
                    },
                    {
                        "name": "Kaizhao Liang"
                    },
                    {
                        "name": "Kuei-Da Liao"
                    },
                    {
                        "name": "Tianren Gao"
                    },
                    {
                        "name": "Wenqian Ye"
                    },
                    {
                        "name": "Jintai Chen"
                    },
                    {
                        "name": "Zhiguang Ding"
                    },
                    {
                        "name": "Jianguo Cao"
                    },
                    {
                        "name": "James M. Rehg"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "arxiv_comment": "Tech Report. The appendix will release soon. arXiv admin note: text\n  overlap with arXiv:2309.11745",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11795v1",
                "updated": "2024-11-18T18:08:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    8,
                    52,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:08:52Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    8,
                    52,
                    0,
                    323,
                    0
                ],
                "title": "Exploring adversarial robustness of JPEG AI: methodology, comparison and\n  new methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring adversarial robustness of JPEG AI: methodology, comparison and\n  new methods"
                },
                "summary": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI - the first standard for\nend-to-end neural image compression (NIC) methods - the question of its\nrobustness has become critically significant. JPEG AI is among the first\ninternational, real-world applications of neural-network-based models to be\nembedded in consumer devices. However, research on NIC robustness has been\nlimited to open-source codecs and a narrow range of attacks. This paper\nproposes a new methodology for measuring NIC robustness to adversarial attacks.\nWe present the first large-scale evaluation of JPEG AI's robustness, comparing\nit with other NIC models. Our evaluation results and code are publicly\navailable online (link is hidden for a blind review).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI - the first standard for\nend-to-end neural image compression (NIC) methods - the question of its\nrobustness has become critically significant. JPEG AI is among the first\ninternational, real-world applications of neural-network-based models to be\nembedded in consumer devices. However, research on NIC robustness has been\nlimited to open-source codecs and a narrow range of attacks. This paper\nproposes a new methodology for measuring NIC robustness to adversarial attacks.\nWe present the first large-scale evaluation of JPEG AI's robustness, comparing\nit with other NIC models. Our evaluation results and code are publicly\navailable online (link is hidden for a blind review)."
                },
                "authors": [
                    {
                        "name": "Egor Kovalev"
                    },
                    {
                        "name": "Georgii Bychkov"
                    },
                    {
                        "name": "Khaled Abud"
                    },
                    {
                        "name": "Aleksandr Gushchin"
                    },
                    {
                        "name": "Anna Chistyakova"
                    },
                    {
                        "name": "Sergey Lavrushkin"
                    },
                    {
                        "name": "Dmitriy Vatolin"
                    },
                    {
                        "name": "Anastasia Antsiferova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Antsiferova"
                },
                "author": "Anastasia Antsiferova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11779v1",
                "updated": "2024-11-18T17:56:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    56,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:56:13Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    56,
                    13,
                    0,
                    323,
                    0
                ],
                "title": "LLM-IE: A Python Package for Generative Information Extraction with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-IE: A Python Package for Generative Information Extraction with\n  Large Language Models"
                },
                "summary": "Objectives: Despite the recent adoption of large language models (LLMs) for\nbiomedical information extraction, challenges in prompt engineering and\nalgorithms persist, with no dedicated software available. To address this, we\ndeveloped LLM-IE: a Python package for building complete information extraction\npipelines. Our key innovation is an interactive LLM agent to support schema\ndefinition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity\nattribute extraction, and relation extraction tasks. We benchmarked on the i2b2\ndatasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best\nperformance while requiring a longer inference time. System evaluation provided\nintuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare\nand has been adopted in internal projects. It should hold great value to the\nbiomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building\nblocks for robust information extraction pipeline construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Despite the recent adoption of large language models (LLMs) for\nbiomedical information extraction, challenges in prompt engineering and\nalgorithms persist, with no dedicated software available. To address this, we\ndeveloped LLM-IE: a Python package for building complete information extraction\npipelines. Our key innovation is an interactive LLM agent to support schema\ndefinition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity\nattribute extraction, and relation extraction tasks. We benchmarked on the i2b2\ndatasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best\nperformance while requiring a longer inference time. System evaluation provided\nintuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare\nand has been adopted in internal projects. It should hold great value to the\nbiomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building\nblocks for robust information extraction pipeline construction."
                },
                "authors": [
                    {
                        "name": "Enshuo Hsu"
                    },
                    {
                        "name": "Kirk Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Kirk Roberts"
                },
                "author": "Kirk Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11778v1",
                "updated": "2024-11-18T17:55:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    55,
                    2,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:55:02Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    55,
                    2,
                    0,
                    323,
                    0
                ],
                "title": "Design And Optimization Of Multi-rendezvous Manoeuvres Based On\n  Reinforcement Learning And Convex Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design And Optimization Of Multi-rendezvous Manoeuvres Based On\n  Reinforcement Learning And Convex Optimization"
                },
                "summary": "Optimizing space vehicle routing is crucial for critical applications such as\non-orbit servicing, constellation deployment, and space debris de-orbiting.\nMulti-target Rendezvous presents a significant challenge in this domain. This\nproblem involves determining the optimal sequence in which to visit a set of\ntargets, and the corresponding optimal trajectories: this results in a\ndemanding NP-hard problem. We introduce a framework for the design and\nrefinement of multi-rendezvous trajectories based on heuristic combinatorial\noptimization and Sequential Convex Programming. Our framework is both highly\nmodular and capable of leveraging candidate solutions obtained with advanced\napproaches and handcrafted heuristics. We demonstrate this flexibility by\nintegrating an Attention-based routing policy trained with Reinforcement\nLearning to improve the performance of the combinatorial optimization process.\nWe show that Reinforcement Learning approaches for combinatorial optimization\ncan be effectively applied to spacecraft routing problems. We apply the\nproposed framework to the UARX Space OSSIE mission: we are able to thoroughly\nexplore the mission design space, finding optimal tours and trajectories for a\nwide variety of mission scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing space vehicle routing is crucial for critical applications such as\non-orbit servicing, constellation deployment, and space debris de-orbiting.\nMulti-target Rendezvous presents a significant challenge in this domain. This\nproblem involves determining the optimal sequence in which to visit a set of\ntargets, and the corresponding optimal trajectories: this results in a\ndemanding NP-hard problem. We introduce a framework for the design and\nrefinement of multi-rendezvous trajectories based on heuristic combinatorial\noptimization and Sequential Convex Programming. Our framework is both highly\nmodular and capable of leveraging candidate solutions obtained with advanced\napproaches and handcrafted heuristics. We demonstrate this flexibility by\nintegrating an Attention-based routing policy trained with Reinforcement\nLearning to improve the performance of the combinatorial optimization process.\nWe show that Reinforcement Learning approaches for combinatorial optimization\ncan be effectively applied to spacecraft routing problems. We apply the\nproposed framework to the UARX Space OSSIE mission: we are able to thoroughly\nexplore the mission design space, finding optimal tours and trajectories for a\nwide variety of mission scenarios."
                },
                "authors": [
                    {
                        "name": "Antonio Lpez Rivera"
                    },
                    {
                        "name": "Lucrezia Marcovaldi"
                    },
                    {
                        "name": "Jess Ramrez"
                    },
                    {
                        "name": "Alex Cuenca"
                    },
                    {
                        "name": "David Bermejo"
                    }
                ],
                "author_detail": {
                    "name": "David Bermejo"
                },
                "author": "David Bermejo",
                "arxiv_comment": "18 pages, 12 figures, 5 tables",
                "arxiv_journal_ref": "Proceedings of the International Astronautical Congress, 75, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11762v1",
                "updated": "2024-11-18T17:40:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    40,
                    43,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:40:43Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    40,
                    43,
                    0,
                    323,
                    0
                ],
                "title": "High-Speed Cornering Control and Real-Vehicle Deployment for Autonomous\n  Electric Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Speed Cornering Control and Real-Vehicle Deployment for Autonomous\n  Electric Vehicles"
                },
                "summary": "Executing drift maneuvers during high-speed cornering presents significant\nchallenges for autonomous vehicles, yet offers the potential to minimize\nturning time and enhance driving dynamics. While reinforcement learning (RL)\nhas shown promising results in simulated environments, discrepancies between\nsimulations and real-world conditions have limited its practical deployment.\nThis study introduces an innovative control framework that integrates\ntrajectory optimization with drift maneuvers, aiming to improve the algorithm's\nadaptability for real-vehicle implementation. We leveraged Bezier-based\npre-trajectory optimization to enhance rewards and optimize the controller\nthrough Twin Delayed Deep Deterministic Policy Gradient (TD3) in a simulated\nenvironment. For real-world deployment, we implement a hybrid RL-MPC fusion\nmechanism, , where TD3-derived maneuvers serve as primary inputs for a Model\nPredictive Controller (MPC). This integration enables precise real-time\ntracking of the optimal trajectory, with MPC providing corrective inputs to\nbridge the gap between simulation and reality. The efficacy of this method is\nvalidated through real-vehicle tests on consumer-grade electric vehicles,\nfocusing on drift U-turns and drift right-angle turns. The control outcomes of\nthese real-vehicle tests are thoroughly documented in the paper, supported by\nsupplementary video evidence (https://youtu.be/5wp67FcpfL8). Notably, this\nstudy is the first to deploy and apply an RL-based transient drift cornering\nalgorithm on consumer-grade electric vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing drift maneuvers during high-speed cornering presents significant\nchallenges for autonomous vehicles, yet offers the potential to minimize\nturning time and enhance driving dynamics. While reinforcement learning (RL)\nhas shown promising results in simulated environments, discrepancies between\nsimulations and real-world conditions have limited its practical deployment.\nThis study introduces an innovative control framework that integrates\ntrajectory optimization with drift maneuvers, aiming to improve the algorithm's\nadaptability for real-vehicle implementation. We leveraged Bezier-based\npre-trajectory optimization to enhance rewards and optimize the controller\nthrough Twin Delayed Deep Deterministic Policy Gradient (TD3) in a simulated\nenvironment. For real-world deployment, we implement a hybrid RL-MPC fusion\nmechanism, , where TD3-derived maneuvers serve as primary inputs for a Model\nPredictive Controller (MPC). This integration enables precise real-time\ntracking of the optimal trajectory, with MPC providing corrective inputs to\nbridge the gap between simulation and reality. The efficacy of this method is\nvalidated through real-vehicle tests on consumer-grade electric vehicles,\nfocusing on drift U-turns and drift right-angle turns. The control outcomes of\nthese real-vehicle tests are thoroughly documented in the paper, supported by\nsupplementary video evidence (https://youtu.be/5wp67FcpfL8). Notably, this\nstudy is the first to deploy and apply an RL-based transient drift cornering\nalgorithm on consumer-grade electric vehicles."
                },
                "authors": [
                    {
                        "name": "Shiyue Zhao"
                    },
                    {
                        "name": "Junzhi Zhang"
                    },
                    {
                        "name": "Neda Masoud"
                    },
                    {
                        "name": "Yuhong Jiang"
                    },
                    {
                        "name": "Heye Huang"
                    },
                    {
                        "name": "Tao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Liu"
                },
                "author": "Tao Liu",
                "arxiv_comment": "In the process of being submitted to the Journal of IEEE Transactions\n  on Industrial Electronics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22339v2",
                "updated": "2024-11-18T17:30:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    30,
                    47,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-11T18:47:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    18,
                    47,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "DAWN: Designing Distributed Agents in a Worldwide Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAWN: Designing Distributed Agents in a Worldwide Network"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has transformed them from\nbasic conversational tools into sophisticated entities capable of complex\nreasoning and decision-making. These advancements have led to the development\nof specialized LLM-based agents designed for diverse tasks such as coding and\nweb browsing. As these agents become more capable, the need for a robust\nframework that facilitates global communication and collaboration among them\ntowards advanced objectives has become increasingly critical. Distributed\nAgents in a Worldwide Network (DAWN) addresses this need by offering a\nversatile framework that integrates LLM-based agents with traditional software\nsystems, enabling the creation of agentic applications suited for a wide range\nof use cases. DAWN enables distributed agents worldwide to register and be\neasily discovered through Gateway Agents. Collaborations among these agents are\ncoordinated by a Principal Agent equipped with reasoning strategies. DAWN\noffers three operational modes: No-LLM Mode for deterministic tasks, Copilot\nfor augmented decision-making, and LLM Agent for autonomous operations.\nAdditionally, DAWN ensures the safety and security of agent collaborations\nglobally through a dedicated safety, security, and compliance layer, protecting\nthe network against attackers and adhering to stringent security and compliance\nstandards. These features make DAWN a robust network for deploying agent-based\napplications across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has transformed them from\nbasic conversational tools into sophisticated entities capable of complex\nreasoning and decision-making. These advancements have led to the development\nof specialized LLM-based agents designed for diverse tasks such as coding and\nweb browsing. As these agents become more capable, the need for a robust\nframework that facilitates global communication and collaboration among them\ntowards advanced objectives has become increasingly critical. Distributed\nAgents in a Worldwide Network (DAWN) addresses this need by offering a\nversatile framework that integrates LLM-based agents with traditional software\nsystems, enabling the creation of agentic applications suited for a wide range\nof use cases. DAWN enables distributed agents worldwide to register and be\neasily discovered through Gateway Agents. Collaborations among these agents are\ncoordinated by a Principal Agent equipped with reasoning strategies. DAWN\noffers three operational modes: No-LLM Mode for deterministic tasks, Copilot\nfor augmented decision-making, and LLM Agent for autonomous operations.\nAdditionally, DAWN ensures the safety and security of agent collaborations\nglobally through a dedicated safety, security, and compliance layer, protecting\nthe network against attackers and adhering to stringent security and compliance\nstandards. These features make DAWN a robust network for deploying agent-based\napplications across various industries."
                },
                "authors": [
                    {
                        "name": "Zahra Aminiranjbar"
                    },
                    {
                        "name": "Jianan Tang"
                    },
                    {
                        "name": "Qiudan Wang"
                    },
                    {
                        "name": "Shubha Pant"
                    },
                    {
                        "name": "Mahesh Viswanathan"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Viswanathan"
                },
                "author": "Mahesh Viswanathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11752v1",
                "updated": "2024-11-18T17:27:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    27,
                    56,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:27:56Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    27,
                    56,
                    0,
                    323,
                    0
                ],
                "title": "sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality\n  Spaces with LLMs and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality\n  Spaces with LLMs and Generative AI"
                },
                "summary": "In mixed reality (MR) environments, understanding space and creating virtual\nobjects is crucial to providing an intuitive and rich user experience. This\npaper introduces sMoRe (Spatial Mapping and Object Rendering Environment), an\nMR application that combines Generative AI (GenAI) with large language models\n(LLMs) to assist users in creating, placing, and managing virtual objects\nwithin physical spaces. sMoRe allows users to use voice or typed text commands\nto create and place virtual objects using GenAI while specifying spatial\nconstraints. The system leverages LLMs to interpret users' commands, analyze\nthe current scene, and identify optimal locations. Additionally, sMoRe\nintegrates text-to-3D generative AI to dynamically create 3D objects based on\nusers' descriptions. Our user study demonstrates the effectiveness of sMoRe in\nenhancing user comprehension, interaction, and organization of the MR\nenvironment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In mixed reality (MR) environments, understanding space and creating virtual\nobjects is crucial to providing an intuitive and rich user experience. This\npaper introduces sMoRe (Spatial Mapping and Object Rendering Environment), an\nMR application that combines Generative AI (GenAI) with large language models\n(LLMs) to assist users in creating, placing, and managing virtual objects\nwithin physical spaces. sMoRe allows users to use voice or typed text commands\nto create and place virtual objects using GenAI while specifying spatial\nconstraints. The system leverages LLMs to interpret users' commands, analyze\nthe current scene, and identify optimal locations. Additionally, sMoRe\nintegrates text-to-3D generative AI to dynamically create 3D objects based on\nusers' descriptions. Our user study demonstrates the effectiveness of sMoRe in\nenhancing user comprehension, interaction, and organization of the MR\nenvironment."
                },
                "authors": [
                    {
                        "name": "Yunhao Xing"
                    },
                    {
                        "name": "Que Liu"
                    },
                    {
                        "name": "Jingwu Wang"
                    },
                    {
                        "name": "Diego Gomez-Zara"
                    }
                ],
                "author_detail": {
                    "name": "Diego Gomez-Zara"
                },
                "author": "Diego Gomez-Zara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06153v2",
                "updated": "2024-11-18T17:25:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    25,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-08T15:52:42Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    52,
                    42,
                    1,
                    282,
                    0
                ],
                "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare."
                },
                "authors": [
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Likai Ma"
                    },
                    {
                        "name": "Jiahe Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11745v1",
                "updated": "2024-11-18T17:16:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:16:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks. Yet the substantial memory footprint of LLMs\nsignificantly hinders their deployment. In this paper, we improve the\naccessibility of LLMs through BitMoD, an algorithm-hardware co-design solution\nthat enables efficient LLM acceleration at low weight precision. On the\nalgorithm side, BitMoD introduces fine-grained data type adaptation that uses a\ndifferent numerical data type to quantize a group of (e.g., 128) weights.\nThrough the careful design of these new data types, BitMoD is able to quantize\nLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining\nhigh accuracy. On the hardware side, BitMoD employs a bit-serial processing\nelement to easily support multiple numerical precisions and data types; our\nhardware design includes two key innovations: First, it employs a unified\nrepresentation to process different weight data types, thus reducing the\nhardware cost. Second, it adopts a bit-serial dequantization unit to rescale\nthe per-group partial sum with minimal hardware overhead. Our evaluation on six\nrepresentative LLMs demonstrates that BitMoD significantly outperforms\nstate-of-the-art LLM quantization and acceleration methods. For discriminative\ntasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss\non average. For generative tasks, BitMoD is able to quantize LLM weights to\n3-bit while achieving better perplexity than prior LLM quantization scheme.\nCombining the superior model performance with an efficient accelerator design,\nBitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared\nto prior LLM accelerators ANT and OliVe, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks. Yet the substantial memory footprint of LLMs\nsignificantly hinders their deployment. In this paper, we improve the\naccessibility of LLMs through BitMoD, an algorithm-hardware co-design solution\nthat enables efficient LLM acceleration at low weight precision. On the\nalgorithm side, BitMoD introduces fine-grained data type adaptation that uses a\ndifferent numerical data type to quantize a group of (e.g., 128) weights.\nThrough the careful design of these new data types, BitMoD is able to quantize\nLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining\nhigh accuracy. On the hardware side, BitMoD employs a bit-serial processing\nelement to easily support multiple numerical precisions and data types; our\nhardware design includes two key innovations: First, it employs a unified\nrepresentation to process different weight data types, thus reducing the\nhardware cost. Second, it adopts a bit-serial dequantization unit to rescale\nthe per-group partial sum with minimal hardware overhead. Our evaluation on six\nrepresentative LLMs demonstrates that BitMoD significantly outperforms\nstate-of-the-art LLM quantization and acceleration methods. For discriminative\ntasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss\non average. For generative tasks, BitMoD is able to quantize LLM weights to\n3-bit while achieving better perplexity than prior LLM quantization scheme.\nCombining the superior model performance with an efficient accelerator design,\nBitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared\nto prior LLM accelerators ANT and OliVe, respectively."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Ahmed F. AbouElhamayed"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Marta Andronic"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02270v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02270v4",
                "updated": "2024-11-18T17:16:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    34,
                    0,
                    323,
                    0
                ],
                "published": "2023-06-04T06:27:17Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    6,
                    27,
                    17,
                    6,
                    155,
                    0
                ],
                "title": "Crypto-Ransomware and Their Defenses: In-depth Behavioral\n  Characterization, Discussion of Deployability, and New Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crypto-Ransomware and Their Defenses: In-depth Behavioral\n  Characterization, Discussion of Deployability, and New Insights"
                },
                "summary": "Crypto-ransomware has caused an unprecedented scope of impact in recent years\nwith an evolving level of sophistication. An extensive range of studies have\nbeen on defending against ransomware and reviewing the efficacy of various\nprotections. However, for practical defenses, deployability holds equal\nsignificance as detection accuracy. Therefore, in this study, we review 117\npublished ransomware defense works, categorize them by the level they are\nimplemented at, and discuss the deployability. API-based solutions are easy to\ndeploy and most existing works focus on machine learning-based classification.\nTo provide more insights, we quantitively characterize the runtime behaviors of\nreal-world ransomware samples. Based on our experimental findings, we present a\npossible future detection direction with our consistency analysis and\nAPI-contrast-based refinement. Moreover, we experimentally evaluate various\ncommercial defenses and identify the security gaps. Our findings help the field\nunderstand the deployability of ransomware defenses and create more effective,\npractical solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crypto-ransomware has caused an unprecedented scope of impact in recent years\nwith an evolving level of sophistication. An extensive range of studies have\nbeen on defending against ransomware and reviewing the efficacy of various\nprotections. However, for practical defenses, deployability holds equal\nsignificance as detection accuracy. Therefore, in this study, we review 117\npublished ransomware defense works, categorize them by the level they are\nimplemented at, and discuss the deployability. API-based solutions are easy to\ndeploy and most existing works focus on machine learning-based classification.\nTo provide more insights, we quantitively characterize the runtime behaviors of\nreal-world ransomware samples. Based on our experimental findings, we present a\npossible future detection direction with our consistency analysis and\nAPI-contrast-based refinement. Moreover, we experimentally evaluate various\ncommercial defenses and identify the security gaps. Our findings help the field\nunderstand the deployability of ransomware defenses and create more effective,\npractical solutions."
                },
                "authors": [
                    {
                        "name": "Wenjia Song"
                    },
                    {
                        "name": "Sanjula Karanam"
                    },
                    {
                        "name": "Ya Xiao"
                    },
                    {
                        "name": "Jingyuan Qi"
                    },
                    {
                        "name": "Nathan Dautenhahn"
                    },
                    {
                        "name": "Na Meng"
                    },
                    {
                        "name": "Elena Ferrari"
                    },
                    {
                        "name": "Danfeng"
                    },
                    {
                        "name": "Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yao"
                },
                "arxiv_affiliation": "Daphne",
                "author": "Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02270v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02270v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15367v2",
                "updated": "2024-11-18T17:00:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    0,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-18T18:36:18Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    18,
                    36,
                    18,
                    2,
                    262,
                    0
                ],
                "title": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss"
                },
                "summary": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation."
                },
                "authors": [
                    {
                        "name": "Andrei Chernov"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Chernov"
                },
                "author": "Andrei Chernov",
                "arxiv_comment": "4 main pages; 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11731v1",
                "updated": "2024-11-18T16:59:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    59,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:59:59Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    59,
                    0,
                    323,
                    0
                ],
                "title": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and\n  Ethical Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and\n  Ethical Alignment"
                },
                "summary": "We explore how large language models (LLMs) can be influenced by prompting\nthem to alter their initial decisions and align them with established ethical\nframeworks. Our study is based on two experiments designed to assess the\nsusceptibility of LLMs to moral persuasion. In the first experiment, we examine\nthe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally\nambiguous scenarios and observing how a Persuader Agent attempts to modify the\nBase Agent's initial decisions. The second experiment evaluates the\nsusceptibility of LLMs to align with predefined ethical frameworks by prompting\nthem to adopt specific value alignments rooted in established philosophical\ntheories. The results demonstrate that LLMs can indeed be persuaded in morally\ncharged scenarios, with the success of persuasion depending on factors such as\nthe model used, the complexity of the scenario, and the conversation length.\nNotably, LLMs of distinct sizes but from the same company produced markedly\ndifferent outcomes, highlighting the variability in their susceptibility to\nethical persuasion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore how large language models (LLMs) can be influenced by prompting\nthem to alter their initial decisions and align them with established ethical\nframeworks. Our study is based on two experiments designed to assess the\nsusceptibility of LLMs to moral persuasion. In the first experiment, we examine\nthe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally\nambiguous scenarios and observing how a Persuader Agent attempts to modify the\nBase Agent's initial decisions. The second experiment evaluates the\nsusceptibility of LLMs to align with predefined ethical frameworks by prompting\nthem to adopt specific value alignments rooted in established philosophical\ntheories. The results demonstrate that LLMs can indeed be persuaded in morally\ncharged scenarios, with the success of persuasion depending on factors such as\nthe model used, the complexity of the scenario, and the conversation length.\nNotably, LLMs of distinct sizes but from the same company produced markedly\ndifferent outcomes, highlighting the variability in their susceptibility to\nethical persuasion."
                },
                "authors": [
                    {
                        "name": "Allison Huang"
                    },
                    {
                        "name": "Yulu Niki Pi"
                    },
                    {
                        "name": "Carlos Mougan"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Mougan"
                },
                "author": "Carlos Mougan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11717v1",
                "updated": "2024-11-18T16:45:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    45,
                    44,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:45:44Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    45,
                    44,
                    0,
                    323,
                    0
                ],
                "title": "RAWMamba: Unified sRGB-to-RAW De-rendering With State Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAWMamba: Unified sRGB-to-RAW De-rendering With State Space Model"
                },
                "summary": "Recent advancements in sRGB-to-RAW de-rendering have increasingly emphasized\nmetadata-driven approaches to reconstruct RAW data from sRGB images,\nsupplemented by partial RAW information. In image-based de-rendering, metadata\nis commonly obtained through sampling, whereas in video tasks, it is typically\nderived from the initial frame. The distinct metadata requirements necessitate\nspecialized network architectures, leading to architectural incompatibilities\nthat increase deployment complexity. In this paper, we propose RAWMamba, a\nMamba-based unified framework developed for sRGB-to-RAW de-rendering across\nboth image and video domains. The core of RAWMamba is the Unified Metadata\nEmbedding (UME) module, which harmonizes diverse metadata types into a unified\nrepresentation. In detail, a multi-perspective affinity modeling method is\nproposed to promote the extraction of reference information. In addition, we\nintroduce the Local Tone-Aware Mamba (LTA-Mamba) module, which captures\nlong-range dependencies to enable effective global propagation of metadata.\nExperimental results demonstrate that the proposed RAWMamba achieves\nstate-of-the-art performance, yielding high-quality RAW data reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in sRGB-to-RAW de-rendering have increasingly emphasized\nmetadata-driven approaches to reconstruct RAW data from sRGB images,\nsupplemented by partial RAW information. In image-based de-rendering, metadata\nis commonly obtained through sampling, whereas in video tasks, it is typically\nderived from the initial frame. The distinct metadata requirements necessitate\nspecialized network architectures, leading to architectural incompatibilities\nthat increase deployment complexity. In this paper, we propose RAWMamba, a\nMamba-based unified framework developed for sRGB-to-RAW de-rendering across\nboth image and video domains. The core of RAWMamba is the Unified Metadata\nEmbedding (UME) module, which harmonizes diverse metadata types into a unified\nrepresentation. In detail, a multi-perspective affinity modeling method is\nproposed to promote the extraction of reference information. In addition, we\nintroduce the Local Tone-Aware Mamba (LTA-Mamba) module, which captures\nlong-range dependencies to enable effective global propagation of metadata.\nExperimental results demonstrate that the proposed RAWMamba achieves\nstate-of-the-art performance, yielding high-quality RAW data reconstruction."
                },
                "authors": [
                    {
                        "name": "Hongjun Chen"
                    },
                    {
                        "name": "Wencheng Han"
                    },
                    {
                        "name": "Huan Zheng"
                    },
                    {
                        "name": "Jianbing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jianbing Shen"
                },
                "author": "Jianbing Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11714v1",
                "updated": "2024-11-18T16:42:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    42,
                    7,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:42:07Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    42,
                    7,
                    0,
                    323,
                    0
                ],
                "title": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via\n  Skill Library and Tactile Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via\n  Skill Library and Tactile Representation"
                },
                "summary": "Deploying robots in open-world environments involves complex tasks\ncharacterized by long sequences and rich interactions, necessitating efficient\ntransfer of robotic skills across diverse and complex scenarios. To address\nthis challenge, we propose a skill library framework based on knowledge graphs,\nwhich endows robots with high-level skill awareness and spatial semantic\nunderstanding. The framework hierarchically organizes operational knowledge by\nconstructing a \"task graph\" and a \"scene graph\" to represent task and scene\nsemantic information, respectively. We introduce a \"state graph\" to facilitate\ninteraction between high-level task planning and low-level scene information.\nFurthermore, we propose a hierarchical transfer framework for operational\nskills. At the task level, the framework integrates contextual learning and\nchain-of-thought prompting within a four-stage prompt paradigm, leveraging\nlarge language models' (LLMs) reasoning and generalization capabilities to\nachieve task-level subtask sequence transfer. At the motion level, an adaptive\ntrajectory transfer method is developed using the A* algorithm and the skill\nlibrary, enabling motion-level adaptive trajectory transfer. At the physical\nlevel, we introduce an adaptive contour extraction and posture perception\nmethod based on tactile perception. This method dynamically obtains\nhigh-precision contour and posture information from visual-tactile texture data\nand adjusts transferred skills, such as contact positions and postures, to\nensure effectiveness in new environments. Experimental results validate the\neffectiveness of the proposed methods. Project\nwebsite:https://github.com/MingchaoQi/skill_transfer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying robots in open-world environments involves complex tasks\ncharacterized by long sequences and rich interactions, necessitating efficient\ntransfer of robotic skills across diverse and complex scenarios. To address\nthis challenge, we propose a skill library framework based on knowledge graphs,\nwhich endows robots with high-level skill awareness and spatial semantic\nunderstanding. The framework hierarchically organizes operational knowledge by\nconstructing a \"task graph\" and a \"scene graph\" to represent task and scene\nsemantic information, respectively. We introduce a \"state graph\" to facilitate\ninteraction between high-level task planning and low-level scene information.\nFurthermore, we propose a hierarchical transfer framework for operational\nskills. At the task level, the framework integrates contextual learning and\nchain-of-thought prompting within a four-stage prompt paradigm, leveraging\nlarge language models' (LLMs) reasoning and generalization capabilities to\nachieve task-level subtask sequence transfer. At the motion level, an adaptive\ntrajectory transfer method is developed using the A* algorithm and the skill\nlibrary, enabling motion-level adaptive trajectory transfer. At the physical\nlevel, we introduce an adaptive contour extraction and posture perception\nmethod based on tactile perception. This method dynamically obtains\nhigh-precision contour and posture information from visual-tactile texture data\nand adjusts transferred skills, such as contact positions and postures, to\nensure effectiveness in new environments. Experimental results validate the\neffectiveness of the proposed methods. Project\nwebsite:https://github.com/MingchaoQi/skill_transfer"
                },
                "authors": [
                    {
                        "name": "Mingchao Qi"
                    },
                    {
                        "name": "Yuanjin Li"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Zhengxiong Liu"
                    },
                    {
                        "name": "Panfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Panfeng Huang"
                },
                "author": "Panfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11707v1",
                "updated": "2024-11-18T16:34:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    34,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:34:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    34,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models"
                },
                "summary": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data."
                },
                "authors": [
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Guoqiang Ma"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11694v1",
                "updated": "2024-11-18T16:15:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:15:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search"
                },
                "summary": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. We thoroughly explore various design considerations necessary for\nimplementing this framework and provide a detailed report of the technical\naspects. To assess the effectiveness of our approach, we focus on mathematical\nreasoning tasks and conduct extensive evaluations on four challenging datasets,\nsignificantly enhancing the reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. We thoroughly explore various design considerations necessary for\nimplementing this framework and provide a detailed report of the technical\naspects. To assess the effectiveness of our approach, we focus on mathematical\nreasoning tasks and conduct extensive evaluations on four challenging datasets,\nsignificantly enhancing the reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Haoxiang Sun"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "LLM;Complex Reasoning;Math",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11937v1",
                "updated": "2024-11-18T16:12:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    12,
                    24,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:12:24Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    12,
                    24,
                    0,
                    323,
                    0
                ],
                "title": "Value Imprint: A Technique for Auditing the Human Values Embedded in\n  RLHF Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Imprint: A Technique for Auditing the Human Values Embedded in\n  RLHF Datasets"
                },
                "summary": "LLMs are increasingly fine-tuned using RLHF datasets to align them with human\npreferences and values. However, very limited research has investigated which\nspecific human values are operationalized through these datasets. In this\npaper, we introduce Value Imprint, a framework for auditing and classifying the\nhuman values embedded within RLHF datasets. To investigate the viability of\nthis framework, we conducted three case study experiments by auditing the\nAnthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM datasets to\nexamine the human values embedded within them. Our analysis involved a\ntwo-phase process. During the first phase, we developed a taxonomy of human\nvalues through an integrated review of prior works from philosophy, axiology,\nand ethics. Then, we applied this taxonomy to annotate 6,501 RLHF preferences.\nDuring the second phase, we employed the labels generated from the annotation\nas ground truth data for training a transformer-based machine learning model to\naudit and classify the three RLHF datasets. Through this approach, we\ndiscovered that information-utility values, including Wisdom/Knowledge and\nInformation Seeking, were the most dominant human values within all three RLHF\ndatasets. In contrast, prosocial and democratic values, including Well-being,\nJustice, and Human/Animal Rights, were the least represented human values.\nThese findings have significant implications for developing language models\nthat align with societal values and norms. We contribute our datasets to\nsupport further research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly fine-tuned using RLHF datasets to align them with human\npreferences and values. However, very limited research has investigated which\nspecific human values are operationalized through these datasets. In this\npaper, we introduce Value Imprint, a framework for auditing and classifying the\nhuman values embedded within RLHF datasets. To investigate the viability of\nthis framework, we conducted three case study experiments by auditing the\nAnthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM datasets to\nexamine the human values embedded within them. Our analysis involved a\ntwo-phase process. During the first phase, we developed a taxonomy of human\nvalues through an integrated review of prior works from philosophy, axiology,\nand ethics. Then, we applied this taxonomy to annotate 6,501 RLHF preferences.\nDuring the second phase, we employed the labels generated from the annotation\nas ground truth data for training a transformer-based machine learning model to\naudit and classify the three RLHF datasets. Through this approach, we\ndiscovered that information-utility values, including Wisdom/Knowledge and\nInformation Seeking, were the most dominant human values within all three RLHF\ndatasets. In contrast, prosocial and democratic values, including Well-being,\nJustice, and Human/Animal Rights, were the least represented human values.\nThese findings have significant implications for developing language models\nthat align with societal values and norms. We contribute our datasets to\nsupport further research in this area."
                },
                "authors": [
                    {
                        "name": "Ike Obi"
                    },
                    {
                        "name": "Rohan Pant"
                    },
                    {
                        "name": "Srishti Shekhar Agrawal"
                    },
                    {
                        "name": "Maham Ghazanfar"
                    },
                    {
                        "name": "Aaron Basiletti"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Basiletti"
                },
                "author": "Aaron Basiletti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v1",
                "updated": "2024-11-18T16:09:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the\n  Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the\n  Physical World"
                },
                "summary": "Robotic manipulation refers to the autonomous handling and interaction of\nrobots with objects using advanced techniques in robotics and artificial\nintelligence. The advent of powerful tools such as large language models (LLMs)\nand large vision-language models (LVLMs) has significantly enhanced the\ncapabilities of these robots in environmental perception and decision-making.\nHowever, the introduction of these intelligent agents has led to security\nthreats such as jailbreak attacks and adversarial attacks.\n  In this research, we take a further step by proposing a backdoor attack\nspecifically targeting robotic manipulation and, for the first time,\nimplementing backdoor attack in the physical world. By embedding a backdoor\nvisual language model into the visual perception module within the robotic\nsystem, we successfully mislead the robotic arm's operation in the physical\nworld, given the presence of common items as triggers. Experimental evaluations\nin the physical world demonstrate the effectiveness of the proposed backdoor\nattack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation refers to the autonomous handling and interaction of\nrobots with objects using advanced techniques in robotics and artificial\nintelligence. The advent of powerful tools such as large language models (LLMs)\nand large vision-language models (LVLMs) has significantly enhanced the\ncapabilities of these robots in environmental perception and decision-making.\nHowever, the introduction of these intelligent agents has led to security\nthreats such as jailbreak attacks and adversarial attacks.\n  In this research, we take a further step by proposing a backdoor attack\nspecifically targeting robotic manipulation and, for the first time,\nimplementing backdoor attack in the physical world. By embedding a backdoor\nvisual language model into the visual perception module within the robotic\nsystem, we successfully mislead the robotic arm's operation in the physical\nworld, given the presence of common items as triggers. Experimental evaluations\nin the physical world demonstrate the effectiveness of the proposed backdoor\nattack."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Wei Wan"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "Initial version with preliminary results. We welcome any feedback or\n  suggestions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11678v1",
                "updated": "2024-11-18T15:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    30,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T15:59:30Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    30,
                    0,
                    323,
                    0
                ],
                "title": "Analysis of Hardware Synthesis Strategies for Machine Learning in\n  Collider Trigger and Data Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Hardware Synthesis Strategies for Machine Learning in\n  Collider Trigger and Data Acquisition"
                },
                "summary": "To fully exploit the physics potential of current and future high energy\nparticle colliders, machine learning (ML) can be implemented in detector\nelectronics for intelligent data processing and acquisition. The implementation\nof ML in real-time at colliders requires very low latencies that are\nunachievable with a software-based approach, requiring optimization and\nsynthesis of ML algorithms for deployment on hardware. An analysis of neural\nnetwork inference efficiency is presented, focusing on the application of\ncollider trigger algorithms in field programmable gate arrays (FPGAs).\nTrade-offs are evaluated between two frameworks, the SLAC Neural Network\nLibrary (SNL) and hls4ml, in terms of resources and latency for different model\nsizes. Results highlight the strengths and limitations of each approach,\noffering valuable insights for optimizing real-time neural network deployments\nat colliders. This work aims to guide researchers and engineers in selecting\nthe most suitable hardware and software configurations for real-time,\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To fully exploit the physics potential of current and future high energy\nparticle colliders, machine learning (ML) can be implemented in detector\nelectronics for intelligent data processing and acquisition. The implementation\nof ML in real-time at colliders requires very low latencies that are\nunachievable with a software-based approach, requiring optimization and\nsynthesis of ML algorithms for deployment on hardware. An analysis of neural\nnetwork inference efficiency is presented, focusing on the application of\ncollider trigger algorithms in field programmable gate arrays (FPGAs).\nTrade-offs are evaluated between two frameworks, the SLAC Neural Network\nLibrary (SNL) and hls4ml, in terms of resources and latency for different model\nsizes. Results highlight the strengths and limitations of each approach,\noffering valuable insights for optimizing real-time neural network deployments\nat colliders. This work aims to guide researchers and engineers in selecting\nthe most suitable hardware and software configurations for real-time,\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Haoyi Jia"
                    },
                    {
                        "name": "Abhilasha Dave"
                    },
                    {
                        "name": "Julia Gonski"
                    },
                    {
                        "name": "Ryan Herbst"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Herbst"
                },
                "author": "Ryan Herbst",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]