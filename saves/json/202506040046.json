[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.12094v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v6",
                "updated": "2025-06-02T11:46:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    46,
                    43,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v3",
                "updated": "2025-06-03T06:43:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    43,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v3",
                "updated": "2025-06-02T02:08:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    8,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v2",
                "updated": "2025-06-02T01:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    8,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v3",
                "updated": "2025-06-01T16:00:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    16,
                    0,
                    34,
                    6,
                    152,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v2",
                "updated": "2025-06-01T10:36:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    10,
                    36,
                    7,
                    6,
                    152,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted to ACL Findings 2025. Please cite the ACL version. Code and\n  data are available at: https://github.com/NiuTrans/LaMaTE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v3",
                "updated": "2025-06-03T01:55:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    55,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17491v2",
                "updated": "2025-05-31T23:01:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    1,
                    0,
                    5,
                    151,
                    0
                ],
                "published": "2024-07-04T02:35:00Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    2,
                    35,
                    0,
                    3,
                    186,
                    0
                ],
                "title": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting"
                },
                "summary": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness."
                },
                "authors": [
                    {
                        "name": "Changdae Oh"
                    },
                    {
                        "name": "Gyeongdeok Seo"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Hosik Choi"
                    },
                    {
                        "name": "Jiyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "Extended work from the CVPR'23 paper: arxiv:2303.14773; This paper\n  has been submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v2",
                "updated": "2025-05-31T22:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    22,
                    12,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v6",
                "updated": "2025-05-31T17:58:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    17,
                    58,
                    24,
                    5,
                    151,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v2",
                "updated": "2025-05-31T13:43:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    13,
                    43,
                    36,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v4",
                "updated": "2025-05-31T04:45:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    45,
                    23,
                    5,
                    151,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "Accepted by ICML25. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v1",
                "updated": "2025-05-30T15:42:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24643v1",
                "updated": "2025-05-30T14:29:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:29:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching"
                },
                "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."
                },
                "authors": [
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Cecilia Bolaños"
                    },
                    {
                        "name": "Juan Tollo"
                    },
                    {
                        "name": "Giovanni Marraffini"
                    },
                    {
                        "name": "Agustín Gianolini"
                    },
                    {
                        "name": "Noe Hsueh"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v2",
                "updated": "2025-05-30T11:43:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    11,
                    43,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v1",
                "updated": "2025-05-30T08:49:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. Code is available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. Code is available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24221v1",
                "updated": "2025-05-30T05:17:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T05:17:44Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "title": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management"
                },
                "summary": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Wenzhe Zhu"
                    },
                    {
                        "name": "Yongkun Li"
                    },
                    {
                        "name": "Yinlong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinlong Xu"
                },
                "author": "Yinlong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v2",
                "updated": "2025-06-03T03:32:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    32,
                    10,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24095v1",
                "updated": "2025-05-30T00:46:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v3",
                "updated": "2025-05-30T00:36:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    36,
                    37,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23970v1",
                "updated": "2025-05-29T19:52:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T19:52:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving"
                },
                "summary": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low."
                },
                "authors": [
                    {
                        "name": "Yuyang Tian"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23938v1",
                "updated": "2025-05-29T18:41:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T18:41:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "Digital Forensic Investigation of the ChatGPT Windows Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Forensic Investigation of the ChatGPT Windows Application"
                },
                "summary": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics."
                },
                "authors": [
                    {
                        "name": "Malithi Wanniarachchi Kankanamge"
                    },
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Santiago Carmona"
                    },
                    {
                        "name": "Syed Mhamudul Hasan"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v1",
                "updated": "2025-05-29T17:12:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v1",
                "updated": "2025-05-29T13:05:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21889v2",
                "updated": "2025-05-29T12:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    59,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T02:07:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse"
                },
                "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Cheater Lin"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Xianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zhang"
                },
                "author": "Xianwei Zhang",
                "arxiv_comment": "31st International European Conference on Parallel and Distributed\n  Computing (Euro-Par 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23351v1",
                "updated": "2025-05-29T11:16:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T11:16:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores"
                },
                "summary": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods."
                },
                "authors": [
                    {
                        "name": "Sudam M. Wasala"
                    },
                    {
                        "name": "Jurre Wolff"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Anuj Pathania"
                    },
                    {
                        "name": "Clemens Grelck"
                    },
                    {
                        "name": "Andy D. Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Andy D. Pimentel"
                },
                "author": "Andy D. Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23275v1",
                "updated": "2025-05-29T09:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception"
                },
                "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v2",
                "updated": "2025-05-29T09:18:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    18,
                    35,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23258v1",
                "updated": "2025-05-29T09:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:06:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System"
                },
                "summary": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading."
                },
                "authors": [
                    {
                        "name": "Haojie Jia"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "9 pages, In Proceedings of IEEE ICCCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v4",
                "updated": "2025-05-29T09:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    1,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "arxiv_comment": "52 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v2",
                "updated": "2025-05-29T03:11:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    11,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v2",
                "updated": "2025-05-29T01:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    34,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22927v1",
                "updated": "2025-05-28T22:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs"
                },
                "summary": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array."
                },
                "authors": [
                    {
                        "name": "Robert Marosi"
                    },
                    {
                        "name": "Muhammed Zuboraj"
                    },
                    {
                        "name": "Filippo Capolino"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Capolino"
                },
                "author": "Filippo Capolino",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v1",
                "updated": "2025-05-28T22:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v6",
                "updated": "2025-05-28T18:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v4",
                "updated": "2025-05-28T12:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    7,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v1",
                "updated": "2025-05-28T09:20:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21919v1",
                "updated": "2025-05-28T03:05:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T03:05:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference"
                },
                "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eun Kyung Lee"
                },
                "author": "Eun Kyung Lee",
                "arxiv_comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v2",
                "updated": "2025-05-28T01:38:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    38,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21669v1",
                "updated": "2025-05-27T18:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T18:47:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "Improved Prefetching Techniques for Linked Data Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Prefetching Techniques for Linked Data Structures"
                },
                "summary": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%."
                },
                "authors": [
                    {
                        "name": "Nikola Vuk Maruszewski"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Vuk Maruszewski"
                },
                "author": "Nikola Vuk Maruszewski",
                "arxiv_comment": "73 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.5.3; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20438v1",
                "updated": "2025-05-26T18:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T18:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMburger: Accelerating LLM Inference via Token Smashing"
                },
                "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v5",
                "updated": "2025-05-26T16:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    16,
                    43,
                    0,
                    146,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v2",
                "updated": "2025-05-26T13:20:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    20,
                    45,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Jiaheng Dai"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Tianqi Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v1",
                "updated": "2025-05-26T12:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 18th International Conference\n  on Cloud Computing (CLOUD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v1",
                "updated": "2025-05-26T11:35:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v2",
                "updated": "2025-05-26T10:07:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    7,
                    5,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xinyu Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17062v3",
                "updated": "2025-05-26T08:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    39,
                    26,
                    0,
                    146,
                    0
                ],
                "published": "2024-05-27T11:31:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    31,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation"
                },
                "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v2",
                "updated": "2025-05-26T07:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    30,
                    17,
                    0,
                    146,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19602v1",
                "updated": "2025-05-26T07:11:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T07:11:42Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression"
                },
                "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."
                },
                "authors": [
                    {
                        "name": "Kunjun Li"
                    },
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Cheng-Yen Yang"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Jenq-Neng Hwang"
                },
                "author": "Jenq-Neng Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v1",
                "updated": "2025-05-26T05:58:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v2",
                "updated": "2025-05-26T05:56:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    56,
                    51,
                    0,
                    146,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v2",
                "updated": "2025-05-26T05:28:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    28,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v2",
                "updated": "2025-05-25T13:03:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    13,
                    3,
                    54,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19089v1",
                "updated": "2025-05-25T10:57:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-25T10:57:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation"
                },
                "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v4",
                "updated": "2025-05-25T05:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    5,
                    26,
                    2,
                    6,
                    145,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v1",
                "updated": "2025-05-24T17:46:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v3",
                "updated": "2025-05-24T17:39:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    39,
                    32,
                    5,
                    144,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "18 pages, 14 figures. Accepted by ICML 2025. The code is available at\n  https://github.com/wenhao728/AsymRnR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16366v2",
                "updated": "2025-05-24T17:04:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    4,
                    26,
                    5,
                    144,
                    0
                ],
                "published": "2024-04-25T07:09:05Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    7,
                    9,
                    5,
                    3,
                    116,
                    0
                ],
                "title": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection"
                },
                "summary": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jinke Shi"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Haishuai Wang"
                    },
                    {
                        "name": "Jiajun Bu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Bu"
                },
                "author": "Jiajun Bu",
                "arxiv_comment": "Accepted by IEEE TNNLS (14 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20334v1",
                "updated": "2025-05-24T10:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query"
                },
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v2",
                "updated": "2025-05-24T09:33:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    33,
                    35,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18610v1",
                "updated": "2025-05-24T09:18:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T09:18:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs"
                },
                "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."
                },
                "authors": [
                    {
                        "name": "Tengxuan Liu"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Jiayi Yang"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18577v1",
                "updated": "2025-05-24T07:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T07:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "title": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance"
                },
                "summary": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies."
                },
                "authors": [
                    {
                        "name": "Dongsuk Oh"
                    },
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Eunjee Na"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Hyunkyu Choi"
                    },
                    {
                        "name": "Seonghyeon Jang"
                    },
                    {
                        "name": "Hanjin Choi"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18554v1",
                "updated": "2025-05-24T06:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T06:45:16Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "title": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads"
                },
                "summary": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively."
                },
                "authors": [
                    {
                        "name": "Jaewon Kwon"
                    },
                    {
                        "name": "Yongju Lee"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Enhyeok Jang"
                    },
                    {
                        "name": "Hongju Kal"
                    },
                    {
                        "name": "Won Woo Ro"
                    }
                ],
                "author_detail": {
                    "name": "Won Woo Ro"
                },
                "arxiv_affiliation": "Yonsei University, Seoul, Republic of Korea",
                "author": "Won Woo Ro",
                "arxiv_comment": "Accepted to ISCA '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v2",
                "updated": "2025-05-24T04:37:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    4,
                    37,
                    34,
                    5,
                    144,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "2025 ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v1",
                "updated": "2025-05-23T18:46:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20325v1",
                "updated": "2025-05-23T18:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:19:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence"
                },
                "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques."
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Keith G. Mills"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v3",
                "updated": "2025-05-23T17:02:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    2,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v3",
                "updated": "2025-05-23T16:36:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    36,
                    12,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18013v1",
                "updated": "2025-05-23T15:15:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T15:15:21Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "title": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence"
                },
                "summary": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively."
                },
                "authors": [
                    {
                        "name": "Hanze Zhang"
                    },
                    {
                        "name": "Kaiming Wang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17934v1",
                "updated": "2025-05-23T14:12:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T14:12:05Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "title": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications"
                },
                "summary": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications."
                },
                "authors": [
                    {
                        "name": "Marcin Lawenda"
                    },
                    {
                        "name": "Łukasz Szustak"
                    },
                    {
                        "name": "László Környei"
                    },
                    {
                        "name": "Flavio Cesar Cunha Galeazzo"
                    },
                    {
                        "name": "Paweł Bratek"
                    }
                ],
                "author_detail": {
                    "name": "Paweł Bratek"
                },
                "author": "Paweł Bratek",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18231v1",
                "updated": "2025-05-23T12:40:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:40:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache"
                },
                "summary": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines."
                },
                "authors": [
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17787v1",
                "updated": "2025-05-23T12:00:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:00:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration"
                },
                "summary": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration."
                },
                "authors": [
                    {
                        "name": "Peilin Chen"
                    },
                    {
                        "name": "Xiaoxuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Yang"
                },
                "author": "Xiaoxuan Yang",
                "arxiv_comment": "Accepted to GLSVLSI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v2",
                "updated": "2025-05-23T11:59:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    59,
                    22,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v4",
                "updated": "2025-05-23T10:45:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    45,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17694v1",
                "updated": "2025-05-23T10:03:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T10:03:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding"
                },
                "summary": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Rui Ning"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Mo Zhou"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v3",
                "updated": "2025-05-23T10:01:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    1,
                    57,
                    4,
                    143,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v3",
                "updated": "2025-05-23T09:33:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    33,
                    32,
                    4,
                    143,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "ACL25 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v2",
                "updated": "2025-05-23T08:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    12,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v2",
                "updated": "2025-05-23T07:07:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    7,
                    7,
                    29,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v2",
                "updated": "2025-05-23T04:58:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    58,
                    47,
                    4,
                    143,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values"
                },
                "summary": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Mohammadreza Nemati"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v1",
                "updated": "2025-05-22T22:54:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17272v1",
                "updated": "2025-05-22T20:39:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T20:39:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zebra-Llama: Towards Extremely Efficient Hybrid Models"
                },
                "summary": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance."
                },
                "authors": [
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v2",
                "updated": "2025-05-22T20:10:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    10,
                    16,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v1",
                "updated": "2025-05-22T17:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.23932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23932v2",
                "updated": "2025-06-02T17:42:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    42,
                    36,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T18:28:02Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    28,
                    2,
                    3,
                    149,
                    0
                ],
                "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue\n  Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue\n  Solving"
                },
                "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io"
                },
                "authors": [
                    {
                        "name": "Wendong Xu"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Qiujiang Chen"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Z. Morley Mao"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23487v2",
                "updated": "2025-06-02T17:37:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    37,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-30T15:41:55Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    15,
                    41,
                    55,
                    6,
                    89,
                    0
                ],
                "title": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners"
                },
                "summary": "Large Language Models (LLMs) have been found to struggle with systematic\nreasoning. Even on tasks where they appear to perform well, their performance\noften depends on shortcuts, rather than on genuine reasoning abilities, leading\nthem to collapse on out-of-distribution (OOD) examples. Post-training\nstrategies based on reinforcement learning and chain-of-thought prompting have\nrecently been hailed as a step change. However, little is known about the\npotential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and\nprogramming-based problem solving, where genuine OOD problems can be sparse. In\nthis paper, we focus on tasks that require systematic relational composition\nfor qualitative spatial and temporal reasoning. The setting allows fine control\nover problem difficulty to precisely measure OOD generalization. We find that,\nzero-shot LRMs generally outperform their LLM counterparts in single-path\nreasoning tasks but struggle in the multi-path setting. Whilst showing\ncomparatively better results, fine-tuned LLMs are also not capable of\nmulti-path generalization. We also provide evidence for the behavioral\ninterpretation for this, i.e., that LRMs are shallow disjunctive reasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been found to struggle with systematic\nreasoning. Even on tasks where they appear to perform well, their performance\noften depends on shortcuts, rather than on genuine reasoning abilities, leading\nthem to collapse on out-of-distribution (OOD) examples. Post-training\nstrategies based on reinforcement learning and chain-of-thought prompting have\nrecently been hailed as a step change. However, little is known about the\npotential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and\nprogramming-based problem solving, where genuine OOD problems can be sparse. In\nthis paper, we focus on tasks that require systematic relational composition\nfor qualitative spatial and temporal reasoning. The setting allows fine control\nover problem difficulty to precisely measure OOD generalization. We find that,\nzero-shot LRMs generally outperform their LLM counterparts in single-path\nreasoning tasks but struggle in the multi-path setting. Whilst showing\ncomparatively better results, fine-tuned LLMs are also not capable of\nmulti-path generalization. We also provide evidence for the behavioral\ninterpretation for this, i.e., that LRMs are shallow disjunctive reasoners."
                },
                "authors": [
                    {
                        "name": "Irtaza Khalid"
                    },
                    {
                        "name": "Amir Masoud Nourollah"
                    },
                    {
                        "name": "Steven Schockaert"
                    }
                ],
                "author_detail": {
                    "name": "Steven Schockaert"
                },
                "author": "Steven Schockaert",
                "arxiv_comment": "ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24803v2",
                "updated": "2025-06-02T17:37:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    37,
                    17,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T17:08:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    8,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "Guiding Generative Storytelling with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Generative Storytelling with Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."
                },
                "authors": [
                    {
                        "name": "Zhijun Pan"
                    },
                    {
                        "name": "Antonios Andronis"
                    },
                    {
                        "name": "Eva Hayek"
                    },
                    {
                        "name": "Oscar AP Wilkinson"
                    },
                    {
                        "name": "Ilya Lasy"
                    },
                    {
                        "name": "Annette Parry"
                    },
                    {
                        "name": "Guy Gadney"
                    },
                    {
                        "name": "Tim J. Smith"
                    },
                    {
                        "name": "Mick Grierson"
                    }
                ],
                "author_detail": {
                    "name": "Mick Grierson"
                },
                "author": "Mick Grierson",
                "arxiv_comment": "This manuscript was submitted for peer review in January 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04864v2",
                "updated": "2025-06-02T17:31:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    31,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-07T09:19:28Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    19,
                    28,
                    0,
                    97,
                    0
                ],
                "title": "Statistical parametric simulation studies based on real data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical parametric simulation studies based on real data"
                },
                "summary": "Simulation studies are indispensable for evaluating and comparing statistical\nmethods. The most common simulation approach is parametric simulation, where\nthe data-generating mechanism (DGM) corresponds to a predefined parametric\nmodel from which observations are drawn. Many statistical simulation studies\naim to provide practical recommendations on a method's suitability for a given\napplication; however, parametric simulations in particular are frequently\ncriticized for being too simplistic and not reflecting reality. To overcome\nthis drawback, it is generally considered a sensible approach to employ real\ndata for constructing the parametric DGMs. However, while the concept of\nreal-data-based parametric DGMs is widely recognized, the specific ways in\nwhich DGM components are inferred from real data vary, and their implications\nmay not always be well understood. Additionally, researchers often rely on a\nlimited selection of real datasets, with the rationale for their selection\noften unclear. This paper addresses these issues by formally discussing how\ncomponents of parametric DGMs can be inferred from real data and how dataset\nselection can be performed more systematically. By doing so, we aim to support\nresearchers in conducting simulation studies with a lower risk of\novergeneralization and misinterpretation. We illustrate the construction of\nparametric DGMs based on a systematically selected set of real datasets using\ntwo examples: one on ordinal outcomes in randomized controlled trials and one\non differential gene expression analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation studies are indispensable for evaluating and comparing statistical\nmethods. The most common simulation approach is parametric simulation, where\nthe data-generating mechanism (DGM) corresponds to a predefined parametric\nmodel from which observations are drawn. Many statistical simulation studies\naim to provide practical recommendations on a method's suitability for a given\napplication; however, parametric simulations in particular are frequently\ncriticized for being too simplistic and not reflecting reality. To overcome\nthis drawback, it is generally considered a sensible approach to employ real\ndata for constructing the parametric DGMs. However, while the concept of\nreal-data-based parametric DGMs is widely recognized, the specific ways in\nwhich DGM components are inferred from real data vary, and their implications\nmay not always be well understood. Additionally, researchers often rely on a\nlimited selection of real datasets, with the rationale for their selection\noften unclear. This paper addresses these issues by formally discussing how\ncomponents of parametric DGMs can be inferred from real data and how dataset\nselection can be performed more systematically. By doing so, we aim to support\nresearchers in conducting simulation studies with a lower risk of\novergeneralization and misinterpretation. We illustrate the construction of\nparametric DGMs based on a systematically selected set of real datasets using\ntwo examples: one on ordinal outcomes in randomized controlled trials and one\non differential gene expression analysis."
                },
                "authors": [
                    {
                        "name": "Christina Sauer"
                    },
                    {
                        "name": "F. Julian D. Lange"
                    },
                    {
                        "name": "Maria Thurow"
                    },
                    {
                        "name": "Ina Dormuth"
                    },
                    {
                        "name": "Anne-Laure Boulesteix"
                    }
                ],
                "author_detail": {
                    "name": "Anne-Laure Boulesteix"
                },
                "author": "Anne-Laure Boulesteix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08826v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08826v3",
                "updated": "2025-06-02T17:15:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    15,
                    8,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-12T22:33:41Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    22,
                    33,
                    41,
                    2,
                    43,
                    0
                ],
                "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Abootorabi"
                    },
                    {
                        "name": "Amirhosein Zobeiri"
                    },
                    {
                        "name": "Mahdi Dehghani"
                    },
                    {
                        "name": "Mohammadali Mohammadkhani"
                    },
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Omid Ghahroodi"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    },
                    {
                        "name": "Ehsaneddin Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Ehsaneddin Asgari"
                },
                "author": "Ehsaneddin Asgari",
                "arxiv_comment": "GitHub repository:\n  https://github.com/llm-lab-org/Multimodal-RAG-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08826v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08826v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17536v2",
                "updated": "2025-06-02T17:10:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    10,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-23T06:41:54Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    6,
                    41,
                    54,
                    4,
                    143,
                    0
                ],
                "title": "Multimodal Conversation Structure Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Conversation Structure Understanding"
                },
                "summary": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure."
                },
                "authors": [
                    {
                        "name": "Kent K. Chang"
                    },
                    {
                        "name": "Mackenzie Hanh Cramer"
                    },
                    {
                        "name": "Anna Ho"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Yilin Yuan"
                    },
                    {
                        "name": "David Bamman"
                    }
                ],
                "author_detail": {
                    "name": "David Bamman"
                },
                "author": "David Bamman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02669v2",
                "updated": "2025-06-02T16:48:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    48,
                    29,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-05T21:36:38Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    21,
                    36,
                    38,
                    6,
                    5,
                    0
                ],
                "title": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate\n  Modality Imbalance in VLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate\n  Modality Imbalance in VLMs?"
                },
                "summary": "Vision Language Models (VLMs) are impressive at visual question answering and\nimage captioning. But they underperform on multi-step visual reasoning -- even\ncompared to LLMs on the same tasks presented in text form -- giving rise to\nperceptions of modality imbalance or brittleness. Towards a systematic study of\nsuch issues, we introduce a synthetic framework for assessing the ability of\nVLMs to perform algorithmic visual reasoning, comprising three tasks: Table\nReadout, Grid Navigation, and Visual Analogy. Each has two levels of\ndifficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for\nfrontier VLMs. We propose strategies for training on the SIMPLE version of\ntasks that improve performance on the corresponding HARD task, i.e.,\nsimple-to-hard (S2H) generalization. This controlled setup, where each task\nalso has an equivalent text-only version, allows a quantification of the\nmodality imbalance and how it is impacted by training strategy. We show that 1)\nexplicit image-to-text conversion is important in promoting S2H generalization\non images, by transferring reasoning from text; 2) conversion can be\ninternalized at test time. We also report results of mechanistic study of this\nphenomenon. We identify measures of gradient alignment that can identify\ntraining strategies that promote better S2H generalization. Ablations highlight\nthe importance of chain-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) are impressive at visual question answering and\nimage captioning. But they underperform on multi-step visual reasoning -- even\ncompared to LLMs on the same tasks presented in text form -- giving rise to\nperceptions of modality imbalance or brittleness. Towards a systematic study of\nsuch issues, we introduce a synthetic framework for assessing the ability of\nVLMs to perform algorithmic visual reasoning, comprising three tasks: Table\nReadout, Grid Navigation, and Visual Analogy. Each has two levels of\ndifficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for\nfrontier VLMs. We propose strategies for training on the SIMPLE version of\ntasks that improve performance on the corresponding HARD task, i.e.,\nsimple-to-hard (S2H) generalization. This controlled setup, where each task\nalso has an equivalent text-only version, allows a quantification of the\nmodality imbalance and how it is impacted by training strategy. We show that 1)\nexplicit image-to-text conversion is important in promoting S2H generalization\non images, by transferring reasoning from text; 2) conversion can be\ninternalized at test time. We also report results of mechanistic study of this\nphenomenon. We identify measures of gradient alignment that can identify\ntraining strategies that promote better S2H generalization. Ablations highlight\nthe importance of chain-of-thought."
                },
                "authors": [
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Abhishek Panigrahi"
                    },
                    {
                        "name": "Yun Cheng"
                    },
                    {
                        "name": "Dingli Yu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19358v3",
                "updated": "2025-06-02T16:30:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    30,
                    23,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-31T18:10:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking"
                },
                "summary": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "The paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15455v2",
                "updated": "2025-06-02T16:26:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    26,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-21T13:30:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    30,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning"
                },
                "summary": "Fine-tuning large language models (LLMs) is computationally expensive, and\nLow-Rank Adaptation (LoRA) provides a cost-effective solution by approximating\nweight updates through low-rank matrices. In real-world scenarios, LLMs are\nfine-tuned on data from multiple domains to perform tasks across various\nfields, embodying multi-task learning (MTL). LoRA often underperforms in such\ncomplex scenarios. To enhance LoRA's capability in multi-task learning, we\npropose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head\nRandomization diversifies the head matrices through Multi-Head Dropout and\nMulti-Head Random Initialization, enabling more efficient learning of\ntask-specific features while maintaining shared knowledge representation. Our\napproach not only improves performance in MTL but also reduces GPU memory usage\nand training time. Experiments show that R-LoRA's gains stem from increased\ndiversity in the head matrices, demonstrating its effectiveness for multi-task\nlearning. The code is available at https://github.com/jinda-liu/R-LoRA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is computationally expensive, and\nLow-Rank Adaptation (LoRA) provides a cost-effective solution by approximating\nweight updates through low-rank matrices. In real-world scenarios, LLMs are\nfine-tuned on data from multiple domains to perform tasks across various\nfields, embodying multi-task learning (MTL). LoRA often underperforms in such\ncomplex scenarios. To enhance LoRA's capability in multi-task learning, we\npropose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head\nRandomization diversifies the head matrices through Multi-Head Dropout and\nMulti-Head Random Initialization, enabling more efficient learning of\ntask-specific features while maintaining shared knowledge representation. Our\napproach not only improves performance in MTL but also reduces GPU memory usage\nand training time. Experiments show that R-LoRA's gains stem from increased\ndiversity in the head matrices, demonstrating its effectiveness for multi-task\nlearning. The code is available at https://github.com/jinda-liu/R-LoRA"
                },
                "authors": [
                    {
                        "name": "Jinda Liu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17779v3",
                "updated": "2025-06-02T16:25:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    25,
                    13,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-23T18:38:38Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    38,
                    38,
                    0,
                    358,
                    0
                ],
                "title": "Ergodic Network Stochastic Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ergodic Network Stochastic Differential Equations"
                },
                "summary": "We propose a novel framework for Network Stochastic Differential Equations\n(N-SDE), where each node in a network is governed by an SDE influenced by\ninteractions with its neighbors. The evolution of each node is driven by the\ninterplay of three key components: the node's intrinsic dynamics\n(\\emph{momentum effect}), feedback from neighboring nodes (\\emph{network\neffect}), and a \\emph{stochastic volatility} term modeled by Brownian motion.\nOur primary objective is to estimate the parameters of the N-SDE system from\nhigh-frequency discrete-time observations. The motivation behind this model\nlies in its ability to analyze very high-dimensional time series by leveraging\nthe inherent sparsity of the underlying network graph. We consider two distinct\nscenarios: \\textit{i) known network structure}: the graph is fully specified,\nand we establish conditions under which the parameters can be identified,\nconsidering the linear growth of the parameter space with the number of edges.\n\\textit{ii) unknown network structure}: the graph must be inferred from the\ndata. For this, we develop an iterative procedure using adaptive Lasso,\ntailored to a specific subclass of N-SDE models. In this work, we assume the\nnetwork graph is oriented, paving the way for novel applications of SDEs in\ncausal inference, enabling the study of cause-effect relationships in dynamic\nsystems. Through extensive simulation studies, we demonstrate the performance\nof our estimators across various graph topologies in high-dimensional settings.\nWe also showcase the framework's applicability to real-world datasets,\nhighlighting its potential for advancing the analysis of complex networked\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for Network Stochastic Differential Equations\n(N-SDE), where each node in a network is governed by an SDE influenced by\ninteractions with its neighbors. The evolution of each node is driven by the\ninterplay of three key components: the node's intrinsic dynamics\n(\\emph{momentum effect}), feedback from neighboring nodes (\\emph{network\neffect}), and a \\emph{stochastic volatility} term modeled by Brownian motion.\nOur primary objective is to estimate the parameters of the N-SDE system from\nhigh-frequency discrete-time observations. The motivation behind this model\nlies in its ability to analyze very high-dimensional time series by leveraging\nthe inherent sparsity of the underlying network graph. We consider two distinct\nscenarios: \\textit{i) known network structure}: the graph is fully specified,\nand we establish conditions under which the parameters can be identified,\nconsidering the linear growth of the parameter space with the number of edges.\n\\textit{ii) unknown network structure}: the graph must be inferred from the\ndata. For this, we develop an iterative procedure using adaptive Lasso,\ntailored to a specific subclass of N-SDE models. In this work, we assume the\nnetwork graph is oriented, paving the way for novel applications of SDEs in\ncausal inference, enabling the study of cause-effect relationships in dynamic\nsystems. Through extensive simulation studies, we demonstrate the performance\nof our estimators across various graph topologies in high-dimensional settings.\nWe also showcase the framework's applicability to real-world datasets,\nhighlighting its potential for advancing the analysis of complex networked\nsystems."
                },
                "authors": [
                    {
                        "name": "Francesco Iafrate"
                    },
                    {
                        "name": "Stefano Iacus"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Iacus"
                },
                "author": "Stefano Iacus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17823v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17823v3",
                "updated": "2025-06-02T16:21:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    21,
                    11,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-29T18:15:49Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    15,
                    49,
                    2,
                    29,
                    0
                ],
                "title": "Robust Multimodal Learning via Cross-Modal Proxy Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Multimodal Learning via Cross-Modal Proxy Tokens"
                },
                "summary": "Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality without requiring explicit modality generation\nor auxiliary networks. To efficiently learn these approximations with minimal\ncomputational overhead, we employ low-rank adapters in frozen unimodal encoders\nand jointly optimize an alignment loss with a task-specific loss. Extensive\nexperiments on five multimodal datasets show that our method outperforms\nstate-of-the-art baselines across various missing rates while achieving\ncompetitive results in complete-modality settings. Overall, our method offers a\nflexible and efficient solution for robust multimodal learning. The code and\npretrained models will be released on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality without requiring explicit modality generation\nor auxiliary networks. To efficiently learn these approximations with minimal\ncomputational overhead, we employ low-rank adapters in frozen unimodal encoders\nand jointly optimize an alignment loss with a task-specific loss. Extensive\nexperiments on five multimodal datasets show that our method outperforms\nstate-of-the-art baselines across various missing rates while achieving\ncompetitive results in complete-modality settings. Overall, our method offers a\nflexible and efficient solution for robust multimodal learning. The code and\npretrained models will be released on GitHub."
                },
                "authors": [
                    {
                        "name": "Md Kaykobad Reza"
                    },
                    {
                        "name": "Ameya Patil"
                    },
                    {
                        "name": "Mashhour Solh"
                    },
                    {
                        "name": "M. Salman Asif"
                    }
                ],
                "author_detail": {
                    "name": "M. Salman Asif"
                },
                "author": "M. Salman Asif",
                "arxiv_comment": "21 Pages, 9 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17823v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17823v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23786v2",
                "updated": "2025-06-02T16:21:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    21,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-24T16:30:37Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    16,
                    30,
                    37,
                    5,
                    144,
                    0
                ],
                "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: A Practical Attack on GGUF Quantization"
                },
                "summary": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\n`ollama` and `llama.cpp` frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\n`ollama` and `llama.cpp` frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense."
                },
                "authors": [
                    {
                        "name": "Kazuki Egashira"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23799v2",
                "updated": "2025-06-02T15:55:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    55,
                    44,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-26T16:53:47Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    53,
                    47,
                    0,
                    146,
                    0
                ],
                "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics"
                },
                "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used."
                },
                "authors": [
                    {
                        "name": "Xiaoyuan Wu"
                    },
                    {
                        "name": "Weiran Lin"
                    },
                    {
                        "name": "Omer Akgul"
                    },
                    {
                        "name": "Lujo Bauer"
                    }
                ],
                "author_detail": {
                    "name": "Lujo Bauer"
                },
                "author": "Lujo Bauer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17004v2",
                "updated": "2025-06-02T15:53:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    53,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-23T18:00:07Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    0,
                    7,
                    2,
                    113,
                    0
                ],
                "title": "(Im)possibility of Automated Hallucination Detection in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Im)possibility of Automated Hallucination Detection in Large Language\n  Models"
                },
                "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."
                },
                "authors": [
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Omar Montasser"
                    },
                    {
                        "name": "John Sous"
                    },
                    {
                        "name": "Grigoris Velegkas"
                    }
                ],
                "author_detail": {
                    "name": "Grigoris Velegkas"
                },
                "author": "Grigoris Velegkas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07580v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07580v3",
                "updated": "2025-06-02T15:44:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    44,
                    17,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-10T17:45:30Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    45,
                    30,
                    0,
                    69,
                    0
                ],
                "title": "BOPO: Neural Combinatorial Optimization via Best-anchored and\n  Objective-guided Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOPO: Neural Combinatorial Optimization via Best-anchored and\n  Objective-guided Preference Optimization"
                },
                "summary": "Neural Combinatorial Optimization (NCO) has emerged as a promising approach\nfor NP-hard problems. However, prevailing RL-based methods suffer from low\nsample efficiency due to sparse rewards and underused solutions. We propose\nBest-anchored and Objective-guided Preference Optimization (BOPO), a training\nparadigm that leverages solution preferences via objective values. It\nintroduces: (1) a best-anchored preference pair construction for better explore\nand exploit solutions, and (2) an objective-guided pairwise loss function that\nadaptively scales gradients via objective differences, removing reliance on\nreward models or reference policies. Experiments on Job-shop Scheduling Problem\n(JSP), Traveling Salesman Problem (TSP), and Flexible Job-shop Scheduling\nProblem (FJSP) show BOPO outperforms state-of-the-art neural methods, reducing\noptimality gaps impressively with efficient inference. BOPO is\narchitecture-agnostic, enabling seamless integration with existing NCO models,\nand establishes preference optimization as a principled framework for\ncombinatorial optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Combinatorial Optimization (NCO) has emerged as a promising approach\nfor NP-hard problems. However, prevailing RL-based methods suffer from low\nsample efficiency due to sparse rewards and underused solutions. We propose\nBest-anchored and Objective-guided Preference Optimization (BOPO), a training\nparadigm that leverages solution preferences via objective values. It\nintroduces: (1) a best-anchored preference pair construction for better explore\nand exploit solutions, and (2) an objective-guided pairwise loss function that\nadaptively scales gradients via objective differences, removing reliance on\nreward models or reference policies. Experiments on Job-shop Scheduling Problem\n(JSP), Traveling Salesman Problem (TSP), and Flexible Job-shop Scheduling\nProblem (FJSP) show BOPO outperforms state-of-the-art neural methods, reducing\noptimality gaps impressively with efficient inference. BOPO is\narchitecture-agnostic, enabling seamless integration with existing NCO models,\nand establishes preference optimization as a principled framework for\ncombinatorial optimization."
                },
                "authors": [
                    {
                        "name": "Zijun Liao"
                    },
                    {
                        "name": "Jinbiao Chen"
                    },
                    {
                        "name": "Debing Wang"
                    },
                    {
                        "name": "Zizhen Zhang"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "arxiv_comment": "This paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07580v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07580v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07071v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07071v3",
                "updated": "2025-06-02T15:40:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    40,
                    42,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-13T05:53:56Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "title": "Value Compass Benchmarks: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Compass Benchmarks: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values"
                },
                "summary": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Benchmarks, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Benchmarks, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values."
                },
                "authors": [
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Shitong Duan"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07071v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07071v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12821v2",
                "updated": "2025-06-02T15:40:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    40,
                    35,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-18T12:32:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    32,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models"
                },
                "summary": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values."
                },
                "authors": [
                    {
                        "name": "Elena Stringli"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Athanasios Voulodimos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04800v2",
                "updated": "2025-06-02T15:39:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    39,
                    49,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-03T06:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    54,
                    5,
                    0,
                    62,
                    0
                ],
                "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH."
                },
                "authors": [
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Tingyue Pan"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Jiaying Lin"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07453v2",
                "updated": "2025-06-02T15:39:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    39,
                    29,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-12T11:35:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    11,
                    35,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "How well do LLMs reason over tabular data, really?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do LLMs reason over tabular data, really?"
                },
                "summary": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs."
                },
                "authors": [
                    {
                        "name": "Cornelius Wolff"
                    },
                    {
                        "name": "Madelon Hulsebos"
                    }
                ],
                "author_detail": {
                    "name": "Madelon Hulsebos"
                },
                "author": "Madelon Hulsebos",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07784v2",
                "updated": "2025-06-02T15:27:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    27,
                    28,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-12T17:37:17Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    37,
                    17,
                    0,
                    132,
                    0
                ],
                "title": "Domain Regeneration: How well do LLMs match syntactic properties of text\n  domains?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Regeneration: How well do LLMs match syntactic properties of text\n  domains?"
                },
                "summary": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals."
                },
                "authors": [
                    {
                        "name": "Da Ju"
                    },
                    {
                        "name": "Hagen Blix"
                    },
                    {
                        "name": "Adina Williams"
                    }
                ],
                "author_detail": {
                    "name": "Adina Williams"
                },
                "author": "Adina Williams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04363v2",
                "updated": "2025-06-02T15:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    1,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-06T12:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    6,
                    54,
                    3,
                    65,
                    0
                ],
                "title": "Causally Reliable Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causally Reliable Concept Bottleneck Models"
                },
                "summary": "Concept-based models are an emerging paradigm in deep learning that\nconstrains the inference process to operate through human-interpretable\nvariables, facilitating explainability and human interaction. However, these\narchitectures, on par with popular opaque neural models, fail to account for\nthe true causal mechanisms underlying the target phenomena represented in the\ndata. This hampers their ability to support causal reasoning tasks, limits\nout-of-distribution generalization, and hinders the implementation of fairness\nconstraints. To overcome these issues, we propose Causally reliable Concept\nBottleneck Models (C$^2$BMs), a class of concept-based architectures that\nenforce reasoning through a bottleneck of concepts structured according to a\nmodel of the real-world causal mechanisms. We also introduce a pipeline to\nautomatically learn this structure from observational data and unstructured\nbackground knowledge (e.g., scientific literature). Experimental evidence\nsuggests that C$^2$BMs are more interpretable, causally reliable, and improve\nresponsiveness to interventions w.r.t. standard opaque and concept-based\nmodels, while maintaining their accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based models are an emerging paradigm in deep learning that\nconstrains the inference process to operate through human-interpretable\nvariables, facilitating explainability and human interaction. However, these\narchitectures, on par with popular opaque neural models, fail to account for\nthe true causal mechanisms underlying the target phenomena represented in the\ndata. This hampers their ability to support causal reasoning tasks, limits\nout-of-distribution generalization, and hinders the implementation of fairness\nconstraints. To overcome these issues, we propose Causally reliable Concept\nBottleneck Models (C$^2$BMs), a class of concept-based architectures that\nenforce reasoning through a bottleneck of concepts structured according to a\nmodel of the real-world causal mechanisms. We also introduce a pipeline to\nautomatically learn this structure from observational data and unstructured\nbackground knowledge (e.g., scientific literature). Experimental evidence\nsuggests that C$^2$BMs are more interpretable, causally reliable, and improve\nresponsiveness to interventions w.r.t. standard opaque and concept-based\nmodels, while maintaining their accuracy."
                },
                "authors": [
                    {
                        "name": "Giovanni De Felice"
                    },
                    {
                        "name": "Arianna Casanova Flores"
                    },
                    {
                        "name": "Francesco De Santis"
                    },
                    {
                        "name": "Silvia Santini"
                    },
                    {
                        "name": "Johannes Schneider"
                    },
                    {
                        "name": "Pietro Barbiero"
                    },
                    {
                        "name": "Alberto Termine"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Termine"
                },
                "author": "Alberto Termine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09837v2",
                "updated": "2025-06-02T14:54:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    54,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-14T23:02:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    2,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "Real-time Adapting Routing (RAR): Improving Efficiency Through\n  Continuous Learning in Software Powered by Layered Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Adapting Routing (RAR): Improving Efficiency Through\n  Continuous Learning in Software Powered by Layered Foundation Models"
                },
                "summary": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM."
                },
                "authors": [
                    {
                        "name": "Kirill Vasilevski"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00071v2",
                "updated": "2025-06-02T14:52:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    52,
                    36,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-27T17:28:12Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    28,
                    12,
                    3,
                    58,
                    0
                ],
                "title": "I see what you mean: Co-Speech Gestures for Reference Resolution in\n  Multimodal Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I see what you mean: Co-Speech Gestures for Reference Resolution in\n  Multimodal Dialogue"
                },
                "summary": "In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction."
                },
                "authors": [
                    {
                        "name": "Esam Ghaleb"
                    },
                    {
                        "name": "Bulat Khaertdinov"
                    },
                    {
                        "name": "Aslı Özyürek"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03517v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03517v4",
                "updated": "2025-06-03T07:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    7,
                    50,
                    11,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-04T15:17:09Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    9,
                    4,
                    94,
                    0
                ],
                "title": "A framework for computing upper bounds in passive learning settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for computing upper bounds in passive learning settings"
                },
                "summary": "The task of inferring logical formulas from examples has garnered significant\nattention as a means to assist engineers in creating formal specifications used\nin the design, synthesis, and verification of computing systems. Among various\napproaches, enumeration algorithms have emerged as some of the most effective\ntechniques for this task. These algorithms employ advanced strategies to\nsystematically enumerate candidate formulas while minimizing redundancies by\navoiding the generation of syntactically different but semantically equivalent\nformulas. However, a notable drawback is that these algorithms typically do not\nprovide guarantees of termination, which poses challenges for their use in\nreal-world applications.\n  This paper develops an abstract framework to bound the size of possible\nsolutions for a logic inference task, thereby providing a termination guarantee\nfor enumeration algorithms through the introduction of a sufficient stopping\ncriterion. The proposed framework is designed with flexibility in mind and is\napplicable to a broad spectrum of practically relevant logical formalisms,\nincluding Modal Logic, Linear Temporal Logic, Computation Tree Logic,\nAlternating-time Temporal Logic, and even selected inference task for finite\nautomata. In addition, our approach enabled us to develop a new class of\nalgorithms that enumerate over the semantics of formulas rather than their\nsyntactic representations, offering new possibilities for reducing redundancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of inferring logical formulas from examples has garnered significant\nattention as a means to assist engineers in creating formal specifications used\nin the design, synthesis, and verification of computing systems. Among various\napproaches, enumeration algorithms have emerged as some of the most effective\ntechniques for this task. These algorithms employ advanced strategies to\nsystematically enumerate candidate formulas while minimizing redundancies by\navoiding the generation of syntactically different but semantically equivalent\nformulas. However, a notable drawback is that these algorithms typically do not\nprovide guarantees of termination, which poses challenges for their use in\nreal-world applications.\n  This paper develops an abstract framework to bound the size of possible\nsolutions for a logic inference task, thereby providing a termination guarantee\nfor enumeration algorithms through the introduction of a sufficient stopping\ncriterion. The proposed framework is designed with flexibility in mind and is\napplicable to a broad spectrum of practically relevant logical formalisms,\nincluding Modal Logic, Linear Temporal Logic, Computation Tree Logic,\nAlternating-time Temporal Logic, and even selected inference task for finite\nautomata. In addition, our approach enabled us to develop a new class of\nalgorithms that enumerate over the semantics of formulas rather than their\nsyntactic representations, offering new possibilities for reducing redundancy."
                },
                "authors": [
                    {
                        "name": "Benjamin Bordais"
                    },
                    {
                        "name": "Daniel Neider"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Neider"
                },
                "author": "Daniel Neider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03517v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03517v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13601v2",
                "updated": "2025-06-02T14:40:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    40,
                    47,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-19T18:00:03Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    18,
                    0,
                    3,
                    0,
                    139,
                    0
                ],
                "title": "Dark Matter in an Evanescent Three-Brane Randall-Sundrum Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark Matter in an Evanescent Three-Brane Randall-Sundrum Scenario"
                },
                "summary": "Apart from its gravitational interactions, dark matter (DM) has remained so\nfar elusive in laboratory searches. One possible explanation is that the\nrelevant interactions to explain its relic abundance are mainly gravitational.\nIn this work we consider an extradimensional Randall-Sundrum scenario with a\nTeV-PeV IR brane, where the Standard Model is located, and a GeV-TeV deep IR\n(DIR) one, where the DM lies. When the curvatures of the bulk to the left and\nright of the IR brane are very similar, the tension of the IR brane is\nsignificantly smaller than that of the other two branes, and therefore we term\nit \\evanescent\". In this setup, the relic abundance of DM arises from the\nfreezeout mechanism, thanks to DM annihilations into radions and gravitons.\nFocusing on a scalar singlet DM candidate, we compute and apply current and\nfuture constraints from direct, indirect and collider-based searches. Our\nfindings demonstrate the viability of this scenario and highlight its potential\ntestability in upcoming experiments. We also discuss the possibility of\ninferring the number of branes if the radion and several Kaluza-Klein graviton\nresonances are detected at a future collider.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apart from its gravitational interactions, dark matter (DM) has remained so\nfar elusive in laboratory searches. One possible explanation is that the\nrelevant interactions to explain its relic abundance are mainly gravitational.\nIn this work we consider an extradimensional Randall-Sundrum scenario with a\nTeV-PeV IR brane, where the Standard Model is located, and a GeV-TeV deep IR\n(DIR) one, where the DM lies. When the curvatures of the bulk to the left and\nright of the IR brane are very similar, the tension of the IR brane is\nsignificantly smaller than that of the other two branes, and therefore we term\nit \\evanescent\". In this setup, the relic abundance of DM arises from the\nfreezeout mechanism, thanks to DM annihilations into radions and gravitons.\nFocusing on a scalar singlet DM candidate, we compute and apply current and\nfuture constraints from direct, indirect and collider-based searches. Our\nfindings demonstrate the viability of this scenario and highlight its potential\ntestability in upcoming experiments. We also discuss the possibility of\ninferring the number of branes if the radion and several Kaluza-Klein graviton\nresonances are detected at a future collider."
                },
                "authors": [
                    {
                        "name": "Andrea Donini"
                    },
                    {
                        "name": "Miguel G. Folgado"
                    },
                    {
                        "name": "Juan Herrero-García"
                    },
                    {
                        "name": "Giacomo Landini"
                    },
                    {
                        "name": "Alejandro Muñoz-Ovalle"
                    },
                    {
                        "name": "Nuria Rius"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Rius"
                },
                "author": "Nuria Rius",
                "arxiv_comment": "42 pages, 14 figures. v2: added references + minor corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06851v3",
                "updated": "2025-06-02T14:38:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    38,
                    8,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-07T11:56:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    56,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "Survey on Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey on Vision-Language-Action Models"
                },
                "summary": "This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable."
                },
                "authors": [
                    {
                        "name": "Adilzhan Adilkhanov"
                    },
                    {
                        "name": "Amir Yelenov"
                    },
                    {
                        "name": "Assylkhan Seitzhanov"
                    },
                    {
                        "name": "Ayan Mazhitov"
                    },
                    {
                        "name": "Azamat Abdikarimov"
                    },
                    {
                        "name": "Danissa Sandykbayeva"
                    },
                    {
                        "name": "Daryn Kenzhebek"
                    },
                    {
                        "name": "Dinmukhammed Mukashev"
                    },
                    {
                        "name": "Ilyas Umurbekov"
                    },
                    {
                        "name": "Jabrail Chumakov"
                    },
                    {
                        "name": "Kamila Spanova"
                    },
                    {
                        "name": "Karina Burunchina"
                    },
                    {
                        "name": "Madina Yergibay"
                    },
                    {
                        "name": "Margulan Issa"
                    },
                    {
                        "name": "Moldir Zabirova"
                    },
                    {
                        "name": "Nurdaulet Zhuzbay"
                    },
                    {
                        "name": "Nurlan Kabdyshev"
                    },
                    {
                        "name": "Nurlan Zhaniyar"
                    },
                    {
                        "name": "Rasul Yermagambet"
                    },
                    {
                        "name": "Rustam Chibar"
                    },
                    {
                        "name": "Saltanat Seitzhan"
                    },
                    {
                        "name": "Soibkhon Khajikhanov"
                    },
                    {
                        "name": "Tasbolat Taunyazov"
                    },
                    {
                        "name": "Temirlan Galimzhanov"
                    },
                    {
                        "name": "Temirlan Kaiyrbay"
                    },
                    {
                        "name": "Tleukhan Mussin"
                    },
                    {
                        "name": "Togzhan Syrymova"
                    },
                    {
                        "name": "Valeriya Kostyukova"
                    },
                    {
                        "name": "Yerkebulan Massalim"
                    },
                    {
                        "name": "Yermakhan Kassym"
                    },
                    {
                        "name": "Zerde Nurbayeva"
                    },
                    {
                        "name": "Zhanat Kappassov"
                    }
                ],
                "author_detail": {
                    "name": "Zhanat Kappassov"
                },
                "author": "Zhanat Kappassov",
                "arxiv_comment": "arXiv admin note: This submission has been withdrawn due to serious\n  violation of arXiv policies for acceptable submissions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04503v3",
                "updated": "2025-06-02T14:34:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    34,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2024-07-05T13:44:09Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    44,
                    9,
                    4,
                    187,
                    0
                ],
                "title": "When LLMs Play the Telephone Game: Cultural Attractors as Conceptual\n  Tools to Evaluate LLMs in Multi-turn Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Play the Telephone Game: Cultural Attractors as Conceptual\n  Tools to Evaluate LLMs in Multi-turn Settings"
                },
                "summary": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics."
                },
                "authors": [
                    {
                        "name": "Jérémy Perez"
                    },
                    {
                        "name": "Grgur Kovač"
                    },
                    {
                        "name": "Corentin Léger"
                    },
                    {
                        "name": "Cédric Colas"
                    },
                    {
                        "name": "Gaia Molinaro"
                    },
                    {
                        "name": "Maxime Derex"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Clément Moulin-Frier"
                    }
                ],
                "author_detail": {
                    "name": "Clément Moulin-Frier"
                },
                "author": "Clément Moulin-Frier",
                "arxiv_comment": "Code available at https://github.com/jeremyperez2/TelephoneGameLLM.\n  Companion website with a Data Explorer tool at\n  https://sites.google.com/view/telephone-game-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18478v2",
                "updated": "2025-06-02T14:26:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    26,
                    19,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-27T16:19:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    19,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform\ndownstream tasks through advanced prompting and high-quality demonstrations.\nHowever, traditional ICL paradigms encounter significant limitations in complex\nreasoning tasks, stemming primarily from their dependence on example quality\nand absence of explicit reasoning guidance. To address these challenges, we\nintroduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in\n**ICL** that shifts focus from specific examples to abstract reasoning\npatterns, thereby extending the conventional concept of \"context\" in ICL. Our\napproach begins by defining five atomic reasoning actions, upon which we employ\nMonte Carlo Tree Search to systematically construct high-level reasoning\npatterns. During inference, HiAR-ICL dynamically selects appropriate reasoning\npatterns based on problem attributes, providing explicit guidance for the\nmodel's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and\nefficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our\nmethod achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's\n77.2% and 57.5%. Our approach enhances performance across models of varying\nsizes while generalizing effectively across domains. Further analysis reveals\nthat HiAR-ICL can also serve as a plug-and-play inference method compatible\nwith post-training techniques like GRPO. Code and data are available at\nhttps://github.com/jinyangwu/HiARICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform\ndownstream tasks through advanced prompting and high-quality demonstrations.\nHowever, traditional ICL paradigms encounter significant limitations in complex\nreasoning tasks, stemming primarily from their dependence on example quality\nand absence of explicit reasoning guidance. To address these challenges, we\nintroduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in\n**ICL** that shifts focus from specific examples to abstract reasoning\npatterns, thereby extending the conventional concept of \"context\" in ICL. Our\napproach begins by defining five atomic reasoning actions, upon which we employ\nMonte Carlo Tree Search to systematically construct high-level reasoning\npatterns. During inference, HiAR-ICL dynamically selects appropriate reasoning\npatterns based on problem attributes, providing explicit guidance for the\nmodel's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and\nefficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our\nmethod achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's\n77.2% and 57.5%. Our approach enhances performance across models of varying\nsizes while generalizing effectively across domains. Further analysis reveals\nthat HiAR-ICL can also serve as a plug-and-play inference method compatible\nwith post-training techniques like GRPO. Code and data are available at\nhttps://github.com/jinyangwu/HiARICL."
                },
                "authors": [
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Zengqi Wen"
                    },
                    {
                        "name": "Chonghua Liao"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18702v2",
                "updated": "2025-06-02T14:23:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    23,
                    25,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-24T12:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    56,
                    1,
                    3,
                    298,
                    0
                ],
                "title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning"
                },
                "summary": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses."
                },
                "authors": [
                    {
                        "name": "Rita Ramos"
                    },
                    {
                        "name": "Everlyn Asiko Chimoto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Natalie Schluter"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Schluter"
                },
                "author": "Natalie Schluter",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17879v2",
                "updated": "2025-06-02T14:17:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    17,
                    22,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-23T13:28:26Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    13,
                    28,
                    26,
                    4,
                    143,
                    0
                ],
                "title": "LLM4SG: Large Language Models for Scatterer Generation via Synesthesia\n  of Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4SG: Large Language Models for Scatterer Generation via Synesthesia\n  of Machines"
                },
                "summary": "Guided by Synesthesia of Machines (SoM), the nonlinear mapping relationship\nbetween sensory and communication information serves as a powerful tool to\nenhance both the accuracy and generalization of vehicle-to-vehicle (V2V)\nmulti-modal intelligent channel modeling (MMICM) in intelligent transportation\nsystems (ITSs). To explore the general mapping relationship between physical\nenvironment and electromagnetic space, a new intelligent sensing-communication\nintegration dataset, named V2V-M3, is constructed for multiple scenarios in V2V\ncommunications with multiple frequency bands and multiple vehicular traffic\ndensities (VTDs). Leveraging the strong representation and cross-modal\ninference capabilities of large language models (LLMs), a novel LLM-based\nmethod for Scatterer Generation (LLM4SG) from light detection and ranging\n(LiDAR) point clouds is developed. To address the inherent and significant\ndifferences across multi-modal data, synergistically optimized four-module\narchitecture, i.e., preprocessor, embedding, backbone, and output modules, are\ndesigned by considering the sensing/channel characteristics and electromagnetic\npropagation mechanism. On the basis of cross-modal representation alignment and\npositional encoding, the network of LLM4SG is fine-tuned to capture the general\nmapping relationship between LiDAR point clouds and scatterers. Simulation\nresults demonstrate that the proposed LLM4SG achieves superior performance in\nfull-sample and generalization testing, significantly outperforming small\nmodels across different frequency bands, scenarios, and VTDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by Synesthesia of Machines (SoM), the nonlinear mapping relationship\nbetween sensory and communication information serves as a powerful tool to\nenhance both the accuracy and generalization of vehicle-to-vehicle (V2V)\nmulti-modal intelligent channel modeling (MMICM) in intelligent transportation\nsystems (ITSs). To explore the general mapping relationship between physical\nenvironment and electromagnetic space, a new intelligent sensing-communication\nintegration dataset, named V2V-M3, is constructed for multiple scenarios in V2V\ncommunications with multiple frequency bands and multiple vehicular traffic\ndensities (VTDs). Leveraging the strong representation and cross-modal\ninference capabilities of large language models (LLMs), a novel LLM-based\nmethod for Scatterer Generation (LLM4SG) from light detection and ranging\n(LiDAR) point clouds is developed. To address the inherent and significant\ndifferences across multi-modal data, synergistically optimized four-module\narchitecture, i.e., preprocessor, embedding, backbone, and output modules, are\ndesigned by considering the sensing/channel characteristics and electromagnetic\npropagation mechanism. On the basis of cross-modal representation alignment and\npositional encoding, the network of LLM4SG is fine-tuned to capture the general\nmapping relationship between LiDAR point clouds and scatterers. Simulation\nresults demonstrate that the proposed LLM4SG achieves superior performance in\nfull-sample and generalization testing, significantly outperforming small\nmodels across different frequency bands, scenarios, and VTDs."
                },
                "authors": [
                    {
                        "name": "Zengrui Han"
                    },
                    {
                        "name": "Lu Bai"
                    },
                    {
                        "name": "Ziwei Huang"
                    },
                    {
                        "name": "Xiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Cheng"
                },
                "author": "Xiang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03340v2",
                "updated": "2025-06-02T14:15:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    15,
                    13,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-05T10:13:05Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    13,
                    5,
                    2,
                    64,
                    0
                ],
                "title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States"
                },
                "summary": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains challenging for Large\nLanguage Models (LLMs). While existing ToM reasoning methods show promise with\nreasoning via perceptual perspective-taking, they often rely excessively on\noff-the-shelf LLMs, reducing their efficiency and limiting their applicability\nto high-order ToM reasoning. To address these issues, we present EnigmaToM, a\nnovel neuro-symbolic framework that enhances ToM reasoning by integrating a\nNeural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired\niterative masking mechanism that facilitates accurate perspective-taking and\n(2) knowledge injection that elicits key entity information. Enigma generates\nstructured knowledge of entity states to build spatial scene graphs for belief\ntracking across various ToM orders and enrich events with fine-grained entity\nstate details. Experimental results on ToMi, HiToM, and FANToM benchmarks show\nthat EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains challenging for Large\nLanguage Models (LLMs). While existing ToM reasoning methods show promise with\nreasoning via perceptual perspective-taking, they often rely excessively on\noff-the-shelf LLMs, reducing their efficiency and limiting their applicability\nto high-order ToM reasoning. To address these issues, we present EnigmaToM, a\nnovel neuro-symbolic framework that enhances ToM reasoning by integrating a\nNeural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired\niterative masking mechanism that facilitates accurate perspective-taking and\n(2) knowledge injection that elicits key entity information. Enigma generates\nstructured knowledge of entity states to build spatial scene graphs for belief\ntracking across various ToM orders and enrich events with fine-grained entity\nstate details. Experimental results on ToMi, HiToM, and FANToM benchmarks show\nthat EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Siya Qi"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Caroline Catmur"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "arxiv_comment": "Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24832v2",
                "updated": "2025-06-02T14:13:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    13,
                    41,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T17:34:03Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    34,
                    3,
                    4,
                    150,
                    0
                ],
                "title": "How much do language models memorize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much do language models memorize?"
                },
                "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference."
                },
                "authors": [
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Chawin Sitawarin"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "Narine Kokhlikyan"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    }
                ],
                "author_detail": {
                    "name": "Saeed Mahloujifar"
                },
                "author": "Saeed Mahloujifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23291v2",
                "updated": "2025-06-02T14:05:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    5,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T09:42:25Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    42,
                    25,
                    3,
                    149,
                    0
                ],
                "title": "ScEdit: Script-based Assessment of Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScEdit: Script-based Assessment of Knowledge Editing"
                },
                "summary": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit."
                },
                "authors": [
                    {
                        "name": "Xinye Li"
                    },
                    {
                        "name": "Zunwen Zheng"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Dekai Zhuang"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    },
                    {
                        "name": "Dianbo Sui"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Sui"
                },
                "author": "Dianbo Sui",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08092v2",
                "updated": "2025-06-02T14:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    2,
                    8,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-12T03:33:06Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    3,
                    33,
                    6,
                    2,
                    43,
                    0
                ],
                "title": "GCoT: Chain-of-Thought Prompt Learning for Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GCoT: Chain-of-Thought Prompt Learning for Graphs"
                },
                "summary": "Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach."
                },
                "authors": [
                    {
                        "name": "Xingtong Yu"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Zhongwei Kuai"
                    },
                    {
                        "name": "Xinming Zhang"
                    },
                    {
                        "name": "Yuan Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Fang"
                },
                "author": "Yuan Fang",
                "arxiv_comment": "Accepted by SIGKDD2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09439v2",
                "updated": "2025-06-02T13:43:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    43,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-14T14:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"
                },
                "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU and\nMMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU and\nMMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance."
                },
                "authors": [
                    {
                        "name": "Andrew Rouditchenko"
                    },
                    {
                        "name": "Saurabhchand Bhati"
                    },
                    {
                        "name": "Edson Araujo"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17662v2",
                "updated": "2025-06-02T13:38:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    38,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-23T09:27:25Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    27,
                    25,
                    4,
                    143,
                    0
                ],
                "title": "Automating Versatile Time-Series Analysis with Tiny Transformers on\n  Embedded FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Versatile Time-Series Analysis with Tiny Transformers on\n  Embedded FPGAs"
                },
                "summary": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS)."
                },
                "authors": [
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Lukas Johannes Haßler"
                    },
                    {
                        "name": "Gregor Schiele"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Schiele"
                },
                "author": "Gregor Schiele",
                "arxiv_comment": "6 pages, 5 figures, 1 table, accepted by IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09266v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09266v2",
                "updated": "2025-06-02T13:23:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    23,
                    32,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-13T12:28:42Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    28,
                    42,
                    3,
                    44,
                    0
                ],
                "title": "Accelerating Bayesian Sampling for Massive Black Hole Binaries with\n  Prior Constraints from Conditional Variational Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Bayesian Sampling for Massive Black Hole Binaries with\n  Prior Constraints from Conditional Variational Autoencoder"
                },
                "summary": "A Conditional Variational Autoencoder (CVAE) model is employed for parameter\ninference on gravitational waves (GW) signals of massive black hole binaries,\nconsidering joint observations with a network of three space-based GW\ndetectors. Our experiments show that the trained CVAE model can estimate the\nposterior distribution of source parameters in approximately one second, while\nthe standard Bayesian sampling method, utilizing parallel computation across 16\nCPU cores, takes an average of 20 hours for a GW signal instance. However, the\nsampling distributions from CVAE exhibit lighter tails, appearing broader when\ncompared to the standard Bayesian sampling results. By using CVAE results to\nconstrain the prior range for Bayesian sampling, the sampling time is reduced\nby a factor of $\\sim$6 while maintaining the similar precision of the Bayesian\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Conditional Variational Autoencoder (CVAE) model is employed for parameter\ninference on gravitational waves (GW) signals of massive black hole binaries,\nconsidering joint observations with a network of three space-based GW\ndetectors. Our experiments show that the trained CVAE model can estimate the\nposterior distribution of source parameters in approximately one second, while\nthe standard Bayesian sampling method, utilizing parallel computation across 16\nCPU cores, takes an average of 20 hours for a GW signal instance. However, the\nsampling distributions from CVAE exhibit lighter tails, appearing broader when\ncompared to the standard Bayesian sampling results. By using CVAE results to\nconstrain the prior range for Bayesian sampling, the sampling time is reduced\nby a factor of $\\sim$6 while maintaining the similar precision of the Bayesian\nresults."
                },
                "authors": [
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Jibo He"
                    }
                ],
                "author_detail": {
                    "name": "Jibo He"
                },
                "author": "Jibo He",
                "arxiv_doi": "10.1103/8r8b-hckx",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/8r8b-hckx",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09266v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 103053 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15278v2",
                "updated": "2025-06-02T13:12:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    12,
                    41,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-25T17:10:50Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    17,
                    10,
                    50,
                    5,
                    25,
                    0
                ],
                "title": "PIP: Perturbation-based Iterative Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP: Perturbation-based Iterative Pruning for Large Language Models"
                },
                "summary": "The rapid increase in the parameter counts of Large Language Models (LLMs),\nreaching billions or even trillions, presents significant challenges for their\npractical deployment, particularly in resource-constrained environments. To\nease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel\ndouble-view structured pruning method to optimize LLMs, which combines\ninformation from two different views: the unperturbed view and the perturbed\nview. With the calculation of gradient differences, PIP iteratively prunes\nthose that struggle to distinguish between these two views. Our experiments\nshow that PIP reduces the parameter count by approximately 20% while retaining\nover 85% of the original model's accuracy across varied benchmarks. In some\ncases, the performance of the pruned model is within 5% of the unpruned\nversion, demonstrating PIP's ability to preserve key aspects of model\neffectiveness. Moreover, PIP consistently outperforms existing state-of-the-art\n(SOTA) structured pruning methods, establishing it as a leading technique for\noptimizing LLMs in environments with constrained resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in the parameter counts of Large Language Models (LLMs),\nreaching billions or even trillions, presents significant challenges for their\npractical deployment, particularly in resource-constrained environments. To\nease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel\ndouble-view structured pruning method to optimize LLMs, which combines\ninformation from two different views: the unperturbed view and the perturbed\nview. With the calculation of gradient differences, PIP iteratively prunes\nthose that struggle to distinguish between these two views. Our experiments\nshow that PIP reduces the parameter count by approximately 20% while retaining\nover 85% of the original model's accuracy across varied benchmarks. In some\ncases, the performance of the pruned model is within 5% of the unpruned\nversion, demonstrating PIP's ability to preserve key aspects of model\neffectiveness. Moreover, PIP consistently outperforms existing state-of-the-art\n(SOTA) structured pruning methods, establishing it as a leading technique for\noptimizing LLMs in environments with constrained resources."
                },
                "authors": [
                    {
                        "name": "Yi Cao"
                    },
                    {
                        "name": "Wei-Jie Xu"
                    },
                    {
                        "name": "Yucheng Shen"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Chi-Min Chan"
                    },
                    {
                        "name": "Jianfeng Qu"
                    },
                    {
                        "name": "Jiajie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Xu"
                },
                "author": "Jiajie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01150v2",
                "updated": "2025-06-02T13:06:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    6,
                    53,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-03T03:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    3,
                    56,
                    3,
                    0,
                    62,
                    0
                ],
                "title": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority\n  Languages"
                },
                "summary": "Large language models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages (LRLs), particularly those spoken by minority\ncommunities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To\nsystematically track the progress in these languages, we introduce MiLiC-Eval,\na benchmark designed for minority languages in China, featuring 24K instances\nacross 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its\nparallelism between tasks and languages can provide a faithful and fine-grained\nassessment of linguistic and problem-solving skills. Our evaluation reveals\nthat open-source LLMs perform poorly on syntax-intensive tasks and multi-script\nlanguages. We further demonstrate how MiLiC-Eval can help advance LRL research\nin handling diverse writing systems and understanding the process of language\nadaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages (LRLs), particularly those spoken by minority\ncommunities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To\nsystematically track the progress in these languages, we introduce MiLiC-Eval,\na benchmark designed for minority languages in China, featuring 24K instances\nacross 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its\nparallelism between tasks and languages can provide a faithful and fine-grained\nassessment of linguistic and problem-solving skills. Our evaluation reveals\nthat open-source LLMs perform poorly on syntax-intensive tasks and multi-script\nlanguages. We further demonstrate how MiLiC-Eval can help advance LRL research\nin handling diverse writing systems and understanding the process of language\nadaptation."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Mingxu Tao"
                    },
                    {
                        "name": "Zhiyuan Liao"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "arxiv_comment": "ACL 2025 (Findings) Code and data available at\n  https://github.com/luciusssss/MiLiC-Eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19396v4",
                "updated": "2025-06-02T13:06:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    6,
                    19,
                    0,
                    153,
                    0
                ],
                "published": "2024-03-28T13:15:08Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    13,
                    15,
                    8,
                    3,
                    88,
                    0
                ],
                "title": "Persistence Diagram Estimation of Multivariate Piecewise\n  Hölder-continuous Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistence Diagram Estimation of Multivariate Piecewise\n  Hölder-continuous Signals"
                },
                "summary": "To our knowledge, the analysis of convergence rates for persistence diagrams\nestimation from noisy signals has predominantly relied on lifting signal\nestimation results through sup-norm (or other functional norm) stability\ntheorems. We believe that moving forward from this approach can lead to\nconsiderable gains. We illustrate it in the setting of nonparametric\nregression. From a minimax perspective, we examine the inference of persistence\ndiagrams (for the sublevel sets filtration). We show that for piecewise\nH\\\"older-continuous functions, with control over the reach of the set of\ndiscontinuities, taking the persistence diagram coming from a simple histogram\nestimator of the signal permits achieving the minimax rates known for\nH\\\"older-continuous functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To our knowledge, the analysis of convergence rates for persistence diagrams\nestimation from noisy signals has predominantly relied on lifting signal\nestimation results through sup-norm (or other functional norm) stability\ntheorems. We believe that moving forward from this approach can lead to\nconsiderable gains. We illustrate it in the setting of nonparametric\nregression. From a minimax perspective, we examine the inference of persistence\ndiagrams (for the sublevel sets filtration). We show that for piecewise\nH\\\"older-continuous functions, with control over the reach of the set of\ndiscontinuities, taking the persistence diagram coming from a simple histogram\nestimator of the signal permits achieving the minimax rates known for\nH\\\"older-continuous functions."
                },
                "authors": [
                    {
                        "name": "Hugo Henneuse"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Henneuse"
                },
                "author": "Hugo Henneuse",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23661v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23661v3",
                "updated": "2025-06-02T13:04:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    4,
                    26,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T17:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation"
                },
                "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni."
                },
                "authors": [
                    {
                        "name": "Size Wu"
                    },
                    {
                        "name": "Zhonghua Wu"
                    },
                    {
                        "name": "Zerui Gong"
                    },
                    {
                        "name": "Qingyi Tao"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Qinyue Li"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23661v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23661v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12276v3",
                "updated": "2025-06-02T12:55:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    12,
                    55,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T19:00:18Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    19,
                    0,
                    18,
                    0,
                    351,
                    0
                ],
                "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An\n  Encoder Decoder Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An\n  Encoder Decoder Perspective"
                },
                "summary": "Autoregressive transformers exhibit adaptive learning through in-context\nlearning (ICL), which begs the question of how. Prior works have shown that\ntransformers represent the ICL tasks as vectors in their representations. In\nthis paper, we leverage the encoding-decoding framework to study how\ntransformers form task vectors during pretraining and how their task encoding\nquality predicts ICL task performance. On synthetic ICL tasks, we analyze the\ntraining dynamics of a small transformer and report the coupled emergence of\ntask encoding and decoding. As the model learns to encode different latent\ntasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable\nrepresentations, it concurrently builds conditional decoding algorithms and\nimproves its ICL performance. We validate this phenomenon across pretrained\nmodels of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the\ncourse of pretraining in OLMo-7B. Further, we demonstrate that the quality of\ntask encoding inferred from representations predicts ICL performance, and that,\nsurprisingly, finetuning the earlier layers can improve the task encoding and\nperformance more than finetuning the latter layers. Our empirical insights shed\nlight into better understanding the success and failure modes of large language\nmodels via their representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive transformers exhibit adaptive learning through in-context\nlearning (ICL), which begs the question of how. Prior works have shown that\ntransformers represent the ICL tasks as vectors in their representations. In\nthis paper, we leverage the encoding-decoding framework to study how\ntransformers form task vectors during pretraining and how their task encoding\nquality predicts ICL task performance. On synthetic ICL tasks, we analyze the\ntraining dynamics of a small transformer and report the coupled emergence of\ntask encoding and decoding. As the model learns to encode different latent\ntasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable\nrepresentations, it concurrently builds conditional decoding algorithms and\nimproves its ICL performance. We validate this phenomenon across pretrained\nmodels of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the\ncourse of pretraining in OLMo-7B. Further, we demonstrate that the quality of\ntask encoding inferred from representations predicts ICL performance, and that,\nsurprisingly, finetuning the earlier layers can improve the task encoding and\nperformance more than finetuning the latter layers. Our empirical insights shed\nlight into better understanding the success and failure modes of large language\nmodels via their representations."
                },
                "authors": [
                    {
                        "name": "Seungwook Han"
                    },
                    {
                        "name": "Jinyeop Song"
                    },
                    {
                        "name": "Jeff Gore"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Pulkit Agrawal"
                },
                "author": "Pulkit Agrawal",
                "arxiv_comment": "https://charming-centaur-089.notion.site/Emergence-and-Effectiveness-of-Task-Vectors-in-In-Context-Learning-An-Encoder-Decoder-Perspective-2054664a1d59814f8401cded3332fce4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05206v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05206v4",
                "updated": "2025-06-02T12:47:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    12,
                    47,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-02T05:14:22Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    5,
                    14,
                    22,
                    6,
                    33,
                    0
                ],
                "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
                },
                "summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models."
                },
                "authors": [
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ye Sun"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Yunhao Chen"
                    },
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Neil Gong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Sarah Erfani"
                    },
                    {
                        "name": "Tim Baldwin"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "James Bailey"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05206v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05206v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17780v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17780v4",
                "updated": "2025-06-02T12:07:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    12,
                    7,
                    55,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-23T18:38:49Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    38,
                    49,
                    0,
                    358,
                    0
                ],
                "title": "PepTune: De Novo Generation of Therapeutic Peptides with\n  Multi-Objective-Guided Discrete Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PepTune: De Novo Generation of Therapeutic Peptides with\n  Multi-Objective-Guided Discrete Diffusion"
                },
                "summary": "We present PepTune, a multi-objective discrete diffusion model for\nsimultaneous generation and optimization of therapeutic peptide SMILES. Built\non the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid\npeptide structures with a novel bond-dependent masking schedule and invalid\nloss function. To guide the diffusion process, we introduce Monte Carlo Tree\nGuidance (MCTG), an inference-time multi-objective guidance algorithm that\nbalances exploration and exploitation to iteratively refine Pareto-optimal\nsequences. MCTG integrates classifier-based rewards with search-tree expansion,\novercoming gradient estimation challenges and data sparsity. Using PepTune, we\ngenerate diverse, chemically-modified peptides simultaneously optimized for\nmultiple therapeutic properties, including target binding affinity, membrane\npermeability, solubility, hemolysis, and non-fouling for various\ndisease-relevant targets. In total, our results demonstrate that MCTG for\nmasked discrete diffusion is a powerful and modular approach for\nmulti-objective sequence design in discrete state spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PepTune, a multi-objective discrete diffusion model for\nsimultaneous generation and optimization of therapeutic peptide SMILES. Built\non the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid\npeptide structures with a novel bond-dependent masking schedule and invalid\nloss function. To guide the diffusion process, we introduce Monte Carlo Tree\nGuidance (MCTG), an inference-time multi-objective guidance algorithm that\nbalances exploration and exploitation to iteratively refine Pareto-optimal\nsequences. MCTG integrates classifier-based rewards with search-tree expansion,\novercoming gradient estimation challenges and data sparsity. Using PepTune, we\ngenerate diverse, chemically-modified peptides simultaneously optimized for\nmultiple therapeutic properties, including target binding affinity, membrane\npermeability, solubility, hemolysis, and non-fouling for various\ndisease-relevant targets. In total, our results demonstrate that MCTG for\nmasked discrete diffusion is a powerful and modular approach for\nmulti-objective sequence design in discrete state spaces."
                },
                "authors": [
                    {
                        "name": "Sophia Tang"
                    },
                    {
                        "name": "Yinuo Zhang"
                    },
                    {
                        "name": "Pranam Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Pranam Chatterjee"
                },
                "author": "Pranam Chatterjee",
                "arxiv_comment": "Published at ICML 2025. (Proceedings of the 42nd International\n  Conference on Machine Learning, Vancouver, Canada)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17780v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17780v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07604v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07604v3",
                "updated": "2025-06-02T12:06:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    12,
                    6,
                    31,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-10T17:58:31Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    58,
                    31,
                    0,
                    69,
                    0
                ],
                "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Reasoning in Transformers is Reasoning through Shortcuts"
                },
                "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Tianhe Lin"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07604v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07604v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v6",
                "updated": "2025-06-02T11:46:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    46,
                    43,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15268v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15268v4",
                "updated": "2025-06-02T11:45:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    45,
                    29,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-17T06:28:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    28,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph"
                },
                "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox."
                },
                "authors": [
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "8 pages of content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15268v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15268v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18403v3",
                "updated": "2025-06-02T11:31:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    31,
                    19,
                    0,
                    153,
                    0
                ],
                "published": "2024-06-26T14:56:13Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    56,
                    13,
                    2,
                    178,
                    0
                ],
                "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks"
                },
                "summary": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators."
                },
                "authors": [
                    {
                        "name": "Anna Bavaresco"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    },
                    {
                        "name": "Leonardo Bertolazzi"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Esam Ghaleb"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "André F. T. Martins"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Aditya K Surikuchi"
                    },
                    {
                        "name": "Ece Takmaz"
                    },
                    {
                        "name": "Alberto Testoni"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Testoni"
                },
                "author": "Alberto Testoni",
                "arxiv_comment": "Accepted to the main conference of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v5",
                "updated": "2025-06-02T11:31:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    31,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large language models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of long-context summarization\ndatasets hinders progress in this area. To address this, we introduce CNNSum, a\nmulti-scale long-context summarization benchmark based on Chinese novels,\nfeaturing human-driven annotations across four subsets totaling 695 samples,\nwith lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct\ndetailed human assessments to summarize abnormal output types. Furthermore, we\nextensively explore how to improve long-context summarization. In our study:\n(1) Advanced LLMs may generate much subjective commentary, leading to vague\nsummaries. (2) Currently, long-context summarization mainly relies on memory\nability. The advantages of Large LLMs are hard to utilize, thus small LLMs are\nmore cost-effective. (3) Different prompt types paired with various version\nmodels may cause large performance gaps. In further fine-tuning, these can be\nmitigated, and the Base version models perform better. (4) LLMs with RoPE-base\nscaled exhibit strong extrapolation potential; using short-context data can\nsignificantly improve long-context summarization performance. However, further\napplying other interpolation methods requires careful selection. (5) CNNSum\nprovides more reliable evaluation results than other benchmarks. We release\nCNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of long-context summarization\ndatasets hinders progress in this area. To address this, we introduce CNNSum, a\nmulti-scale long-context summarization benchmark based on Chinese novels,\nfeaturing human-driven annotations across four subsets totaling 695 samples,\nwith lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct\ndetailed human assessments to summarize abnormal output types. Furthermore, we\nextensively explore how to improve long-context summarization. In our study:\n(1) Advanced LLMs may generate much subjective commentary, leading to vague\nsummaries. (2) Currently, long-context summarization mainly relies on memory\nability. The advantages of Large LLMs are hard to utilize, thus small LLMs are\nmore cost-effective. (3) Different prompt types paired with various version\nmodels may cause large performance gaps. In further fine-tuning, these can be\nmitigated, and the Base version models perform better. (4) LLMs with RoPE-base\nscaled exhibit strong extrapolation potential; using short-context data can\nsignificantly improve long-context summarization performance. However, further\napplying other interpolation methods requires careful selection. (5) CNNSum\nprovides more reliable evaluation results than other benchmarks. We release\nCNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)"
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Accepted to ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08985v2",
                "updated": "2025-06-02T11:22:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    22,
                    49,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-12T06:38:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    38,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11743v2",
                "updated": "2025-06-02T11:22:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    22,
                    35,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T13:02:17Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    2,
                    17,
                    0,
                    351,
                    0
                ],
                "title": "Generalized Bayesian deep reinforcement learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Bayesian deep reinforcement learning"
                },
                "summary": "Bayesian reinforcement learning (BRL) is a method that merges principles from\nBayesian statistics and reinforcement learning to make optimal decisions in\nuncertain environments. As a model-based RL method, it has two key components:\n(1) inferring the posterior distribution of the model for the data-generating\nprocess (DGP) and (2) policy learning using the learned posterior. We propose\nto model the dynamics of the unknown environment through deep generative\nmodels, assuming Markov dependence. In the absence of likelihood functions for\nthese models, we train them by learning a generalized predictive-sequential (or\nprequential) scoring rule (SR) posterior. We used sequential Monte Carlo (SMC)\nsamplers to draw samples from this generalized Bayesian posterior distribution.\nIn conjunction, to achieve scalability in the high-dimensional parameter space\nof the neural networks, we use the gradient-based Markov kernels within SMC. To\njustify the use of the prequential scoring rule posterior, we prove a\nBernstein-von Mises-type theorem. For policy learning, we propose expected\nThompson sampling (ETS) to learn the optimal policy by maximising the expected\nvalue function with respect to the posterior distribution. This improves upon\ntraditional Thompson sampling (TS) and its extensions, which utilize only one\nsample drawn from the posterior distribution. This improvement is studied both\ntheoretically and using simulation studies, assuming a discrete action space.\nFinally, we successfully extended our setup for a challenging problem with a\ncontinuous action space without theoretical guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian reinforcement learning (BRL) is a method that merges principles from\nBayesian statistics and reinforcement learning to make optimal decisions in\nuncertain environments. As a model-based RL method, it has two key components:\n(1) inferring the posterior distribution of the model for the data-generating\nprocess (DGP) and (2) policy learning using the learned posterior. We propose\nto model the dynamics of the unknown environment through deep generative\nmodels, assuming Markov dependence. In the absence of likelihood functions for\nthese models, we train them by learning a generalized predictive-sequential (or\nprequential) scoring rule (SR) posterior. We used sequential Monte Carlo (SMC)\nsamplers to draw samples from this generalized Bayesian posterior distribution.\nIn conjunction, to achieve scalability in the high-dimensional parameter space\nof the neural networks, we use the gradient-based Markov kernels within SMC. To\njustify the use of the prequential scoring rule posterior, we prove a\nBernstein-von Mises-type theorem. For policy learning, we propose expected\nThompson sampling (ETS) to learn the optimal policy by maximising the expected\nvalue function with respect to the posterior distribution. This improves upon\ntraditional Thompson sampling (TS) and its extensions, which utilize only one\nsample drawn from the posterior distribution. This improvement is studied both\ntheoretically and using simulation studies, assuming a discrete action space.\nFinally, we successfully extended our setup for a challenging problem with a\ncontinuous action space without theoretical guarantees."
                },
                "authors": [
                    {
                        "name": "Shreya Sinha Roy"
                    },
                    {
                        "name": "Richard G. Everitt"
                    },
                    {
                        "name": "Christian P. Robert"
                    },
                    {
                        "name": "Ritabrata Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Ritabrata Dutta"
                },
                "author": "Ritabrata Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00763v2",
                "updated": "2025-06-02T11:11:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    11,
                    2,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-01T18:00:01Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    1,
                    3,
                    121,
                    0
                ],
                "title": "JFlow: Model-Independent Spherical Jeans Analysis using Equivariant\n  Continuous Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JFlow: Model-Independent Spherical Jeans Analysis using Equivariant\n  Continuous Normalizing Flows"
                },
                "summary": "The kinematics of stars in dwarf spheroidal galaxies have been studied to\nunderstand the structure of dark matter halos. However, the kinematic\ninformation of these stars is often limited to celestial positions and\nline-of-sight velocities, making full phase space analysis challenging.\nConventional methods rely on projected analytic phase space density models with\nseveral parameters and infer dark matter halo structures by solving the\nspherical Jeans equation. In this paper, we introduce an unsupervised machine\nlearning method for solving the spherical Jeans equation in a model-independent\nway as a first step toward model-independent analysis of dwarf spheroidal\ngalaxies. Using equivariant continuous normalizing flows, we demonstrate that\nspherically symmetric stellar phase space densities and velocity dispersions\ncan be estimated without model assumptions. As a proof of concept, we apply our\nmethod to Gaia challenge datasets for spherical models and measure dark matter\nmass densities for given velocity anisotropy profiles. Our method can identify\nhalo structures accurately, even with a small number of tracer stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kinematics of stars in dwarf spheroidal galaxies have been studied to\nunderstand the structure of dark matter halos. However, the kinematic\ninformation of these stars is often limited to celestial positions and\nline-of-sight velocities, making full phase space analysis challenging.\nConventional methods rely on projected analytic phase space density models with\nseveral parameters and infer dark matter halo structures by solving the\nspherical Jeans equation. In this paper, we introduce an unsupervised machine\nlearning method for solving the spherical Jeans equation in a model-independent\nway as a first step toward model-independent analysis of dwarf spheroidal\ngalaxies. Using equivariant continuous normalizing flows, we demonstrate that\nspherically symmetric stellar phase space densities and velocity dispersions\ncan be estimated without model assumptions. As a proof of concept, we apply our\nmethod to Gaia challenge datasets for spherical models and measure dark matter\nmass densities for given velocity anisotropy profiles. Our method can identify\nhalo structures accurately, even with a small number of tracer stars."
                },
                "authors": [
                    {
                        "name": "Sung Hak Lim"
                    },
                    {
                        "name": "Kohei Hayashi"
                    },
                    {
                        "name": "Shun'ichi Horigome"
                    },
                    {
                        "name": "Shigeki Matsumoto"
                    },
                    {
                        "name": "Mihoko M. Nojiri"
                    }
                ],
                "author_detail": {
                    "name": "Mihoko M. Nojiri"
                },
                "author": "Mihoko M. Nojiri",
                "arxiv_comment": "10 pages, 3 figures, 1 table, revised version for the journal\n  submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v4",
                "updated": "2025-06-02T11:03:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    3,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "arxiv_comment": "Accepted to the Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10208v3",
                "updated": "2025-06-02T11:03:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    3,
                    20,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-13T15:31:17Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    31,
                    17,
                    4,
                    348,
                    0
                ],
                "title": "Efficient Generative Modeling with Residual Vector Quantization-Based\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Generative Modeling with Residual Vector Quantization-Based\n  Tokens"
                },
                "summary": "We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based\ngenerative model for high-fidelity generation with fast sampling. RVQ improves\ndata fidelity by increasing the number of quantization steps, referred to as\ndepth, but deeper quantization typically increases inference steps in\ngenerative models. To address this, ResGen directly predicts the vector\nembedding of collective tokens rather than individual ones, ensuring that\ninference steps remain independent of RVQ depth. Additionally, we formulate\ntoken masking and multi-token prediction within a probabilistic framework using\ndiscrete diffusion and variational inference. We validate the efficacy and\ngeneralizability of the proposed method on two challenging tasks across\ndifferent modalities: conditional image generation on ImageNet 256x256 and\nzero-shot text-to-speech synthesis. Experimental results demonstrate that\nResGen outperforms autoregressive counterparts in both tasks, delivering\nsuperior performance without compromising sampling speed. Furthermore, as we\nscale the depth of RVQ, our generative models exhibit enhanced generation\nfidelity or faster sampling speeds compared to similarly sized baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based\ngenerative model for high-fidelity generation with fast sampling. RVQ improves\ndata fidelity by increasing the number of quantization steps, referred to as\ndepth, but deeper quantization typically increases inference steps in\ngenerative models. To address this, ResGen directly predicts the vector\nembedding of collective tokens rather than individual ones, ensuring that\ninference steps remain independent of RVQ depth. Additionally, we formulate\ntoken masking and multi-token prediction within a probabilistic framework using\ndiscrete diffusion and variational inference. We validate the efficacy and\ngeneralizability of the proposed method on two challenging tasks across\ndifferent modalities: conditional image generation on ImageNet 256x256 and\nzero-shot text-to-speech synthesis. Experimental results demonstrate that\nResGen outperforms autoregressive counterparts in both tasks, delivering\nsuperior performance without compromising sampling speed. Furthermore, as we\nscale the depth of RVQ, our generative models exhibit enhanced generation\nfidelity or faster sampling speeds compared to similarly sized baseline models."
                },
                "authors": [
                    {
                        "name": "Jaehyeon Kim"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Keon Lee"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18557v2",
                "updated": "2025-06-02T11:00:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    0,
                    28,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-24T06:51:03Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    51,
                    3,
                    5,
                    144,
                    0
                ],
                "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through\n  Structure-based Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through\n  Structure-based Augmentation"
                },
                "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks."
                },
                "authors": [
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Zhiwen Ruan"
                    },
                    {
                        "name": "Junyou Su"
                    },
                    {
                        "name": "Xingwei He"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Wenjia Zhang"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v5",
                "updated": "2025-06-02T10:58:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    58,
                    16,
                    0,
                    153,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large\nlanguage models (LLMs) into human-interpretable latent directions or features.\nAs we increase the number of features in the SAE, hierarchical features tend to\nsplit into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.),\na phenomenon referred to as feature splitting. However, we show that sparse\ndecomposition and splitting of hierarchical features is not robust.\nSpecifically, we show that seemingly monosemantic features fail to fire where\nthey should, and instead get \"absorbed\" into their children features. We coin\nthis phenomenon feature absorption, and show that it is caused by optimizing\nfor sparsity in SAEs whenever the underlying features form a hierarchy. We\nintroduce a metric to detect absorption in SAEs, and validate our findings\nempirically on hundreds of LLM SAEs. Our investigation suggests that varying\nSAE sizes or sparsity is insufficient to solve this issue. We discuss the\nimplications of feature absorption in SAEs and some potential approaches to\nsolve the fundamental theoretical issues before SAEs can be used for\ninterpreting LLMs robustly and at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large\nlanguage models (LLMs) into human-interpretable latent directions or features.\nAs we increase the number of features in the SAE, hierarchical features tend to\nsplit into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.),\na phenomenon referred to as feature splitting. However, we show that sparse\ndecomposition and splitting of hierarchical features is not robust.\nSpecifically, we show that seemingly monosemantic features fail to fire where\nthey should, and instead get \"absorbed\" into their children features. We coin\nthis phenomenon feature absorption, and show that it is caused by optimizing\nfor sparsity in SAEs whenever the underlying features form a hierarchy. We\nintroduce a metric to detect absorption in SAEs, and validate our findings\nempirically on hundreds of LLM SAEs. Our investigation suggests that varying\nSAE sizes or sparsity is insufficient to solve this issue. We discuss the\nimplications of feature absorption in SAEs and some potential approaches to\nsolve the fundamental theoretical issues before SAEs can be used for\ninterpreting LLMs robustly and at scale."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17077v2",
                "updated": "2025-06-02T10:38:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    38,
                    54,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-28T17:02:16Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    2,
                    16,
                    1,
                    28,
                    0
                ],
                "title": "Inducing, Detecting and Characterising Neural Modules: A Pipeline for\n  Functional Interpretability in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing, Detecting and Characterising Neural Modules: A Pipeline for\n  Functional Interpretability in Reinforcement Learning"
                },
                "summary": "Interpretability is crucial for ensuring RL systems align with human values.\nHowever, it remains challenging to achieve in complex decision making domains.\nExisting methods frequently attempt interpretability at the level of\nfundamental model units, such as neurons or decision nodes: an approach which\nscales poorly to large models. Here, we instead propose an approach to\ninterpretability at the level of functional modularity. We show how encouraging\nsparsity and locality in network weights leads to the emergence of functional\nmodules in RL policy networks. To detect these modules, we develop an extended\nLouvain algorithm which uses a novel `correlation alignment' metric to overcome\nthe limitations of standard network analysis techniques when applied to neural\nnetwork architectures. Applying these methods to 2D and 3D MiniGrid\nenvironments reveals the consistent emergence of distinct navigational modules\nfor different axes, and we further demonstrate how these functions can be\nvalidated through direct interventions on network weights prior to inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability is crucial for ensuring RL systems align with human values.\nHowever, it remains challenging to achieve in complex decision making domains.\nExisting methods frequently attempt interpretability at the level of\nfundamental model units, such as neurons or decision nodes: an approach which\nscales poorly to large models. Here, we instead propose an approach to\ninterpretability at the level of functional modularity. We show how encouraging\nsparsity and locality in network weights leads to the emergence of functional\nmodules in RL policy networks. To detect these modules, we develop an extended\nLouvain algorithm which uses a novel `correlation alignment' metric to overcome\nthe limitations of standard network analysis techniques when applied to neural\nnetwork architectures. Applying these methods to 2D and 3D MiniGrid\nenvironments reveals the consistent emergence of distinct navigational modules\nfor different axes, and we further demonstrate how these functions can be\nvalidated through direct interventions on network weights prior to inference."
                },
                "authors": [
                    {
                        "name": "Anna Soligo"
                    },
                    {
                        "name": "Pietro Ferraro"
                    },
                    {
                        "name": "David Boyle"
                    }
                ],
                "author_detail": {
                    "name": "David Boyle"
                },
                "author": "David Boyle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07217v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07217v3",
                "updated": "2025-06-02T10:38:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    38,
                    46,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-10T11:57:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    57,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "ReelWave: Multi-Agentic Movie Sound Generation through Multimodal LLM\n  Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReelWave: Multi-Agentic Movie Sound Generation through Multimodal LLM\n  Conversation"
                },
                "summary": "Current audio generation conditioned by text or video focuses on aligning\naudio with text/video modalities. Despite excellent alignment results, these\nmultimodal frameworks still cannot be directly applied to compelling movie\nstorytelling involving multiple scenes, where \"on-screen\" sounds require\ntemporally-aligned audio generation, while \"off-screen\" sounds contribute to\nappropriate environment sounds accompanied by background music when applicable.\nInspired by professional movie production, this paper proposes a multi-agentic\nframework for audio generation supervised by an autonomous Sound Director\nagent, engaging multi-turn conversations with other agents for on-screen and\noff-screen sound generation through multimodal LLM. To address on-screen sound\ngeneration, after detecting any talking humans in videos, we capture\nsemantically and temporally synchronized sound by training a prediction model\nthat forecasts interpretable, time-varying audio control signals: loudness,\npitch, and timbre, which are used by a Foley Artist agent to condition a\ncross-attention module in the sound generation. The Foley Artist works\ncooperatively with the Composer and Voice Actor agents, and together they\nautonomously generate off-screen sound to complement the overall production.\nEach agent takes on specific roles similar to those of a movie production team.\nTo temporally ground audio language models, in ReelWave, text/video conditions\nare decomposed into atomic, specific sound generation instructions synchronized\nwith visuals when applicable. Consequently, our framework can generate rich and\nrelevant audio content conditioned on video clips extracted from movies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current audio generation conditioned by text or video focuses on aligning\naudio with text/video modalities. Despite excellent alignment results, these\nmultimodal frameworks still cannot be directly applied to compelling movie\nstorytelling involving multiple scenes, where \"on-screen\" sounds require\ntemporally-aligned audio generation, while \"off-screen\" sounds contribute to\nappropriate environment sounds accompanied by background music when applicable.\nInspired by professional movie production, this paper proposes a multi-agentic\nframework for audio generation supervised by an autonomous Sound Director\nagent, engaging multi-turn conversations with other agents for on-screen and\noff-screen sound generation through multimodal LLM. To address on-screen sound\ngeneration, after detecting any talking humans in videos, we capture\nsemantically and temporally synchronized sound by training a prediction model\nthat forecasts interpretable, time-varying audio control signals: loudness,\npitch, and timbre, which are used by a Foley Artist agent to condition a\ncross-attention module in the sound generation. The Foley Artist works\ncooperatively with the Composer and Voice Actor agents, and together they\nautonomously generate off-screen sound to complement the overall production.\nEach agent takes on specific roles similar to those of a movie production team.\nTo temporally ground audio language models, in ReelWave, text/video conditions\nare decomposed into atomic, specific sound generation instructions synchronized\nwith visuals when applicable. Consequently, our framework can generate rich and\nrelevant audio content conditioned on video clips extracted from movies."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "arxiv_comment": "Project page: https://vincent2311.github.io/ReelWave_demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07217v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07217v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24362v2",
                "updated": "2025-06-02T10:26:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    26,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T08:54:28Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    54,
                    28,
                    4,
                    150,
                    0
                ],
                "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion"
                },
                "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits."
                },
                "authors": [
                    {
                        "name": "Anum Afzal"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Yftah Ziser"
                    }
                ],
                "author_detail": {
                    "name": "Yftah Ziser"
                },
                "author": "Yftah Ziser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15865v2",
                "updated": "2025-06-02T10:13:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    13,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-21T12:56:15Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    56,
                    15,
                    4,
                    52,
                    0
                ],
                "title": "Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must\n  Prioritize Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must\n  Prioritize Risk"
                },
                "summary": "Standard benchmarks fixate on how well large language model (LLM) agents\nperform in finance, yet say little about whether they are safe to deploy. We\nargue that accuracy metrics and return-based scores provide an illusion of\nreliability, overlooking vulnerabilities such as hallucinated facts, stale\ndata, and adversarial prompt manipulation. We take a firm position: financial\nLLM agents should be evaluated first and foremost on their risk profile, not on\ntheir point-estimate performance. Drawing on risk-engineering principles, we\noutline a three-level agenda: model, workflow, and system, for stress-testing\nLLM agents under realistic failure modes. To illustrate why this shift is\nurgent, we audit six API-based and open-weights LLM agents on three high-impact\ntasks and uncover hidden weaknesses that conventional benchmarks miss. We\nconclude with actionable recommendations for researchers, practitioners, and\nregulators: audit risk-aware metrics in future studies, publish stress\nscenarios alongside datasets, and treat ``safety budget'' as a primary success\ncriterion. Only by redefining what ``good'' looks like can the community\nresponsibly advance AI-driven finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard benchmarks fixate on how well large language model (LLM) agents\nperform in finance, yet say little about whether they are safe to deploy. We\nargue that accuracy metrics and return-based scores provide an illusion of\nreliability, overlooking vulnerabilities such as hallucinated facts, stale\ndata, and adversarial prompt manipulation. We take a firm position: financial\nLLM agents should be evaluated first and foremost on their risk profile, not on\ntheir point-estimate performance. Drawing on risk-engineering principles, we\noutline a three-level agenda: model, workflow, and system, for stress-testing\nLLM agents under realistic failure modes. To illustrate why this shift is\nurgent, we audit six API-based and open-weights LLM agents on three high-impact\ntasks and uncover hidden weaknesses that conventional benchmarks miss. We\nconclude with actionable recommendations for researchers, practitioners, and\nregulators: audit risk-aware metrics in future studies, publish stress\nscenarios alongside datasets, and treat ``safety budget'' as a primary success\ncriterion. Only by redefining what ``good'' looks like can the community\nresponsibly advance AI-driven finance."
                },
                "authors": [
                    {
                        "name": "Zichen Chen"
                    },
                    {
                        "name": "Jiaao Chen"
                    },
                    {
                        "name": "Jianda Chen"
                    },
                    {
                        "name": "Misha Sra"
                    }
                ],
                "author_detail": {
                    "name": "Misha Sra"
                },
                "author": "Misha Sra",
                "arxiv_comment": "46 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16310v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16310v4",
                "updated": "2025-06-02T10:06:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    6,
                    31,
                    0,
                    153,
                    0
                ],
                "published": "2024-01-29T17:13:44Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    17,
                    13,
                    44,
                    0,
                    29,
                    0
                ],
                "title": "An Insight into Security Code Review with LLMs: Capabilities, Obstacles,\n  and Influential Factors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Insight into Security Code Review with LLMs: Capabilities, Obstacles,\n  and Influential Factors"
                },
                "summary": "Security code review is a time-consuming and labor-intensive process\ntypically requiring integration with automated security defect detection tools.\nHowever, existing security analysis tools struggle with poor generalization,\nhigh false positive rates, and coarse detection granularity. Large Language\nModels (LLMs) have been considered promising candidates for addressing those\nchallenges. In this study, we conducted an empirical study to explore the\npotential of LLMs in detecting security defects during code review.\nSpecifically, we evaluated the performance of six LLMs under five different\nprompts and compared them with state-of-the-art static analysis tools. We also\nperformed linguistic and regression analyses for the best-performing LLM to\nidentify quality problems in its responses and factors influencing its\nperformance. Our findings showthat: (1) existing pre-trained LLMs have limited\ncapability in security code review but significantly outperformthe\nstate-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs\nwhen provided with a CWE list for reference. (3) GPT-4 frequently generates\nverbose or non-compliant responses with the task requirements given in the\nprompts. (4) GPT-4 is more adept at identifying security defects in code files\nwith fewer tokens, containing functional logic, or written by developers with\nless involvement in the project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security code review is a time-consuming and labor-intensive process\ntypically requiring integration with automated security defect detection tools.\nHowever, existing security analysis tools struggle with poor generalization,\nhigh false positive rates, and coarse detection granularity. Large Language\nModels (LLMs) have been considered promising candidates for addressing those\nchallenges. In this study, we conducted an empirical study to explore the\npotential of LLMs in detecting security defects during code review.\nSpecifically, we evaluated the performance of six LLMs under five different\nprompts and compared them with state-of-the-art static analysis tools. We also\nperformed linguistic and regression analyses for the best-performing LLM to\nidentify quality problems in its responses and factors influencing its\nperformance. Our findings showthat: (1) existing pre-trained LLMs have limited\ncapability in security code review but significantly outperformthe\nstate-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs\nwhen provided with a CWE list for reference. (3) GPT-4 frequently generates\nverbose or non-compliant responses with the task requirements given in the\nprompts. (4) GPT-4 is more adept at identifying security defects in code files\nwith fewer tokens, containing functional logic, or written by developers with\nless involvement in the project."
                },
                "authors": [
                    {
                        "name": "Jiaxin Yu"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Amjed Tahir"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Yangxiao Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yangxiao Cai"
                },
                "author": "Yangxiao Cai",
                "arxiv_comment": "21 pages, 5 images, 8 tables, Manuscript submitted to a journal\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16310v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16310v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15351v2",
                "updated": "2025-06-02T10:04:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    4,
                    32,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-19T15:48:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    48,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models"
                },
                "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals."
                },
                "authors": [
                    {
                        "name": "I-Fan Lin"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06204v2",
                "updated": "2025-06-02T09:56:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    56,
                    25,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-10T07:03:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    3,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Non-literal Understanding of Number Words by Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-literal Understanding of Number Words by Language Models"
                },
                "summary": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Polina Tsvilodub"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jan-Philipp Fränken"
                    },
                    {
                        "name": "Michael Franke"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "12 pages, 10 figures. To appear in the Proceedings of CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13899v2",
                "updated": "2025-06-02T09:43:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    43,
                    49,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-21T07:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "Schemato -- An LLM for Netlist-to-Schematic Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schemato -- An LLM for Netlist-to-Schematic Conversion"
                },
                "summary": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in\nconverting netlists to .asc files, text-based schematic description used in\nLTSpice. Experiments on our circuit dataset show that Schemato achieves up to\n76% compilation success rate, surpassing 63% scored by the state-of-the-art\nLLMs. Furthermore, our experiments show that Schemato generates schematics with\nan average graph edit distance score and mean structural similarity index\nmeasure, scaled by the compilation success rate that are 1.8x and 4.3x higher\nthan the best performing LLMs respectively, demonstrating its ability to\ngenerate schematics that are more accurately connected and are closer to the\nreference human design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in\nconverting netlists to .asc files, text-based schematic description used in\nLTSpice. Experiments on our circuit dataset show that Schemato achieves up to\n76% compilation success rate, surpassing 63% scored by the state-of-the-art\nLLMs. Furthermore, our experiments show that Schemato generates schematics with\nan average graph edit distance score and mean structural similarity index\nmeasure, scaled by the compilation success rate that are 1.8x and 4.3x higher\nthan the best performing LLMs respectively, demonstrating its ability to\ngenerate schematics that are more accurately connected and are closer to the\nreference human design."
                },
                "authors": [
                    {
                        "name": "Ryoga Matsuo"
                    },
                    {
                        "name": "Stefan Uhlich"
                    },
                    {
                        "name": "Arun Venkitaraman"
                    },
                    {
                        "name": "Andrea Bonetti"
                    },
                    {
                        "name": "Chia-Yu Hsieh"
                    },
                    {
                        "name": "Ali Momeni"
                    },
                    {
                        "name": "Lukas Mauch"
                    },
                    {
                        "name": "Augusto Capone"
                    },
                    {
                        "name": "Eisaku Ohbuchi"
                    },
                    {
                        "name": "Lorenzo Servadei"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Servadei"
                },
                "author": "Lorenzo Servadei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17358v3",
                "updated": "2025-06-02T09:40:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    40,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-24T17:36:49Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    36,
                    49,
                    0,
                    55,
                    0
                ],
                "title": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data"
                },
                "summary": "How can we verify whether copyrighted content was used to train a large\nvision-language model (VLM) without direct access to its training data?\nMotivated by the hypothesis that a VLM is able to recognize images from its\ntraining corpus, we propose DIS-CO, a novel approach to infer the inclusion of\ncopyrighted content during the model's development. By repeatedly querying a\nVLM with specific frames from targeted copyrighted material, DIS-CO extracts\nthe content's identity through free-form text completions. To assess its\neffectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames\npaired with detailed captions, drawn from films released both before and after\na model's training cutoff. Our results show that DIS-CO significantly improves\ndetection performance, nearly doubling the average AUC of the best prior method\non models with logits available. Our findings also highlight a broader concern:\nall tested models appear to have been exposed to some extent to copyrighted\ncontent. Our code and data are available at\nhttps://github.com/avduarte333/DIS-CO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we verify whether copyrighted content was used to train a large\nvision-language model (VLM) without direct access to its training data?\nMotivated by the hypothesis that a VLM is able to recognize images from its\ntraining corpus, we propose DIS-CO, a novel approach to infer the inclusion of\ncopyrighted content during the model's development. By repeatedly querying a\nVLM with specific frames from targeted copyrighted material, DIS-CO extracts\nthe content's identity through free-form text completions. To assess its\neffectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames\npaired with detailed captions, drawn from films released both before and after\na model's training cutoff. Our results show that DIS-CO significantly improves\ndetection performance, nearly doubling the average AUC of the best prior method\non models with logits available. Our findings also highlight a broader concern:\nall tested models appear to have been exposed to some extent to copyrighted\ncontent. Our code and data are available at\nhttps://github.com/avduarte333/DIS-CO"
                },
                "authors": [
                    {
                        "name": "André V. Duarte"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Arlindo L. Oliveira"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07089v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07089v3",
                "updated": "2025-06-02T09:38:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    38,
                    26,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-09T17:58:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "OmniCaptioner: One Captioner to Rule Them All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCaptioner: One Captioner to Rule Them All"
                },
                "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."
                },
                "authors": [
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Xinyue Li"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Yuewen Cao"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Tianshuo Peng"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "arxiv_comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07089v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07089v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14301v2",
                "updated": "2025-06-02T09:23:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    23,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-20T06:32:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    32,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models"
                },
                "summary": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and culturally representative evaluation suite\nfor the SEA languages has not been developed thus far. Here, we present\nSEA-HELM, a holistic linguistic and cultural LLM evaluation suite that\nemphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2)\nLLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM\ncurrently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also\nintroduce the SEA-HELM leaderboard, which allows users to understand models'\nmultilingual and multicultural performance in a systematic and user-friendly\nmanner. We make the SEA-HELM evaluation code publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and culturally representative evaluation suite\nfor the SEA languages has not been developed thus far. Here, we present\nSEA-HELM, a holistic linguistic and cultural LLM evaluation suite that\nemphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2)\nLLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM\ncurrently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also\nintroduce the SEA-HELM leaderboard, which allows users to understand models'\nmultilingual and multicultural performance in a systematic and user-friendly\nmanner. We make the SEA-HELM evaluation code publicly available."
                },
                "authors": [
                    {
                        "name": "Yosephine Susanto"
                    },
                    {
                        "name": "Adithya Venkatadri Hulagadri"
                    },
                    {
                        "name": "Jann Railey Montalan"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Xian Bin Yong"
                    },
                    {
                        "name": "Weiqi Leong"
                    },
                    {
                        "name": "Hamsawardhini Rengarajan"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Yifan Mai"
                    },
                    {
                        "name": "William Chandra Tjhi"
                    }
                ],
                "author_detail": {
                    "name": "William Chandra Tjhi"
                },
                "author": "William Chandra Tjhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19107v2",
                "updated": "2025-06-02T09:19:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    19,
                    36,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-31T13:04:37Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    37,
                    4,
                    31,
                    0
                ],
                "title": "Brain network science modelling of sparse neural networks enables\n  Transformers and LLMs to perform as fully connected",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain network science modelling of sparse neural networks enables\n  Transformers and LLMs to perform as fully connected"
                },
                "summary": "Dynamic sparse training (DST) can reduce the computational demands in ANNs,\nbut faces difficulties in keeping peak performance at high sparsity levels. The\nCannistraci-Hebb training (CHT) is a brain-inspired method for growing\nconnectivity in DST. CHT leverages a gradient-free, topology-driven link\nregrowth, which has shown ultra-sparse (less than 1% connectivity) advantage\nacross various tasks compared to fully connected networks. Yet, CHT suffers two\nmain drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d\nnode degree - restricting it to ultra-sparse regimes. (ii) it selects top link\nprediction scores, which is inappropriate for the early training epochs, when\nthe network presents unreliable connections. Here, we design the first\nbrain-inspired network model - termed bipartite receptive field (BRF) - to\ninitialize the connectivity of sparse artificial neural networks. We further\nintroduce a GPU-friendly matrix-based approximation of CH link prediction,\nreducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training\nsoft rule (CHTs), which adopts a flexible strategy for sampling connections in\nboth link removal and regrowth, balancing the exploration and exploitation of\nnetwork topology. Additionally, we integrate CHTs with a sigmoid gradual\ndensity decay (CHTss). Empirical results show that BRF offers performance\nadvantages over previous network science models. Using 1% of connections, CHTs\noutperforms fully connected networks in MLP architectures on image\nclassification tasks, compressing some networks to less than 30% of the nodes.\nUsing 5% of the connections, CHTss outperforms fully connected networks in two\nTransformer-based machine translation tasks. Finally, at 30% connectivity, both\nCHTs and CHTss outperform other DST methods in language modeling and even\nexceed fully connected baselines in zero-shot tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic sparse training (DST) can reduce the computational demands in ANNs,\nbut faces difficulties in keeping peak performance at high sparsity levels. The\nCannistraci-Hebb training (CHT) is a brain-inspired method for growing\nconnectivity in DST. CHT leverages a gradient-free, topology-driven link\nregrowth, which has shown ultra-sparse (less than 1% connectivity) advantage\nacross various tasks compared to fully connected networks. Yet, CHT suffers two\nmain drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d\nnode degree - restricting it to ultra-sparse regimes. (ii) it selects top link\nprediction scores, which is inappropriate for the early training epochs, when\nthe network presents unreliable connections. Here, we design the first\nbrain-inspired network model - termed bipartite receptive field (BRF) - to\ninitialize the connectivity of sparse artificial neural networks. We further\nintroduce a GPU-friendly matrix-based approximation of CH link prediction,\nreducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training\nsoft rule (CHTs), which adopts a flexible strategy for sampling connections in\nboth link removal and regrowth, balancing the exploration and exploitation of\nnetwork topology. Additionally, we integrate CHTs with a sigmoid gradual\ndensity decay (CHTss). Empirical results show that BRF offers performance\nadvantages over previous network science models. Using 1% of connections, CHTs\noutperforms fully connected networks in MLP architectures on image\nclassification tasks, compressing some networks to less than 30% of the nodes.\nUsing 5% of the connections, CHTss outperforms fully connected networks in two\nTransformer-based machine translation tasks. Finally, at 30% connectivity, both\nCHTs and CHTss outperform other DST methods in language modeling and even\nexceed fully connected baselines in zero-shot tasks."
                },
                "authors": [
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Diego Cerretti"
                    },
                    {
                        "name": "Jialin Zhao"
                    },
                    {
                        "name": "Wenjing Wu"
                    },
                    {
                        "name": "Ziheng Liao"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Carlo Vittorio Cannistraci"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Vittorio Cannistraci"
                },
                "author": "Carlo Vittorio Cannistraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08219v2",
                "updated": "2025-06-02T09:12:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    12,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-14T16:02:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Paul Joe Maliakel"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14830v3",
                "updated": "2025-06-02T09:09:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    9,
                    36,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-20T18:45:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs"
                },
                "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."
                },
                "authors": [
                    {
                        "name": "Danni Liu"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00113v3",
                "updated": "2025-06-02T09:08:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    8,
                    56,
                    0,
                    153,
                    0
                ],
                "published": "2024-08-27T19:27:43Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    19,
                    27,
                    43,
                    1,
                    240,
                    0
                ],
                "title": "Wait, that's not an option: LLMs Robustness with Incorrect\n  Multiple-Choice Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wait, that's not an option: LLMs Robustness with Incorrect\n  Multiple-Choice Options"
                },
                "summary": "This work introduces a novel framework for evaluating LLMs' capacity to\nbalance instruction-following with critical reasoning when presented with\nmultiple-choice questions containing no valid answers. Through systematic\nevaluation across arithmetic, domain-specific knowledge, and high-stakes\nmedical decision tasks, we demonstrate that post-training aligned models often\ndefault to selecting invalid options, while base models exhibit improved\nrefusal capabilities that scale with model size. Our analysis reveals that\nalignment techniques, though intended to enhance helpfulness, can inadvertently\nimpair models' reflective judgment--the ability to override default behaviors\nwhen faced with invalid options. We additionally conduct a parallel human study\nshowing similar instruction-following biases, with implications for how these\nbiases may propagate through human feedback datasets used in alignment. We\nprovide extensive ablation studies examining the impact of model size, training\ntechniques, and prompt engineering. Our findings highlight fundamental tensions\nbetween alignment optimization and preservation of critical reasoning\ncapabilities, with important implications for developing more robust AI systems\nfor real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a novel framework for evaluating LLMs' capacity to\nbalance instruction-following with critical reasoning when presented with\nmultiple-choice questions containing no valid answers. Through systematic\nevaluation across arithmetic, domain-specific knowledge, and high-stakes\nmedical decision tasks, we demonstrate that post-training aligned models often\ndefault to selecting invalid options, while base models exhibit improved\nrefusal capabilities that scale with model size. Our analysis reveals that\nalignment techniques, though intended to enhance helpfulness, can inadvertently\nimpair models' reflective judgment--the ability to override default behaviors\nwhen faced with invalid options. We additionally conduct a parallel human study\nshowing similar instruction-following biases, with implications for how these\nbiases may propagate through human feedback datasets used in alignment. We\nprovide extensive ablation studies examining the impact of model size, training\ntechniques, and prompt engineering. Our findings highlight fundamental tensions\nbetween alignment optimization and preservation of critical reasoning\ncapabilities, with important implications for developing more robust AI systems\nfor real-world deployment."
                },
                "authors": [
                    {
                        "name": "Gracjan Góral"
                    },
                    {
                        "name": "Emilia Wiśnios"
                    },
                    {
                        "name": "Piotr Sankowski"
                    },
                    {
                        "name": "Paweł Budzianowski"
                    }
                ],
                "author_detail": {
                    "name": "Paweł Budzianowski"
                },
                "author": "Paweł Budzianowski",
                "arxiv_comment": "Accepted for ACL 2025 Main Conference and NeurIPS 2024 FM-EduAssess\n  Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06560v2",
                "updated": "2025-06-02T08:49:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    49,
                    20,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-10T15:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    25,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation"
                },
                "summary": "The recent surge in high-quality open-source Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, have opened\nthe possibility of creating high-quality personalized models that generate text\nattuned to a specific individual's needs and are capable of credibly imitating\ntheir writing style by refining an open-source model with that person's own\ndata. The technology to create such models is accessible to private\nindividuals, and training and running such models can be done cheaply on\nconsumer-grade hardware. While these advancements are a huge gain for usability\nand privacy, this position paper argues that the practical feasibility of\nimpersonating specific individuals also introduces novel safety risks. For\ninstance, this technology enables the creation of phishing emails or fraudulent\nsocial media accounts, based on small amounts of publicly available text, or by\nthe individuals themselves to escape AI text detection. We further argue that\nthese risks are complementary to - and distinct from - the much-discussed risks\nof other impersonation attacks such as image, voice, or video deepfakes, and\nare not adequately addressed by the larger research community, or the current\ngeneration of open- and closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge in high-quality open-source Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, have opened\nthe possibility of creating high-quality personalized models that generate text\nattuned to a specific individual's needs and are capable of credibly imitating\ntheir writing style by refining an open-source model with that person's own\ndata. The technology to create such models is accessible to private\nindividuals, and training and running such models can be done cheaply on\nconsumer-grade hardware. While these advancements are a huge gain for usability\nand privacy, this position paper argues that the practical feasibility of\nimpersonating specific individuals also introduces novel safety risks. For\ninstance, this technology enables the creation of phishing emails or fraudulent\nsocial media accounts, based on small amounts of publicly available text, or by\nthe individuals themselves to escape AI text detection. We further argue that\nthese risks are complementary to - and distinct from - the much-discussed risks\nof other impersonation attacks such as image, voice, or video deepfakes, and\nare not adequately addressed by the larger research community, or the current\ngeneration of open- and closed-source models."
                },
                "authors": [
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13248v2",
                "updated": "2025-06-02T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    41,
                    9,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-17T06:15:00Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    6,
                    15,
                    0,
                    3,
                    291,
                    0
                ],
                "title": "Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation"
                },
                "summary": "Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. The datasets and benchmark\nimplementation are available at: https://github.com/jchanxtarov/sent_xrec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. The datasets and benchmark\nimplementation are available at: https://github.com/jchanxtarov/sent_xrec."
                },
                "authors": [
                    {
                        "name": "Ryotaro Shimizu"
                    },
                    {
                        "name": "Takashi Wada"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Johannes Kruse"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Sai HtaungKham"
                    },
                    {
                        "name": "Linxin Song"
                    },
                    {
                        "name": "Yuya Yoshikawa"
                    },
                    {
                        "name": "Yuki Saito"
                    },
                    {
                        "name": "Fugee Tsung"
                    },
                    {
                        "name": "Masayuki Goto"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "This manuscript has been accepted for presentation at The Web\n  Conference (WWW) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17081v2",
                "updated": "2025-06-02T08:41:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    41,
                    8,
                    0,
                    153,
                    0
                ],
                "published": "2024-08-30T08:09:19Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    9,
                    19,
                    4,
                    243,
                    0
                ],
                "title": "Stochastic Layer-Wise Shuffle for Improving Vision Mamba Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Layer-Wise Shuffle for Improving Vision Mamba Training"
                },
                "summary": "Recent Vision Mamba (Vim) models exhibit nearly linear complexity in sequence\nlength, making them highly attractive for processing visual data. However, the\ntraining methodologies and their potential are still not sufficiently explored.\nIn this paper, we investigate strategies for Vim and propose Stochastic\nLayer-Wise Shuffle (SLWS), a novel regularization method that can effectively\nimprove the Vim training. Without architectural modifications, this approach\nenables the non-hierarchical Vim to get leading performance on ImageNet-1K\ncompared with the similar type counterparts. Our method operates through four\nsimple steps per layer: probability allocation to assign layer-dependent\nshuffle rates, operation sampling via Bernoulli trials, sequence shuffling of\ninput tokens, and order restoration of outputs. SLWS distinguishes itself\nthrough three principles: \\textit{(1) Plug-and-play:} No architectural\nmodifications are needed, and it is deactivated during inference. \\textit{(2)\nSimple but effective:} The four-step process introduces only random\npermutations and negligible overhead. \\textit{(3) Intuitive design:} Shuffling\nprobabilities grow linearly with layer depth, aligning with the hierarchical\nsemantic abstraction in vision models. Our work underscores the importance of\ntailored training strategies for Vim models and provides a helpful way to\nexplore their scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision Mamba (Vim) models exhibit nearly linear complexity in sequence\nlength, making them highly attractive for processing visual data. However, the\ntraining methodologies and their potential are still not sufficiently explored.\nIn this paper, we investigate strategies for Vim and propose Stochastic\nLayer-Wise Shuffle (SLWS), a novel regularization method that can effectively\nimprove the Vim training. Without architectural modifications, this approach\nenables the non-hierarchical Vim to get leading performance on ImageNet-1K\ncompared with the similar type counterparts. Our method operates through four\nsimple steps per layer: probability allocation to assign layer-dependent\nshuffle rates, operation sampling via Bernoulli trials, sequence shuffling of\ninput tokens, and order restoration of outputs. SLWS distinguishes itself\nthrough three principles: \\textit{(1) Plug-and-play:} No architectural\nmodifications are needed, and it is deactivated during inference. \\textit{(2)\nSimple but effective:} The four-step process introduces only random\npermutations and negligible overhead. \\textit{(3) Intuitive design:} Shuffling\nprobabilities grow linearly with layer depth, aligning with the hierarchical\nsemantic abstraction in vision models. Our work underscores the importance of\ntailored training strategies for Vim models and provides a helpful way to\nexplore their scalability."
                },
                "authors": [
                    {
                        "name": "Zizheng Huang"
                    },
                    {
                        "name": "Haoxing Chen"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Jun Lan"
                    },
                    {
                        "name": "Huijia Zhu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "accpeted to ICML25",
                "arxiv_journal_ref": "Proceedings of the 42nd International Conference on Machine\n  Learning, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23405v2",
                "updated": "2025-06-02T08:21:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    21,
                    31,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T12:52:43Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    52,
                    43,
                    3,
                    149,
                    0
                ],
                "title": "A Practical Guide for Supporting Formative Assessment and Feedback Using\n  Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Guide for Supporting Formative Assessment and Feedback Using\n  Generative AI"
                },
                "summary": "Formative assessment is a cornerstone of effective teaching and learning,\nproviding students with feedback to guide their learning. While there has been\nan exponential growth in the application of generative AI in scaling various\naspects of formative assessment, ranging from automatic question generation to\nintelligent tutoring systems and personalized feedback, few have directly\naddressed the core pedagogical principles of formative assessment. Here, we\ncritically examined how generative AI, especially large-language models (LLMs)\nsuch as ChatGPT, can support key components of formative assessment: helping\nstudents, teachers, and peers understand \"where learners are going,\" \"where\nlearners currently are,\" and \"how to move learners forward\" in the learning\nprocess. With the rapid emergence of new prompting techniques and LLM\ncapabilities, we also provide guiding principles for educators to effectively\nleverage cost-free LLMs in formative assessments while remaining grounded in\npedagogical best practices. Furthermore, we reviewed the role of LLMs in\ngenerating feedback, highlighting limitations in current evaluation metrics\nthat inadequately capture the nuances of formative feedback, such as\ndistinguishing feedback at the task, process, and self-regulatory levels.\nFinally, we offer practical guidelines for educators and researchers, including\nconcrete classroom strategies and future directions such as developing robust\nmetrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic\nand cultural barriers to formative assessment, and designing AI-aware\nassessment strategies that promote transferable skills while mitigating\noverreliance on LLM-generated responses. By structuring the discussion within\nan established formative assessment framework, this review provides a\ncomprehensive foundation for integrating LLMs into formative assessment in a\npedagogically informed manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formative assessment is a cornerstone of effective teaching and learning,\nproviding students with feedback to guide their learning. While there has been\nan exponential growth in the application of generative AI in scaling various\naspects of formative assessment, ranging from automatic question generation to\nintelligent tutoring systems and personalized feedback, few have directly\naddressed the core pedagogical principles of formative assessment. Here, we\ncritically examined how generative AI, especially large-language models (LLMs)\nsuch as ChatGPT, can support key components of formative assessment: helping\nstudents, teachers, and peers understand \"where learners are going,\" \"where\nlearners currently are,\" and \"how to move learners forward\" in the learning\nprocess. With the rapid emergence of new prompting techniques and LLM\ncapabilities, we also provide guiding principles for educators to effectively\nleverage cost-free LLMs in formative assessments while remaining grounded in\npedagogical best practices. Furthermore, we reviewed the role of LLMs in\ngenerating feedback, highlighting limitations in current evaluation metrics\nthat inadequately capture the nuances of formative feedback, such as\ndistinguishing feedback at the task, process, and self-regulatory levels.\nFinally, we offer practical guidelines for educators and researchers, including\nconcrete classroom strategies and future directions such as developing robust\nmetrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic\nand cultural barriers to formative assessment, and designing AI-aware\nassessment strategies that promote transferable skills while mitigating\noverreliance on LLM-generated responses. By structuring the discussion within\nan established formative assessment framework, this review provides a\ncomprehensive foundation for integrating LLMs into formative assessment in a\npedagogically informed manner."
                },
                "authors": [
                    {
                        "name": "Sapolnach Prompiengchai"
                    },
                    {
                        "name": "Charith Narreddy"
                    },
                    {
                        "name": "Steve Joordens"
                    }
                ],
                "author_detail": {
                    "name": "Steve Joordens"
                },
                "author": "Steve Joordens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05367v2",
                "updated": "2025-06-02T08:18:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    18,
                    21,
                    0,
                    153,
                    0
                ],
                "published": "2024-09-09T06:55:37Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    37,
                    0,
                    253,
                    0
                ],
                "title": "STRICTA: Structured Reasoning in Critical Text Assessment for Peer\n  Review and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRICTA: Structured Reasoning in Critical Text Assessment for Peer\n  Review and Beyond"
                },
                "summary": "Critical text assessment is at the core of many expert activities, such as\nfact-checking, peer review, and essay grading. Yet, existing work treats\ncritical text assessment as a black box problem, limiting interpretability and\nhuman-AI collaboration. To close this gap, we introduce Structured Reasoning In\nCritical Text Assessment (STRICTA), a novel specification framework to model\ntext assessment as an explicit, step-wise reasoning process. STRICTA breaks\ndown the assessment into a graph of interconnected reasoning steps drawing on\ncausality theory (Pearl, 1995). This graph is populated based on expert\ninteraction data and used to study the assessment process and facilitate\nhuman-AI collaboration. We formally define STRICTA and apply it in a study on\nbiomedical paper assessment, resulting in a dataset of over 4000 reasoning\nsteps from roughly 40 biomedical experts on more than 20 papers. We use this\ndataset to empirically study expert reasoning in critical text assessment, and\ninvestigate if LLMs are able to imitate and support experts within these\nworkflows. The resulting tools and datasets pave the way for studying\ncollaborative expert-AI reasoning in text assessment, in peer review and\nbeyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical text assessment is at the core of many expert activities, such as\nfact-checking, peer review, and essay grading. Yet, existing work treats\ncritical text assessment as a black box problem, limiting interpretability and\nhuman-AI collaboration. To close this gap, we introduce Structured Reasoning In\nCritical Text Assessment (STRICTA), a novel specification framework to model\ntext assessment as an explicit, step-wise reasoning process. STRICTA breaks\ndown the assessment into a graph of interconnected reasoning steps drawing on\ncausality theory (Pearl, 1995). This graph is populated based on expert\ninteraction data and used to study the assessment process and facilitate\nhuman-AI collaboration. We formally define STRICTA and apply it in a study on\nbiomedical paper assessment, resulting in a dataset of over 4000 reasoning\nsteps from roughly 40 biomedical experts on more than 20 papers. We use this\ndataset to empirically study expert reasoning in critical text assessment, and\ninvestigate if LLMs are able to imitate and support experts within these\nworkflows. The resulting tools and datasets pave the way for studying\ncollaborative expert-AI reasoning in text assessment, in peer review and\nbeyond."
                },
                "authors": [
                    {
                        "name": "Nils Dycke"
                    },
                    {
                        "name": "Matej Zečević"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Beatrix Suess"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09923v2",
                "updated": "2025-06-02T08:10:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    10,
                    54,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-14T06:32:45Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    32,
                    45,
                    0,
                    104,
                    0
                ],
                "title": "Guiding Reasoning in Small Language Models with LLM Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Reasoning in Small Language Models with LLM Assistance"
                },
                "summary": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone."
                },
                "authors": [
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Euiin Yi"
                    },
                    {
                        "name": "Minu Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    },
                    {
                        "name": "Taehyeon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taehyeon Kim"
                },
                "author": "Taehyeon Kim",
                "arxiv_comment": "20 pages, 12 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11364v2",
                "updated": "2025-06-02T07:52:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    52,
                    45,
                    0,
                    153,
                    0
                ],
                "published": "2024-09-17T17:13:20Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    13,
                    20,
                    1,
                    261,
                    0
                ],
                "title": "On the number of elements beyond the ones actually observed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the number of elements beyond the ones actually observed"
                },
                "summary": "In this work, a variant of the birth and death chain with constant\nintensities, originally introduced by\n  Bruno de Finetti way back in 1957, is revisited. This fact is also underlined\nby the choice of the title,\n  which is clearly a literal translation of the original one. Characteristic of\nthe variant is that it\n  allows negative jumps of any magnitude. And this, as explained in the paper,\nmight be useful in offering\n  some insight into the issue, arising in numerous situations, of inferring the\nnumber of the undetected\n  elements of a given population. One thinks, for example, of problems\nconcerning abundance or richness of\n  species.\n  The author's purpose is twofold: to align the original de Finetti's\nconstruction with the modern,\n  well-established theory of the continuous-time Markov chains with discrete\nstate space and show how it\n  could be used to make probabilistic previsions on the number of the unseen\nelements of a population.\n  With the aim of enhancing the possible practical applications of the model,\none discusses the statistical\n  point estimation of the rates which characterize its infinitesimal\ndescription.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a variant of the birth and death chain with constant\nintensities, originally introduced by\n  Bruno de Finetti way back in 1957, is revisited. This fact is also underlined\nby the choice of the title,\n  which is clearly a literal translation of the original one. Characteristic of\nthe variant is that it\n  allows negative jumps of any magnitude. And this, as explained in the paper,\nmight be useful in offering\n  some insight into the issue, arising in numerous situations, of inferring the\nnumber of the undetected\n  elements of a given population. One thinks, for example, of problems\nconcerning abundance or richness of\n  species.\n  The author's purpose is twofold: to align the original de Finetti's\nconstruction with the modern,\n  well-established theory of the continuous-time Markov chains with discrete\nstate space and show how it\n  could be used to make probabilistic previsions on the number of the unseen\nelements of a population.\n  With the aim of enhancing the possible practical applications of the model,\none discusses the statistical\n  point estimation of the rates which characterize its infinitesimal\ndescription."
                },
                "authors": [
                    {
                        "name": "Eugenio Regazzini"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Regazzini"
                },
                "author": "Eugenio Regazzini",
                "arxiv_comment": "33 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60J27 (Primary) 62F10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11671v2",
                "updated": "2025-06-02T07:51:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    51,
                    20,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-17T11:00:40Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    0,
                    40,
                    0,
                    48,
                    0
                ],
                "title": "Diversity-oriented Data Augmentation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity-oriented Data Augmentation with Large Language Models"
                },
                "summary": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points."
                },
                "authors": [
                    {
                        "name": "Zaitian Wang"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Xinhao Zhang"
                    },
                    {
                        "name": "Kunpeng Liu"
                    },
                    {
                        "name": "Pengfei Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03835v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03835v4",
                "updated": "2025-06-03T09:02:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    2,
                    22,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-07T14:45:30Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    45,
                    30,
                    1,
                    7,
                    0
                ],
                "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification"
                },
                "summary": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR."
                },
                "authors": [
                    {
                        "name": "Yindu Su"
                    },
                    {
                        "name": "Huike Zou"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Haiyang Yang"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Qingheng Zhang"
                    },
                    {
                        "name": "Shuguang Han"
                    },
                    {
                        "name": "Jufeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Chen"
                },
                "author": "Jufeng Chen",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03835v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03835v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10201v2",
                "updated": "2025-06-02T07:26:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    26,
                    26,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-14T14:52:41Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    52,
                    41,
                    4,
                    45,
                    0
                ],
                "title": "Prediction hubs are context-informed frequent tokens in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction hubs are context-informed frequent tokens in LLMs"
                },
                "summary": "Hubness, the tendency for a few points to be among the nearest neighbours of\na disproportionate number of other points, commonly arises when applying\nstandard distance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first prove that the only large-scale representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appearance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction.\nHowever, when other distances are used to compare LLM representations, we do\nnot have the same theoretical guarantees, and, indeed, we see nuisance hubs\nappear. There are two main takeaways. First, hubness, while omnipresent in\nhigh-dimensional spaces, is not a negative property that needs to be mitigated\nwhen LLMs are being used for next token prediction. Second, when comparing\nrepresentations from LLMs using Euclidean or cosine distance, there is a high\nrisk of nuisance hubs and practitioners should use mitigation techniques if\nrelevant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubness, the tendency for a few points to be among the nearest neighbours of\na disproportionate number of other points, commonly arises when applying\nstandard distance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first prove that the only large-scale representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appearance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction.\nHowever, when other distances are used to compare LLM representations, we do\nnot have the same theoretical guarantees, and, indeed, we see nuisance hubs\nappear. There are two main takeaways. First, hubness, while omnipresent in\nhigh-dimensional spaces, is not a negative property that needs to be mitigated\nwhen LLMs are being used for next token prediction. Second, when comparing\nrepresentations from LLMs using Euclidean or cosine distance, there is a high\nrisk of nuisance hubs and practitioners should use mitigation techniques if\nrelevant."
                },
                "authors": [
                    {
                        "name": "Beatrix M. G. Nielsen"
                    },
                    {
                        "name": "Iuri Macocco"
                    },
                    {
                        "name": "Marco Baroni"
                    }
                ],
                "author_detail": {
                    "name": "Marco Baroni"
                },
                "author": "Marco Baroni",
                "arxiv_comment": "Published as a conference paper at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24223v2",
                "updated": "2025-06-02T07:21:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    21,
                    17,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T05:23:01Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    23,
                    1,
                    4,
                    150,
                    0
                ],
                "title": "Automated Structured Radiology Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Structured Radiology Report Generation"
                },
                "summary": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments."
                },
                "authors": [
                    {
                        "name": "Jean-Benoit Delbrouck"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Johannes Moll"
                    },
                    {
                        "name": "Alois Thomas"
                    },
                    {
                        "name": "Zhihong Chen"
                    },
                    {
                        "name": "Sophie Ostmeier"
                    },
                    {
                        "name": "Asfandyar Azhar"
                    },
                    {
                        "name": "Kelvin Zhenghao Li"
                    },
                    {
                        "name": "Andrew Johnston"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Eduardo Reis"
                    },
                    {
                        "name": "Mohamed Muneer"
                    },
                    {
                        "name": "Maya Varma"
                    },
                    {
                        "name": "Curtis Langlotz"
                    }
                ],
                "author_detail": {
                    "name": "Curtis Langlotz"
                },
                "author": "Curtis Langlotz",
                "arxiv_comment": "Accepted to ACL Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18744v3",
                "updated": "2025-06-02T07:16:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    16,
                    11,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-26T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    1,
                    36,
                    40,
                    2,
                    57,
                    0
                ],
                "title": "ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation\n  Preference Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation\n  Preference Dataset Construction"
                },
                "summary": "Recent efforts in LLM alignment have focused on constructing large-scale\npreference datasets via human or Artificial Intelligence (AI) annotators.\nHowever, such approaches rely on instance-wise supervision, incurring\nsubstantial annotation cost and limited interpretability. In this paper, we\npropose ZEBRA - a model behavior-wise zero-annotation framework that constructs\npreference data by leveraging model behavior knowledge derived from benchmark\nperformances. ZEBRA binarizes response pairs by evaluating the quality and\nsimilarity of their origin models, entirely bypassing instance-level\nannotation. This allows scalable, controllable, and cost-effective alignment\ndata generation. Empirical results show that ZEBRA achieves alignment\nperformance comparable to instance-supervised methods, despite requiring no\nmanual or model-based labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts in LLM alignment have focused on constructing large-scale\npreference datasets via human or Artificial Intelligence (AI) annotators.\nHowever, such approaches rely on instance-wise supervision, incurring\nsubstantial annotation cost and limited interpretability. In this paper, we\npropose ZEBRA - a model behavior-wise zero-annotation framework that constructs\npreference data by leveraging model behavior knowledge derived from benchmark\nperformances. ZEBRA binarizes response pairs by evaluating the quality and\nsimilarity of their origin models, entirely bypassing instance-level\nannotation. This allows scalable, controllable, and cost-effective alignment\ndata generation. Empirical results show that ZEBRA achieves alignment\nperformance comparable to instance-supervised methods, despite requiring no\nmanual or model-based labeling."
                },
                "authors": [
                    {
                        "name": "Jeesu Jung"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Sangkeun Jung"
                    }
                ],
                "author_detail": {
                    "name": "Sangkeun Jung"
                },
                "author": "Sangkeun Jung",
                "arxiv_comment": "16 pages,7 figures,5 tables,4 graphs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18311v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18311v3",
                "updated": "2025-06-02T07:05:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    5,
                    23,
                    0,
                    153,
                    0
                ],
                "published": "2024-05-28T16:02:11Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    16,
                    2,
                    11,
                    1,
                    149,
                    0
                ],
                "title": "Deterministic and statistical calibration of constitutive models from\n  full-field data with parametric physics-informed neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic and statistical calibration of constitutive models from\n  full-field data with parametric physics-informed neural networks"
                },
                "summary": "The calibration of constitutive models from full-field data has recently\ngained increasing interest due to improvements in full-field measurement\ncapabilities. In addition to the experimental characterization of novel\nmaterials, continuous structural health monitoring is another application that\nis of great interest. However, monitoring is usually associated with severe\ntime constraints, difficult to meet with standard numerical approaches.\nTherefore, parametric physics-informed neural networks (PINNs) for constitutive\nmodel calibration from full-field displacement data are investigated. In an\noffline stage, a parametric PINN can be trained to learn a parameterized\nsolution of the underlying partial differential equation. In the subsequent\nonline stage, the parametric PINN then acts as a surrogate for the\nparameters-to-state map in calibration. We test the proposed approach for the\ndeterministic least-squares calibration of a linear elastic as well as a\nhyperelastic constitutive model from noisy synthetic displacement data. We\nfurther carry out Markov chain Monte Carlo-based Bayesian inference to quantify\nthe uncertainty. A proper statistical evaluation of the results underlines the\nhigh accuracy of the deterministic calibration and that the estimated\nuncertainty is valid. Finally, we consider experimental data and show that the\nresults are in good agreement with a finite element method-based calibration.\nDue to the fast evaluation of PINNs, calibration can be performed in near\nreal-time. This advantage is particularly evident in many-query applications\nsuch as Markov chain Monte Carlo-based Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The calibration of constitutive models from full-field data has recently\ngained increasing interest due to improvements in full-field measurement\ncapabilities. In addition to the experimental characterization of novel\nmaterials, continuous structural health monitoring is another application that\nis of great interest. However, monitoring is usually associated with severe\ntime constraints, difficult to meet with standard numerical approaches.\nTherefore, parametric physics-informed neural networks (PINNs) for constitutive\nmodel calibration from full-field displacement data are investigated. In an\noffline stage, a parametric PINN can be trained to learn a parameterized\nsolution of the underlying partial differential equation. In the subsequent\nonline stage, the parametric PINN then acts as a surrogate for the\nparameters-to-state map in calibration. We test the proposed approach for the\ndeterministic least-squares calibration of a linear elastic as well as a\nhyperelastic constitutive model from noisy synthetic displacement data. We\nfurther carry out Markov chain Monte Carlo-based Bayesian inference to quantify\nthe uncertainty. A proper statistical evaluation of the results underlines the\nhigh accuracy of the deterministic calibration and that the estimated\nuncertainty is valid. Finally, we consider experimental data and show that the\nresults are in good agreement with a finite element method-based calibration.\nDue to the fast evaluation of PINNs, calibration can be performed in near\nreal-time. This advantage is particularly evident in many-query applications\nsuch as Markov chain Monte Carlo-based Bayesian inference."
                },
                "authors": [
                    {
                        "name": "David Anton"
                    },
                    {
                        "name": "Jendrik-Alexander Tröger"
                    },
                    {
                        "name": "Henning Wessels"
                    },
                    {
                        "name": "Ulrich Römer"
                    },
                    {
                        "name": "Alexander Henkes"
                    },
                    {
                        "name": "Stefan Hartmann"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Hartmann"
                },
                "author": "Stefan Hartmann",
                "arxiv_doi": "10.1186/s40323-025-00285-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s40323-025-00285-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.18311v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18311v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03819v2",
                "updated": "2025-06-02T06:56:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    56,
                    42,
                    0,
                    153,
                    0
                ],
                "published": "2024-08-07T14:55:04Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    14,
                    55,
                    4,
                    2,
                    220,
                    0
                ],
                "title": "Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning"
                },
                "summary": "Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL."
                },
                "authors": [
                    {
                        "name": "Simret Araya Gebreegziabher"
                    },
                    {
                        "name": "Kuangshi Ai"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Elena L. Glassman"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-Jun Li"
                },
                "author": "Toby Jia-Jun Li",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02508v2",
                "updated": "2025-06-02T06:42:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    42,
                    17,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-04T17:26:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    26,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced."
                },
                "authors": [
                    {
                        "name": "Maohao Shen"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Zhenfang Chen"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Gregory Wornell"
                    },
                    {
                        "name": "Subhro Das"
                    },
                    {
                        "name": "David Cox"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21847v2",
                "updated": "2025-06-02T06:39:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    39,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-28T00:27:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    27,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural\n  Reparameterization on Feedforward Network Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePaViT: Scalable Vision Transformer Acceleration via Structural\n  Reparameterization on Feedforward Network Layers"
                },
                "summary": "We reveal that feedforward network (FFN) layers, rather than attention\nlayers, are the primary contributors to Vision Transformer (ViT) inference\nlatency, with their impact signifying as model size increases. This finding\nhighlights a critical opportunity for optimizing the efficiency of large-scale\nViTs by focusing on FFN layers. In this work, we propose a novel channel idle\nmechanism that facilitates post-training structural reparameterization for\nefficient FFN layers during testing. Specifically, a set of feature channels\nremains idle and bypasses the nonlinear activation function in each FFN layer,\nthereby forming a linear pathway that enables structural reparameterization\nduring inference. This mechanism results in a family of ReParameterizable\nVision Transformers (RePaViTs), which achieve remarkable latency reductions\nwith acceptable sacrifices (sometimes gains) in accuracy across various ViTs.\nThe benefits of our method scale consistently with model sizes, demonstrating\ngreater speed improvements and progressively narrowing accuracy gaps or even\nhigher accuracies on larger models. In particular, RePa-ViT-Large and\nRePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1\naccuracies under the same training strategy, respectively. RePaViT is the first\nto employ structural reparameterization on FFN layers to expedite ViTs to our\nbest knowledge, and we believe that it represents an auspicious direction for\nefficient ViTs. Source code is available at\nhttps://github.com/Ackesnal/RePaViT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reveal that feedforward network (FFN) layers, rather than attention\nlayers, are the primary contributors to Vision Transformer (ViT) inference\nlatency, with their impact signifying as model size increases. This finding\nhighlights a critical opportunity for optimizing the efficiency of large-scale\nViTs by focusing on FFN layers. In this work, we propose a novel channel idle\nmechanism that facilitates post-training structural reparameterization for\nefficient FFN layers during testing. Specifically, a set of feature channels\nremains idle and bypasses the nonlinear activation function in each FFN layer,\nthereby forming a linear pathway that enables structural reparameterization\nduring inference. This mechanism results in a family of ReParameterizable\nVision Transformers (RePaViTs), which achieve remarkable latency reductions\nwith acceptable sacrifices (sometimes gains) in accuracy across various ViTs.\nThe benefits of our method scale consistently with model sizes, demonstrating\ngreater speed improvements and progressively narrowing accuracy gaps or even\nhigher accuracies on larger models. In particular, RePa-ViT-Large and\nRePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1\naccuracies under the same training strategy, respectively. RePaViT is the first\nto employ structural reparameterization on FFN layers to expedite ViTs to our\nbest knowledge, and we believe that it represents an auspicious direction for\nefficient ViTs. Source code is available at\nhttps://github.com/Ackesnal/RePaViT."
                },
                "authors": [
                    {
                        "name": "Xuwei Xu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Jiajun Liu"
                    },
                    {
                        "name": "Sen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sen Wang"
                },
                "author": "Sen Wang",
                "arxiv_comment": "Accepted to ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22954v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22954v3",
                "updated": "2025-06-02T06:34:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    34,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-30T12:09:29Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    9,
                    29,
                    2,
                    304,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Estimation of Source Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Estimation of Source Reliability"
                },
                "summary": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines."
                },
                "authors": [
                    {
                        "name": "Jeongyeon Hwang"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Hyejin Park"
                    },
                    {
                        "name": "Dongwoo Kim"
                    },
                    {
                        "name": "Sangdon Park"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22954v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22954v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13169v2",
                "updated": "2025-06-02T06:24:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    24,
                    38,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-17T18:46:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    46,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study"
                },
                "summary": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness."
                },
                "authors": [
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Berk Yoztyurk"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Markus Herklotz"
                    },
                    {
                        "name": "Frauke Kreuter"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Assenmacher"
                },
                "author": "Matthias Assenmacher",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07155v2",
                "updated": "2025-06-02T05:57:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    57,
                    53,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-12T00:15:02Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    0,
                    15,
                    2,
                    0,
                    132,
                    0
                ],
                "title": "Reassessing Large Language Model Boolean Query Generation for Systematic\n  Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reassessing Large Language Model Boolean Query Generation for Systematic\n  Reviews"
                },
                "summary": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Harrisen Scells"
                    },
                    {
                        "name": "Bevan Koopman"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "arxiv_comment": "Accepted in SIGIR-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14318v2",
                "updated": "2025-06-02T05:56:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    56,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-20T13:05:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    5,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "RADAR: Enhancing Radiology Report Generation with Supplementary\n  Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADAR: Enhancing Radiology Report Generation with Supplementary\n  Knowledge Injection"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration. To address this\nlimitation, we propose Radar, a framework for enhancing radiology report\ngeneration with supplementary knowledge injection. Radar improves report\ngeneration by systematically leveraging both the internal knowledge of an LLM\nand externally retrieved information. Specifically, it first extracts the\nmodel's acquired knowledge that aligns with expert image-based classification\noutputs. It then retrieves relevant supplementary knowledge to further enrich\nthis information. Finally, by aggregating both sources, Radar generates more\naccurate and informative radiology reports. Extensive experiments on MIMIC-CXR,\nCheXpert-Plus, and IU X-ray demonstrate that our model outperforms\nstate-of-the-art LLMs in both language quality and clinical accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration. To address this\nlimitation, we propose Radar, a framework for enhancing radiology report\ngeneration with supplementary knowledge injection. Radar improves report\ngeneration by systematically leveraging both the internal knowledge of an LLM\nand externally retrieved information. Specifically, it first extracts the\nmodel's acquired knowledge that aligns with expert image-based classification\noutputs. It then retrieves relevant supplementary knowledge to further enrich\nthis information. Finally, by aggregating both sources, Radar generates more\naccurate and informative radiology reports. Extensive experiments on MIMIC-CXR,\nCheXpert-Plus, and IU X-ray demonstrate that our model outperforms\nstate-of-the-art LLMs in both language quality and clinical accuracy."
                },
                "authors": [
                    {
                        "name": "Wenjun Hou"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Kaishuai Xu"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Jiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Liu"
                },
                "author": "Jiang Liu",
                "arxiv_comment": "Accepted to ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17451v2",
                "updated": "2025-06-02T05:46:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    46,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-26T14:08:34Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    8,
                    34,
                    1,
                    331,
                    0
                ],
                "title": "VL-RewardBench: A Challenging Benchmark for Vision-Language Generative\n  Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-RewardBench: A Challenging Benchmark for Vision-Language Generative\n  Reward Models"
                },
                "summary": "Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline that combines\nsample selection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe VL-GenRMs limitations. Comprehensive evaluation\nacross 16 leading large vision-language models demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r $>$ 0.9) with MMMU-Pro accuracy using\nBest-of-N sampling with VL-GenRMs. Analysis experiments uncover three critical\ninsights for improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline that combines\nsample selection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe VL-GenRMs limitations. Comprehensive evaluation\nacross 16 leading large vision-language models demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r $>$ 0.9) with MMMU-Pro accuracy using\nBest-of-N sampling with VL-GenRMs. Analysis experiments uncover three critical\ninsights for improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs."
                },
                "authors": [
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yuancheng Wei"
                    },
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Xuqing Yang"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Sujian Li"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "arxiv_comment": "CVPR 2025 Camera Ready Version. Project page:\n  https://vl-rewardbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17605v2",
                "updated": "2025-06-02T05:22:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    22,
                    27,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-26T17:17:41Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    17,
                    41,
                    1,
                    331,
                    0
                ],
                "title": "Distractor-free Generalizable 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distractor-free Generalizable 3D Gaussian Splatting"
                },
                "summary": "We present DGGS, a novel framework that addresses the previously unexplored\nchallenge: $\\textbf{Distractor-free Generalizable 3D Gaussian Splatting}$\n(3DGS). It mitigates 3D inconsistency and training instability caused by\ndistractor data in the cross-scenes generalizable train setting while enabling\nfeedforward inference for 3DGS and distractor masks from references in the\nunseen scenes. To achieve these objectives, DGGS proposes a scene-agnostic\nreference-based mask prediction and refinement module during the training\nphase, effectively eliminating the impact of distractor on training stability.\nMoreover, we combat distractor-induced artifacts and holes at inference time\nthrough a novel two-stage inference framework for references scoring and\nre-selection, complemented by a distractor pruning mechanism that further\nremoves residual distractor 3DGS-primitive influences. Extensive feedforward\nexperiments on the real and our synthetic data show DGGS's reconstruction\ncapability when dealing with novel distractor scenes. Moreover, our\ngeneralizable mask prediction even achieves an accuracy superior to existing\nscene-specific training methods. Homepage is https://github.com/bbbbby-99/DGGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DGGS, a novel framework that addresses the previously unexplored\nchallenge: $\\textbf{Distractor-free Generalizable 3D Gaussian Splatting}$\n(3DGS). It mitigates 3D inconsistency and training instability caused by\ndistractor data in the cross-scenes generalizable train setting while enabling\nfeedforward inference for 3DGS and distractor masks from references in the\nunseen scenes. To achieve these objectives, DGGS proposes a scene-agnostic\nreference-based mask prediction and refinement module during the training\nphase, effectively eliminating the impact of distractor on training stability.\nMoreover, we combat distractor-induced artifacts and holes at inference time\nthrough a novel two-stage inference framework for references scoring and\nre-selection, complemented by a distractor pruning mechanism that further\nremoves residual distractor 3DGS-primitive influences. Extensive feedforward\nexperiments on the real and our synthetic data show DGGS's reconstruction\ncapability when dealing with novel distractor scenes. Moreover, our\ngeneralizable mask prediction even achieves an accuracy superior to existing\nscene-specific training methods. Homepage is https://github.com/bbbbby-99/DGGS."
                },
                "authors": [
                    {
                        "name": "Yanqi Bao"
                    },
                    {
                        "name": "Jing Liao"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02006v2",
                "updated": "2025-06-02T05:17:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    17,
                    34,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-04T11:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    50,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey"
                },
                "summary": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents"
                },
                "authors": [
                    {
                        "name": "Biao Wu"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19630v3",
                "updated": "2025-06-02T05:05:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    5,
                    30,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-27T13:19:35Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    19,
                    35,
                    4,
                    362,
                    0
                ],
                "title": "ATiM: Autotuning Tensor Programs for Processing-in-DRAM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATiM: Autotuning Tensor Programs for Processing-in-DRAM"
                },
                "summary": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present ATiM, a search-based optimizing tensor compiler\nfor UPMEM. Key features of ATiM include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 6.18$\\times$\nfor various UPMEM benchmark kernels and 8.21$\\times$ for GPT-J layers. To the\nbest of our knowledge, ATiM is the first tensor compiler to provide fully\nautomated, autotuning-integrated code generation support for a DRAM-PIM system.\nBy bridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, ATiM establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present ATiM, a search-based optimizing tensor compiler\nfor UPMEM. Key features of ATiM include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 6.18$\\times$\nfor various UPMEM benchmark kernels and 8.21$\\times$ for GPT-J layers. To the\nbest of our knowledge, ATiM is the first tensor compiler to provide fully\nautomated, autotuning-integrated code generation support for a DRAM-PIM system.\nBy bridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, ATiM establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization."
                },
                "authors": [
                    {
                        "name": "Yongwon Shin"
                    },
                    {
                        "name": "Dookyung Kang"
                    },
                    {
                        "name": "Hyojin Sung"
                    }
                ],
                "author_detail": {
                    "name": "Hyojin Sung"
                },
                "author": "Hyojin Sung",
                "arxiv_comment": "17 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08662v3",
                "updated": "2025-06-02T04:54:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    54,
                    0,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-10T09:34:15Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    34,
                    15,
                    0,
                    41,
                    0
                ],
                "title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs"
                },
                "summary": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner\n(https://github.com/soyoung97/RoToR)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner\n(https://github.com/soyoung97/RoToR)"
                },
                "authors": [
                    {
                        "name": "Soyoung Yoon"
                    },
                    {
                        "name": "Dongha Ahn"
                    },
                    {
                        "name": "Youngwon Lee"
                    },
                    {
                        "name": "Minkyu Jung"
                    },
                    {
                        "name": "HyungJoo Jang"
                    },
                    {
                        "name": "Seung-won Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Seung-won Hwang"
                },
                "author": "Seung-won Hwang",
                "arxiv_comment": "Accepted at ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23802v2",
                "updated": "2025-06-02T04:19:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    19,
                    10,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-26T22:55:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    22,
                    55,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks"
                },
                "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this."
                },
                "authors": [
                    {
                        "name": "Suhana Bedi"
                    },
                    {
                        "name": "Hejie Cui"
                    },
                    {
                        "name": "Miguel Fuentes"
                    },
                    {
                        "name": "Alyssa Unell"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Juan M. Banda"
                    },
                    {
                        "name": "Nikesh Kotecha"
                    },
                    {
                        "name": "Timothy Keyes"
                    },
                    {
                        "name": "Yifan Mai"
                    },
                    {
                        "name": "Mert Oez"
                    },
                    {
                        "name": "Hao Qiu"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Leonardo Schettini"
                    },
                    {
                        "name": "Mehr Kashyap"
                    },
                    {
                        "name": "Jason Alan Fries"
                    },
                    {
                        "name": "Akshay Swaminathan"
                    },
                    {
                        "name": "Philip Chung"
                    },
                    {
                        "name": "Fateme Nateghi"
                    },
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Ashwin Nayak"
                    },
                    {
                        "name": "Shivam Vedak"
                    },
                    {
                        "name": "Sneha S. Jain"
                    },
                    {
                        "name": "Birju Patel"
                    },
                    {
                        "name": "Oluseyi Fayanju"
                    },
                    {
                        "name": "Shreya Shah"
                    },
                    {
                        "name": "Ethan Goh"
                    },
                    {
                        "name": "Dong-han Yao"
                    },
                    {
                        "name": "Brian Soetikno"
                    },
                    {
                        "name": "Eduardo Reis"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Vasu Divi"
                    },
                    {
                        "name": "Robson Capasso"
                    },
                    {
                        "name": "Rachna Saralkar"
                    },
                    {
                        "name": "Chia-Chun Chiang"
                    },
                    {
                        "name": "Jenelle Jindal"
                    },
                    {
                        "name": "Tho Pham"
                    },
                    {
                        "name": "Faraz Ghoddusi"
                    },
                    {
                        "name": "Steven Lin"
                    },
                    {
                        "name": "Albert S. Chiou"
                    },
                    {
                        "name": "Christy Hong"
                    },
                    {
                        "name": "Mohana Roy"
                    },
                    {
                        "name": "Michael F. Gensheimer"
                    },
                    {
                        "name": "Hinesh Patel"
                    },
                    {
                        "name": "Kevin Schulman"
                    },
                    {
                        "name": "Dev Dash"
                    },
                    {
                        "name": "Danton Char"
                    },
                    {
                        "name": "Lance Downing"
                    },
                    {
                        "name": "Francois Grolleau"
                    },
                    {
                        "name": "Kameron Black"
                    },
                    {
                        "name": "Bethel Mieso"
                    },
                    {
                        "name": "Aydin Zahedivash"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Harshita Sharma"
                    },
                    {
                        "name": "Tony Lee"
                    },
                    {
                        "name": "Hannah Kirsch"
                    },
                    {
                        "name": "Jennifer Lee"
                    },
                    {
                        "name": "Nerissa Ambers"
                    },
                    {
                        "name": "Carlene Lugtu"
                    },
                    {
                        "name": "Aditya Sharma"
                    },
                    {
                        "name": "Bilal Mawji"
                    },
                    {
                        "name": "Alex Alekseyev"
                    },
                    {
                        "name": "Vicky Zhou"
                    },
                    {
                        "name": "Vikas Kakkar"
                    },
                    {
                        "name": "Jarrod Helzer"
                    },
                    {
                        "name": "Anurang Revri"
                    },
                    {
                        "name": "Yair Bannett"
                    },
                    {
                        "name": "Roxana Daneshjou"
                    },
                    {
                        "name": "Jonathan Chen"
                    },
                    {
                        "name": "Emily Alsentzer"
                    },
                    {
                        "name": "Keith Morse"
                    },
                    {
                        "name": "Nirmal Ravi"
                    },
                    {
                        "name": "Nima Aghaeepour"
                    },
                    {
                        "name": "Vanessa Kennedy"
                    },
                    {
                        "name": "Akshay Chaudhari"
                    },
                    {
                        "name": "Thomas Wang"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Matthew P. Lungren"
                    },
                    {
                        "name": "Eric Horvitz"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Mike Pfeffer"
                    },
                    {
                        "name": "Nigam H. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Nigam H. Shah"
                },
                "author": "Nigam H. Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24238v2",
                "updated": "2025-06-02T04:16:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    16,
                    4,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T05:54:36Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    54,
                    36,
                    4,
                    150,
                    0
                ],
                "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM"
                },
                "summary": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models."
                },
                "authors": [
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Minheng Ni"
                    },
                    {
                        "name": "Zitong Huang"
                    },
                    {
                        "name": "Guanglei Yang"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v3",
                "updated": "2025-06-02T04:02:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    2,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00212v3",
                "updated": "2025-06-02T03:25:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    3,
                    25,
                    55,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-30T23:09:44Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    23,
                    9,
                    44,
                    2,
                    120,
                    0
                ],
                "title": "Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems"
                },
                "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution"
                },
                "authors": [
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Jieyu Zhang"
                    },
                    {
                        "name": "Jiale Liu"
                    },
                    {
                        "name": "Zhiguang Han"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Qingyun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wu"
                },
                "author": "Qingyun Wu",
                "arxiv_comment": "camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.23932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23932v2",
                "updated": "2025-06-02T17:42:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    42,
                    36,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T18:28:02Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    28,
                    2,
                    3,
                    149,
                    0
                ],
                "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue\n  Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue\n  Solving"
                },
                "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io"
                },
                "authors": [
                    {
                        "name": "Wendong Xu"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Qiujiang Chen"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Z. Morley Mao"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23487v2",
                "updated": "2025-06-02T17:37:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    37,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-30T15:41:55Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    15,
                    41,
                    55,
                    6,
                    89,
                    0
                ],
                "title": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners"
                },
                "summary": "Large Language Models (LLMs) have been found to struggle with systematic\nreasoning. Even on tasks where they appear to perform well, their performance\noften depends on shortcuts, rather than on genuine reasoning abilities, leading\nthem to collapse on out-of-distribution (OOD) examples. Post-training\nstrategies based on reinforcement learning and chain-of-thought prompting have\nrecently been hailed as a step change. However, little is known about the\npotential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and\nprogramming-based problem solving, where genuine OOD problems can be sparse. In\nthis paper, we focus on tasks that require systematic relational composition\nfor qualitative spatial and temporal reasoning. The setting allows fine control\nover problem difficulty to precisely measure OOD generalization. We find that,\nzero-shot LRMs generally outperform their LLM counterparts in single-path\nreasoning tasks but struggle in the multi-path setting. Whilst showing\ncomparatively better results, fine-tuned LLMs are also not capable of\nmulti-path generalization. We also provide evidence for the behavioral\ninterpretation for this, i.e., that LRMs are shallow disjunctive reasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been found to struggle with systematic\nreasoning. Even on tasks where they appear to perform well, their performance\noften depends on shortcuts, rather than on genuine reasoning abilities, leading\nthem to collapse on out-of-distribution (OOD) examples. Post-training\nstrategies based on reinforcement learning and chain-of-thought prompting have\nrecently been hailed as a step change. However, little is known about the\npotential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and\nprogramming-based problem solving, where genuine OOD problems can be sparse. In\nthis paper, we focus on tasks that require systematic relational composition\nfor qualitative spatial and temporal reasoning. The setting allows fine control\nover problem difficulty to precisely measure OOD generalization. We find that,\nzero-shot LRMs generally outperform their LLM counterparts in single-path\nreasoning tasks but struggle in the multi-path setting. Whilst showing\ncomparatively better results, fine-tuned LLMs are also not capable of\nmulti-path generalization. We also provide evidence for the behavioral\ninterpretation for this, i.e., that LRMs are shallow disjunctive reasoners."
                },
                "authors": [
                    {
                        "name": "Irtaza Khalid"
                    },
                    {
                        "name": "Amir Masoud Nourollah"
                    },
                    {
                        "name": "Steven Schockaert"
                    }
                ],
                "author_detail": {
                    "name": "Steven Schockaert"
                },
                "author": "Steven Schockaert",
                "arxiv_comment": "ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24803v2",
                "updated": "2025-06-02T17:37:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    37,
                    17,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T17:08:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    8,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "Guiding Generative Storytelling with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Generative Storytelling with Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."
                },
                "authors": [
                    {
                        "name": "Zhijun Pan"
                    },
                    {
                        "name": "Antonios Andronis"
                    },
                    {
                        "name": "Eva Hayek"
                    },
                    {
                        "name": "Oscar AP Wilkinson"
                    },
                    {
                        "name": "Ilya Lasy"
                    },
                    {
                        "name": "Annette Parry"
                    },
                    {
                        "name": "Guy Gadney"
                    },
                    {
                        "name": "Tim J. Smith"
                    },
                    {
                        "name": "Mick Grierson"
                    }
                ],
                "author_detail": {
                    "name": "Mick Grierson"
                },
                "author": "Mick Grierson",
                "arxiv_comment": "This manuscript was submitted for peer review in January 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08826v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08826v3",
                "updated": "2025-06-02T17:15:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    15,
                    8,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-12T22:33:41Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    22,
                    33,
                    41,
                    2,
                    43,
                    0
                ],
                "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Abootorabi"
                    },
                    {
                        "name": "Amirhosein Zobeiri"
                    },
                    {
                        "name": "Mahdi Dehghani"
                    },
                    {
                        "name": "Mohammadali Mohammadkhani"
                    },
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Omid Ghahroodi"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    },
                    {
                        "name": "Ehsaneddin Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Ehsaneddin Asgari"
                },
                "author": "Ehsaneddin Asgari",
                "arxiv_comment": "GitHub repository:\n  https://github.com/llm-lab-org/Multimodal-RAG-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08826v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08826v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17536v2",
                "updated": "2025-06-02T17:10:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    10,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-23T06:41:54Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    6,
                    41,
                    54,
                    4,
                    143,
                    0
                ],
                "title": "Multimodal Conversation Structure Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Conversation Structure Understanding"
                },
                "summary": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure."
                },
                "authors": [
                    {
                        "name": "Kent K. Chang"
                    },
                    {
                        "name": "Mackenzie Hanh Cramer"
                    },
                    {
                        "name": "Anna Ho"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Yilin Yuan"
                    },
                    {
                        "name": "David Bamman"
                    }
                ],
                "author_detail": {
                    "name": "David Bamman"
                },
                "author": "David Bamman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02669v2",
                "updated": "2025-06-02T16:48:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    48,
                    29,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-05T21:36:38Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    21,
                    36,
                    38,
                    6,
                    5,
                    0
                ],
                "title": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate\n  Modality Imbalance in VLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate\n  Modality Imbalance in VLMs?"
                },
                "summary": "Vision Language Models (VLMs) are impressive at visual question answering and\nimage captioning. But they underperform on multi-step visual reasoning -- even\ncompared to LLMs on the same tasks presented in text form -- giving rise to\nperceptions of modality imbalance or brittleness. Towards a systematic study of\nsuch issues, we introduce a synthetic framework for assessing the ability of\nVLMs to perform algorithmic visual reasoning, comprising three tasks: Table\nReadout, Grid Navigation, and Visual Analogy. Each has two levels of\ndifficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for\nfrontier VLMs. We propose strategies for training on the SIMPLE version of\ntasks that improve performance on the corresponding HARD task, i.e.,\nsimple-to-hard (S2H) generalization. This controlled setup, where each task\nalso has an equivalent text-only version, allows a quantification of the\nmodality imbalance and how it is impacted by training strategy. We show that 1)\nexplicit image-to-text conversion is important in promoting S2H generalization\non images, by transferring reasoning from text; 2) conversion can be\ninternalized at test time. We also report results of mechanistic study of this\nphenomenon. We identify measures of gradient alignment that can identify\ntraining strategies that promote better S2H generalization. Ablations highlight\nthe importance of chain-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) are impressive at visual question answering and\nimage captioning. But they underperform on multi-step visual reasoning -- even\ncompared to LLMs on the same tasks presented in text form -- giving rise to\nperceptions of modality imbalance or brittleness. Towards a systematic study of\nsuch issues, we introduce a synthetic framework for assessing the ability of\nVLMs to perform algorithmic visual reasoning, comprising three tasks: Table\nReadout, Grid Navigation, and Visual Analogy. Each has two levels of\ndifficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for\nfrontier VLMs. We propose strategies for training on the SIMPLE version of\ntasks that improve performance on the corresponding HARD task, i.e.,\nsimple-to-hard (S2H) generalization. This controlled setup, where each task\nalso has an equivalent text-only version, allows a quantification of the\nmodality imbalance and how it is impacted by training strategy. We show that 1)\nexplicit image-to-text conversion is important in promoting S2H generalization\non images, by transferring reasoning from text; 2) conversion can be\ninternalized at test time. We also report results of mechanistic study of this\nphenomenon. We identify measures of gradient alignment that can identify\ntraining strategies that promote better S2H generalization. Ablations highlight\nthe importance of chain-of-thought."
                },
                "authors": [
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Abhishek Panigrahi"
                    },
                    {
                        "name": "Yun Cheng"
                    },
                    {
                        "name": "Dingli Yu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23516v2",
                "updated": "2025-06-02T16:37:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    37,
                    1,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T14:56:26Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    56,
                    26,
                    3,
                    149,
                    0
                ],
                "title": "The CASE Framework -- A New Architecture for Participatory Research and\n  Digital Health Surveillance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CASE Framework -- A New Architecture for Participatory Research and\n  Digital Health Surveillance"
                },
                "summary": "We present the CASE framework, an open-source platform for adaptive,\ncontext-aware participatory research, and pandemic preparedness. CASE\nimplements an event-driven architecture that enables dynamic survey workflows,\nallowing real-time adaptation based on participant responses, external data,\ntemporal conditions, and evolving user states. The framework supports a broad\nrange of research needs, from simple one-time questionnaires to complex\nlongitudinal studies with advanced conditional logic. Built on over a decade of\npractical experience, CASE underwent a major architectural rework in 2024,\ntransitioning from a microservice-based design to a streamlined monolithic\narchitecture. This evolution significantly improved maintainability,\nflexibility, and accessibility to deployment, particularly for institutions\nwith limited technical capacity. CASE has been successfully deployed across\ndiverse domains, powering national disease surveillance platforms, supporting\npost-COVID cohort studies, and enabling real-time sentiment analysis during\npolitical events. These applications, involving tens of thousands of\nparticipants, demonstrate the framework's scalability, versatility, and\npractical value. This paper describes the foundations of CASE, details its\narchitectural evolution, and presents lessons learned from real-world\ndeployments. We establish CASE as a mature and reusable research infrastructure\nthat balances sophisticated functionality with practical implementation,\naddressing the critical global need for sustainable and institutionally\ncontrolled data collection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the CASE framework, an open-source platform for adaptive,\ncontext-aware participatory research, and pandemic preparedness. CASE\nimplements an event-driven architecture that enables dynamic survey workflows,\nallowing real-time adaptation based on participant responses, external data,\ntemporal conditions, and evolving user states. The framework supports a broad\nrange of research needs, from simple one-time questionnaires to complex\nlongitudinal studies with advanced conditional logic. Built on over a decade of\npractical experience, CASE underwent a major architectural rework in 2024,\ntransitioning from a microservice-based design to a streamlined monolithic\narchitecture. This evolution significantly improved maintainability,\nflexibility, and accessibility to deployment, particularly for institutions\nwith limited technical capacity. CASE has been successfully deployed across\ndiverse domains, powering national disease surveillance platforms, supporting\npost-COVID cohort studies, and enabling real-time sentiment analysis during\npolitical events. These applications, involving tens of thousands of\nparticipants, demonstrate the framework's scalability, versatility, and\npractical value. This paper describes the foundations of CASE, details its\narchitectural evolution, and presents lessons learned from real-world\ndeployments. We establish CASE as a mature and reusable research infrastructure\nthat balances sophisticated functionality with practical implementation,\naddressing the critical global need for sustainable and institutionally\ncontrolled data collection systems."
                },
                "authors": [
                    {
                        "name": "Marco Hirsch"
                    },
                    {
                        "name": "Peter Hevesi"
                    },
                    {
                        "name": "Paul Lukowicz"
                    }
                ],
                "author_detail": {
                    "name": "Paul Lukowicz"
                },
                "author": "Paul Lukowicz",
                "arxiv_comment": "10 pages, 5 figures. Submitted as a preprint to arXiv (no prior\n  publication)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; D.2.11; H.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19358v3",
                "updated": "2025-06-02T16:30:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    30,
                    23,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-31T18:10:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking"
                },
                "summary": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "The paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15455v2",
                "updated": "2025-06-02T16:26:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    26,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-21T13:30:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    30,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning"
                },
                "summary": "Fine-tuning large language models (LLMs) is computationally expensive, and\nLow-Rank Adaptation (LoRA) provides a cost-effective solution by approximating\nweight updates through low-rank matrices. In real-world scenarios, LLMs are\nfine-tuned on data from multiple domains to perform tasks across various\nfields, embodying multi-task learning (MTL). LoRA often underperforms in such\ncomplex scenarios. To enhance LoRA's capability in multi-task learning, we\npropose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head\nRandomization diversifies the head matrices through Multi-Head Dropout and\nMulti-Head Random Initialization, enabling more efficient learning of\ntask-specific features while maintaining shared knowledge representation. Our\napproach not only improves performance in MTL but also reduces GPU memory usage\nand training time. Experiments show that R-LoRA's gains stem from increased\ndiversity in the head matrices, demonstrating its effectiveness for multi-task\nlearning. The code is available at https://github.com/jinda-liu/R-LoRA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is computationally expensive, and\nLow-Rank Adaptation (LoRA) provides a cost-effective solution by approximating\nweight updates through low-rank matrices. In real-world scenarios, LLMs are\nfine-tuned on data from multiple domains to perform tasks across various\nfields, embodying multi-task learning (MTL). LoRA often underperforms in such\ncomplex scenarios. To enhance LoRA's capability in multi-task learning, we\npropose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head\nRandomization diversifies the head matrices through Multi-Head Dropout and\nMulti-Head Random Initialization, enabling more efficient learning of\ntask-specific features while maintaining shared knowledge representation. Our\napproach not only improves performance in MTL but also reduces GPU memory usage\nand training time. Experiments show that R-LoRA's gains stem from increased\ndiversity in the head matrices, demonstrating its effectiveness for multi-task\nlearning. The code is available at https://github.com/jinda-liu/R-LoRA"
                },
                "authors": [
                    {
                        "name": "Jinda Liu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23786v2",
                "updated": "2025-06-02T16:21:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    21,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-24T16:30:37Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    16,
                    30,
                    37,
                    5,
                    144,
                    0
                ],
                "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: A Practical Attack on GGUF Quantization"
                },
                "summary": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\n`ollama` and `llama.cpp` frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\n`ollama` and `llama.cpp` frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense."
                },
                "authors": [
                    {
                        "name": "Kazuki Egashira"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23799v2",
                "updated": "2025-06-02T15:55:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    55,
                    44,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-26T16:53:47Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    53,
                    47,
                    0,
                    146,
                    0
                ],
                "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics"
                },
                "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used."
                },
                "authors": [
                    {
                        "name": "Xiaoyuan Wu"
                    },
                    {
                        "name": "Weiran Lin"
                    },
                    {
                        "name": "Omer Akgul"
                    },
                    {
                        "name": "Lujo Bauer"
                    }
                ],
                "author_detail": {
                    "name": "Lujo Bauer"
                },
                "author": "Lujo Bauer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17004v2",
                "updated": "2025-06-02T15:53:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    53,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-23T18:00:07Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    0,
                    7,
                    2,
                    113,
                    0
                ],
                "title": "(Im)possibility of Automated Hallucination Detection in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Im)possibility of Automated Hallucination Detection in Large Language\n  Models"
                },
                "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."
                },
                "authors": [
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Omar Montasser"
                    },
                    {
                        "name": "John Sous"
                    },
                    {
                        "name": "Grigoris Velegkas"
                    }
                ],
                "author_detail": {
                    "name": "Grigoris Velegkas"
                },
                "author": "Grigoris Velegkas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07071v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07071v3",
                "updated": "2025-06-02T15:40:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    40,
                    42,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-13T05:53:56Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "title": "Value Compass Benchmarks: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Compass Benchmarks: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values"
                },
                "summary": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Benchmarks, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Benchmarks, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values."
                },
                "authors": [
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Shitong Duan"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07071v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07071v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12821v2",
                "updated": "2025-06-02T15:40:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    40,
                    35,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-18T12:32:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    32,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models"
                },
                "summary": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values."
                },
                "authors": [
                    {
                        "name": "Elena Stringli"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Athanasios Voulodimos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04800v2",
                "updated": "2025-06-02T15:39:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    39,
                    49,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-03T06:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    54,
                    5,
                    0,
                    62,
                    0
                ],
                "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH."
                },
                "authors": [
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Tingyue Pan"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Jiaying Lin"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07453v2",
                "updated": "2025-06-02T15:39:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    39,
                    29,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-12T11:35:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    11,
                    35,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "How well do LLMs reason over tabular data, really?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do LLMs reason over tabular data, really?"
                },
                "summary": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs."
                },
                "authors": [
                    {
                        "name": "Cornelius Wolff"
                    },
                    {
                        "name": "Madelon Hulsebos"
                    }
                ],
                "author_detail": {
                    "name": "Madelon Hulsebos"
                },
                "author": "Madelon Hulsebos",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07784v2",
                "updated": "2025-06-02T15:27:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    27,
                    28,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-12T17:37:17Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    37,
                    17,
                    0,
                    132,
                    0
                ],
                "title": "Domain Regeneration: How well do LLMs match syntactic properties of text\n  domains?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Regeneration: How well do LLMs match syntactic properties of text\n  domains?"
                },
                "summary": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals."
                },
                "authors": [
                    {
                        "name": "Da Ju"
                    },
                    {
                        "name": "Hagen Blix"
                    },
                    {
                        "name": "Adina Williams"
                    }
                ],
                "author_detail": {
                    "name": "Adina Williams"
                },
                "author": "Adina Williams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00639v2",
                "updated": "2025-06-02T15:22:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    22,
                    19,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-01T01:36:41Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    36,
                    41,
                    6,
                    336,
                    0
                ],
                "title": "Needle: A Generative AI-Powered Multi-modal Database for Answering\n  Complex Natural Language Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Needle: A Generative AI-Powered Multi-modal Database for Answering\n  Complex Natural Language Queries"
                },
                "summary": "Multi-modal datasets, like those involving images, often miss the detailed\ndescriptions that properly capture the rich information encoded in each item.\nThis makes answering complex natural language queries a major challenge in this\ndomain. In particular, unlike the traditional nearest neighbor search, where\nthe tuples and the query are represented as points in a single metric space,\nthese settings involve queries and tuples embedded in fundamentally different\nspaces, making the traditional query answering methods inapplicable. Existing\nliterature addresses this challenge for image datasets through vector\nrepresentations jointly trained on natural language and images. This technique,\nhowever, underperforms for complex queries due to various reasons.\n  This paper takes a step towards addressing this challenge by introducing a\nGenerative-based Monte Carlo method that utilizes foundation models to generate\nsynthetic samples that capture the complexity of the natural language query and\nrepresent it in the same metric space as the multi-modal data.\n  Following this method, we propose Needle, a database for image data\nretrieval. Instead of relying on contrastive learning or metadata-searching\napproaches, our system is based on synthetic data generation to capture the\ncomplexities of natural language queries. Our system is open-source and ready\nfor deployment, designed to be easily adopted by researchers and developers.\nThe comprehensive experiments on various benchmark datasets verify that this\nsystem significantly outperforms state-of-the-art text-to-image retrieval\nmethods in the literature. Any foundation model and embedder can be easily\nintegrated into Needle to improve the performance, piggybacking on the\nadvancements in these technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal datasets, like those involving images, often miss the detailed\ndescriptions that properly capture the rich information encoded in each item.\nThis makes answering complex natural language queries a major challenge in this\ndomain. In particular, unlike the traditional nearest neighbor search, where\nthe tuples and the query are represented as points in a single metric space,\nthese settings involve queries and tuples embedded in fundamentally different\nspaces, making the traditional query answering methods inapplicable. Existing\nliterature addresses this challenge for image datasets through vector\nrepresentations jointly trained on natural language and images. This technique,\nhowever, underperforms for complex queries due to various reasons.\n  This paper takes a step towards addressing this challenge by introducing a\nGenerative-based Monte Carlo method that utilizes foundation models to generate\nsynthetic samples that capture the complexity of the natural language query and\nrepresent it in the same metric space as the multi-modal data.\n  Following this method, we propose Needle, a database for image data\nretrieval. Instead of relying on contrastive learning or metadata-searching\napproaches, our system is based on synthetic data generation to capture the\ncomplexities of natural language queries. Our system is open-source and ready\nfor deployment, designed to be easily adopted by researchers and developers.\nThe comprehensive experiments on various benchmark datasets verify that this\nsystem significantly outperforms state-of-the-art text-to-image retrieval\nmethods in the literature. Any foundation model and embedder can be easily\nintegrated into Needle to improve the performance, piggybacking on the\nadvancements in these technologies."
                },
                "authors": [
                    {
                        "name": "Mahdi Erfanian"
                    },
                    {
                        "name": "Mohsen Dehghankar"
                    },
                    {
                        "name": "Abolfazl Asudeh"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Asudeh"
                },
                "author": "Abolfazl Asudeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09837v2",
                "updated": "2025-06-02T14:54:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    54,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-14T23:02:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    2,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "Real-time Adapting Routing (RAR): Improving Efficiency Through\n  Continuous Learning in Software Powered by Layered Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Adapting Routing (RAR): Improving Efficiency Through\n  Continuous Learning in Software Powered by Layered Foundation Models"
                },
                "summary": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM."
                },
                "authors": [
                    {
                        "name": "Kirill Vasilevski"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06851v3",
                "updated": "2025-06-02T14:38:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    38,
                    8,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-07T11:56:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    56,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "Survey on Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey on Vision-Language-Action Models"
                },
                "summary": "This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable."
                },
                "authors": [
                    {
                        "name": "Adilzhan Adilkhanov"
                    },
                    {
                        "name": "Amir Yelenov"
                    },
                    {
                        "name": "Assylkhan Seitzhanov"
                    },
                    {
                        "name": "Ayan Mazhitov"
                    },
                    {
                        "name": "Azamat Abdikarimov"
                    },
                    {
                        "name": "Danissa Sandykbayeva"
                    },
                    {
                        "name": "Daryn Kenzhebek"
                    },
                    {
                        "name": "Dinmukhammed Mukashev"
                    },
                    {
                        "name": "Ilyas Umurbekov"
                    },
                    {
                        "name": "Jabrail Chumakov"
                    },
                    {
                        "name": "Kamila Spanova"
                    },
                    {
                        "name": "Karina Burunchina"
                    },
                    {
                        "name": "Madina Yergibay"
                    },
                    {
                        "name": "Margulan Issa"
                    },
                    {
                        "name": "Moldir Zabirova"
                    },
                    {
                        "name": "Nurdaulet Zhuzbay"
                    },
                    {
                        "name": "Nurlan Kabdyshev"
                    },
                    {
                        "name": "Nurlan Zhaniyar"
                    },
                    {
                        "name": "Rasul Yermagambet"
                    },
                    {
                        "name": "Rustam Chibar"
                    },
                    {
                        "name": "Saltanat Seitzhan"
                    },
                    {
                        "name": "Soibkhon Khajikhanov"
                    },
                    {
                        "name": "Tasbolat Taunyazov"
                    },
                    {
                        "name": "Temirlan Galimzhanov"
                    },
                    {
                        "name": "Temirlan Kaiyrbay"
                    },
                    {
                        "name": "Tleukhan Mussin"
                    },
                    {
                        "name": "Togzhan Syrymova"
                    },
                    {
                        "name": "Valeriya Kostyukova"
                    },
                    {
                        "name": "Yerkebulan Massalim"
                    },
                    {
                        "name": "Yermakhan Kassym"
                    },
                    {
                        "name": "Zerde Nurbayeva"
                    },
                    {
                        "name": "Zhanat Kappassov"
                    }
                ],
                "author_detail": {
                    "name": "Zhanat Kappassov"
                },
                "author": "Zhanat Kappassov",
                "arxiv_comment": "arXiv admin note: This submission has been withdrawn due to serious\n  violation of arXiv policies for acceptable submissions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04503v3",
                "updated": "2025-06-02T14:34:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    34,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2024-07-05T13:44:09Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    44,
                    9,
                    4,
                    187,
                    0
                ],
                "title": "When LLMs Play the Telephone Game: Cultural Attractors as Conceptual\n  Tools to Evaluate LLMs in Multi-turn Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Play the Telephone Game: Cultural Attractors as Conceptual\n  Tools to Evaluate LLMs in Multi-turn Settings"
                },
                "summary": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) start interacting with each other and\ngenerating an increasing amount of text online, it becomes crucial to better\nunderstand how information is transformed as it passes from one LLM to the\nnext. While significant research has examined individual LLM behaviors,\nexisting studies have largely overlooked the collective behaviors and\ninformation distortions arising from iterated LLM interactions. Small biases,\nnegligible at the single output level, risk being amplified in iterated\ninteractions, potentially leading the content to evolve towards attractor\nstates. In a series of telephone game experiments, we apply a transmission\nchain design borrowed from the human cultural evolution literature: LLM agents\niteratively receive, produce, and transmit texts from the previous to the next\nagent in the chain. By tracking the evolution of text toxicity, positivity,\ndifficulty, and length across transmission chains, we uncover the existence of\nbiases and attractors, and study their dependence on the initial text, the\ninstructions, language model, and model size. For instance, we find that more\nopen-ended instructions lead to stronger attraction effects compared to more\nconstrained tasks. We also find that different text properties display\ndifferent sensitivity to attraction effects, with toxicity leading to stronger\nattractors than length. These findings highlight the importance of accounting\nfor multi-step transmission dynamics and represent a first step towards a more\ncomprehensive understanding of LLM cultural dynamics."
                },
                "authors": [
                    {
                        "name": "Jérémy Perez"
                    },
                    {
                        "name": "Grgur Kovač"
                    },
                    {
                        "name": "Corentin Léger"
                    },
                    {
                        "name": "Cédric Colas"
                    },
                    {
                        "name": "Gaia Molinaro"
                    },
                    {
                        "name": "Maxime Derex"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Clément Moulin-Frier"
                    }
                ],
                "author_detail": {
                    "name": "Clément Moulin-Frier"
                },
                "author": "Clément Moulin-Frier",
                "arxiv_comment": "Code available at https://github.com/jeremyperez2/TelephoneGameLLM.\n  Companion website with a Data Explorer tool at\n  https://sites.google.com/view/telephone-game-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18478v2",
                "updated": "2025-06-02T14:26:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    26,
                    19,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-27T16:19:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    19,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform\ndownstream tasks through advanced prompting and high-quality demonstrations.\nHowever, traditional ICL paradigms encounter significant limitations in complex\nreasoning tasks, stemming primarily from their dependence on example quality\nand absence of explicit reasoning guidance. To address these challenges, we\nintroduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in\n**ICL** that shifts focus from specific examples to abstract reasoning\npatterns, thereby extending the conventional concept of \"context\" in ICL. Our\napproach begins by defining five atomic reasoning actions, upon which we employ\nMonte Carlo Tree Search to systematically construct high-level reasoning\npatterns. During inference, HiAR-ICL dynamically selects appropriate reasoning\npatterns based on problem attributes, providing explicit guidance for the\nmodel's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and\nefficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our\nmethod achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's\n77.2% and 57.5%. Our approach enhances performance across models of varying\nsizes while generalizing effectively across domains. Further analysis reveals\nthat HiAR-ICL can also serve as a plug-and-play inference method compatible\nwith post-training techniques like GRPO. Code and data are available at\nhttps://github.com/jinyangwu/HiARICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform\ndownstream tasks through advanced prompting and high-quality demonstrations.\nHowever, traditional ICL paradigms encounter significant limitations in complex\nreasoning tasks, stemming primarily from their dependence on example quality\nand absence of explicit reasoning guidance. To address these challenges, we\nintroduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in\n**ICL** that shifts focus from specific examples to abstract reasoning\npatterns, thereby extending the conventional concept of \"context\" in ICL. Our\napproach begins by defining five atomic reasoning actions, upon which we employ\nMonte Carlo Tree Search to systematically construct high-level reasoning\npatterns. During inference, HiAR-ICL dynamically selects appropriate reasoning\npatterns based on problem attributes, providing explicit guidance for the\nmodel's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and\nefficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our\nmethod achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's\n77.2% and 57.5%. Our approach enhances performance across models of varying\nsizes while generalizing effectively across domains. Further analysis reveals\nthat HiAR-ICL can also serve as a plug-and-play inference method compatible\nwith post-training techniques like GRPO. Code and data are available at\nhttps://github.com/jinyangwu/HiARICL."
                },
                "authors": [
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Zengqi Wen"
                    },
                    {
                        "name": "Chonghua Liao"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18702v2",
                "updated": "2025-06-02T14:23:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    23,
                    25,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-24T12:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    56,
                    1,
                    3,
                    298,
                    0
                ],
                "title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning"
                },
                "summary": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses."
                },
                "authors": [
                    {
                        "name": "Rita Ramos"
                    },
                    {
                        "name": "Everlyn Asiko Chimoto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Natalie Schluter"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Schluter"
                },
                "author": "Natalie Schluter",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17879v2",
                "updated": "2025-06-02T14:17:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    17,
                    22,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-23T13:28:26Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    13,
                    28,
                    26,
                    4,
                    143,
                    0
                ],
                "title": "LLM4SG: Large Language Models for Scatterer Generation via Synesthesia\n  of Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4SG: Large Language Models for Scatterer Generation via Synesthesia\n  of Machines"
                },
                "summary": "Guided by Synesthesia of Machines (SoM), the nonlinear mapping relationship\nbetween sensory and communication information serves as a powerful tool to\nenhance both the accuracy and generalization of vehicle-to-vehicle (V2V)\nmulti-modal intelligent channel modeling (MMICM) in intelligent transportation\nsystems (ITSs). To explore the general mapping relationship between physical\nenvironment and electromagnetic space, a new intelligent sensing-communication\nintegration dataset, named V2V-M3, is constructed for multiple scenarios in V2V\ncommunications with multiple frequency bands and multiple vehicular traffic\ndensities (VTDs). Leveraging the strong representation and cross-modal\ninference capabilities of large language models (LLMs), a novel LLM-based\nmethod for Scatterer Generation (LLM4SG) from light detection and ranging\n(LiDAR) point clouds is developed. To address the inherent and significant\ndifferences across multi-modal data, synergistically optimized four-module\narchitecture, i.e., preprocessor, embedding, backbone, and output modules, are\ndesigned by considering the sensing/channel characteristics and electromagnetic\npropagation mechanism. On the basis of cross-modal representation alignment and\npositional encoding, the network of LLM4SG is fine-tuned to capture the general\nmapping relationship between LiDAR point clouds and scatterers. Simulation\nresults demonstrate that the proposed LLM4SG achieves superior performance in\nfull-sample and generalization testing, significantly outperforming small\nmodels across different frequency bands, scenarios, and VTDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by Synesthesia of Machines (SoM), the nonlinear mapping relationship\nbetween sensory and communication information serves as a powerful tool to\nenhance both the accuracy and generalization of vehicle-to-vehicle (V2V)\nmulti-modal intelligent channel modeling (MMICM) in intelligent transportation\nsystems (ITSs). To explore the general mapping relationship between physical\nenvironment and electromagnetic space, a new intelligent sensing-communication\nintegration dataset, named V2V-M3, is constructed for multiple scenarios in V2V\ncommunications with multiple frequency bands and multiple vehicular traffic\ndensities (VTDs). Leveraging the strong representation and cross-modal\ninference capabilities of large language models (LLMs), a novel LLM-based\nmethod for Scatterer Generation (LLM4SG) from light detection and ranging\n(LiDAR) point clouds is developed. To address the inherent and significant\ndifferences across multi-modal data, synergistically optimized four-module\narchitecture, i.e., preprocessor, embedding, backbone, and output modules, are\ndesigned by considering the sensing/channel characteristics and electromagnetic\npropagation mechanism. On the basis of cross-modal representation alignment and\npositional encoding, the network of LLM4SG is fine-tuned to capture the general\nmapping relationship between LiDAR point clouds and scatterers. Simulation\nresults demonstrate that the proposed LLM4SG achieves superior performance in\nfull-sample and generalization testing, significantly outperforming small\nmodels across different frequency bands, scenarios, and VTDs."
                },
                "authors": [
                    {
                        "name": "Zengrui Han"
                    },
                    {
                        "name": "Lu Bai"
                    },
                    {
                        "name": "Ziwei Huang"
                    },
                    {
                        "name": "Xiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Cheng"
                },
                "author": "Xiang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03340v2",
                "updated": "2025-06-02T14:15:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    15,
                    13,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-05T10:13:05Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    13,
                    5,
                    2,
                    64,
                    0
                ],
                "title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States"
                },
                "summary": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains challenging for Large\nLanguage Models (LLMs). While existing ToM reasoning methods show promise with\nreasoning via perceptual perspective-taking, they often rely excessively on\noff-the-shelf LLMs, reducing their efficiency and limiting their applicability\nto high-order ToM reasoning. To address these issues, we present EnigmaToM, a\nnovel neuro-symbolic framework that enhances ToM reasoning by integrating a\nNeural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired\niterative masking mechanism that facilitates accurate perspective-taking and\n(2) knowledge injection that elicits key entity information. Enigma generates\nstructured knowledge of entity states to build spatial scene graphs for belief\ntracking across various ToM orders and enrich events with fine-grained entity\nstate details. Experimental results on ToMi, HiToM, and FANToM benchmarks show\nthat EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains challenging for Large\nLanguage Models (LLMs). While existing ToM reasoning methods show promise with\nreasoning via perceptual perspective-taking, they often rely excessively on\noff-the-shelf LLMs, reducing their efficiency and limiting their applicability\nto high-order ToM reasoning. To address these issues, we present EnigmaToM, a\nnovel neuro-symbolic framework that enhances ToM reasoning by integrating a\nNeural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired\niterative masking mechanism that facilitates accurate perspective-taking and\n(2) knowledge injection that elicits key entity information. Enigma generates\nstructured knowledge of entity states to build spatial scene graphs for belief\ntracking across various ToM orders and enrich events with fine-grained entity\nstate details. Experimental results on ToMi, HiToM, and FANToM benchmarks show\nthat EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Siya Qi"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Caroline Catmur"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "arxiv_comment": "Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23291v2",
                "updated": "2025-06-02T14:05:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    5,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T09:42:25Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    42,
                    25,
                    3,
                    149,
                    0
                ],
                "title": "ScEdit: Script-based Assessment of Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScEdit: Script-based Assessment of Knowledge Editing"
                },
                "summary": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit."
                },
                "authors": [
                    {
                        "name": "Xinye Li"
                    },
                    {
                        "name": "Zunwen Zheng"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Dekai Zhuang"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    },
                    {
                        "name": "Dianbo Sui"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Sui"
                },
                "author": "Dianbo Sui",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08154v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08154v3",
                "updated": "2025-06-02T13:54:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    54,
                    10,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-11T08:10:03Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    10,
                    3,
                    1,
                    70,
                    0
                ],
                "title": "S2A: A Unified Framework for Parameter and Memory Efficient Transfer\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2A: A Unified Framework for Parameter and Memory Efficient Transfer\n  Learning"
                },
                "summary": "Parameter-efficient transfer learning (PETL) aims to reduce the scales of\npretrained models for multiple downstream tasks. However, as the models keep\nscaling up, the memory footprint of existing PETL methods is not significantly\nreduced compared to the reduction of learnable parameters. This limitation\nhinders the practical deployment of PETL methods on memory-constrained devices.\nTo this end, we proposed a new PETL framework, called Structure to Activation\n(S2A), to reduce the memory footprint of activation during fine-tuning.\nSpecifically, our framework consists of: 1) Activation modules design(i.e.,\nbias, prompt and side modules) in the parametric model structure, which results\nin a significant reduction of adjustable parameters and activation memory; 2)\n4-bit quantization of activations based on their derivatives for non-parametric\nstructures (e.g., nonlinear functions), which maintains accuracy while\nsignificantly reducing memory usage. Our S2A method consequently offers a\nlightweight solution in terms of both parameters and memory footprint. We\nevaluated S2A with different backbones and performed extensive experiments on\nvarious datasets to evaluate the effectiveness. The results show that our\nmethods not only outperform existing PETL techniques, achieving a fourfold\nreduction in GPU memory footprint on average, but also shows competitive\nperformance in accuracy with fewer tunable parameters. These demonstrate that\nour method is highly suitable for practical transfer learning on\nhardware-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient transfer learning (PETL) aims to reduce the scales of\npretrained models for multiple downstream tasks. However, as the models keep\nscaling up, the memory footprint of existing PETL methods is not significantly\nreduced compared to the reduction of learnable parameters. This limitation\nhinders the practical deployment of PETL methods on memory-constrained devices.\nTo this end, we proposed a new PETL framework, called Structure to Activation\n(S2A), to reduce the memory footprint of activation during fine-tuning.\nSpecifically, our framework consists of: 1) Activation modules design(i.e.,\nbias, prompt and side modules) in the parametric model structure, which results\nin a significant reduction of adjustable parameters and activation memory; 2)\n4-bit quantization of activations based on their derivatives for non-parametric\nstructures (e.g., nonlinear functions), which maintains accuracy while\nsignificantly reducing memory usage. Our S2A method consequently offers a\nlightweight solution in terms of both parameters and memory footprint. We\nevaluated S2A with different backbones and performed extensive experiments on\nvarious datasets to evaluate the effectiveness. The results show that our\nmethods not only outperform existing PETL techniques, achieving a fourfold\nreduction in GPU memory footprint on average, but also shows competitive\nperformance in accuracy with fewer tunable parameters. These demonstrate that\nour method is highly suitable for practical transfer learning on\nhardware-constrained devices."
                },
                "authors": [
                    {
                        "name": "Tian Jin"
                    },
                    {
                        "name": "Enjun Du"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Wenhao Xu"
                    },
                    {
                        "name": "Ding Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ding Luo"
                },
                "author": "Ding Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08154v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08154v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09439v2",
                "updated": "2025-06-02T13:43:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    43,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-14T14:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"
                },
                "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU and\nMMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU and\nMMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance."
                },
                "authors": [
                    {
                        "name": "Andrew Rouditchenko"
                    },
                    {
                        "name": "Saurabhchand Bhati"
                    },
                    {
                        "name": "Edson Araujo"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17662v2",
                "updated": "2025-06-02T13:38:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    38,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-23T09:27:25Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    27,
                    25,
                    4,
                    143,
                    0
                ],
                "title": "Automating Versatile Time-Series Analysis with Tiny Transformers on\n  Embedded FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Versatile Time-Series Analysis with Tiny Transformers on\n  Embedded FPGAs"
                },
                "summary": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS)."
                },
                "authors": [
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Lukas Johannes Haßler"
                    },
                    {
                        "name": "Gregor Schiele"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Schiele"
                },
                "author": "Gregor Schiele",
                "arxiv_comment": "6 pages, 5 figures, 1 table, accepted by IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15278v2",
                "updated": "2025-06-02T13:12:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    12,
                    41,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-25T17:10:50Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    17,
                    10,
                    50,
                    5,
                    25,
                    0
                ],
                "title": "PIP: Perturbation-based Iterative Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP: Perturbation-based Iterative Pruning for Large Language Models"
                },
                "summary": "The rapid increase in the parameter counts of Large Language Models (LLMs),\nreaching billions or even trillions, presents significant challenges for their\npractical deployment, particularly in resource-constrained environments. To\nease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel\ndouble-view structured pruning method to optimize LLMs, which combines\ninformation from two different views: the unperturbed view and the perturbed\nview. With the calculation of gradient differences, PIP iteratively prunes\nthose that struggle to distinguish between these two views. Our experiments\nshow that PIP reduces the parameter count by approximately 20% while retaining\nover 85% of the original model's accuracy across varied benchmarks. In some\ncases, the performance of the pruned model is within 5% of the unpruned\nversion, demonstrating PIP's ability to preserve key aspects of model\neffectiveness. Moreover, PIP consistently outperforms existing state-of-the-art\n(SOTA) structured pruning methods, establishing it as a leading technique for\noptimizing LLMs in environments with constrained resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in the parameter counts of Large Language Models (LLMs),\nreaching billions or even trillions, presents significant challenges for their\npractical deployment, particularly in resource-constrained environments. To\nease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel\ndouble-view structured pruning method to optimize LLMs, which combines\ninformation from two different views: the unperturbed view and the perturbed\nview. With the calculation of gradient differences, PIP iteratively prunes\nthose that struggle to distinguish between these two views. Our experiments\nshow that PIP reduces the parameter count by approximately 20% while retaining\nover 85% of the original model's accuracy across varied benchmarks. In some\ncases, the performance of the pruned model is within 5% of the unpruned\nversion, demonstrating PIP's ability to preserve key aspects of model\neffectiveness. Moreover, PIP consistently outperforms existing state-of-the-art\n(SOTA) structured pruning methods, establishing it as a leading technique for\noptimizing LLMs in environments with constrained resources."
                },
                "authors": [
                    {
                        "name": "Yi Cao"
                    },
                    {
                        "name": "Wei-Jie Xu"
                    },
                    {
                        "name": "Yucheng Shen"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Chi-Min Chan"
                    },
                    {
                        "name": "Jianfeng Qu"
                    },
                    {
                        "name": "Jiajie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Xu"
                },
                "author": "Jiajie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01150v2",
                "updated": "2025-06-02T13:06:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    6,
                    53,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-03T03:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    3,
                    56,
                    3,
                    0,
                    62,
                    0
                ],
                "title": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority\n  Languages"
                },
                "summary": "Large language models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages (LRLs), particularly those spoken by minority\ncommunities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To\nsystematically track the progress in these languages, we introduce MiLiC-Eval,\na benchmark designed for minority languages in China, featuring 24K instances\nacross 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its\nparallelism between tasks and languages can provide a faithful and fine-grained\nassessment of linguistic and problem-solving skills. Our evaluation reveals\nthat open-source LLMs perform poorly on syntax-intensive tasks and multi-script\nlanguages. We further demonstrate how MiLiC-Eval can help advance LRL research\nin handling diverse writing systems and understanding the process of language\nadaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages (LRLs), particularly those spoken by minority\ncommunities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To\nsystematically track the progress in these languages, we introduce MiLiC-Eval,\na benchmark designed for minority languages in China, featuring 24K instances\nacross 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its\nparallelism between tasks and languages can provide a faithful and fine-grained\nassessment of linguistic and problem-solving skills. Our evaluation reveals\nthat open-source LLMs perform poorly on syntax-intensive tasks and multi-script\nlanguages. We further demonstrate how MiLiC-Eval can help advance LRL research\nin handling diverse writing systems and understanding the process of language\nadaptation."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Mingxu Tao"
                    },
                    {
                        "name": "Zhiyuan Liao"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "arxiv_comment": "ACL 2025 (Findings) Code and data available at\n  https://github.com/luciusssss/MiLiC-Eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23661v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23661v3",
                "updated": "2025-06-02T13:04:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    4,
                    26,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T17:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation"
                },
                "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni."
                },
                "authors": [
                    {
                        "name": "Size Wu"
                    },
                    {
                        "name": "Zhonghua Wu"
                    },
                    {
                        "name": "Zerui Gong"
                    },
                    {
                        "name": "Qingyi Tao"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Qinyue Li"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23661v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23661v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20237v2",
                "updated": "2025-06-02T12:59:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    12,
                    59,
                    54,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-26T17:17:08Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    17,
                    17,
                    8,
                    0,
                    146,
                    0
                ],
                "title": "Efficient Speech Translation through Model Compression and Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Speech Translation through Model Compression and Knowledge\n  Distillation"
                },
                "summary": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models."
                },
                "authors": [
                    {
                        "name": "Yasmin Moslem"
                    }
                ],
                "author_detail": {
                    "name": "Yasmin Moslem"
                },
                "author": "Yasmin Moslem",
                "arxiv_comment": "IWSLT 2025",
                "arxiv_journal_ref": "Proceedings of the 22nd International Conference on Spoken\n  Language Translation (IWSLT 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05206v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05206v4",
                "updated": "2025-06-02T12:47:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    12,
                    47,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-02T05:14:22Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    5,
                    14,
                    22,
                    6,
                    33,
                    0
                ],
                "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
                },
                "summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models."
                },
                "authors": [
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ye Sun"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Yunhao Chen"
                    },
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Neil Gong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Sarah Erfani"
                    },
                    {
                        "name": "Tim Baldwin"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "James Bailey"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05206v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05206v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v6",
                "updated": "2025-06-02T11:46:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    46,
                    43,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15268v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15268v4",
                "updated": "2025-06-02T11:45:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    45,
                    29,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-17T06:28:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    28,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph"
                },
                "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox."
                },
                "authors": [
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "8 pages of content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15268v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15268v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18403v3",
                "updated": "2025-06-02T11:31:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    31,
                    19,
                    0,
                    153,
                    0
                ],
                "published": "2024-06-26T14:56:13Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    56,
                    13,
                    2,
                    178,
                    0
                ],
                "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks"
                },
                "summary": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators."
                },
                "authors": [
                    {
                        "name": "Anna Bavaresco"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    },
                    {
                        "name": "Leonardo Bertolazzi"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Esam Ghaleb"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "André F. T. Martins"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Aditya K Surikuchi"
                    },
                    {
                        "name": "Ece Takmaz"
                    },
                    {
                        "name": "Alberto Testoni"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Testoni"
                },
                "author": "Alberto Testoni",
                "arxiv_comment": "Accepted to the main conference of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v5",
                "updated": "2025-06-02T11:31:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    31,
                    7,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large language models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of long-context summarization\ndatasets hinders progress in this area. To address this, we introduce CNNSum, a\nmulti-scale long-context summarization benchmark based on Chinese novels,\nfeaturing human-driven annotations across four subsets totaling 695 samples,\nwith lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct\ndetailed human assessments to summarize abnormal output types. Furthermore, we\nextensively explore how to improve long-context summarization. In our study:\n(1) Advanced LLMs may generate much subjective commentary, leading to vague\nsummaries. (2) Currently, long-context summarization mainly relies on memory\nability. The advantages of Large LLMs are hard to utilize, thus small LLMs are\nmore cost-effective. (3) Different prompt types paired with various version\nmodels may cause large performance gaps. In further fine-tuning, these can be\nmitigated, and the Base version models perform better. (4) LLMs with RoPE-base\nscaled exhibit strong extrapolation potential; using short-context data can\nsignificantly improve long-context summarization performance. However, further\napplying other interpolation methods requires careful selection. (5) CNNSum\nprovides more reliable evaluation results than other benchmarks. We release\nCNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of long-context summarization\ndatasets hinders progress in this area. To address this, we introduce CNNSum, a\nmulti-scale long-context summarization benchmark based on Chinese novels,\nfeaturing human-driven annotations across four subsets totaling 695 samples,\nwith lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct\ndetailed human assessments to summarize abnormal output types. Furthermore, we\nextensively explore how to improve long-context summarization. In our study:\n(1) Advanced LLMs may generate much subjective commentary, leading to vague\nsummaries. (2) Currently, long-context summarization mainly relies on memory\nability. The advantages of Large LLMs are hard to utilize, thus small LLMs are\nmore cost-effective. (3) Different prompt types paired with various version\nmodels may cause large performance gaps. In further fine-tuning, these can be\nmitigated, and the Base version models perform better. (4) LLMs with RoPE-base\nscaled exhibit strong extrapolation potential; using short-context data can\nsignificantly improve long-context summarization performance. However, further\napplying other interpolation methods requires careful selection. (5) CNNSum\nprovides more reliable evaluation results than other benchmarks. We release\nCNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)"
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Accepted to ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04760v2",
                "updated": "2025-06-02T11:28:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    28,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-07T14:58:51Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    58,
                    51,
                    3,
                    312,
                    0
                ],
                "title": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural\n  Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks\nthat efficiently extract temporal information while offering promising gains in\nterms of energy efficiency and latency when deployed on neuromorphic devices.\nHowever, SNN model parameters are sensitive to temporal resolution, leading to\nsignificant performance drops when the temporal resolution of target data at\nthe edge is not the same with that of the pre-deployment source data used for\ntraining, especially when fine-tuning is not possible at the edge. To address\nthis challenge, we propose three novel domain adaptation methods for adapting\nneuron parameters to account for the change in time resolution without\nre-training on target time-resolution. The proposed methods are based on a\nmapping between neuron dynamics in SNNs and State Space Models (SSMs); and are\napplicable to general neuron models. We evaluate the proposed methods under\nspatio-temporal data tasks, namely the audio keyword spotting datasets SHD and\nMSWC as well as the image classification NMINST dataset. Our methods provide an\nalternative to - and in majority of the cases significantly outperform - the\nexisting reference method that simply scales the time constant. Moreover, our\nresults show that high accuracy on high temporal resolution data can be\nobtained by time efficient training on lower temporal resolution data and model\nadaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks\nthat efficiently extract temporal information while offering promising gains in\nterms of energy efficiency and latency when deployed on neuromorphic devices.\nHowever, SNN model parameters are sensitive to temporal resolution, leading to\nsignificant performance drops when the temporal resolution of target data at\nthe edge is not the same with that of the pre-deployment source data used for\ntraining, especially when fine-tuning is not possible at the edge. To address\nthis challenge, we propose three novel domain adaptation methods for adapting\nneuron parameters to account for the change in time resolution without\nre-training on target time-resolution. The proposed methods are based on a\nmapping between neuron dynamics in SNNs and State Space Models (SSMs); and are\napplicable to general neuron models. We evaluate the proposed methods under\nspatio-temporal data tasks, namely the audio keyword spotting datasets SHD and\nMSWC as well as the image classification NMINST dataset. Our methods provide an\nalternative to - and in majority of the cases significantly outperform - the\nexisting reference method that simply scales the time constant. Moreover, our\nresults show that high accuracy on high temporal resolution data can be\nobtained by time efficient training on lower temporal resolution data and model\nadaptation."
                },
                "authors": [
                    {
                        "name": "Sanja Karilanova"
                    },
                    {
                        "name": "Maxime Fabre"
                    },
                    {
                        "name": "Emre Neftci"
                    },
                    {
                        "name": "Ayça Özçelikkale"
                    }
                ],
                "author_detail": {
                    "name": "Ayça Özçelikkale"
                },
                "author": "Ayça Özçelikkale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08985v2",
                "updated": "2025-06-02T11:22:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    22,
                    49,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-12T06:38:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    38,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v4",
                "updated": "2025-06-02T11:03:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    3,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "arxiv_comment": "Accepted to the Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18557v2",
                "updated": "2025-06-02T11:00:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    0,
                    28,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-24T06:51:03Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    51,
                    3,
                    5,
                    144,
                    0
                ],
                "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through\n  Structure-based Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through\n  Structure-based Augmentation"
                },
                "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks."
                },
                "authors": [
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Zhiwen Ruan"
                    },
                    {
                        "name": "Junyou Su"
                    },
                    {
                        "name": "Xingwei He"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Wenjia Zhang"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v5",
                "updated": "2025-06-02T10:58:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    58,
                    16,
                    0,
                    153,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large\nlanguage models (LLMs) into human-interpretable latent directions or features.\nAs we increase the number of features in the SAE, hierarchical features tend to\nsplit into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.),\na phenomenon referred to as feature splitting. However, we show that sparse\ndecomposition and splitting of hierarchical features is not robust.\nSpecifically, we show that seemingly monosemantic features fail to fire where\nthey should, and instead get \"absorbed\" into their children features. We coin\nthis phenomenon feature absorption, and show that it is caused by optimizing\nfor sparsity in SAEs whenever the underlying features form a hierarchy. We\nintroduce a metric to detect absorption in SAEs, and validate our findings\nempirically on hundreds of LLM SAEs. Our investigation suggests that varying\nSAE sizes or sparsity is insufficient to solve this issue. We discuss the\nimplications of feature absorption in SAEs and some potential approaches to\nsolve the fundamental theoretical issues before SAEs can be used for\ninterpreting LLMs robustly and at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large\nlanguage models (LLMs) into human-interpretable latent directions or features.\nAs we increase the number of features in the SAE, hierarchical features tend to\nsplit into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.),\na phenomenon referred to as feature splitting. However, we show that sparse\ndecomposition and splitting of hierarchical features is not robust.\nSpecifically, we show that seemingly monosemantic features fail to fire where\nthey should, and instead get \"absorbed\" into their children features. We coin\nthis phenomenon feature absorption, and show that it is caused by optimizing\nfor sparsity in SAEs whenever the underlying features form a hierarchy. We\nintroduce a metric to detect absorption in SAEs, and validate our findings\nempirically on hundreds of LLM SAEs. Our investigation suggests that varying\nSAE sizes or sparsity is insufficient to solve this issue. We discuss the\nimplications of feature absorption in SAEs and some potential approaches to\nsolve the fundamental theoretical issues before SAEs can be used for\ninterpreting LLMs robustly and at scale."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07217v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07217v3",
                "updated": "2025-06-02T10:38:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    38,
                    46,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-10T11:57:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    57,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "ReelWave: Multi-Agentic Movie Sound Generation through Multimodal LLM\n  Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReelWave: Multi-Agentic Movie Sound Generation through Multimodal LLM\n  Conversation"
                },
                "summary": "Current audio generation conditioned by text or video focuses on aligning\naudio with text/video modalities. Despite excellent alignment results, these\nmultimodal frameworks still cannot be directly applied to compelling movie\nstorytelling involving multiple scenes, where \"on-screen\" sounds require\ntemporally-aligned audio generation, while \"off-screen\" sounds contribute to\nappropriate environment sounds accompanied by background music when applicable.\nInspired by professional movie production, this paper proposes a multi-agentic\nframework for audio generation supervised by an autonomous Sound Director\nagent, engaging multi-turn conversations with other agents for on-screen and\noff-screen sound generation through multimodal LLM. To address on-screen sound\ngeneration, after detecting any talking humans in videos, we capture\nsemantically and temporally synchronized sound by training a prediction model\nthat forecasts interpretable, time-varying audio control signals: loudness,\npitch, and timbre, which are used by a Foley Artist agent to condition a\ncross-attention module in the sound generation. The Foley Artist works\ncooperatively with the Composer and Voice Actor agents, and together they\nautonomously generate off-screen sound to complement the overall production.\nEach agent takes on specific roles similar to those of a movie production team.\nTo temporally ground audio language models, in ReelWave, text/video conditions\nare decomposed into atomic, specific sound generation instructions synchronized\nwith visuals when applicable. Consequently, our framework can generate rich and\nrelevant audio content conditioned on video clips extracted from movies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current audio generation conditioned by text or video focuses on aligning\naudio with text/video modalities. Despite excellent alignment results, these\nmultimodal frameworks still cannot be directly applied to compelling movie\nstorytelling involving multiple scenes, where \"on-screen\" sounds require\ntemporally-aligned audio generation, while \"off-screen\" sounds contribute to\nappropriate environment sounds accompanied by background music when applicable.\nInspired by professional movie production, this paper proposes a multi-agentic\nframework for audio generation supervised by an autonomous Sound Director\nagent, engaging multi-turn conversations with other agents for on-screen and\noff-screen sound generation through multimodal LLM. To address on-screen sound\ngeneration, after detecting any talking humans in videos, we capture\nsemantically and temporally synchronized sound by training a prediction model\nthat forecasts interpretable, time-varying audio control signals: loudness,\npitch, and timbre, which are used by a Foley Artist agent to condition a\ncross-attention module in the sound generation. The Foley Artist works\ncooperatively with the Composer and Voice Actor agents, and together they\nautonomously generate off-screen sound to complement the overall production.\nEach agent takes on specific roles similar to those of a movie production team.\nTo temporally ground audio language models, in ReelWave, text/video conditions\nare decomposed into atomic, specific sound generation instructions synchronized\nwith visuals when applicable. Consequently, our framework can generate rich and\nrelevant audio content conditioned on video clips extracted from movies."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "arxiv_comment": "Project page: https://vincent2311.github.io/ReelWave_demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07217v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07217v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24362v2",
                "updated": "2025-06-02T10:26:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    26,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T08:54:28Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    54,
                    28,
                    4,
                    150,
                    0
                ],
                "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion"
                },
                "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits."
                },
                "authors": [
                    {
                        "name": "Anum Afzal"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Yftah Ziser"
                    }
                ],
                "author_detail": {
                    "name": "Yftah Ziser"
                },
                "author": "Yftah Ziser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15865v2",
                "updated": "2025-06-02T10:13:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    13,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-21T12:56:15Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    56,
                    15,
                    4,
                    52,
                    0
                ],
                "title": "Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must\n  Prioritize Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must\n  Prioritize Risk"
                },
                "summary": "Standard benchmarks fixate on how well large language model (LLM) agents\nperform in finance, yet say little about whether they are safe to deploy. We\nargue that accuracy metrics and return-based scores provide an illusion of\nreliability, overlooking vulnerabilities such as hallucinated facts, stale\ndata, and adversarial prompt manipulation. We take a firm position: financial\nLLM agents should be evaluated first and foremost on their risk profile, not on\ntheir point-estimate performance. Drawing on risk-engineering principles, we\noutline a three-level agenda: model, workflow, and system, for stress-testing\nLLM agents under realistic failure modes. To illustrate why this shift is\nurgent, we audit six API-based and open-weights LLM agents on three high-impact\ntasks and uncover hidden weaknesses that conventional benchmarks miss. We\nconclude with actionable recommendations for researchers, practitioners, and\nregulators: audit risk-aware metrics in future studies, publish stress\nscenarios alongside datasets, and treat ``safety budget'' as a primary success\ncriterion. Only by redefining what ``good'' looks like can the community\nresponsibly advance AI-driven finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard benchmarks fixate on how well large language model (LLM) agents\nperform in finance, yet say little about whether they are safe to deploy. We\nargue that accuracy metrics and return-based scores provide an illusion of\nreliability, overlooking vulnerabilities such as hallucinated facts, stale\ndata, and adversarial prompt manipulation. We take a firm position: financial\nLLM agents should be evaluated first and foremost on their risk profile, not on\ntheir point-estimate performance. Drawing on risk-engineering principles, we\noutline a three-level agenda: model, workflow, and system, for stress-testing\nLLM agents under realistic failure modes. To illustrate why this shift is\nurgent, we audit six API-based and open-weights LLM agents on three high-impact\ntasks and uncover hidden weaknesses that conventional benchmarks miss. We\nconclude with actionable recommendations for researchers, practitioners, and\nregulators: audit risk-aware metrics in future studies, publish stress\nscenarios alongside datasets, and treat ``safety budget'' as a primary success\ncriterion. Only by redefining what ``good'' looks like can the community\nresponsibly advance AI-driven finance."
                },
                "authors": [
                    {
                        "name": "Zichen Chen"
                    },
                    {
                        "name": "Jiaao Chen"
                    },
                    {
                        "name": "Jianda Chen"
                    },
                    {
                        "name": "Misha Sra"
                    }
                ],
                "author_detail": {
                    "name": "Misha Sra"
                },
                "author": "Misha Sra",
                "arxiv_comment": "46 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16310v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16310v4",
                "updated": "2025-06-02T10:06:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    6,
                    31,
                    0,
                    153,
                    0
                ],
                "published": "2024-01-29T17:13:44Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    17,
                    13,
                    44,
                    0,
                    29,
                    0
                ],
                "title": "An Insight into Security Code Review with LLMs: Capabilities, Obstacles,\n  and Influential Factors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Insight into Security Code Review with LLMs: Capabilities, Obstacles,\n  and Influential Factors"
                },
                "summary": "Security code review is a time-consuming and labor-intensive process\ntypically requiring integration with automated security defect detection tools.\nHowever, existing security analysis tools struggle with poor generalization,\nhigh false positive rates, and coarse detection granularity. Large Language\nModels (LLMs) have been considered promising candidates for addressing those\nchallenges. In this study, we conducted an empirical study to explore the\npotential of LLMs in detecting security defects during code review.\nSpecifically, we evaluated the performance of six LLMs under five different\nprompts and compared them with state-of-the-art static analysis tools. We also\nperformed linguistic and regression analyses for the best-performing LLM to\nidentify quality problems in its responses and factors influencing its\nperformance. Our findings showthat: (1) existing pre-trained LLMs have limited\ncapability in security code review but significantly outperformthe\nstate-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs\nwhen provided with a CWE list for reference. (3) GPT-4 frequently generates\nverbose or non-compliant responses with the task requirements given in the\nprompts. (4) GPT-4 is more adept at identifying security defects in code files\nwith fewer tokens, containing functional logic, or written by developers with\nless involvement in the project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security code review is a time-consuming and labor-intensive process\ntypically requiring integration with automated security defect detection tools.\nHowever, existing security analysis tools struggle with poor generalization,\nhigh false positive rates, and coarse detection granularity. Large Language\nModels (LLMs) have been considered promising candidates for addressing those\nchallenges. In this study, we conducted an empirical study to explore the\npotential of LLMs in detecting security defects during code review.\nSpecifically, we evaluated the performance of six LLMs under five different\nprompts and compared them with state-of-the-art static analysis tools. We also\nperformed linguistic and regression analyses for the best-performing LLM to\nidentify quality problems in its responses and factors influencing its\nperformance. Our findings showthat: (1) existing pre-trained LLMs have limited\ncapability in security code review but significantly outperformthe\nstate-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs\nwhen provided with a CWE list for reference. (3) GPT-4 frequently generates\nverbose or non-compliant responses with the task requirements given in the\nprompts. (4) GPT-4 is more adept at identifying security defects in code files\nwith fewer tokens, containing functional logic, or written by developers with\nless involvement in the project."
                },
                "authors": [
                    {
                        "name": "Jiaxin Yu"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Amjed Tahir"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Yangxiao Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yangxiao Cai"
                },
                "author": "Yangxiao Cai",
                "arxiv_comment": "21 pages, 5 images, 8 tables, Manuscript submitted to a journal\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16310v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16310v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15351v2",
                "updated": "2025-06-02T10:04:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    4,
                    32,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-19T15:48:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    48,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models"
                },
                "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals."
                },
                "authors": [
                    {
                        "name": "I-Fan Lin"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06204v2",
                "updated": "2025-06-02T09:56:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    56,
                    25,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-10T07:03:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    3,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Non-literal Understanding of Number Words by Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-literal Understanding of Number Words by Language Models"
                },
                "summary": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Polina Tsvilodub"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jan-Philipp Fränken"
                    },
                    {
                        "name": "Michael Franke"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "12 pages, 10 figures. To appear in the Proceedings of CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11230v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11230v3",
                "updated": "2025-06-02T09:45:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    45,
                    53,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-15T14:30:26Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    30,
                    26,
                    1,
                    105,
                    0
                ],
                "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image"
                },
                "summary": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page."
                },
                "authors": [
                    {
                        "name": "Jingshun Huang"
                    },
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhu"
                },
                "author": "Yi Zhu",
                "arxiv_comment": "To appear in CVPR 2025 (Highlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11230v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13899v2",
                "updated": "2025-06-02T09:43:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    43,
                    49,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-21T07:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "Schemato -- An LLM for Netlist-to-Schematic Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schemato -- An LLM for Netlist-to-Schematic Conversion"
                },
                "summary": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in\nconverting netlists to .asc files, text-based schematic description used in\nLTSpice. Experiments on our circuit dataset show that Schemato achieves up to\n76% compilation success rate, surpassing 63% scored by the state-of-the-art\nLLMs. Furthermore, our experiments show that Schemato generates schematics with\nan average graph edit distance score and mean structural similarity index\nmeasure, scaled by the compilation success rate that are 1.8x and 4.3x higher\nthan the best performing LLMs respectively, demonstrating its ability to\ngenerate schematics that are more accurately connected and are closer to the\nreference human design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in\nconverting netlists to .asc files, text-based schematic description used in\nLTSpice. Experiments on our circuit dataset show that Schemato achieves up to\n76% compilation success rate, surpassing 63% scored by the state-of-the-art\nLLMs. Furthermore, our experiments show that Schemato generates schematics with\nan average graph edit distance score and mean structural similarity index\nmeasure, scaled by the compilation success rate that are 1.8x and 4.3x higher\nthan the best performing LLMs respectively, demonstrating its ability to\ngenerate schematics that are more accurately connected and are closer to the\nreference human design."
                },
                "authors": [
                    {
                        "name": "Ryoga Matsuo"
                    },
                    {
                        "name": "Stefan Uhlich"
                    },
                    {
                        "name": "Arun Venkitaraman"
                    },
                    {
                        "name": "Andrea Bonetti"
                    },
                    {
                        "name": "Chia-Yu Hsieh"
                    },
                    {
                        "name": "Ali Momeni"
                    },
                    {
                        "name": "Lukas Mauch"
                    },
                    {
                        "name": "Augusto Capone"
                    },
                    {
                        "name": "Eisaku Ohbuchi"
                    },
                    {
                        "name": "Lorenzo Servadei"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Servadei"
                },
                "author": "Lorenzo Servadei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07089v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07089v3",
                "updated": "2025-06-02T09:38:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    38,
                    26,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-09T17:58:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "OmniCaptioner: One Captioner to Rule Them All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCaptioner: One Captioner to Rule Them All"
                },
                "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."
                },
                "authors": [
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Xinyue Li"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Yuewen Cao"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Tianshuo Peng"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "arxiv_comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07089v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07089v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14301v2",
                "updated": "2025-06-02T09:23:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    23,
                    18,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-20T06:32:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    32,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models"
                },
                "summary": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and culturally representative evaluation suite\nfor the SEA languages has not been developed thus far. Here, we present\nSEA-HELM, a holistic linguistic and cultural LLM evaluation suite that\nemphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2)\nLLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM\ncurrently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also\nintroduce the SEA-HELM leaderboard, which allows users to understand models'\nmultilingual and multicultural performance in a systematic and user-friendly\nmanner. We make the SEA-HELM evaluation code publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and culturally representative evaluation suite\nfor the SEA languages has not been developed thus far. Here, we present\nSEA-HELM, a holistic linguistic and cultural LLM evaluation suite that\nemphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2)\nLLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM\ncurrently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also\nintroduce the SEA-HELM leaderboard, which allows users to understand models'\nmultilingual and multicultural performance in a systematic and user-friendly\nmanner. We make the SEA-HELM evaluation code publicly available."
                },
                "authors": [
                    {
                        "name": "Yosephine Susanto"
                    },
                    {
                        "name": "Adithya Venkatadri Hulagadri"
                    },
                    {
                        "name": "Jann Railey Montalan"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Xian Bin Yong"
                    },
                    {
                        "name": "Weiqi Leong"
                    },
                    {
                        "name": "Hamsawardhini Rengarajan"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Yifan Mai"
                    },
                    {
                        "name": "William Chandra Tjhi"
                    }
                ],
                "author_detail": {
                    "name": "William Chandra Tjhi"
                },
                "author": "William Chandra Tjhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19107v2",
                "updated": "2025-06-02T09:19:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    19,
                    36,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-31T13:04:37Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    37,
                    4,
                    31,
                    0
                ],
                "title": "Brain network science modelling of sparse neural networks enables\n  Transformers and LLMs to perform as fully connected",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain network science modelling of sparse neural networks enables\n  Transformers and LLMs to perform as fully connected"
                },
                "summary": "Dynamic sparse training (DST) can reduce the computational demands in ANNs,\nbut faces difficulties in keeping peak performance at high sparsity levels. The\nCannistraci-Hebb training (CHT) is a brain-inspired method for growing\nconnectivity in DST. CHT leverages a gradient-free, topology-driven link\nregrowth, which has shown ultra-sparse (less than 1% connectivity) advantage\nacross various tasks compared to fully connected networks. Yet, CHT suffers two\nmain drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d\nnode degree - restricting it to ultra-sparse regimes. (ii) it selects top link\nprediction scores, which is inappropriate for the early training epochs, when\nthe network presents unreliable connections. Here, we design the first\nbrain-inspired network model - termed bipartite receptive field (BRF) - to\ninitialize the connectivity of sparse artificial neural networks. We further\nintroduce a GPU-friendly matrix-based approximation of CH link prediction,\nreducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training\nsoft rule (CHTs), which adopts a flexible strategy for sampling connections in\nboth link removal and regrowth, balancing the exploration and exploitation of\nnetwork topology. Additionally, we integrate CHTs with a sigmoid gradual\ndensity decay (CHTss). Empirical results show that BRF offers performance\nadvantages over previous network science models. Using 1% of connections, CHTs\noutperforms fully connected networks in MLP architectures on image\nclassification tasks, compressing some networks to less than 30% of the nodes.\nUsing 5% of the connections, CHTss outperforms fully connected networks in two\nTransformer-based machine translation tasks. Finally, at 30% connectivity, both\nCHTs and CHTss outperform other DST methods in language modeling and even\nexceed fully connected baselines in zero-shot tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic sparse training (DST) can reduce the computational demands in ANNs,\nbut faces difficulties in keeping peak performance at high sparsity levels. The\nCannistraci-Hebb training (CHT) is a brain-inspired method for growing\nconnectivity in DST. CHT leverages a gradient-free, topology-driven link\nregrowth, which has shown ultra-sparse (less than 1% connectivity) advantage\nacross various tasks compared to fully connected networks. Yet, CHT suffers two\nmain drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d\nnode degree - restricting it to ultra-sparse regimes. (ii) it selects top link\nprediction scores, which is inappropriate for the early training epochs, when\nthe network presents unreliable connections. Here, we design the first\nbrain-inspired network model - termed bipartite receptive field (BRF) - to\ninitialize the connectivity of sparse artificial neural networks. We further\nintroduce a GPU-friendly matrix-based approximation of CH link prediction,\nreducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training\nsoft rule (CHTs), which adopts a flexible strategy for sampling connections in\nboth link removal and regrowth, balancing the exploration and exploitation of\nnetwork topology. Additionally, we integrate CHTs with a sigmoid gradual\ndensity decay (CHTss). Empirical results show that BRF offers performance\nadvantages over previous network science models. Using 1% of connections, CHTs\noutperforms fully connected networks in MLP architectures on image\nclassification tasks, compressing some networks to less than 30% of the nodes.\nUsing 5% of the connections, CHTss outperforms fully connected networks in two\nTransformer-based machine translation tasks. Finally, at 30% connectivity, both\nCHTs and CHTss outperform other DST methods in language modeling and even\nexceed fully connected baselines in zero-shot tasks."
                },
                "authors": [
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Diego Cerretti"
                    },
                    {
                        "name": "Jialin Zhao"
                    },
                    {
                        "name": "Wenjing Wu"
                    },
                    {
                        "name": "Ziheng Liao"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Carlo Vittorio Cannistraci"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Vittorio Cannistraci"
                },
                "author": "Carlo Vittorio Cannistraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08219v2",
                "updated": "2025-06-02T09:12:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    12,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-14T16:02:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Paul Joe Maliakel"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14830v3",
                "updated": "2025-06-02T09:09:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    9,
                    36,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-20T18:45:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs"
                },
                "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."
                },
                "authors": [
                    {
                        "name": "Danni Liu"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00113v3",
                "updated": "2025-06-02T09:08:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    8,
                    56,
                    0,
                    153,
                    0
                ],
                "published": "2024-08-27T19:27:43Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    19,
                    27,
                    43,
                    1,
                    240,
                    0
                ],
                "title": "Wait, that's not an option: LLMs Robustness with Incorrect\n  Multiple-Choice Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wait, that's not an option: LLMs Robustness with Incorrect\n  Multiple-Choice Options"
                },
                "summary": "This work introduces a novel framework for evaluating LLMs' capacity to\nbalance instruction-following with critical reasoning when presented with\nmultiple-choice questions containing no valid answers. Through systematic\nevaluation across arithmetic, domain-specific knowledge, and high-stakes\nmedical decision tasks, we demonstrate that post-training aligned models often\ndefault to selecting invalid options, while base models exhibit improved\nrefusal capabilities that scale with model size. Our analysis reveals that\nalignment techniques, though intended to enhance helpfulness, can inadvertently\nimpair models' reflective judgment--the ability to override default behaviors\nwhen faced with invalid options. We additionally conduct a parallel human study\nshowing similar instruction-following biases, with implications for how these\nbiases may propagate through human feedback datasets used in alignment. We\nprovide extensive ablation studies examining the impact of model size, training\ntechniques, and prompt engineering. Our findings highlight fundamental tensions\nbetween alignment optimization and preservation of critical reasoning\ncapabilities, with important implications for developing more robust AI systems\nfor real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a novel framework for evaluating LLMs' capacity to\nbalance instruction-following with critical reasoning when presented with\nmultiple-choice questions containing no valid answers. Through systematic\nevaluation across arithmetic, domain-specific knowledge, and high-stakes\nmedical decision tasks, we demonstrate that post-training aligned models often\ndefault to selecting invalid options, while base models exhibit improved\nrefusal capabilities that scale with model size. Our analysis reveals that\nalignment techniques, though intended to enhance helpfulness, can inadvertently\nimpair models' reflective judgment--the ability to override default behaviors\nwhen faced with invalid options. We additionally conduct a parallel human study\nshowing similar instruction-following biases, with implications for how these\nbiases may propagate through human feedback datasets used in alignment. We\nprovide extensive ablation studies examining the impact of model size, training\ntechniques, and prompt engineering. Our findings highlight fundamental tensions\nbetween alignment optimization and preservation of critical reasoning\ncapabilities, with important implications for developing more robust AI systems\nfor real-world deployment."
                },
                "authors": [
                    {
                        "name": "Gracjan Góral"
                    },
                    {
                        "name": "Emilia Wiśnios"
                    },
                    {
                        "name": "Piotr Sankowski"
                    },
                    {
                        "name": "Paweł Budzianowski"
                    }
                ],
                "author_detail": {
                    "name": "Paweł Budzianowski"
                },
                "author": "Paweł Budzianowski",
                "arxiv_comment": "Accepted for ACL 2025 Main Conference and NeurIPS 2024 FM-EduAssess\n  Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06560v2",
                "updated": "2025-06-02T08:49:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    49,
                    20,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-10T15:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    25,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation"
                },
                "summary": "The recent surge in high-quality open-source Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, have opened\nthe possibility of creating high-quality personalized models that generate text\nattuned to a specific individual's needs and are capable of credibly imitating\ntheir writing style by refining an open-source model with that person's own\ndata. The technology to create such models is accessible to private\nindividuals, and training and running such models can be done cheaply on\nconsumer-grade hardware. While these advancements are a huge gain for usability\nand privacy, this position paper argues that the practical feasibility of\nimpersonating specific individuals also introduces novel safety risks. For\ninstance, this technology enables the creation of phishing emails or fraudulent\nsocial media accounts, based on small amounts of publicly available text, or by\nthe individuals themselves to escape AI text detection. We further argue that\nthese risks are complementary to - and distinct from - the much-discussed risks\nof other impersonation attacks such as image, voice, or video deepfakes, and\nare not adequately addressed by the larger research community, or the current\ngeneration of open- and closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge in high-quality open-source Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, have opened\nthe possibility of creating high-quality personalized models that generate text\nattuned to a specific individual's needs and are capable of credibly imitating\ntheir writing style by refining an open-source model with that person's own\ndata. The technology to create such models is accessible to private\nindividuals, and training and running such models can be done cheaply on\nconsumer-grade hardware. While these advancements are a huge gain for usability\nand privacy, this position paper argues that the practical feasibility of\nimpersonating specific individuals also introduces novel safety risks. For\ninstance, this technology enables the creation of phishing emails or fraudulent\nsocial media accounts, based on small amounts of publicly available text, or by\nthe individuals themselves to escape AI text detection. We further argue that\nthese risks are complementary to - and distinct from - the much-discussed risks\nof other impersonation attacks such as image, voice, or video deepfakes, and\nare not adequately addressed by the larger research community, or the current\ngeneration of open- and closed-source models."
                },
                "authors": [
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13248v2",
                "updated": "2025-06-02T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    41,
                    9,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-17T06:15:00Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    6,
                    15,
                    0,
                    3,
                    291,
                    0
                ],
                "title": "Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation"
                },
                "summary": "Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. The datasets and benchmark\nimplementation are available at: https://github.com/jchanxtarov/sent_xrec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. The datasets and benchmark\nimplementation are available at: https://github.com/jchanxtarov/sent_xrec."
                },
                "authors": [
                    {
                        "name": "Ryotaro Shimizu"
                    },
                    {
                        "name": "Takashi Wada"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Johannes Kruse"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Sai HtaungKham"
                    },
                    {
                        "name": "Linxin Song"
                    },
                    {
                        "name": "Yuya Yoshikawa"
                    },
                    {
                        "name": "Yuki Saito"
                    },
                    {
                        "name": "Fugee Tsung"
                    },
                    {
                        "name": "Masayuki Goto"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "This manuscript has been accepted for presentation at The Web\n  Conference (WWW) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23405v2",
                "updated": "2025-06-02T08:21:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    21,
                    31,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T12:52:43Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    52,
                    43,
                    3,
                    149,
                    0
                ],
                "title": "A Practical Guide for Supporting Formative Assessment and Feedback Using\n  Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Guide for Supporting Formative Assessment and Feedback Using\n  Generative AI"
                },
                "summary": "Formative assessment is a cornerstone of effective teaching and learning,\nproviding students with feedback to guide their learning. While there has been\nan exponential growth in the application of generative AI in scaling various\naspects of formative assessment, ranging from automatic question generation to\nintelligent tutoring systems and personalized feedback, few have directly\naddressed the core pedagogical principles of formative assessment. Here, we\ncritically examined how generative AI, especially large-language models (LLMs)\nsuch as ChatGPT, can support key components of formative assessment: helping\nstudents, teachers, and peers understand \"where learners are going,\" \"where\nlearners currently are,\" and \"how to move learners forward\" in the learning\nprocess. With the rapid emergence of new prompting techniques and LLM\ncapabilities, we also provide guiding principles for educators to effectively\nleverage cost-free LLMs in formative assessments while remaining grounded in\npedagogical best practices. Furthermore, we reviewed the role of LLMs in\ngenerating feedback, highlighting limitations in current evaluation metrics\nthat inadequately capture the nuances of formative feedback, such as\ndistinguishing feedback at the task, process, and self-regulatory levels.\nFinally, we offer practical guidelines for educators and researchers, including\nconcrete classroom strategies and future directions such as developing robust\nmetrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic\nand cultural barriers to formative assessment, and designing AI-aware\nassessment strategies that promote transferable skills while mitigating\noverreliance on LLM-generated responses. By structuring the discussion within\nan established formative assessment framework, this review provides a\ncomprehensive foundation for integrating LLMs into formative assessment in a\npedagogically informed manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formative assessment is a cornerstone of effective teaching and learning,\nproviding students with feedback to guide their learning. While there has been\nan exponential growth in the application of generative AI in scaling various\naspects of formative assessment, ranging from automatic question generation to\nintelligent tutoring systems and personalized feedback, few have directly\naddressed the core pedagogical principles of formative assessment. Here, we\ncritically examined how generative AI, especially large-language models (LLMs)\nsuch as ChatGPT, can support key components of formative assessment: helping\nstudents, teachers, and peers understand \"where learners are going,\" \"where\nlearners currently are,\" and \"how to move learners forward\" in the learning\nprocess. With the rapid emergence of new prompting techniques and LLM\ncapabilities, we also provide guiding principles for educators to effectively\nleverage cost-free LLMs in formative assessments while remaining grounded in\npedagogical best practices. Furthermore, we reviewed the role of LLMs in\ngenerating feedback, highlighting limitations in current evaluation metrics\nthat inadequately capture the nuances of formative feedback, such as\ndistinguishing feedback at the task, process, and self-regulatory levels.\nFinally, we offer practical guidelines for educators and researchers, including\nconcrete classroom strategies and future directions such as developing robust\nmetrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic\nand cultural barriers to formative assessment, and designing AI-aware\nassessment strategies that promote transferable skills while mitigating\noverreliance on LLM-generated responses. By structuring the discussion within\nan established formative assessment framework, this review provides a\ncomprehensive foundation for integrating LLMs into formative assessment in a\npedagogically informed manner."
                },
                "authors": [
                    {
                        "name": "Sapolnach Prompiengchai"
                    },
                    {
                        "name": "Charith Narreddy"
                    },
                    {
                        "name": "Steve Joordens"
                    }
                ],
                "author_detail": {
                    "name": "Steve Joordens"
                },
                "author": "Steve Joordens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05367v2",
                "updated": "2025-06-02T08:18:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    18,
                    21,
                    0,
                    153,
                    0
                ],
                "published": "2024-09-09T06:55:37Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    37,
                    0,
                    253,
                    0
                ],
                "title": "STRICTA: Structured Reasoning in Critical Text Assessment for Peer\n  Review and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRICTA: Structured Reasoning in Critical Text Assessment for Peer\n  Review and Beyond"
                },
                "summary": "Critical text assessment is at the core of many expert activities, such as\nfact-checking, peer review, and essay grading. Yet, existing work treats\ncritical text assessment as a black box problem, limiting interpretability and\nhuman-AI collaboration. To close this gap, we introduce Structured Reasoning In\nCritical Text Assessment (STRICTA), a novel specification framework to model\ntext assessment as an explicit, step-wise reasoning process. STRICTA breaks\ndown the assessment into a graph of interconnected reasoning steps drawing on\ncausality theory (Pearl, 1995). This graph is populated based on expert\ninteraction data and used to study the assessment process and facilitate\nhuman-AI collaboration. We formally define STRICTA and apply it in a study on\nbiomedical paper assessment, resulting in a dataset of over 4000 reasoning\nsteps from roughly 40 biomedical experts on more than 20 papers. We use this\ndataset to empirically study expert reasoning in critical text assessment, and\ninvestigate if LLMs are able to imitate and support experts within these\nworkflows. The resulting tools and datasets pave the way for studying\ncollaborative expert-AI reasoning in text assessment, in peer review and\nbeyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical text assessment is at the core of many expert activities, such as\nfact-checking, peer review, and essay grading. Yet, existing work treats\ncritical text assessment as a black box problem, limiting interpretability and\nhuman-AI collaboration. To close this gap, we introduce Structured Reasoning In\nCritical Text Assessment (STRICTA), a novel specification framework to model\ntext assessment as an explicit, step-wise reasoning process. STRICTA breaks\ndown the assessment into a graph of interconnected reasoning steps drawing on\ncausality theory (Pearl, 1995). This graph is populated based on expert\ninteraction data and used to study the assessment process and facilitate\nhuman-AI collaboration. We formally define STRICTA and apply it in a study on\nbiomedical paper assessment, resulting in a dataset of over 4000 reasoning\nsteps from roughly 40 biomedical experts on more than 20 papers. We use this\ndataset to empirically study expert reasoning in critical text assessment, and\ninvestigate if LLMs are able to imitate and support experts within these\nworkflows. The resulting tools and datasets pave the way for studying\ncollaborative expert-AI reasoning in text assessment, in peer review and\nbeyond."
                },
                "authors": [
                    {
                        "name": "Nils Dycke"
                    },
                    {
                        "name": "Matej Zečević"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Beatrix Suess"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09923v2",
                "updated": "2025-06-02T08:10:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    10,
                    54,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-14T06:32:45Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    32,
                    45,
                    0,
                    104,
                    0
                ],
                "title": "Guiding Reasoning in Small Language Models with LLM Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Reasoning in Small Language Models with LLM Assistance"
                },
                "summary": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone."
                },
                "authors": [
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Euiin Yi"
                    },
                    {
                        "name": "Minu Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    },
                    {
                        "name": "Taehyeon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taehyeon Kim"
                },
                "author": "Taehyeon Kim",
                "arxiv_comment": "20 pages, 12 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11671v2",
                "updated": "2025-06-02T07:51:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    51,
                    20,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-17T11:00:40Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    0,
                    40,
                    0,
                    48,
                    0
                ],
                "title": "Diversity-oriented Data Augmentation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity-oriented Data Augmentation with Large Language Models"
                },
                "summary": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points."
                },
                "authors": [
                    {
                        "name": "Zaitian Wang"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Xinhao Zhang"
                    },
                    {
                        "name": "Kunpeng Liu"
                    },
                    {
                        "name": "Pengfei Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03835v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03835v4",
                "updated": "2025-06-03T09:02:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    2,
                    22,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-07T14:45:30Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    45,
                    30,
                    1,
                    7,
                    0
                ],
                "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification"
                },
                "summary": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR."
                },
                "authors": [
                    {
                        "name": "Yindu Su"
                    },
                    {
                        "name": "Huike Zou"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Haiyang Yang"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Qingheng Zhang"
                    },
                    {
                        "name": "Shuguang Han"
                    },
                    {
                        "name": "Jufeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Chen"
                },
                "author": "Jufeng Chen",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03835v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03835v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10201v2",
                "updated": "2025-06-02T07:26:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    26,
                    26,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-14T14:52:41Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    52,
                    41,
                    4,
                    45,
                    0
                ],
                "title": "Prediction hubs are context-informed frequent tokens in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction hubs are context-informed frequent tokens in LLMs"
                },
                "summary": "Hubness, the tendency for a few points to be among the nearest neighbours of\na disproportionate number of other points, commonly arises when applying\nstandard distance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first prove that the only large-scale representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appearance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction.\nHowever, when other distances are used to compare LLM representations, we do\nnot have the same theoretical guarantees, and, indeed, we see nuisance hubs\nappear. There are two main takeaways. First, hubness, while omnipresent in\nhigh-dimensional spaces, is not a negative property that needs to be mitigated\nwhen LLMs are being used for next token prediction. Second, when comparing\nrepresentations from LLMs using Euclidean or cosine distance, there is a high\nrisk of nuisance hubs and practitioners should use mitigation techniques if\nrelevant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubness, the tendency for a few points to be among the nearest neighbours of\na disproportionate number of other points, commonly arises when applying\nstandard distance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first prove that the only large-scale representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appearance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction.\nHowever, when other distances are used to compare LLM representations, we do\nnot have the same theoretical guarantees, and, indeed, we see nuisance hubs\nappear. There are two main takeaways. First, hubness, while omnipresent in\nhigh-dimensional spaces, is not a negative property that needs to be mitigated\nwhen LLMs are being used for next token prediction. Second, when comparing\nrepresentations from LLMs using Euclidean or cosine distance, there is a high\nrisk of nuisance hubs and practitioners should use mitigation techniques if\nrelevant."
                },
                "authors": [
                    {
                        "name": "Beatrix M. G. Nielsen"
                    },
                    {
                        "name": "Iuri Macocco"
                    },
                    {
                        "name": "Marco Baroni"
                    }
                ],
                "author_detail": {
                    "name": "Marco Baroni"
                },
                "author": "Marco Baroni",
                "arxiv_comment": "Published as a conference paper at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24223v2",
                "updated": "2025-06-02T07:21:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    21,
                    17,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T05:23:01Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    23,
                    1,
                    4,
                    150,
                    0
                ],
                "title": "Automated Structured Radiology Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Structured Radiology Report Generation"
                },
                "summary": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments."
                },
                "authors": [
                    {
                        "name": "Jean-Benoit Delbrouck"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Johannes Moll"
                    },
                    {
                        "name": "Alois Thomas"
                    },
                    {
                        "name": "Zhihong Chen"
                    },
                    {
                        "name": "Sophie Ostmeier"
                    },
                    {
                        "name": "Asfandyar Azhar"
                    },
                    {
                        "name": "Kelvin Zhenghao Li"
                    },
                    {
                        "name": "Andrew Johnston"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Eduardo Reis"
                    },
                    {
                        "name": "Mohamed Muneer"
                    },
                    {
                        "name": "Maya Varma"
                    },
                    {
                        "name": "Curtis Langlotz"
                    }
                ],
                "author_detail": {
                    "name": "Curtis Langlotz"
                },
                "author": "Curtis Langlotz",
                "arxiv_comment": "Accepted to ACL Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18744v3",
                "updated": "2025-06-02T07:16:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    7,
                    16,
                    11,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-26T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    1,
                    36,
                    40,
                    2,
                    57,
                    0
                ],
                "title": "ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation\n  Preference Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation\n  Preference Dataset Construction"
                },
                "summary": "Recent efforts in LLM alignment have focused on constructing large-scale\npreference datasets via human or Artificial Intelligence (AI) annotators.\nHowever, such approaches rely on instance-wise supervision, incurring\nsubstantial annotation cost and limited interpretability. In this paper, we\npropose ZEBRA - a model behavior-wise zero-annotation framework that constructs\npreference data by leveraging model behavior knowledge derived from benchmark\nperformances. ZEBRA binarizes response pairs by evaluating the quality and\nsimilarity of their origin models, entirely bypassing instance-level\nannotation. This allows scalable, controllable, and cost-effective alignment\ndata generation. Empirical results show that ZEBRA achieves alignment\nperformance comparable to instance-supervised methods, despite requiring no\nmanual or model-based labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts in LLM alignment have focused on constructing large-scale\npreference datasets via human or Artificial Intelligence (AI) annotators.\nHowever, such approaches rely on instance-wise supervision, incurring\nsubstantial annotation cost and limited interpretability. In this paper, we\npropose ZEBRA - a model behavior-wise zero-annotation framework that constructs\npreference data by leveraging model behavior knowledge derived from benchmark\nperformances. ZEBRA binarizes response pairs by evaluating the quality and\nsimilarity of their origin models, entirely bypassing instance-level\nannotation. This allows scalable, controllable, and cost-effective alignment\ndata generation. Empirical results show that ZEBRA achieves alignment\nperformance comparable to instance-supervised methods, despite requiring no\nmanual or model-based labeling."
                },
                "authors": [
                    {
                        "name": "Jeesu Jung"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Sangkeun Jung"
                    }
                ],
                "author_detail": {
                    "name": "Sangkeun Jung"
                },
                "author": "Sangkeun Jung",
                "arxiv_comment": "16 pages,7 figures,5 tables,4 graphs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03819v2",
                "updated": "2025-06-02T06:56:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    56,
                    42,
                    0,
                    153,
                    0
                ],
                "published": "2024-08-07T14:55:04Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    14,
                    55,
                    4,
                    2,
                    220,
                    0
                ],
                "title": "Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning"
                },
                "summary": "Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL."
                },
                "authors": [
                    {
                        "name": "Simret Araya Gebreegziabher"
                    },
                    {
                        "name": "Kuangshi Ai"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Elena L. Glassman"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-Jun Li"
                },
                "author": "Toby Jia-Jun Li",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02508v2",
                "updated": "2025-06-02T06:42:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    42,
                    17,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-04T17:26:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    26,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced."
                },
                "authors": [
                    {
                        "name": "Maohao Shen"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Zhenfang Chen"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Gregory Wornell"
                    },
                    {
                        "name": "Subhro Das"
                    },
                    {
                        "name": "David Cox"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22954v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22954v3",
                "updated": "2025-06-02T06:34:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    34,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-30T12:09:29Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    9,
                    29,
                    2,
                    304,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Estimation of Source Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Estimation of Source Reliability"
                },
                "summary": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines."
                },
                "authors": [
                    {
                        "name": "Jeongyeon Hwang"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Hyejin Park"
                    },
                    {
                        "name": "Dongwoo Kim"
                    },
                    {
                        "name": "Sangdon Park"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22954v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22954v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13169v2",
                "updated": "2025-06-02T06:24:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    24,
                    38,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-17T18:46:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    46,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study"
                },
                "summary": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness."
                },
                "authors": [
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Berk Yoztyurk"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Markus Herklotz"
                    },
                    {
                        "name": "Frauke Kreuter"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Assenmacher"
                },
                "author": "Matthias Assenmacher",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07155v2",
                "updated": "2025-06-02T05:57:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    57,
                    53,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-12T00:15:02Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    0,
                    15,
                    2,
                    0,
                    132,
                    0
                ],
                "title": "Reassessing Large Language Model Boolean Query Generation for Systematic\n  Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reassessing Large Language Model Boolean Query Generation for Systematic\n  Reviews"
                },
                "summary": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Harrisen Scells"
                    },
                    {
                        "name": "Bevan Koopman"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "arxiv_comment": "Accepted in SIGIR-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14318v2",
                "updated": "2025-06-02T05:56:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    56,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-20T13:05:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    5,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "RADAR: Enhancing Radiology Report Generation with Supplementary\n  Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADAR: Enhancing Radiology Report Generation with Supplementary\n  Knowledge Injection"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration. To address this\nlimitation, we propose Radar, a framework for enhancing radiology report\ngeneration with supplementary knowledge injection. Radar improves report\ngeneration by systematically leveraging both the internal knowledge of an LLM\nand externally retrieved information. Specifically, it first extracts the\nmodel's acquired knowledge that aligns with expert image-based classification\noutputs. It then retrieves relevant supplementary knowledge to further enrich\nthis information. Finally, by aggregating both sources, Radar generates more\naccurate and informative radiology reports. Extensive experiments on MIMIC-CXR,\nCheXpert-Plus, and IU X-ray demonstrate that our model outperforms\nstate-of-the-art LLMs in both language quality and clinical accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration. To address this\nlimitation, we propose Radar, a framework for enhancing radiology report\ngeneration with supplementary knowledge injection. Radar improves report\ngeneration by systematically leveraging both the internal knowledge of an LLM\nand externally retrieved information. Specifically, it first extracts the\nmodel's acquired knowledge that aligns with expert image-based classification\noutputs. It then retrieves relevant supplementary knowledge to further enrich\nthis information. Finally, by aggregating both sources, Radar generates more\naccurate and informative radiology reports. Extensive experiments on MIMIC-CXR,\nCheXpert-Plus, and IU X-ray demonstrate that our model outperforms\nstate-of-the-art LLMs in both language quality and clinical accuracy."
                },
                "authors": [
                    {
                        "name": "Wenjun Hou"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Kaishuai Xu"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Jiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Liu"
                },
                "author": "Jiang Liu",
                "arxiv_comment": "Accepted to ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02006v2",
                "updated": "2025-06-02T05:17:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    17,
                    34,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-04T11:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    50,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey"
                },
                "summary": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents"
                },
                "authors": [
                    {
                        "name": "Biao Wu"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19630v3",
                "updated": "2025-06-02T05:05:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    5,
                    30,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-27T13:19:35Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    19,
                    35,
                    4,
                    362,
                    0
                ],
                "title": "ATiM: Autotuning Tensor Programs for Processing-in-DRAM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATiM: Autotuning Tensor Programs for Processing-in-DRAM"
                },
                "summary": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present ATiM, a search-based optimizing tensor compiler\nfor UPMEM. Key features of ATiM include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 6.18$\\times$\nfor various UPMEM benchmark kernels and 8.21$\\times$ for GPT-J layers. To the\nbest of our knowledge, ATiM is the first tensor compiler to provide fully\nautomated, autotuning-integrated code generation support for a DRAM-PIM system.\nBy bridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, ATiM establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present ATiM, a search-based optimizing tensor compiler\nfor UPMEM. Key features of ATiM include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 6.18$\\times$\nfor various UPMEM benchmark kernels and 8.21$\\times$ for GPT-J layers. To the\nbest of our knowledge, ATiM is the first tensor compiler to provide fully\nautomated, autotuning-integrated code generation support for a DRAM-PIM system.\nBy bridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, ATiM establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization."
                },
                "authors": [
                    {
                        "name": "Yongwon Shin"
                    },
                    {
                        "name": "Dookyung Kang"
                    },
                    {
                        "name": "Hyojin Sung"
                    }
                ],
                "author_detail": {
                    "name": "Hyojin Sung"
                },
                "author": "Hyojin Sung",
                "arxiv_comment": "17 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23802v2",
                "updated": "2025-06-02T04:19:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    19,
                    10,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-26T22:55:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    22,
                    55,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks"
                },
                "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this."
                },
                "authors": [
                    {
                        "name": "Suhana Bedi"
                    },
                    {
                        "name": "Hejie Cui"
                    },
                    {
                        "name": "Miguel Fuentes"
                    },
                    {
                        "name": "Alyssa Unell"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Juan M. Banda"
                    },
                    {
                        "name": "Nikesh Kotecha"
                    },
                    {
                        "name": "Timothy Keyes"
                    },
                    {
                        "name": "Yifan Mai"
                    },
                    {
                        "name": "Mert Oez"
                    },
                    {
                        "name": "Hao Qiu"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Leonardo Schettini"
                    },
                    {
                        "name": "Mehr Kashyap"
                    },
                    {
                        "name": "Jason Alan Fries"
                    },
                    {
                        "name": "Akshay Swaminathan"
                    },
                    {
                        "name": "Philip Chung"
                    },
                    {
                        "name": "Fateme Nateghi"
                    },
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Ashwin Nayak"
                    },
                    {
                        "name": "Shivam Vedak"
                    },
                    {
                        "name": "Sneha S. Jain"
                    },
                    {
                        "name": "Birju Patel"
                    },
                    {
                        "name": "Oluseyi Fayanju"
                    },
                    {
                        "name": "Shreya Shah"
                    },
                    {
                        "name": "Ethan Goh"
                    },
                    {
                        "name": "Dong-han Yao"
                    },
                    {
                        "name": "Brian Soetikno"
                    },
                    {
                        "name": "Eduardo Reis"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Vasu Divi"
                    },
                    {
                        "name": "Robson Capasso"
                    },
                    {
                        "name": "Rachna Saralkar"
                    },
                    {
                        "name": "Chia-Chun Chiang"
                    },
                    {
                        "name": "Jenelle Jindal"
                    },
                    {
                        "name": "Tho Pham"
                    },
                    {
                        "name": "Faraz Ghoddusi"
                    },
                    {
                        "name": "Steven Lin"
                    },
                    {
                        "name": "Albert S. Chiou"
                    },
                    {
                        "name": "Christy Hong"
                    },
                    {
                        "name": "Mohana Roy"
                    },
                    {
                        "name": "Michael F. Gensheimer"
                    },
                    {
                        "name": "Hinesh Patel"
                    },
                    {
                        "name": "Kevin Schulman"
                    },
                    {
                        "name": "Dev Dash"
                    },
                    {
                        "name": "Danton Char"
                    },
                    {
                        "name": "Lance Downing"
                    },
                    {
                        "name": "Francois Grolleau"
                    },
                    {
                        "name": "Kameron Black"
                    },
                    {
                        "name": "Bethel Mieso"
                    },
                    {
                        "name": "Aydin Zahedivash"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Harshita Sharma"
                    },
                    {
                        "name": "Tony Lee"
                    },
                    {
                        "name": "Hannah Kirsch"
                    },
                    {
                        "name": "Jennifer Lee"
                    },
                    {
                        "name": "Nerissa Ambers"
                    },
                    {
                        "name": "Carlene Lugtu"
                    },
                    {
                        "name": "Aditya Sharma"
                    },
                    {
                        "name": "Bilal Mawji"
                    },
                    {
                        "name": "Alex Alekseyev"
                    },
                    {
                        "name": "Vicky Zhou"
                    },
                    {
                        "name": "Vikas Kakkar"
                    },
                    {
                        "name": "Jarrod Helzer"
                    },
                    {
                        "name": "Anurang Revri"
                    },
                    {
                        "name": "Yair Bannett"
                    },
                    {
                        "name": "Roxana Daneshjou"
                    },
                    {
                        "name": "Jonathan Chen"
                    },
                    {
                        "name": "Emily Alsentzer"
                    },
                    {
                        "name": "Keith Morse"
                    },
                    {
                        "name": "Nirmal Ravi"
                    },
                    {
                        "name": "Nima Aghaeepour"
                    },
                    {
                        "name": "Vanessa Kennedy"
                    },
                    {
                        "name": "Akshay Chaudhari"
                    },
                    {
                        "name": "Thomas Wang"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Matthew P. Lungren"
                    },
                    {
                        "name": "Eric Horvitz"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Mike Pfeffer"
                    },
                    {
                        "name": "Nigam H. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Nigam H. Shah"
                },
                "author": "Nigam H. Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24238v2",
                "updated": "2025-06-02T04:16:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    16,
                    4,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T05:54:36Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    54,
                    36,
                    4,
                    150,
                    0
                ],
                "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM"
                },
                "summary": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models."
                },
                "authors": [
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Minheng Ni"
                    },
                    {
                        "name": "Zitong Huang"
                    },
                    {
                        "name": "Guanglei Yang"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v3",
                "updated": "2025-06-02T04:02:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    2,
                    39,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00212v3",
                "updated": "2025-06-02T03:25:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    3,
                    25,
                    55,
                    0,
                    153,
                    0
                ],
                "published": "2025-04-30T23:09:44Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    23,
                    9,
                    44,
                    2,
                    120,
                    0
                ],
                "title": "Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems"
                },
                "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution"
                },
                "authors": [
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Jieyu Zhang"
                    },
                    {
                        "name": "Jiale Liu"
                    },
                    {
                        "name": "Zhiguang Han"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Qingyun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wu"
                },
                "author": "Qingyun Wu",
                "arxiv_comment": "camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19439v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19439v3",
                "updated": "2025-06-02T03:24:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    3,
                    24,
                    21,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-26T02:56:22Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    2,
                    56,
                    22,
                    0,
                    146,
                    0
                ],
                "title": "Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers"
                },
                "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers. Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: the powerful base\nmodel is like an excellent student who has already mastered mathematical and\nlogical reasoning skills, but performs poorly on the test paper, it simply\nneeds to develop good answering habits to achieve outstanding results in exams,\nto unlock the capabilities it already possesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers. Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: the powerful base\nmodel is like an excellent student who has already mastered mathematical and\nlogical reasoning skills, but performs poorly on the test paper, it simply\nneeds to develop good answering habits to achieve outstanding results in exams,\nto unlock the capabilities it already possesses."
                },
                "authors": [
                    {
                        "name": "Rihui Xin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Zecheng Wang"
                    },
                    {
                        "name": "Yupeng Zhang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Bingning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bingning Wang"
                },
                "author": "Bingning Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19439v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19439v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18970v2",
                "updated": "2025-06-02T02:47:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    47,
                    57,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-25T04:25:28Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    4,
                    25,
                    28,
                    6,
                    145,
                    0
                ],
                "title": "Learning to Explain: Prototype-Based Surrogate Models for LLM\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Explain: Prototype-Based Surrogate Models for LLM\n  Classification"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications."
                },
                "authors": [
                    {
                        "name": "Bowen Wei"
                    },
                    {
                        "name": "Mehrdad Fazli"
                    },
                    {
                        "name": "Ziwei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Zhu"
                },
                "author": "Ziwei Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00600v2",
                "updated": "2025-06-02T02:31:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    31,
                    38,
                    0,
                    153,
                    0
                ],
                "published": "2025-03-01T19:59:25Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    19,
                    59,
                    25,
                    5,
                    60,
                    0
                ],
                "title": "Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented\n  Data Processing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented\n  Data Processing Systems"
                },
                "summary": "AI-augmented data processing systems (DPSs) integrate large language models\n(LLMs) into query pipelines, allowing powerful semantic operations on\nstructured and unstructured data. However, the reliability (a.k.a. trust) of\nthese systems is fundamentally challenged by the potential for LLMs to produce\nerrors, limiting their adoption in critical domains. To help address this\nreliability bottleneck, we introduce semantic integrity constraints (SICs) -- a\ndeclarative abstraction for specifying and enforcing correctness conditions\nover LLM outputs in semantic queries. SICs generalize traditional database\nintegrity constraints to semantic settings, supporting common types of\nconstraints, such as grounding, soundness, and exclusion, with both proactive\nand reactive enforcement strategies.\n  We argue that SICs provide a foundation for building reliable and auditable\nAI-augmented data systems. Specifically, we present a system design for\nintegrating SICs into query planning and runtime execution and discuss its\nrealization in AI-augmented DPSs. To guide and evaluate the vision, we outline\nseveral design goals -- covering criteria around expressiveness, runtime\nsemantics, integration, performance, and enterprise-scale applicability -- and\ndiscuss how our framework addresses each, along with open research challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-augmented data processing systems (DPSs) integrate large language models\n(LLMs) into query pipelines, allowing powerful semantic operations on\nstructured and unstructured data. However, the reliability (a.k.a. trust) of\nthese systems is fundamentally challenged by the potential for LLMs to produce\nerrors, limiting their adoption in critical domains. To help address this\nreliability bottleneck, we introduce semantic integrity constraints (SICs) -- a\ndeclarative abstraction for specifying and enforcing correctness conditions\nover LLM outputs in semantic queries. SICs generalize traditional database\nintegrity constraints to semantic settings, supporting common types of\nconstraints, such as grounding, soundness, and exclusion, with both proactive\nand reactive enforcement strategies.\n  We argue that SICs provide a foundation for building reliable and auditable\nAI-augmented data systems. Specifically, we present a system design for\nintegrating SICs into query planning and runtime execution and discuss its\nrealization in AI-augmented DPSs. To guide and evaluate the vision, we outline\nseveral design goals -- covering criteria around expressiveness, runtime\nsemantics, integration, performance, and enterprise-scale applicability -- and\ndiscuss how our framework addresses each, along with open research challenges."
                },
                "authors": [
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Justin Chan"
                    },
                    {
                        "name": "Michael Fu"
                    },
                    {
                        "name": "Nicolas Kim"
                    },
                    {
                        "name": "Akshay Mehta"
                    },
                    {
                        "name": "Deepti Raghavan"
                    },
                    {
                        "name": "Ugur Cetintemel"
                    }
                ],
                "author_detail": {
                    "name": "Ugur Cetintemel"
                },
                "author": "Ugur Cetintemel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11693v3",
                "updated": "2025-06-02T02:28:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    28,
                    27,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-15T15:26:28Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    26,
                    28,
                    1,
                    289,
                    0
                ],
                "title": "BridG MT: Enhancing LLMs' Machine Translation Capabilities with Sentence\n  Bridging and Gradual MT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BridG MT: Enhancing LLMs' Machine Translation Capabilities with Sentence\n  Bridging and Gradual MT"
                },
                "summary": "Recent Large Language Models (LLMs) have demonstrated impressive translation\nperformance without requiring fine-tuning on additional parallel corpora.\nHowever, they still face significant challenges in certain scenarios,\nparticularly when translating low-resource languages. A common approach to\naddress this issue is to provide external knowledge, such as few-shot examples,\nto assist LLMs in translating specific source sentences. However, this method\nis fundamentally limited by the quality or quantity of relevant sources, which\ncannot always be guaranteed. To reduce LLMs' reliance on external sources, we\npropose BridG MT, a method that combines Sentence Bridging, which generates a\nsequence of sentences as a bridge that gradually transition from\neasy-to-translate to more difficult, and Gradual MT, which sequentially\ntranslates these sentences using earlier translations as few-shot examples for\nsubsequent ones. Experiments conducted on four LLMs across seven languages\ndemonstrate that our method effectively enhances translation performance, even\noutperforming translation methods that rely on a large number of few-shot\nexamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Models (LLMs) have demonstrated impressive translation\nperformance without requiring fine-tuning on additional parallel corpora.\nHowever, they still face significant challenges in certain scenarios,\nparticularly when translating low-resource languages. A common approach to\naddress this issue is to provide external knowledge, such as few-shot examples,\nto assist LLMs in translating specific source sentences. However, this method\nis fundamentally limited by the quality or quantity of relevant sources, which\ncannot always be guaranteed. To reduce LLMs' reliance on external sources, we\npropose BridG MT, a method that combines Sentence Bridging, which generates a\nsequence of sentences as a bridge that gradually transition from\neasy-to-translate to more difficult, and Gradual MT, which sequentially\ntranslates these sentences using earlier translations as few-shot examples for\nsubsequent ones. Experiments conducted on four LLMs across seven languages\ndemonstrate that our method effectively enhances translation performance, even\noutperforming translation methods that rely on a large number of few-shot\nexamples."
                },
                "authors": [
                    {
                        "name": "Seung-Woo Choi"
                    },
                    {
                        "name": "Ga-Hyun Yoo"
                    },
                    {
                        "name": "Jay-Yoon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jay-Yoon Lee"
                },
                "author": "Jay-Yoon Lee",
                "arxiv_comment": "Accepted to ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04529v2",
                "updated": "2025-06-02T02:27:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    27,
                    34,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-07T16:02:46Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    2,
                    46,
                    2,
                    127,
                    0
                ],
                "title": "RAFT: Robust Augmentation of FeaTures for Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAFT: Robust Augmentation of FeaTures for Image Segmentation"
                },
                "summary": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU."
                },
                "authors": [
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Uttej Kallakuri"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02160v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02160v3",
                "updated": "2025-06-02T02:16:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    16,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2024-02-03T14:20:20Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    14,
                    20,
                    20,
                    5,
                    34,
                    0
                ],
                "title": "Data Poisoning for In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Poisoning for In-context Learning"
                },
                "summary": "In the domain of large language models (LLMs), in-context learning (ICL) has\nbeen recognized for its innovative ability to adapt to new tasks, relying on\nexamples rather than retraining or fine-tuning. This paper delves into the\ncritical issue of ICL's susceptibility to data poisoning attacks, an area not\nyet fully explored. We wonder whether ICL is vulnerable, with adversaries\ncapable of manipulating example data to degrade model performance. To address\nthis, we introduce ICLPoison, a specialized attacking framework conceived to\nexploit the learning mechanisms of ICL. Our approach uniquely employs discrete\ntext perturbations to strategically influence the hidden states of LLMs during\nthe ICL process. We outline three representative strategies to implement\nattacks under our framework, each rigorously evaluated across a variety of\nmodels and tasks. Our comprehensive tests, including trials on the\nsophisticated GPT-4 model, demonstrate that ICL's performance is significantly\ncompromised under our framework. These revelations indicate an urgent need for\nenhanced defense mechanisms to safeguard the integrity and reliability of LLMs\nin applications relying on in-context learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of large language models (LLMs), in-context learning (ICL) has\nbeen recognized for its innovative ability to adapt to new tasks, relying on\nexamples rather than retraining or fine-tuning. This paper delves into the\ncritical issue of ICL's susceptibility to data poisoning attacks, an area not\nyet fully explored. We wonder whether ICL is vulnerable, with adversaries\ncapable of manipulating example data to degrade model performance. To address\nthis, we introduce ICLPoison, a specialized attacking framework conceived to\nexploit the learning mechanisms of ICL. Our approach uniquely employs discrete\ntext perturbations to strategically influence the hidden states of LLMs during\nthe ICL process. We outline three representative strategies to implement\nattacks under our framework, each rigorously evaluated across a variety of\nmodels and tasks. Our comprehensive tests, including trials on the\nsophisticated GPT-4 model, demonstrate that ICL's performance is significantly\ncompromised under our framework. These revelations indicate an urgent need for\nenhanced defense mechanisms to safeguard the integrity and reliability of LLMs\nin applications relying on in-context learning."
                },
                "authors": [
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Makoto Yamada"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02160v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02160v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23657v2",
                "updated": "2025-06-02T02:11:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    11,
                    11,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T17:07:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    7,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation"
                },
                "summary": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v3",
                "updated": "2025-06-02T02:08:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    8,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09411v2",
                "updated": "2025-06-02T02:02:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    2,
                    33,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-12T07:38:01Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    38,
                    1,
                    5,
                    286,
                    0
                ],
                "title": "Towards the Effect of Examples on In-Context Learning: A Theoretical\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Effect of Examples on In-Context Learning: A Theoretical\n  Case Study"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful capability for large\nlanguage models (LLMs) to adapt to downstream tasks by leveraging a few\n(demonstration) examples. Despite its effectiveness, the mechanism behind ICL\nremains underexplored. To better understand how ICL integrates the examples\nwith the knowledge learned by the LLM during pre-training (i.e., pre-training\nknowledge) and how the examples impact ICL, this paper conducts a theoretical\nstudy in binary classification tasks. In particular, we introduce a\nprobabilistic model extending from the Gaussian mixture model to exactly\nquantify the impact of pre-training knowledge, label frequency, and label noise\non the prediction accuracy. Based on our analysis, when the pre-training\nknowledge contradicts the knowledge in the examples, whether ICL prediction\nrelies more on the pre-training knowledge or the examples depends on the number\nof examples. In addition, the label frequency and label noise of the examples\nboth affect the accuracy of the ICL prediction, where the minor class has a\nlower accuracy, and how the label noise impacts the accuracy is determined by\nthe specific noise level of the two classes. Extensive simulations are\nconducted to verify the correctness of the theoretical results, and real-data\nexperiments also align with the theoretical insights. Our work reveals the role\nof pre-training knowledge and examples in ICL, offering a deeper understanding\nof LLMs' behaviors in classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful capability for large\nlanguage models (LLMs) to adapt to downstream tasks by leveraging a few\n(demonstration) examples. Despite its effectiveness, the mechanism behind ICL\nremains underexplored. To better understand how ICL integrates the examples\nwith the knowledge learned by the LLM during pre-training (i.e., pre-training\nknowledge) and how the examples impact ICL, this paper conducts a theoretical\nstudy in binary classification tasks. In particular, we introduce a\nprobabilistic model extending from the Gaussian mixture model to exactly\nquantify the impact of pre-training knowledge, label frequency, and label noise\non the prediction accuracy. Based on our analysis, when the pre-training\nknowledge contradicts the knowledge in the examples, whether ICL prediction\nrelies more on the pre-training knowledge or the examples depends on the number\nof examples. In addition, the label frequency and label noise of the examples\nboth affect the accuracy of the ICL prediction, where the minor class has a\nlower accuracy, and how the label noise impacts the accuracy is determined by\nthe specific noise level of the two classes. Extensive simulations are\nconducted to verify the correctness of the theoretical results, and real-data\nexperiments also align with the theoretical insights. Our work reveals the role\nof pre-training knowledge and examples in ICL, offering a deeper understanding\nof LLMs' behaviors in classification tasks."
                },
                "authors": [
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Yingqian Cui"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Makoto Yamada"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Yue Xing"
                    }
                ],
                "author_detail": {
                    "name": "Yue Xing"
                },
                "author": "Yue Xing",
                "arxiv_comment": "Accepted to Stat. Vol 14, Issue 1. Presented on JSM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24785v2",
                "updated": "2025-06-02T01:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    59,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T16:46:29Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    46,
                    29,
                    4,
                    150,
                    0
                ],
                "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXP-Bench: Can AI Conduct AI Research Experiments?"
                },
                "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench."
                },
                "authors": [
                    {
                        "name": "Patrick Tser Jern Kon"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Xinyi Zhu"
                    },
                    {
                        "name": "Qiuyi Ding"
                    },
                    {
                        "name": "Jingjia Peng"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Yibo Huang"
                    },
                    {
                        "name": "Yiming Qiu"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Ang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ang Chen"
                },
                "author": "Ang Chen",
                "arxiv_comment": "45 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23807v3",
                "updated": "2025-06-03T03:06:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    6,
                    29,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-27T07:35:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    7,
                    35,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLP: Dynamic Layerwise Pruning in Large Language Models"
                },
                "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yuli Chen"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Jiale Han"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Yingting Li"
                    },
                    {
                        "name": "Shuhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuhao Zhang"
                },
                "author": "Shuhao Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14847v2",
                "updated": "2025-06-02T01:51:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    51,
                    9,
                    0,
                    153,
                    0
                ],
                "published": "2025-02-20T18:55:39Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    39,
                    3,
                    51,
                    0
                ],
                "title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks"
                },
                "summary": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Yupin Lin"
                    },
                    {
                        "name": "Shen Dong"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24034v2",
                "updated": "2025-06-02T01:49:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    49,
                    51,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-29T22:14:15Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    22,
                    14,
                    15,
                    3,
                    149,
                    0
                ],
                "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for\n  Efficient Large-scale LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for\n  Efficient Large-scale LLM Training"
                },
                "summary": "Reinforcement Learning (RL) has become the most effective post-training\napproach for improving the capabilities of Large Language Models (LLMs). In\npractice, because of the high demands on latency and memory, it is particularly\nchallenging to develop an efficient RL framework that reliably manages policy\nmodels with hundreds to thousands of billions of parameters.\n  In this paper, we present LlamaRL, a fully distributed, asynchronous RL\nframework optimized for efficient training of large-scale LLMs with various\nmodel sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a\nhandful to thousands of devices. LlamaRL introduces a streamlined,\nsingle-controller architecture built entirely on native PyTorch, enabling\nmodularity, ease of use, and seamless scalability to thousands of GPUs. We also\nprovide a theoretical analysis of LlamaRL's efficiency, including a formal\nproof that its asynchronous design leads to strict RL speed-up. Empirically\nduring the Llama 3 post-training, by leveraging best practices such as\ncolocated model offloading, asynchronous off-policy training, and distributed\ndirect memory access for weight synchronization, LlamaRL achieves significant\nefficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like\nsystems on a 405B-parameter policy model. Furthermore, the efficiency advantage\ncontinues to grow with increasing model scale, demonstrating the framework's\nsuitability for future large-scale RL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become the most effective post-training\napproach for improving the capabilities of Large Language Models (LLMs). In\npractice, because of the high demands on latency and memory, it is particularly\nchallenging to develop an efficient RL framework that reliably manages policy\nmodels with hundreds to thousands of billions of parameters.\n  In this paper, we present LlamaRL, a fully distributed, asynchronous RL\nframework optimized for efficient training of large-scale LLMs with various\nmodel sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a\nhandful to thousands of devices. LlamaRL introduces a streamlined,\nsingle-controller architecture built entirely on native PyTorch, enabling\nmodularity, ease of use, and seamless scalability to thousands of GPUs. We also\nprovide a theoretical analysis of LlamaRL's efficiency, including a formal\nproof that its asynchronous design leads to strict RL speed-up. Empirically\nduring the Llama 3 post-training, by leveraging best practices such as\ncolocated model offloading, asynchronous off-policy training, and distributed\ndirect memory access for weight synchronization, LlamaRL achieves significant\nefficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like\nsystems on a 405B-parameter policy model. Furthermore, the efficiency advantage\ncontinues to grow with increasing model scale, demonstrating the framework's\nsuitability for future large-scale RL training."
                },
                "authors": [
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Sid Wang"
                    },
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Jia Ding"
                    },
                    {
                        "name": "Eryk Helenowski"
                    },
                    {
                        "name": "Liang Tan"
                    },
                    {
                        "name": "Tengyu Xu"
                    },
                    {
                        "name": "Tushar Gowda"
                    },
                    {
                        "name": "Zhengxing Chen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Xiaocheng Tang"
                    },
                    {
                        "name": "Yundi Qian"
                    },
                    {
                        "name": "Beibei Zhu"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08534v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08534v3",
                "updated": "2025-06-02T01:21:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    21,
                    35,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-13T11:31:02Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    11,
                    31,
                    2,
                    2,
                    318,
                    0
                ],
                "title": "Neural Topic Modeling with Large Language Models in the Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Topic Modeling with Large Language Models in the Loop"
                },
                "summary": "Topic modeling is a fundamental task in natural language processing, allowing\nthe discovery of latent thematic structures in text corpora. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities in topic\ndiscovery, their direct application to topic modeling suffers from issues such\nas incomplete topic coverage, misalignment of topics, and inefficiency. To\naddress these limitations, we propose LLM-ITL, a novel LLM-in-the-loop\nframework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,\nglobal topics and document representations are learned through the NTM.\nMeanwhile, an LLM refines these topics using an Optimal Transport (OT)-based\nalignment objective, where the refinement is dynamically adjusted based on the\nLLM's confidence in suggesting topical words for each set of input words. With\nthe flexibility of being integrated into many existing NTMs, the proposed\napproach enhances the interpretability of topics while preserving the\nefficiency of NTMs in learning topics and document representations. Extensive\nexperiments demonstrate that LLM-ITL helps NTMs significantly improve their\ntopic interpretability while maintaining the quality of document\nrepresentation. Our code and datasets are available at\nhttps://github.com/Xiaohao-Yang/LLM-ITL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic modeling is a fundamental task in natural language processing, allowing\nthe discovery of latent thematic structures in text corpora. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities in topic\ndiscovery, their direct application to topic modeling suffers from issues such\nas incomplete topic coverage, misalignment of topics, and inefficiency. To\naddress these limitations, we propose LLM-ITL, a novel LLM-in-the-loop\nframework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,\nglobal topics and document representations are learned through the NTM.\nMeanwhile, an LLM refines these topics using an Optimal Transport (OT)-based\nalignment objective, where the refinement is dynamically adjusted based on the\nLLM's confidence in suggesting topical words for each set of input words. With\nthe flexibility of being integrated into many existing NTMs, the proposed\napproach enhances the interpretability of topics while preserving the\nefficiency of NTMs in learning topics and document representations. Extensive\nexperiments demonstrate that LLM-ITL helps NTMs significantly improve their\ntopic interpretability while maintaining the quality of document\nrepresentation. Our code and datasets are available at\nhttps://github.com/Xiaohao-Yang/LLM-ITL"
                },
                "authors": [
                    {
                        "name": "Xiaohao Yang"
                    },
                    {
                        "name": "He Zhao"
                    },
                    {
                        "name": "Weijie Xu"
                    },
                    {
                        "name": "Yuanyuan Qi"
                    },
                    {
                        "name": "Jueqing Lu"
                    },
                    {
                        "name": "Dinh Phung"
                    },
                    {
                        "name": "Lan Du"
                    }
                ],
                "author_detail": {
                    "name": "Lan Du"
                },
                "author": "Lan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08534v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08534v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20368v2",
                "updated": "2025-06-02T01:12:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    12,
                    15,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-26T11:08:23Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    8,
                    23,
                    0,
                    146,
                    0
                ],
                "title": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial\n  Question Answering on Standardized Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial\n  Question Answering on Standardized Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) based large language models (LLMs) are\nwidely used in finance for their excellent performance on knowledge-intensive\ntasks. However, standardized documents (e.g., SEC filing) share similar formats\nsuch as repetitive boilerplate texts, and similar table structures. This\nsimilarity forces traditional RAG methods to misidentify near-duplicate text,\nleading to duplicate retrieval that undermines accuracy and completeness. To\naddress these issues, we propose the Hierarchical Retrieval with Evidence\nCuration (HiREC) framework. Our approach first performs hierarchical retrieval\nto reduce confusion among similar texts. It first retrieve related documents\nand then selects the most relevant passages from the documents. The evidence\ncuration process removes irrelevant passages. When necessary, it automatically\ngenerates complementary queries to collect missing information. To evaluate our\napproach, we construct and release a Large-scale Open-domain Financial (LOFin)\nquestion answering benchmark that includes 145,897 SEC documents and 1,595\nquestion-answer pairs. Our code and data are available at\nhttps://github.com/deep-over/LOFin-bench-HiREC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) based large language models (LLMs) are\nwidely used in finance for their excellent performance on knowledge-intensive\ntasks. However, standardized documents (e.g., SEC filing) share similar formats\nsuch as repetitive boilerplate texts, and similar table structures. This\nsimilarity forces traditional RAG methods to misidentify near-duplicate text,\nleading to duplicate retrieval that undermines accuracy and completeness. To\naddress these issues, we propose the Hierarchical Retrieval with Evidence\nCuration (HiREC) framework. Our approach first performs hierarchical retrieval\nto reduce confusion among similar texts. It first retrieve related documents\nand then selects the most relevant passages from the documents. The evidence\ncuration process removes irrelevant passages. When necessary, it automatically\ngenerates complementary queries to collect missing information. To evaluate our\napproach, we construct and release a Large-scale Open-domain Financial (LOFin)\nquestion answering benchmark that includes 145,897 SEC documents and 1,595\nquestion-answer pairs. Our code and data are available at\nhttps://github.com/deep-over/LOFin-bench-HiREC."
                },
                "authors": [
                    {
                        "name": "Jaeyoung Choe"
                    },
                    {
                        "name": "Jihoon Kim"
                    },
                    {
                        "name": "Woohwan Jung"
                    }
                ],
                "author_detail": {
                    "name": "Woohwan Jung"
                },
                "author": "Woohwan Jung",
                "arxiv_comment": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v2",
                "updated": "2025-06-02T01:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    8,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08291v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08291v4",
                "updated": "2025-06-02T00:45:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    0,
                    45,
                    4,
                    0,
                    153,
                    0
                ],
                "published": "2024-03-13T06:54:15Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    6,
                    54,
                    15,
                    2,
                    73,
                    0
                ],
                "title": "CleanAgent: Automating Data Standardization with LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanAgent: Automating Data Standardization with LLM-based Agents"
                },
                "summary": "Data standardization is a crucial part of the data science life cycle. While\ntools like Pandas offer robust functionalities, their complexity and the manual\neffort required for customizing code to diverse column types pose significant\nchallenges. Although large language models (LLMs) like ChatGPT have shown\npromise in automating this process through natural language understanding and\ncode generation, it still demands expert-level programming knowledge and\ncontinuous interaction for prompt refinement. To solve these challenges, our\nkey idea is to propose a Python library with declarative, unified APIs for\nstandardizing different column types, simplifying the LLM's code generation\nwith concise API calls. We first propose Dataprep.Clean, a component of the\nDataprep Python Library, significantly reduces the coding complexity by\nenabling the standardization of specific column types with a single line of\ncode. Then, we introduce the CleanAgent framework integrating Dataprep.Clean\nand LLM-based agents to automate the data standardization process. With\nCleanAgent, data scientists only need to provide their requirements once,\nallowing for a hands-free process. To demonstrate the practical utility of\nCleanAgent, we developed a user-friendly web application, allowing users to\ninteract with it using real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data standardization is a crucial part of the data science life cycle. While\ntools like Pandas offer robust functionalities, their complexity and the manual\neffort required for customizing code to diverse column types pose significant\nchallenges. Although large language models (LLMs) like ChatGPT have shown\npromise in automating this process through natural language understanding and\ncode generation, it still demands expert-level programming knowledge and\ncontinuous interaction for prompt refinement. To solve these challenges, our\nkey idea is to propose a Python library with declarative, unified APIs for\nstandardizing different column types, simplifying the LLM's code generation\nwith concise API calls. We first propose Dataprep.Clean, a component of the\nDataprep Python Library, significantly reduces the coding complexity by\nenabling the standardization of specific column types with a single line of\ncode. Then, we introduce the CleanAgent framework integrating Dataprep.Clean\nand LLM-based agents to automate the data standardization process. With\nCleanAgent, data scientists only need to provide their requirements once,\nallowing for a hands-free process. To demonstrate the practical utility of\nCleanAgent, we developed a user-friendly web application, allowing users to\ninteract with it using real-world datasets."
                },
                "authors": [
                    {
                        "name": "Danrui Qi"
                    },
                    {
                        "name": "Zhengjie Miao"
                    },
                    {
                        "name": "Jiannan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiannan Wang"
                },
                "author": "Jiannan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08291v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08291v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18547v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18547v5",
                "updated": "2025-06-02T00:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    0,
                    44,
                    9,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-24T16:55:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Token-Budget-Aware LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Budget-Aware LLM Reasoning"
                },
                "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance\nLLM performance by decomposing problems into intermediate steps, they also\nincur significant overhead in token usage, leading to increased costs. We find\nthat the reasoning process of current LLMs is unnecessarily lengthy and it can\nbe compressed by including a reasonable token budget in the prompt, but the\nchoice of token budget plays a crucial role in the actual compression\neffectiveness. We then propose a token-budget-aware LLM reasoning framework\nthat dynamically adjusts the number of reasoning tokens based on the reasoning\ncomplexity of each problem. Experiments show that our method effectively\nreduces token costs in CoT reasoning with only a slight performance reduction,\noffering a practical solution to balance efficiency and accuracy in LLM\nreasoning. Code: https://github.com/GeniusHTX/TALE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance\nLLM performance by decomposing problems into intermediate steps, they also\nincur significant overhead in token usage, leading to increased costs. We find\nthat the reasoning process of current LLMs is unnecessarily lengthy and it can\nbe compressed by including a reasonable token budget in the prompt, but the\nchoice of token budget plays a crucial role in the actual compression\neffectiveness. We then propose a token-budget-aware LLM reasoning framework\nthat dynamically adjusts the number of reasoning tokens based on the reasoning\ncomplexity of each problem. Experiments show that our method effectively\nreduces token costs in CoT reasoning with only a slight performance reduction,\noffering a practical solution to balance efficiency and accuracy in LLM\nreasoning. Code: https://github.com/GeniusHTX/TALE"
                },
                "authors": [
                    {
                        "name": "Tingxu Han"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18547v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18547v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18492v3",
                "updated": "2025-06-03T03:40:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    40,
                    51,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-24T03:52:25Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    3,
                    52,
                    25,
                    5,
                    144,
                    0
                ],
                "title": "Enumerate-Conjecture-Prove: Formally Solving Answer-Construction\n  Problems in Math Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enumerate-Conjecture-Prove: Formally Solving Answer-Construction\n  Problems in Math Competitions"
                },
                "summary": "Mathematical reasoning lies at the heart of artificial intelligence,\nunderpinning applications in education, program verification, and\nresearch-level mathematical discovery. Mathematical competitions, in\nparticular, present two challenging problem types: theorem proving, which\nrequires rigorous proofs of stated conclusions, and answer construction, which\ninvolves hypothesizing and formally verifying mathematical objects. Large\nLanguage Models (LLMs) effectively generate creative candidate answers but\nstruggle with formal verification, while symbolic provers ensure rigor but\ncannot efficiently handle creative conjecture generation. We introduce the\nEnumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method\nintegrating LLM-based enumeration and pattern-driven conjecturing with formal\ntheorem proving. We present ConstructiveBench, a dataset of 3,431\nanswer-construction problems in various math competitions with verified Lean\nformalizations. On the ConstructiveBench dataset, ECP improves the accuracy of\nanswer construction from a Chain-of-Thought (CoT) baseline of 14.54% to 45.06%\nwith the gpt-4.1-mini model. Moreover, combined with ECP's constructed answers,\nthe state-of-the-art DeepSeek-Prover-V2-7B model generates correct proofs for\n858 of the 3,431 constructive problems in Lean, achieving 25.01% accuracy\ncompared to 9.86% for symbolic-only baselines. Our code and dataset are\npublicly available at https://github.com/JackSun200312/ECP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning lies at the heart of artificial intelligence,\nunderpinning applications in education, program verification, and\nresearch-level mathematical discovery. Mathematical competitions, in\nparticular, present two challenging problem types: theorem proving, which\nrequires rigorous proofs of stated conclusions, and answer construction, which\ninvolves hypothesizing and formally verifying mathematical objects. Large\nLanguage Models (LLMs) effectively generate creative candidate answers but\nstruggle with formal verification, while symbolic provers ensure rigor but\ncannot efficiently handle creative conjecture generation. We introduce the\nEnumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method\nintegrating LLM-based enumeration and pattern-driven conjecturing with formal\ntheorem proving. We present ConstructiveBench, a dataset of 3,431\nanswer-construction problems in various math competitions with verified Lean\nformalizations. On the ConstructiveBench dataset, ECP improves the accuracy of\nanswer construction from a Chain-of-Thought (CoT) baseline of 14.54% to 45.06%\nwith the gpt-4.1-mini model. Moreover, combined with ECP's constructed answers,\nthe state-of-the-art DeepSeek-Prover-V2-7B model generates correct proofs for\n858 of the 3,431 constructive problems in Lean, achieving 25.01% accuracy\ncompared to 9.86% for symbolic-only baselines. Our code and dataset are\npublicly available at https://github.com/JackSun200312/ECP."
                },
                "authors": [
                    {
                        "name": "Jialiang Sun"
                    },
                    {
                        "name": "Yuzhi Tang"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Chris J. Maddison"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13089v2",
                "updated": "2025-06-01T23:03:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    3,
                    12,
                    6,
                    152,
                    0
                ],
                "published": "2025-03-17T11:52:16Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    52,
                    16,
                    0,
                    76,
                    0
                ],
                "title": "ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning"
                },
                "summary": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Seyyed Hadi Hashemi"
                    },
                    {
                        "name": "Stefan Vasilev"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "ACL camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16415v2",
                "updated": "2025-06-01T22:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    22,
                    42,
                    30,
                    6,
                    152,
                    0
                ],
                "published": "2025-05-22T09:04:03Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    4,
                    3,
                    3,
                    142,
                    0
                ],
                "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven\n  Mechanistic Study of Context Attribution in Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attributing Response to Context: A Jensen-Shannon Divergence Driven\n  Mechanistic Study of Context Attribution in Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels. Our code is available at https://github.com/ruizheliUOA/ARC_JSD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels. Our code is available at https://github.com/ruizheliUOA/ARC_JSD"
                },
                "authors": [
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Emine Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Emine Yilmaz"
                },
                "author": "Emine Yilmaz",
                "arxiv_comment": "Work in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]