[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v1",
                "updated": "2024-10-05T03:47:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Lukáš Ondráček"
                    },
                    {
                        "name": "Ondřej Mička"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Mička"
                },
                "author": "Ondřej Mička",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v2",
                "updated": "2024-09-27T03:31:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    31,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.07177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07177v1",
                "updated": "2024-10-09T17:59:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Ego: Towards Building Egocentric Multimodal LLMs"
                },
                "summary": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding."
                },
                "authors": [
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Jiasen Lu"
                    },
                    {
                        "name": "Yinfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Yang"
                },
                "author": "Yinfei Yang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07176v1",
                "updated": "2024-10-09T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    58,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Xingchen Wan"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Sercan Ö. Arık"
                    }
                ],
                "author_detail": {
                    "name": "Sercan Ö. Arık"
                },
                "author": "Sercan Ö. Arık",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07173v1",
                "updated": "2024-10-09T17:59:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:33Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    33,
                    2,
                    283,
                    0
                ],
                "title": "Do better language models have crisper vision?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do better language models have crisper vision?"
                },
                "summary": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released."
                },
                "authors": [
                    {
                        "name": "Jona Ruthardt"
                    },
                    {
                        "name": "Gertjan J. Burghouts"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07172v1",
                "updated": "2024-10-09T17:59:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    14,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:14Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    14,
                    2,
                    283,
                    0
                ],
                "title": "Glider: Global and Local Instruction-Driven Expert Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glider: Global and Local Instruction-Driven Expert Router"
                },
                "summary": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to particular domains. This\nhas enabled the creation of powerful and adaptive routing-based \"Model\nMoErging\" methods with the goal of using expert modules to create an aggregate\nsystem with improved performance or generalization. However, existing MoErging\nmethods often prioritize generalization to unseen tasks at the expense of\nperformance on held-in tasks, which limits its practical applicability in\nreal-world deployment scenarios. We observe that current token-level routing\nmechanisms neglect the global semantic context of the input task. This\ntoken-wise independence hinders effective expert selection for held-in tasks,\nas routing decisions fail to incorporate the semantic properties of the task.\nTo address this, we propose, Global and Local Instruction Driven Expert Router\n(GLIDER) that integrates a multi-scale routing mechanism, encompassing a\nsemantic global router and a learned local router. The global router leverages\nLLM's advanced reasoning capabilities for semantic-related contexts to enhance\nexpert selection. Given the input query and LLM, the router generates semantic\ntask instructions that guide the retrieval of the most relevant experts across\nall layers. This global guidance is complemented by a local router that\nfacilitates token-level routing decisions within each module, enabling finer\ncontrol and enhanced performance on unseen tasks. Our experiments using\nT5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves\nsubstantially improved held-in performance while maintaining strong\ngeneralization on held-out tasks. We also perform ablations experiments to dive\ndeeper into the components of GLIDER. Our experiments highlight the importance\nof our multi-scale routing that leverages LLM-driven semantic reasoning for\nMoErging methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to particular domains. This\nhas enabled the creation of powerful and adaptive routing-based \"Model\nMoErging\" methods with the goal of using expert modules to create an aggregate\nsystem with improved performance or generalization. However, existing MoErging\nmethods often prioritize generalization to unseen tasks at the expense of\nperformance on held-in tasks, which limits its practical applicability in\nreal-world deployment scenarios. We observe that current token-level routing\nmechanisms neglect the global semantic context of the input task. This\ntoken-wise independence hinders effective expert selection for held-in tasks,\nas routing decisions fail to incorporate the semantic properties of the task.\nTo address this, we propose, Global and Local Instruction Driven Expert Router\n(GLIDER) that integrates a multi-scale routing mechanism, encompassing a\nsemantic global router and a learned local router. The global router leverages\nLLM's advanced reasoning capabilities for semantic-related contexts to enhance\nexpert selection. Given the input query and LLM, the router generates semantic\ntask instructions that guide the retrieval of the most relevant experts across\nall layers. This global guidance is complemented by a local router that\nfacilitates token-level routing decisions within each module, enabling finer\ncontrol and enhanced performance on unseen tasks. Our experiments using\nT5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves\nsubstantially improved held-in performance while maintaining strong\ngeneralization on held-out tasks. We also perform ablations experiments to dive\ndeeper into the components of GLIDER. Our experiments highlight the importance\nof our multi-scale routing that leverages LLM-driven semantic reasoning for\nMoErging methods."
                },
                "authors": [
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Prateek Yadav"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Yi-Lin Sung"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "Our code is available at https://github.com/UNITES-Lab/glider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07167v1",
                "updated": "2024-10-09T17:59:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    4,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:04Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    4,
                    2,
                    283,
                    0
                ],
                "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate"
                },
                "summary": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate."
                },
                "authors": [
                    {
                        "name": "Qidong Huang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "Project page: https://github.com/shikiw/Modality-Integration-Rate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07166v1",
                "updated": "2024-10-09T17:59:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    0,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:00Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    0,
                    2,
                    283,
                    0
                ],
                "title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making"
                },
                "summary": "We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making."
                },
                "authors": [
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Qineng Wang"
                    },
                    {
                        "name": "Kangrui Wang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Sanjana Srivastava"
                    },
                    {
                        "name": "Cem Gokmen"
                    },
                    {
                        "name": "Tony Lee"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Ruohan Zhang"
                    },
                    {
                        "name": "Weiyu Liu"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Li Fei-Fei"
                    },
                    {
                        "name": "Jiayuan Mao"
                    },
                    {
                        "name": "Jiajun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Wu"
                },
                "author": "Jiajun Wu",
                "arxiv_comment": "Accepted for oral presentation at NeurIPS 2024 in the Datasets and\n  Benchmarks track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07164v1",
                "updated": "2024-10-09T17:58:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    56,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:58:56Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    56,
                    2,
                    283,
                    0
                ],
                "title": "AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation"
                },
                "summary": "Recent advancements in diffusion models have led to significant improvements\nin the generation and animation of 4D full-body human-object interactions\n(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion\ngeneration, which is limited by the scarcity of realistic large-scale\ninteraction data. This constraint affects their ability to create everyday HOI\nscenes. This paper addresses this challenge using a zero-shot approach with a\npre-trained diffusion model. Despite this potential, achieving our goals is\ndifficult due to the diffusion model's lack of understanding of ''where'' and\n''how'' objects interact with the human body. To tackle these issues, we\nintroduce AvatarGO, a novel framework designed to generate animatable 4D HOI\nscenes directly from textual inputs. Specifically, 1) for the ''where''\nchallenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to\nidentify the contact body part from text prompts, ensuring precise\nrepresentation of human-object spatial relations. 2) For the ''how'' challenge,\nwe introduce correspondence-aware motion optimization that constructs motion\nfields for both human and object models using the linear blend skinning\nfunction from SMPL-X. Our framework not only generates coherent compositional\nmotions, but also exhibits greater robustness in handling penetration issues.\nExtensive experiments with existing methods validate AvatarGO's superior\ngeneration and animation capabilities on a variety of human-object pairs and\ndiverse poses. As the first attempt to synthesize 4D avatars with object\ninteractions, we hope AvatarGO could open new doors for human-centric 4D\ncontent creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion models have led to significant improvements\nin the generation and animation of 4D full-body human-object interactions\n(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion\ngeneration, which is limited by the scarcity of realistic large-scale\ninteraction data. This constraint affects their ability to create everyday HOI\nscenes. This paper addresses this challenge using a zero-shot approach with a\npre-trained diffusion model. Despite this potential, achieving our goals is\ndifficult due to the diffusion model's lack of understanding of ''where'' and\n''how'' objects interact with the human body. To tackle these issues, we\nintroduce AvatarGO, a novel framework designed to generate animatable 4D HOI\nscenes directly from textual inputs. Specifically, 1) for the ''where''\nchallenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to\nidentify the contact body part from text prompts, ensuring precise\nrepresentation of human-object spatial relations. 2) For the ''how'' challenge,\nwe introduce correspondence-aware motion optimization that constructs motion\nfields for both human and object models using the linear blend skinning\nfunction from SMPL-X. Our framework not only generates coherent compositional\nmotions, but also exhibits greater robustness in handling penetration issues.\nExtensive experiments with existing methods validate AvatarGO's superior\ngeneration and animation capabilities on a variety of human-object pairs and\ndiverse poses. As the first attempt to synthesize 4D avatars with object\ninteractions, we hope AvatarGO could open new doors for human-centric 4D\ncontent creation."
                },
                "authors": [
                    {
                        "name": "Yukang Cao"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Project page: https://yukangcao.github.io/AvatarGO/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07163v1",
                "updated": "2024-10-09T17:58:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    12,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning"
                },
                "summary": "In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple."
                },
                "authors": [
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Jiancheng Liu"
                    },
                    {
                        "name": "Licong Lin"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06813v2",
                "updated": "2024-10-09T17:57:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    57,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-09T12:37:54Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    12,
                    37,
                    54,
                    1,
                    191,
                    0
                ],
                "title": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy"
                },
                "summary": "Diplomacy is one of the most sophisticated activities in human society. The\ncomplex interactions among multiple parties/ agents involve various abilities\nlike social reasoning, negotiation arts, and long-term strategy planning.\nPrevious AI agents surely have proved their capability of handling multi-step\ngames and larger action spaces on tasks involving multiple agents. However,\ndiplomacy involves a staggering magnitude of decision spaces, especially\nconsidering the negotiation stage required. Recently, LLM agents have shown\ntheir potential for extending the boundary of previous agents on a couple of\napplications, however, it is still not enough to handle a very long planning\nperiod in a complex multi-agent environment. Empowered with cutting-edge LLM\ntechnology, we make the first stab to explore AI's upper bound towards a\nhuman-like agent for such a highly comprehensive multi-agent mission by\ncombining three core and essential capabilities for stronger LLM-based societal\nagents: 1) strategic planner with memory and reflection; 2) goal-oriented\nnegotiate with social reasoning; 3) augmenting memory by self-play games to\nself-evolving without any human in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diplomacy is one of the most sophisticated activities in human society. The\ncomplex interactions among multiple parties/ agents involve various abilities\nlike social reasoning, negotiation arts, and long-term strategy planning.\nPrevious AI agents surely have proved their capability of handling multi-step\ngames and larger action spaces on tasks involving multiple agents. However,\ndiplomacy involves a staggering magnitude of decision spaces, especially\nconsidering the negotiation stage required. Recently, LLM agents have shown\ntheir potential for extending the boundary of previous agents on a couple of\napplications, however, it is still not enough to handle a very long planning\nperiod in a complex multi-agent environment. Empowered with cutting-edge LLM\ntechnology, we make the first stab to explore AI's upper bound towards a\nhuman-like agent for such a highly comprehensive multi-agent mission by\ncombining three core and essential capabilities for stronger LLM-based societal\nagents: 1) strategic planner with memory and reflection; 2) goal-oriented\nnegotiate with social reasoning; 3) augmenting memory by self-play games to\nself-evolving without any human in the loop."
                },
                "authors": [
                    {
                        "name": "Zhenyu Guan"
                    },
                    {
                        "name": "Xiangyu Kong"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_journal_ref": "NuerIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07145v1",
                "updated": "2024-10-09T17:54:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    54,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:54:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    54,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based\n  Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based\n  Long-Context Modeling"
                },
                "summary": "One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling."
                },
                "authors": [
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Xinrong Zhang"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "21 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07137v1",
                "updated": "2024-10-09T17:53:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    53,
                    6,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:53:06Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    53,
                    6,
                    2,
                    283,
                    0
                ],
                "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates"
                },
                "summary": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks."
                },
                "authors": [
                    {
                        "name": "Xiaosen Zheng"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07129v1",
                "updated": "2024-10-09T17:51:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    55,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:51:55Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    55,
                    2,
                    283,
                    0
                ],
                "title": "Mental Disorders Detection in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental Disorders Detection in the Era of Large Language Models"
                },
                "summary": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications."
                },
                "authors": [
                    {
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "name": "Petr Strepetov"
                    },
                    {
                        "name": "Maksim Stankevich"
                    },
                    {
                        "name": "Ivan Smirnov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12877v2",
                "updated": "2024-10-09T17:51:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    44,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-16T08:25:26Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    25,
                    26,
                    1,
                    198,
                    0
                ],
                "title": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models"
                },
                "summary": "Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code ."
                },
                "authors": [
                    {
                        "name": "Yaswanth Narsupalli"
                    },
                    {
                        "name": "Abhranil Chandra"
                    },
                    {
                        "name": "Sreevatsa Muppirala"
                    },
                    {
                        "name": "Manish Gupta"
                    },
                    {
                        "name": "Pawan Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Goyal"
                },
                "author": "Pawan Goyal",
                "arxiv_comment": "Paper Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07118v1",
                "updated": "2024-10-09T17:48:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    48,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:48:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    48,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Exploring the Readiness of Prominent Small Language Models for the\n  Democratization of Financial Literacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Readiness of Prominent Small Language Models for the\n  Democratization of Financial Literacy"
                },
                "summary": "The use of small language models (SLMs), herein defined as models with less\nthan three billion parameters, is increasing across various domains and\napplications. Due to their ability to run on more accessible hardware and\npreserve user privacy, SLMs possess the potential to democratize access to\nlanguage models for individuals of different socioeconomic status and with\ndifferent privacy preferences. This study assesses several state-of-the-art\nSLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama\nproject) for use in the financial domain to support the development of\nfinancial literacy LMs. Democratizing access to quality financial information\nfor those who are financially under educated is greatly needed in society,\nparticularly as new financial markets and products emerge and participation in\nfinancial markets increases due to ease of access. We are the first to examine\nthe use of open-source SLMs to democratize access to financial question\nanswering capabilities for individuals and students. To this end, we provide an\nanalysis of the memory usage, inference time, similarity comparisons to\nground-truth answers, and output readability of prominent SLMs to determine\nwhich models are most accessible and capable of supporting access to financial\ninformation. We analyze zero-shot and few-shot learning variants of the models.\nThe results suggest that some off-the-shelf SLMs merit further exploration and\nfine-tuning to prepare them for individual use, while others may have limits to\ntheir democratization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of small language models (SLMs), herein defined as models with less\nthan three billion parameters, is increasing across various domains and\napplications. Due to their ability to run on more accessible hardware and\npreserve user privacy, SLMs possess the potential to democratize access to\nlanguage models for individuals of different socioeconomic status and with\ndifferent privacy preferences. This study assesses several state-of-the-art\nSLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama\nproject) for use in the financial domain to support the development of\nfinancial literacy LMs. Democratizing access to quality financial information\nfor those who are financially under educated is greatly needed in society,\nparticularly as new financial markets and products emerge and participation in\nfinancial markets increases due to ease of access. We are the first to examine\nthe use of open-source SLMs to democratize access to financial question\nanswering capabilities for individuals and students. To this end, we provide an\nanalysis of the memory usage, inference time, similarity comparisons to\nground-truth answers, and output readability of prominent SLMs to determine\nwhich models are most accessible and capable of supporting access to financial\ninformation. We analyze zero-shot and few-shot learning variants of the models.\nThe results suggest that some off-the-shelf SLMs merit further exploration and\nfine-tuning to prepare them for individual use, while others may have limits to\ntheir democratization."
                },
                "authors": [
                    {
                        "name": "Tagore Rao Kosireddy"
                    },
                    {
                        "name": "Jeffrey D. Wall"
                    },
                    {
                        "name": "Evan Lucas"
                    }
                ],
                "author_detail": {
                    "name": "Evan Lucas"
                },
                "author": "Evan Lucas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07112v1",
                "updated": "2024-10-09T17:46:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    46,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:46:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    46,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "VHELM: A Holistic Evaluation of Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VHELM: A Holistic Evaluation of Vision Language Models"
                },
                "summary": "Current benchmarks for assessing vision-language models (VLMs) often focus on\ntheir perception or problem-solving capabilities and neglect other critical\naspects such as fairness, multilinguality, or toxicity. Furthermore, they\ndiffer in their evaluation procedures and the scope of the evaluation, making\nit difficult to compare models. To address these issues, we extend the HELM\nframework to VLMs to present the Holistic Evaluation of Vision Language Models\n(VHELM). VHELM aggregates various datasets to cover one or more of the 9\naspects: visual perception, knowledge, reasoning, bias, fairness,\nmultilinguality, robustness, toxicity, and safety. In doing so, we produce a\ncomprehensive, multi-dimensional view of the capabilities of the VLMs across\nthese important factors. In addition, we standardize the standard inference\nparameters, methods of prompting, and evaluation metrics to enable fair\ncomparisons across models. Our framework is designed to be lightweight and\nautomatic so that evaluation runs are cheap and fast. Our initial run evaluates\n22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.\nWe uncover new key findings, such as the fact that efficiency-focused models\n(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than\ntheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark\nbut not when evaluated on the other aspects. For transparency, we release the\nraw model generations and complete results on our website\n(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living\nbenchmark, and we hope to continue adding new datasets and models over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current benchmarks for assessing vision-language models (VLMs) often focus on\ntheir perception or problem-solving capabilities and neglect other critical\naspects such as fairness, multilinguality, or toxicity. Furthermore, they\ndiffer in their evaluation procedures and the scope of the evaluation, making\nit difficult to compare models. To address these issues, we extend the HELM\nframework to VLMs to present the Holistic Evaluation of Vision Language Models\n(VHELM). VHELM aggregates various datasets to cover one or more of the 9\naspects: visual perception, knowledge, reasoning, bias, fairness,\nmultilinguality, robustness, toxicity, and safety. In doing so, we produce a\ncomprehensive, multi-dimensional view of the capabilities of the VLMs across\nthese important factors. In addition, we standardize the standard inference\nparameters, methods of prompting, and evaluation metrics to enable fair\ncomparisons across models. Our framework is designed to be lightweight and\nautomatic so that evaluation runs are cheap and fast. Our initial run evaluates\n22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.\nWe uncover new key findings, such as the fact that efficiency-focused models\n(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than\ntheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark\nbut not when evaluated on the other aspects. For transparency, we release the\nraw model generations and complete results on our website\n(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living\nbenchmark, and we hope to continue adding new datasets and models over time."
                },
                "authors": [
                    {
                        "name": "Tony Lee"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Chi Heem Wong"
                    },
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Yifan Mai"
                    },
                    {
                        "name": "Josselin Somerville Roberts"
                    },
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Percy Liang"
                    }
                ],
                "author_detail": {
                    "name": "Percy Liang"
                },
                "author": "Percy Liang",
                "arxiv_comment": "NeurIPS 2024. First three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07109v1",
                "updated": "2024-10-09T17:45:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    47,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:45:47Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    47,
                    2,
                    283,
                    0
                ],
                "title": "I Want to Break Free! Anti-Social Behavior and Persuasion Ability of\n  LLMs in Multi-Agent Settings with Social Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Want to Break Free! Anti-Social Behavior and Persuasion Ability of\n  LLMs in Multi-Agent Settings with Social Hierarchy"
                },
                "summary": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact."
                },
                "authors": [
                    {
                        "name": "Gian Maria Campedelli"
                    },
                    {
                        "name": "Nicolò Penzo"
                    },
                    {
                        "name": "Massimo Stefan"
                    },
                    {
                        "name": "Roberto Dessì"
                    },
                    {
                        "name": "Marco Guerini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12108v2",
                "updated": "2024-10-09T17:45:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    7,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-16T18:28:40Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    28,
                    40,
                    1,
                    198,
                    0
                ],
                "title": "Private prediction for large-scale synthetic text generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private prediction for large-scale synthetic text generation"
                },
                "summary": "We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data."
                },
                "authors": [
                    {
                        "name": "Kareem Amin"
                    },
                    {
                        "name": "Alex Bie"
                    },
                    {
                        "name": "Weiwei Kong"
                    },
                    {
                        "name": "Alexey Kurakin"
                    },
                    {
                        "name": "Natalia Ponomareva"
                    },
                    {
                        "name": "Umar Syed"
                    },
                    {
                        "name": "Andreas Terzis"
                    },
                    {
                        "name": "Sergei Vassilvitskii"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Vassilvitskii"
                },
                "author": "Sergei Vassilvitskii",
                "arxiv_comment": "20 pages; updated figure + some new experiments from EMNLP 2024\n  findings camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07103v1",
                "updated": "2024-10-09T17:41:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    41,
                    53,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:41:53Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    41,
                    53,
                    2,
                    283,
                    0
                ],
                "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models\n  through Repetition of Misordered Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Multi-Hop Reasoning Potential in Large Language Models\n  through Repetition of Misordered Context"
                },
                "summary": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning."
                },
                "authors": [
                    {
                        "name": "Sangwon Yu"
                    },
                    {
                        "name": "Ik-hwan Kim"
                    },
                    {
                        "name": "Jongyoon Song"
                    },
                    {
                        "name": "Saehyung Lee"
                    },
                    {
                        "name": "Junsung Park"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00260v2",
                "updated": "2024-10-09T17:39:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    39,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-30T22:15:58Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    15,
                    58,
                    0,
                    274,
                    0
                ],
                "title": "DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data\n  Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data\n  Mining"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline."
                },
                "authors": [
                    {
                        "name": "Vinayak Arannil"
                    },
                    {
                        "name": "Neha Narwal"
                    },
                    {
                        "name": "Sourav Sanjukta Bhabesh"
                    },
                    {
                        "name": "Sai Nikhil Thirandas"
                    },
                    {
                        "name": "Darren Yow-Bang Wang"
                    },
                    {
                        "name": "Graham Horwood"
                    },
                    {
                        "name": "Alex Anto Chirayath"
                    },
                    {
                        "name": "Gouri Pandeshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gouri Pandeshwar"
                },
                "author": "Gouri Pandeshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2207.02894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2207.02894v3",
                "updated": "2024-10-09T17:38:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    38,
                    58,
                    2,
                    283,
                    0
                ],
                "published": "2022-07-06T18:14:59Z",
                "published_parsed": [
                    2022,
                    7,
                    6,
                    18,
                    14,
                    59,
                    2,
                    187,
                    0
                ],
                "title": "Expert-Guided Inverse Optimization for Convex Constraint Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Guided Inverse Optimization for Convex Constraint Inference"
                },
                "summary": "Conventional inverse optimization inputs a solution and finds the parameters\nof an optimization model that render a given solution optimal. The literature\nmostly focuses on inferring the objective function in linear problems when\naccepted solutions are provided as input. In this paper, we propose an inverse\noptimization model that inputs several accepted and rejected solutions and\nrecovers the underlying convex optimization model that can be used to generate\nsuch solutions. The novelty of our model is two-fold: First, we focus on\ninferring the parameters of the underlying convex feasible region. Second, the\nproposed model learns the convex constraint set from a set of past observations\nthat are either accepted or rejected by an expert. The resulting inverse model\nis a mixed-integer nonlinear problem that is complex to solve. To mitigate the\ninverse problem complexity, we employ variational inequalities and the\ntheoretical properties of the solutions to derive a reduced formulation that\nretains the complexity of its forward counterpart. Using realistic breast\ncancer patient data, we demonstrate that our inverse model can utilize a subset\nof past accepted and rejected treatment plans to infer clinical criteria that\ncan lead to nearly guaranteed acceptable treatment plans for future patients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional inverse optimization inputs a solution and finds the parameters\nof an optimization model that render a given solution optimal. The literature\nmostly focuses on inferring the objective function in linear problems when\naccepted solutions are provided as input. In this paper, we propose an inverse\noptimization model that inputs several accepted and rejected solutions and\nrecovers the underlying convex optimization model that can be used to generate\nsuch solutions. The novelty of our model is two-fold: First, we focus on\ninferring the parameters of the underlying convex feasible region. Second, the\nproposed model learns the convex constraint set from a set of past observations\nthat are either accepted or rejected by an expert. The resulting inverse model\nis a mixed-integer nonlinear problem that is complex to solve. To mitigate the\ninverse problem complexity, we employ variational inequalities and the\ntheoretical properties of the solutions to derive a reduced formulation that\nretains the complexity of its forward counterpart. Using realistic breast\ncancer patient data, we demonstrate that our inverse model can utilize a subset\nof past accepted and rejected treatment plans to infer clinical criteria that\ncan lead to nearly guaranteed acceptable treatment plans for future patients."
                },
                "authors": [
                    {
                        "name": "Houra Mahmoudzadeh"
                    },
                    {
                        "name": "Kimia Ghobadi"
                    }
                ],
                "author_detail": {
                    "name": "Kimia Ghobadi"
                },
                "author": "Kimia Ghobadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2207.02894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2207.02894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07087v2",
                "updated": "2024-10-10T05:02:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    2,
                    4,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-09T17:29:01Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    29,
                    1,
                    2,
                    283,
                    0
                ],
                "title": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark,\n  and Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark,\n  and Methodology"
                },
                "summary": "Developing agents capable of navigating to a target location based on\nlanguage instructions and visual information, known as vision-language\nnavigation (VLN), has attracted widespread interest. Most research has focused\non ground-based agents, while UAV-based VLN remains relatively underexplored.\nRecent efforts in UAV vision-language navigation predominantly adopt\nground-based VLN settings, relying on predefined discrete action spaces and\nneglecting the inherent disparities in agent movement dynamics and the\ncomplexity of navigation tasks between ground and aerial environments. To\naddress these disparities and challenges, we propose solutions from three\nperspectives: platform, benchmark, and methodology. To enable realistic UAV\ntrajectory simulation in VLN tasks, we propose the OpenUAV platform, which\nfeatures diverse environments, realistic flight control, and extensive\nalgorithmic support. We further construct a target-oriented VLN dataset\nconsisting of approximately 12k trajectories on this platform, serving as the\nfirst dataset specifically designed for realistic UAV VLN tasks. To tackle the\nchallenges posed by complex aerial environments, we propose an assistant-guided\nUAV object search benchmark called UAV-Need-Help, which provides varying levels\nof guidance information to help UAVs better accomplish realistic VLN tasks. We\nalso propose a UAV navigation LLM that, given multi-view images, task\ndescriptions, and assistant instructions, leverages the multimodal\nunderstanding capabilities of the MLLM to jointly process visual and textual\ninformation, and performs hierarchical trajectory generation. The evaluation\nresults of our method significantly outperform the baseline models, while there\nremains a considerable gap between our results and those achieved by human\noperators, underscoring the challenge presented by the UAV-Need-Help task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing agents capable of navigating to a target location based on\nlanguage instructions and visual information, known as vision-language\nnavigation (VLN), has attracted widespread interest. Most research has focused\non ground-based agents, while UAV-based VLN remains relatively underexplored.\nRecent efforts in UAV vision-language navigation predominantly adopt\nground-based VLN settings, relying on predefined discrete action spaces and\nneglecting the inherent disparities in agent movement dynamics and the\ncomplexity of navigation tasks between ground and aerial environments. To\naddress these disparities and challenges, we propose solutions from three\nperspectives: platform, benchmark, and methodology. To enable realistic UAV\ntrajectory simulation in VLN tasks, we propose the OpenUAV platform, which\nfeatures diverse environments, realistic flight control, and extensive\nalgorithmic support. We further construct a target-oriented VLN dataset\nconsisting of approximately 12k trajectories on this platform, serving as the\nfirst dataset specifically designed for realistic UAV VLN tasks. To tackle the\nchallenges posed by complex aerial environments, we propose an assistant-guided\nUAV object search benchmark called UAV-Need-Help, which provides varying levels\nof guidance information to help UAVs better accomplish realistic VLN tasks. We\nalso propose a UAV navigation LLM that, given multi-view images, task\ndescriptions, and assistant instructions, leverages the multimodal\nunderstanding capabilities of the MLLM to jointly process visual and textual\ninformation, and performs hierarchical trajectory generation. The evaluation\nresults of our method significantly outperform the baseline models, while there\nremains a considerable gap between our results and those achieved by human\noperators, underscoring the challenge presented by the UAV-Need-Help task."
                },
                "authors": [
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Donglin Yang"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Hohin Kwan"
                    },
                    {
                        "name": "Jinyu Chen"
                    },
                    {
                        "name": "Wenjun Wu"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12040v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12040v4",
                "updated": "2024-10-09T17:28:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    28,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-01T17:59:55Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    55,
                    0,
                    183,
                    0
                ],
                "title": "Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and\n  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and\n  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments"
                },
                "summary": "This study extensively evaluated You Only Look Once (YOLO) object detection\nalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and\nYOLO11 for green fruit detection in commercial orchards. The research also\nvalidated in-field fruitlet counting using an iPhone and machine vision sensors\nacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.\nAmong the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base\noutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In\nterms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9\nconfigurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n\nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,\nsignificantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,\nand YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This\ncomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,\noffering researchers essential insights to choose the best-suited model for\nfruitlet detection and possible automation in commercial orchards. For\nreal-time automation related work in relevant datasets, we recommend using\nYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,\nYOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet\nDetection, Greenfruit Detection, Green Apple Detection, Agricultural\nAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot\nDetection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study extensively evaluated You Only Look Once (YOLO) object detection\nalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and\nYOLO11 for green fruit detection in commercial orchards. The research also\nvalidated in-field fruitlet counting using an iPhone and machine vision sensors\nacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.\nAmong the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base\noutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In\nterms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9\nconfigurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n\nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,\nsignificantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,\nand YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This\ncomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,\noffering researchers essential insights to choose the best-suited model for\nfruitlet detection and possible automation in commercial orchards. For\nreal-time automation related work in relevant datasets, we recommend using\nYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,\nYOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet\nDetection, Greenfruit Detection, Green Apple Detection, Agricultural\nAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot\nDetection"
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Zhichao Meng"
                    },
                    {
                        "name": "Martin Churuvija"
                    },
                    {
                        "name": "Xiaoqiang Du"
                    },
                    {
                        "name": "Zenghong Ma"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "arxiv_comment": "15 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12040v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12040v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07084v1",
                "updated": "2024-10-09T17:24:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    24,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:24:33Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    24,
                    33,
                    2,
                    283,
                    0
                ],
                "title": "Constraining the dispersion measure redshift relation with\n  simulation-based inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the dispersion measure redshift relation with\n  simulation-based inference"
                },
                "summary": "We use the dispersion measure (DM) of localised Fast Radio Bursts (FRBs) to\nconstrain cosmological and host galaxy parameters using simulation-based\ninference (SBI) for the first time. By simulating the large-scale structure of\nthe electron density with the Generator for Large-Scale Structure (GLASS), we\ngenerate log-normal realisations of the free electron density field, accurately\ncapturing the correlations between different FRBs. For the host galaxy\ncontribution, we rigorously test various models, including log-normal,\ntruncated Gaussian and Gamma distributions, while modelling the Milky Way\ncomponent using pulsar data. Through these simulations, we employ the truncated\nsequential neural posterior estimation method to obtain the posterior. Using\ncurrent observational data, we successfully recover the amplitude of the\nDM-redshift relation, consistent with Planck, while also fitting both the mean\nhost contribution and its shape. Notably, we find no clear preference for a\nspecific model of the host galaxy contribution. Although SBI may not yet be\nstrictly necessary for FRB inference, this work lays the groundwork for the\nfuture, as the increasing volume of FRB data will demand precise modelling of\nboth the host and large-scale structure components. Our modular simulation\npipeline offers flexibility, allowing for easy integration of improved models\nas they become available, ensuring scalability and adaptability for upcoming\nanalyses using FRBs. The pipeline is made publicly available under\nhttps://github.com/koustav-konar/FastNeuralBurst.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use the dispersion measure (DM) of localised Fast Radio Bursts (FRBs) to\nconstrain cosmological and host galaxy parameters using simulation-based\ninference (SBI) for the first time. By simulating the large-scale structure of\nthe electron density with the Generator for Large-Scale Structure (GLASS), we\ngenerate log-normal realisations of the free electron density field, accurately\ncapturing the correlations between different FRBs. For the host galaxy\ncontribution, we rigorously test various models, including log-normal,\ntruncated Gaussian and Gamma distributions, while modelling the Milky Way\ncomponent using pulsar data. Through these simulations, we employ the truncated\nsequential neural posterior estimation method to obtain the posterior. Using\ncurrent observational data, we successfully recover the amplitude of the\nDM-redshift relation, consistent with Planck, while also fitting both the mean\nhost contribution and its shape. Notably, we find no clear preference for a\nspecific model of the host galaxy contribution. Although SBI may not yet be\nstrictly necessary for FRB inference, this work lays the groundwork for the\nfuture, as the increasing volume of FRB data will demand precise modelling of\nboth the host and large-scale structure components. Our modular simulation\npipeline offers flexibility, allowing for easy integration of improved models\nas they become available, ensuring scalability and adaptability for upcoming\nanalyses using FRBs. The pipeline is made publicly available under\nhttps://github.com/koustav-konar/FastNeuralBurst."
                },
                "authors": [
                    {
                        "name": "Koustav Konar"
                    },
                    {
                        "name": "Robert Reischke"
                    },
                    {
                        "name": "Steffen Hagstotz"
                    },
                    {
                        "name": "Andrina Nicola"
                    },
                    {
                        "name": "Hendrik Hildebrandt"
                    }
                ],
                "author_detail": {
                    "name": "Hendrik Hildebrandt"
                },
                "author": "Hendrik Hildebrandt",
                "arxiv_comment": "Submitted to OJA, 14 pages, 10 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07083v1",
                "updated": "2024-10-09T17:24:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    24,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:24:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    24,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stanceformer: Target-Aware Transformer for Stance Detection"
                },
                "summary": "The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}"
                },
                "authors": [
                    {
                        "name": "Krishna Garg"
                    },
                    {
                        "name": "Cornelia Caragea"
                    }
                ],
                "author_detail": {
                    "name": "Cornelia Caragea"
                },
                "author": "Cornelia Caragea",
                "arxiv_comment": "16 pages, 2 figures, 14 tables including Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07076v1",
                "updated": "2024-10-09T17:19:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    58,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:19:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses"
                },
                "summary": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations."
                },
                "authors": [
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Wanhao Liu"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "Code and Benchmark are available at\n  https://github.com/ZonglinY/MOOSE-Chem.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14279v2",
                "updated": "2024-10-09T17:19:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    47,
                    2,
                    283,
                    0
                ],
                "published": "2024-01-25T16:10:33Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    10,
                    33,
                    3,
                    25,
                    0
                ],
                "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs"
                },
                "summary": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1."
                },
                "authors": [
                    {
                        "name": "Azmain Kabir"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Muhammad Asaduzzaman"
                    },
                    {
                        "name": "Wenbin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Zhang"
                },
                "author": "Wenbin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07074v1",
                "updated": "2024-10-09T17:19:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:19:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning"
                },
                "summary": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Yichuan Li"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Kyumin Lee"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07073v1",
                "updated": "2024-10-09T17:16:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    16,
                    22,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:16:22Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    16,
                    22,
                    2,
                    283,
                    0
                ],
                "title": "Pixtral 12B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixtral 12B"
                },
                "summary": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "Pravesh Agrawal"
                    },
                    {
                        "name": "Szymon Antoniak"
                    },
                    {
                        "name": "Emma Bou Hanna"
                    },
                    {
                        "name": "Devendra Chaplot"
                    },
                    {
                        "name": "Jessica Chudnovsky"
                    },
                    {
                        "name": "Saurabh Garg"
                    },
                    {
                        "name": "Theophile Gervet"
                    },
                    {
                        "name": "Soham Ghosh"
                    },
                    {
                        "name": "Amélie Héliou"
                    },
                    {
                        "name": "Paul Jacob"
                    },
                    {
                        "name": "Albert Q. Jiang"
                    },
                    {
                        "name": "Timothée Lacroix"
                    },
                    {
                        "name": "Guillaume Lample"
                    },
                    {
                        "name": "Diego Las Casas"
                    },
                    {
                        "name": "Thibaut Lavril"
                    },
                    {
                        "name": "Teven Le Scao"
                    },
                    {
                        "name": "Andy Lo"
                    },
                    {
                        "name": "William Marshall"
                    },
                    {
                        "name": "Louis Martin"
                    },
                    {
                        "name": "Arthur Mensch"
                    },
                    {
                        "name": "Pavankumar Muddireddy"
                    },
                    {
                        "name": "Valera Nemychnikova"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Patrick Von Platen"
                    },
                    {
                        "name": "Nikhil Raghuraman"
                    },
                    {
                        "name": "Baptiste Rozière"
                    },
                    {
                        "name": "Alexandre Sablayrolles"
                    },
                    {
                        "name": "Lucile Saulnier"
                    },
                    {
                        "name": "Romain Sauvestre"
                    },
                    {
                        "name": "Wendy Shang"
                    },
                    {
                        "name": "Roman Soletskyi"
                    },
                    {
                        "name": "Lawrence Stewart"
                    },
                    {
                        "name": "Pierre Stock"
                    },
                    {
                        "name": "Joachim Studnia"
                    },
                    {
                        "name": "Sandeep Subramanian"
                    },
                    {
                        "name": "Sagar Vaze"
                    },
                    {
                        "name": "Thomas Wang"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wang"
                },
                "author": "Thomas Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06809v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06809v3",
                "updated": "2024-10-09T17:16:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    16,
                    15,
                    2,
                    283,
                    0
                ],
                "published": "2024-04-10T07:56:26Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    7,
                    56,
                    26,
                    2,
                    101,
                    0
                ],
                "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation"
                },
                "summary": "The rapid development of large language models has led to the widespread\nadoption of Retrieval-Augmented Generation (RAG), which integrates external\nknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.\nHowever, the existing RAG paradigm inevitably suffers from the impact of flawed\ninformation introduced during the retrieval phrase, thereby diminishing the\nreliability and correctness of the generated outcomes. In this paper, we\npropose Credibility-aware Generation (CAG), a universally applicable framework\ndesigned to mitigate the impact of flawed information in RAG. At its core, CAG\naims to equip models with the ability to discern and process information based\non its credibility. To this end, we propose an innovative data transformation\nframework that generates data based on credibility, thereby effectively\nendowing models with the capability of CAG. Furthermore, to accurately evaluate\nthe models' capabilities of CAG, we construct a comprehensive benchmark\ncovering three critical real-world scenarios. Experimental results demonstrate\nthat our model can effectively understand and utilize credibility for\ngeneration, significantly outperform other models with retrieval augmentation,\nand exhibit resilience against the disruption caused by noisy documents,\nthereby maintaining robust performance. Moreover, our model supports customized\ncredibility, offering a wide range of potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models has led to the widespread\nadoption of Retrieval-Augmented Generation (RAG), which integrates external\nknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.\nHowever, the existing RAG paradigm inevitably suffers from the impact of flawed\ninformation introduced during the retrieval phrase, thereby diminishing the\nreliability and correctness of the generated outcomes. In this paper, we\npropose Credibility-aware Generation (CAG), a universally applicable framework\ndesigned to mitigate the impact of flawed information in RAG. At its core, CAG\naims to equip models with the ability to discern and process information based\non its credibility. To this end, we propose an innovative data transformation\nframework that generates data based on credibility, thereby effectively\nendowing models with the capability of CAG. Furthermore, to accurately evaluate\nthe models' capabilities of CAG, we construct a comprehensive benchmark\ncovering three critical real-world scenarios. Experimental results demonstrate\nthat our model can effectively understand and utilize credibility for\ngeneration, significantly outperform other models with retrieval augmentation,\nand exhibit resilience against the disruption caused by noisy documents,\nthereby maintaining robust performance. Moreover, our model supports customized\ncredibility, offering a wide range of potential applications."
                },
                "authors": [
                    {
                        "name": "Ruotong Pan"
                    },
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Sirui Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Our code, benchmark, and\n  models are available at https://github.com/panruotong/CAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06809v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06809v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07069v1",
                "updated": "2024-10-09T17:14:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    14,
                    50,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:14:50Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    14,
                    50,
                    2,
                    283,
                    0
                ],
                "title": "ReIFE: Re-evaluating Instruction-Following Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReIFE: Re-evaluating Instruction-Following Evaluation"
                },
                "summary": "The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Kejian Shi"
                    },
                    {
                        "name": "Alexander R. Fabbri"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result\n  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07063v1",
                "updated": "2024-10-09T17:05:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    5,
                    15,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:05:15Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    5,
                    15,
                    2,
                    283,
                    0
                ],
                "title": "InAttention: Linear Context Scaling for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InAttention: Linear Context Scaling for Transformers"
                },
                "summary": "VRAM requirements for transformer models scale quadratically with context\nlength due to the self-attention mechanism. In this paper we modify the\ndecoder-only transformer, replacing self-attention with InAttention, which\nscales linearly with context length during inference by having tokens attend\nonly to initial states. Benchmarking shows that InAttention significantly\nreduces VRAM usage during inference, enabling handling of long sequences on\nconsumer GPUs. We corroborate that fine-tuning extends context length\nefficiently, improving performance on long sequences without high training\ncosts. InAttention offers a scalable solution for long-range dependencies in\ntransformer models, paving the way for further optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRAM requirements for transformer models scale quadratically with context\nlength due to the self-attention mechanism. In this paper we modify the\ndecoder-only transformer, replacing self-attention with InAttention, which\nscales linearly with context length during inference by having tokens attend\nonly to initial states. Benchmarking shows that InAttention significantly\nreduces VRAM usage during inference, enabling handling of long sequences on\nconsumer GPUs. We corroborate that fine-tuning extends context length\nefficiently, improving performance on long sequences without high training\ncosts. InAttention offers a scalable solution for long-range dependencies in\ntransformer models, paving the way for further optimization."
                },
                "authors": [
                    {
                        "name": "Joseph Eisner"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Eisner"
                },
                "author": "Joseph Eisner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07062v1",
                "updated": "2024-10-09T17:03:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    3,
                    49,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    3,
                    49,
                    2,
                    283,
                    0
                ],
                "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection"
                },
                "summary": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo"
                },
                "authors": [
                    {
                        "name": "Cristian Gutierrez"
                    }
                ],
                "author_detail": {
                    "name": "Cristian Gutierrez"
                },
                "author": "Cristian Gutierrez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11126v2",
                "updated": "2024-10-09T16:52:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    52,
                    12,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-17T01:04:02Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    1,
                    4,
                    2,
                    0,
                    169,
                    0
                ],
                "title": "Mitigating the binary viewing angle bias for standard sirens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the binary viewing angle bias for standard sirens"
                },
                "summary": "The inconsistency between experiments in the measurements of the local\nUniverse expansion rate, the Hubble constant, suggests unknown systematics in\nthe existing experiments or new physics. Gravitational-wave standard sirens, a\nmethod to independently provide direct measurements of the Hubble constant,\nhave the potential to address this tension. Before that, it is critical to\nensure there is no substantial systematics in the standard siren method. A\nsignificant systematic has been identified when the viewing angle of the\ngravitational-wave sources, the compact binary coalescences, is inferred\ninaccurately from electromagnetic observations of the sources. Such systematic\nhas led to more than 10% discrepancy in the standard siren Hubble constant\nmeasurements with the observations of binary neutron star merger, GW170817. In\nthis Letter, we develop a new formalism to infer and mitigate this systematic.\nWe demonstrate that the systematic uncertainty of the Hubble constant\nmeasurements can be reduced to smaller than their statistical uncertainty with\n5, 10, and 20 binary neutron star merger observations. We show that our\nformalism successfully reduces the systematics even if the shape of the biased\nviewing angle distribution does not follow precisely the model we choose. Our\nformalism ensures unbiased standard siren Hubble constant measurements when the\nbinary viewing angles are inferred from electromagnetic observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inconsistency between experiments in the measurements of the local\nUniverse expansion rate, the Hubble constant, suggests unknown systematics in\nthe existing experiments or new physics. Gravitational-wave standard sirens, a\nmethod to independently provide direct measurements of the Hubble constant,\nhave the potential to address this tension. Before that, it is critical to\nensure there is no substantial systematics in the standard siren method. A\nsignificant systematic has been identified when the viewing angle of the\ngravitational-wave sources, the compact binary coalescences, is inferred\ninaccurately from electromagnetic observations of the sources. Such systematic\nhas led to more than 10% discrepancy in the standard siren Hubble constant\nmeasurements with the observations of binary neutron star merger, GW170817. In\nthis Letter, we develop a new formalism to infer and mitigate this systematic.\nWe demonstrate that the systematic uncertainty of the Hubble constant\nmeasurements can be reduced to smaller than their statistical uncertainty with\n5, 10, and 20 binary neutron star merger observations. We show that our\nformalism successfully reduces the systematics even if the shape of the biased\nviewing angle distribution does not follow precisely the model we choose. Our\nformalism ensures unbiased standard siren Hubble constant measurements when the\nbinary viewing angles are inferred from electromagnetic observations."
                },
                "authors": [
                    {
                        "name": "Alberto Salvarese"
                    },
                    {
                        "name": "Hsin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Yu Chen"
                },
                "author": "Hsin-Yu Chen",
                "arxiv_doi": "10.3847/2041-8213/ad7bbc",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad7bbc",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted version by The Astrophysical Journal Letters",
                "arxiv_journal_ref": "The Astrophysical Journal Letters, 974, 1, L16, 2024",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07054v1",
                "updated": "2024-10-09T16:51:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    21,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:51:21Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    21,
                    2,
                    283,
                    0
                ],
                "title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing"
                },
                "summary": "Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases."
                },
                "authors": [
                    {
                        "name": "Weichuan Wang"
                    },
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Chen Ma"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "arxiv_comment": "20 pages, EMNLP'2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07053v1",
                "updated": "2024-10-09T16:51:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    10,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:51:10Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    10,
                    2,
                    283,
                    0
                ],
                "title": "Robots in the Middle: Evaluating LLMs in Dispute Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots in the Middle: Evaluating LLMs in Dispute Resolution"
                },
                "summary": "Mediation is a dispute resolution method featuring a neutral third-party\n(mediator) who intervenes to help the individuals resolve their dispute. In\nthis paper, we investigate to which extent large language models (LLMs) are\nable to act as mediators. We investigate whether LLMs are able to analyze\ndispute conversations, select suitable intervention types, and generate\nappropriate intervention messages. Using a novel, manually created dataset of\n50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human\nannotators across several key metrics. Overall, the LLMs showed strong\nperformance, even outperforming our human annotators across dimensions.\nSpecifically, in 62% of the cases, the LLMs chose intervention types that were\nrated as better than or equivalent to those chosen by humans. Moreover, in 84%\nof the cases, the intervention messages generated by the LLMs were rated as\nbetter than or equal to the intervention messages written by humans. LLMs\nlikewise performed favourably on metrics such as impartiality, understanding\nand contextualization. Our results demonstrate the potential of integrating AI\nin online dispute resolution (ODR) platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediation is a dispute resolution method featuring a neutral third-party\n(mediator) who intervenes to help the individuals resolve their dispute. In\nthis paper, we investigate to which extent large language models (LLMs) are\nable to act as mediators. We investigate whether LLMs are able to analyze\ndispute conversations, select suitable intervention types, and generate\nappropriate intervention messages. Using a novel, manually created dataset of\n50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human\nannotators across several key metrics. Overall, the LLMs showed strong\nperformance, even outperforming our human annotators across dimensions.\nSpecifically, in 62% of the cases, the LLMs chose intervention types that were\nrated as better than or equivalent to those chosen by humans. Moreover, in 84%\nof the cases, the intervention messages generated by the LLMs were rated as\nbetter than or equal to the intervention messages written by humans. LLMs\nlikewise performed favourably on metrics such as impartiality, understanding\nand contextualization. Our results demonstrate the potential of integrating AI\nin online dispute resolution (ODR) platforms."
                },
                "authors": [
                    {
                        "name": "Jinzhe Tan"
                    },
                    {
                        "name": "Hannes Westermann"
                    },
                    {
                        "name": "Nikhil Reddy Pottanigari"
                    },
                    {
                        "name": "Jaromír Šavelka"
                    },
                    {
                        "name": "Sébastien Meeùs"
                    },
                    {
                        "name": "Mia Godet"
                    },
                    {
                        "name": "Karim Benyekhlef"
                    }
                ],
                "author_detail": {
                    "name": "Karim Benyekhlef"
                },
                "author": "Karim Benyekhlef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07035v1",
                "updated": "2024-10-09T16:15:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    15,
                    36,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:15:36Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    15,
                    36,
                    2,
                    283,
                    0
                ],
                "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Feiyu Duan"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "arxiv_comment": "39 pages. CP-Bench and LenCtrl-Bench are available in\n  https://huggingface.co/datasets/ZenMoore/CP-Bench and\n  https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07030v1",
                "updated": "2024-10-09T16:13:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    13,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:13:19Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    13,
                    19,
                    2,
                    283,
                    0
                ],
                "title": "Clean Evaluations on Contaminated Visual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clean Evaluations on Contaminated Visual Language Models"
                },
                "summary": "How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication."
                },
                "authors": [
                    {
                        "name": "Hongyuan Lu"
                    },
                    {
                        "name": "Shujie Miao"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00202v2",
                "updated": "2024-10-09T16:08:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    8,
                    51,
                    2,
                    283,
                    0
                ],
                "published": "2024-01-31T21:54:34Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    21,
                    54,
                    34,
                    2,
                    31,
                    0
                ],
                "title": "Generalized Universal Inference on Risk Minimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Universal Inference on Risk Minimizers"
                },
                "summary": "A common goal in statistics and machine learning is estimation of unknowns.\nPoint estimates alone are of little value without an accompanying measure of\nuncertainty, but traditional uncertainty quantification methods, such as\nconfidence sets and p-values, often require strong distributional or structural\nassumptions that may not be justified in modern problems. The present paper\nconsiders a very common case in machine learning, where the quantity of\ninterest is the minimizer of a given risk (expected loss) function. For such\ncases, we propose a generalization of the recently developed universal\ninference procedure that is designed for inference on risk minimizers. Notably,\nour generalized universal inference attains finite-sample frequentist validity\nguarantees under a condition common in the statistical learning literature. One\nversion of our procedure is also anytime-valid in the sense that it maintains\nthe finite-sample validity properties regardless of the stopping rule used for\nthe data collection process, thereby providing a link between safe inference\nand fast convergence rates in statistical learning. Practical use of our\nproposal requires tuning, and we offer a data-driven procedure with strong\nempirical performance across a broad range of challenging statistical and\nmachine learning examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common goal in statistics and machine learning is estimation of unknowns.\nPoint estimates alone are of little value without an accompanying measure of\nuncertainty, but traditional uncertainty quantification methods, such as\nconfidence sets and p-values, often require strong distributional or structural\nassumptions that may not be justified in modern problems. The present paper\nconsiders a very common case in machine learning, where the quantity of\ninterest is the minimizer of a given risk (expected loss) function. For such\ncases, we propose a generalization of the recently developed universal\ninference procedure that is designed for inference on risk minimizers. Notably,\nour generalized universal inference attains finite-sample frequentist validity\nguarantees under a condition common in the statistical learning literature. One\nversion of our procedure is also anytime-valid in the sense that it maintains\nthe finite-sample validity properties regardless of the stopping rule used for\nthe data collection process, thereby providing a link between safe inference\nand fast convergence rates in statistical learning. Practical use of our\nproposal requires tuning, and we offer a data-driven procedure with strong\nempirical performance across a broad range of challenging statistical and\nmachine learning examples."
                },
                "authors": [
                    {
                        "name": "Neil Dey"
                    },
                    {
                        "name": "Ryan Martin"
                    },
                    {
                        "name": "Jonathan P. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan P. Williams"
                },
                "author": "Jonathan P. Williams",
                "arxiv_comment": "40 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07025v1",
                "updated": "2024-10-09T16:07:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    7,
                    11,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:07:11Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    7,
                    11,
                    2,
                    283,
                    0
                ],
                "title": "Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation\n  Models Without Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation\n  Models Without Human Feedback"
                },
                "summary": "Radiologists play a crucial role by translating medical images into medical\nreports. However, the field faces staffing shortages and increasing workloads.\nWhile automated approaches using vision-language models (VLMs) show promise as\nassistants, they require exceptionally high accuracy. Most current VLMs in\nradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the\ngeneral domain, additional preference fine-tuning has become standard practice.\nThe challenge in radiology lies in the prohibitive cost of obtaining\nradiologist feedback. We propose a scalable automated preference alignment\ntechnique for VLMs in radiology, focusing on chest X-ray (CXR) report\ngeneration. Our method leverages publicly available datasets with an\nLLM-as-a-Judge mechanism, eliminating the need for additional expert\nradiologist feedback. We evaluate and benchmark five direct alignment\nalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN\nscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in\nan average across six metrics (domain specific and general), compared to the\nSFT baseline. We study reward overoptimization via length exploitation, with\nreports lengthening by up to 3.2x. To assess a potential alignment tax, we\nbenchmark on six additional diverse tasks, finding no significant degradations.\nA reader study involving four board-certified radiologists indicates win rates\nof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.\nOur analysis provides actionable insights for the development of VLMs in\nhigh-stakes fields like radiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiologists play a crucial role by translating medical images into medical\nreports. However, the field faces staffing shortages and increasing workloads.\nWhile automated approaches using vision-language models (VLMs) show promise as\nassistants, they require exceptionally high accuracy. Most current VLMs in\nradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the\ngeneral domain, additional preference fine-tuning has become standard practice.\nThe challenge in radiology lies in the prohibitive cost of obtaining\nradiologist feedback. We propose a scalable automated preference alignment\ntechnique for VLMs in radiology, focusing on chest X-ray (CXR) report\ngeneration. Our method leverages publicly available datasets with an\nLLM-as-a-Judge mechanism, eliminating the need for additional expert\nradiologist feedback. We evaluate and benchmark five direct alignment\nalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN\nscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in\nan average across six metrics (domain specific and general), compared to the\nSFT baseline. We study reward overoptimization via length exploitation, with\nreports lengthening by up to 3.2x. To assess a potential alignment tax, we\nbenchmark on six additional diverse tasks, finding no significant degradations.\nA reader study involving four board-certified radiologists indicates win rates\nof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.\nOur analysis provides actionable insights for the development of VLMs in\nhigh-stakes fields like radiology."
                },
                "authors": [
                    {
                        "name": "Dennis Hein"
                    },
                    {
                        "name": "Zhihong Chen"
                    },
                    {
                        "name": "Sophie Ostmeier"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Maya Varma"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Arne Edward Michalson"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Hyun Joo Shin"
                    },
                    {
                        "name": "Curtis Langlotz"
                    },
                    {
                        "name": "Akshay S Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Akshay S Chaudhari"
                },
                "author": "Akshay S Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.02554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.02554v2",
                "updated": "2024-10-09T16:07:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    7,
                    4,
                    2,
                    283,
                    0
                ],
                "published": "2022-08-04T09:53:22Z",
                "published_parsed": [
                    2022,
                    8,
                    4,
                    9,
                    53,
                    22,
                    3,
                    216,
                    0
                ],
                "title": "Vocabulary Transfer for Medical Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vocabulary Transfer for Medical Texts"
                },
                "summary": "Working within specific NLP subdomains presents significant challenges,\nprimarily due to a persistent deficit of data. Stringent privacy concerns and\nlimited data accessibility often drive this shortage. Additionally, the medical\ndomain demands high accuracy, where even marginal improvements in model\nperformance can have profound impacts. In this study, we investigate the\npotential of vocabulary transfer to enhance model performance in biomedical NLP\ntasks. Specifically, we focus on vocabulary extension, a technique that\ninvolves expanding the target vocabulary to incorporate domain-specific\nbiomedical terms. Our findings demonstrate that vocabulary extension, leads to\nmeasurable improvements in both downstream model performance and inference\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Working within specific NLP subdomains presents significant challenges,\nprimarily due to a persistent deficit of data. Stringent privacy concerns and\nlimited data accessibility often drive this shortage. Additionally, the medical\ndomain demands high accuracy, where even marginal improvements in model\nperformance can have profound impacts. In this study, we investigate the\npotential of vocabulary transfer to enhance model performance in biomedical NLP\ntasks. Specifically, we focus on vocabulary extension, a technique that\ninvolves expanding the target vocabulary to incorporate domain-specific\nbiomedical terms. Our findings demonstrate that vocabulary extension, leads to\nmeasurable improvements in both downstream model performance and inference\ntime."
                },
                "authors": [
                    {
                        "name": "Priyanka Singh"
                    },
                    {
                        "name": "Vladislav D. Mosin"
                    },
                    {
                        "name": "Ivan P. Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan P. Yamshchikov"
                },
                "author": "Ivan P. Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.02554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.02554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12746v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12746v5",
                "updated": "2024-10-09T16:04:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    4,
                    39,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-18T16:06:38Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    16,
                    6,
                    38,
                    1,
                    170,
                    0
                ],
                "title": "Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies\n  for Zero-shot Knowledge-based VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies\n  for Zero-shot Knowledge-based VQA"
                },
                "summary": "Knowledge-based Visual Question-answering (K-VQA) often requires the use of\nbackground knowledge beyond the image. However, we discover that a single\nknowledge generation strategy is often insufficient for all K-VQA questions. To\nthis end, we propose Diversification, Evidence Truncation, and Combination for\nKnowledge-based Elucidation (DietCoke), which utilizes a bundle of\ncomplementary question-answering tactics and aggregates their answers using\ntextual rationales. DietCoke comprises of three stages: diversification,\nrationalization, and ensemble. The diversification stage generates three\ndistinctive decision contexts, each leading to its own answer candidate. The\nrationalization stage generates two rationales, the automatic rationale and the\nmechanistic rationale, for each answer candidate using decorrelated techniques.\nFinally, in the ensemble stage, an LLM informed by the rationales selects one\nanswer from the three candidates. Experiments show that DietCoke significantly\noutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on\nA-OKVOA and that the strategies in the ensembles are highly complementary. Code\nis available at: https://github.com/limiaoyu/DietCoke",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Visual Question-answering (K-VQA) often requires the use of\nbackground knowledge beyond the image. However, we discover that a single\nknowledge generation strategy is often insufficient for all K-VQA questions. To\nthis end, we propose Diversification, Evidence Truncation, and Combination for\nKnowledge-based Elucidation (DietCoke), which utilizes a bundle of\ncomplementary question-answering tactics and aggregates their answers using\ntextual rationales. DietCoke comprises of three stages: diversification,\nrationalization, and ensemble. The diversification stage generates three\ndistinctive decision contexts, each leading to its own answer candidate. The\nrationalization stage generates two rationales, the automatic rationale and the\nmechanistic rationale, for each answer candidate using decorrelated techniques.\nFinally, in the ensemble stage, an LLM informed by the rationales selects one\nanswer from the three candidates. Experiments show that DietCoke significantly\noutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on\nA-OKVOA and that the strategies in the ensembles are highly complementary. Code\nis available at: https://github.com/limiaoyu/DietCoke"
                },
                "authors": [
                    {
                        "name": "Miaoyu Li"
                    },
                    {
                        "name": "Haoxin Li"
                    },
                    {
                        "name": "Zilin Du"
                    },
                    {
                        "name": "Boyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Boyang Li"
                },
                "author": "Boyang Li",
                "arxiv_comment": "Accepted to Findings of EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12746v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12746v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00795v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00795v4",
                "updated": "2024-10-09T16:02:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    2,
                    13,
                    2,
                    283,
                    0
                ],
                "published": "2024-02-01T17:28:10Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    17,
                    28,
                    10,
                    3,
                    32,
                    0
                ],
                "title": "LLMs learn governing principles of dynamical systems, revealing an\n  in-context neural scaling law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs learn governing principles of dynamical systems, revealing an\n  in-context neural scaling law"
                },
                "summary": "Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. We study LLMs' ability to\nextrapolate the behavior of dynamical systems whose evolution is governed by\nprinciples of physical interest. Our results show that LLaMA 2, a language\nmodel trained primarily on texts, achieves accurate predictions of dynamical\nsystem time series without fine-tuning or prompt engineering. Moreover, the\naccuracy of the learned physical rules increases with the length of the input\ncontext window, revealing an in-context version of neural scaling law. Along\nthe way, we present a flexible and efficient algorithm for extracting\nprobability density functions of multi-digit numbers directly from LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. We study LLMs' ability to\nextrapolate the behavior of dynamical systems whose evolution is governed by\nprinciples of physical interest. Our results show that LLaMA 2, a language\nmodel trained primarily on texts, achieves accurate predictions of dynamical\nsystem time series without fine-tuning or prompt engineering. Moreover, the\naccuracy of the learned physical rules increases with the length of the input\ncontext window, revealing an in-context version of neural scaling law. Along\nthe way, we present a flexible and efficient algorithm for extracting\nprobability density functions of multi-digit numbers directly from LLMs."
                },
                "authors": [
                    {
                        "name": "Toni J. B. Liu"
                    },
                    {
                        "name": "Nicolas Boullé"
                    },
                    {
                        "name": "Raphaël Sarfati"
                    },
                    {
                        "name": "Christopher J. Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J. Earls"
                },
                "author": "Christopher J. Earls",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00795v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00795v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07018v1",
                "updated": "2024-10-09T16:00:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    0,
                    21,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:00:21Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    0,
                    21,
                    2,
                    283,
                    0
                ],
                "title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series\n  OOD Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series\n  OOD Generalization"
                },
                "summary": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning\narea of study. Its primary goal is to enhance the adaptability and resilience\nof machine learning models when faced with new, unseen, and potentially\nadversarial data that significantly diverges from their original training\ndatasets. In this paper, we investigate time series OOD generalization via\npre-trained Large Language Models (LLMs). We first propose a novel\n\\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries\n\\textbf{O}OD generalization, termed TTSO, which considers both sample-level and\ngroup-level uncertainties. This formula offers a fresh theoretic perspective\nfor formulating and analyzing OOD generalization problem. In addition, we\nprovide a theoretical analysis to justify this method is well motivated. We\nthen develop a stratified localization algorithm tailored for this tri-level\noptimization problem, theoretically demonstrating the guaranteed convergence of\nthe proposed algorithm. Our analysis also reveals that the iteration complexity\nto obtain an $\\epsilon$-stationary point is bounded by\nO($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have\nbeen conducted to elucidate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning\narea of study. Its primary goal is to enhance the adaptability and resilience\nof machine learning models when faced with new, unseen, and potentially\nadversarial data that significantly diverges from their original training\ndatasets. In this paper, we investigate time series OOD generalization via\npre-trained Large Language Models (LLMs). We first propose a novel\n\\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries\n\\textbf{O}OD generalization, termed TTSO, which considers both sample-level and\ngroup-level uncertainties. This formula offers a fresh theoretic perspective\nfor formulating and analyzing OOD generalization problem. In addition, we\nprovide a theoretical analysis to justify this method is well motivated. We\nthen develop a stratified localization algorithm tailored for this tri-level\noptimization problem, theoretically demonstrating the guaranteed convergence of\nthe proposed algorithm. Our analysis also reveals that the iteration complexity\nto obtain an $\\epsilon$-stationary point is bounded by\nO($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have\nbeen conducted to elucidate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Chengtao Jian"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yang Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Jiao"
                },
                "author": "Yang Jiao",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07009v1",
                "updated": "2024-10-09T15:52:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    52,
                    48,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:52:48Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    52,
                    48,
                    2,
                    283,
                    0
                ],
                "title": "Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based\n  Outline-guided Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based\n  Outline-guided Generation"
                },
                "summary": "The patent domain is gaining attention in natural language processing\nresearch, offering practical applications in streamlining the patenting process\nand providing challenging benchmarks for large language models (LLMs). However,\nthe generation of the description sections of patents, which constitute more\nthan 90% of the patent document, has not been studied to date. We address this\ngap by introducing the task of outline-guided paper-to-patent generation, where\nan academic paper provides the technical specification of the invention and an\noutline conveys the desired patent structure. We present PAP2PAT, a new\nchallenging benchmark of 1.8k patent-paper pairs with document outlines,\ncollected using heuristics that reflect typical research lab practices. Our\nexperiments with current open-weight LLMs and outline-guided chunk-based\ngeneration show that they can effectively use information from the paper but\nstruggle with repetitions, likely due to the inherent repetitiveness of patent\nlanguage. We release our data and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The patent domain is gaining attention in natural language processing\nresearch, offering practical applications in streamlining the patenting process\nand providing challenging benchmarks for large language models (LLMs). However,\nthe generation of the description sections of patents, which constitute more\nthan 90% of the patent document, has not been studied to date. We address this\ngap by introducing the task of outline-guided paper-to-patent generation, where\nan academic paper provides the technical specification of the invention and an\noutline conveys the desired patent structure. We present PAP2PAT, a new\nchallenging benchmark of 1.8k patent-paper pairs with document outlines,\ncollected using heuristics that reflect typical research lab practices. Our\nexperiments with current open-weight LLMs and outline-guided chunk-based\ngeneration show that they can effectively use information from the paper but\nstruggle with repetitions, likely due to the inherent repetitiveness of patent\nlanguage. We release our data and code."
                },
                "authors": [
                    {
                        "name": "Valentin Knappich"
                    },
                    {
                        "name": "Simon Razniewski"
                    },
                    {
                        "name": "Anna Hätty"
                    },
                    {
                        "name": "Annemarie Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Annemarie Friedrich"
                },
                "author": "Annemarie Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10054v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10054v3",
                "updated": "2024-10-09T15:44:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    44,
                    36,
                    2,
                    283,
                    0
                ],
                "published": "2023-11-16T17:48:55Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    17,
                    48,
                    55,
                    3,
                    320,
                    0
                ],
                "title": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models"
                },
                "summary": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles."
                },
                "authors": [
                    {
                        "name": "Mingqian Zheng"
                    },
                    {
                        "name": "Jiaxin Pei"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "arxiv_comment": "Accepted by Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10054v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06997v1",
                "updated": "2024-10-09T15:44:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    44,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    44,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one\n  Single X-ray",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one\n  Single X-ray"
                },
                "summary": "Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays\nare commonly used for its diagnosis due to their cost-effectiveness. Magnetic\nResonance Imaging (MRI), on the other hand, offers detailed soft tissue\nvisualization and has become a valuable supplementary diagnostic tool for KOA.\nUnfortunately, the high cost and limited accessibility of MRI hinder its\nwidespread use, leaving many patients with KOA reliant solely on X-ray imaging.\nIn this study, we introduce a novel diffusion-based Xray2MRI model capable of\ngenerating pseudo-MRI volumes from one single X-ray image. In addition to using\nX-rays as conditional input, our model integrates target depth, KOA probability\ndistribution, and image intensity distribution modules to guide the synthesis\nprocess, ensuring that the generated corresponding slices accurately correspond\nto the anatomical structures. Experimental results demonstrate that by\nintegrating information from X-rays with additional input data, our proposed\napproach is capable of generating pseudo-MRI sequences that approximate real\nMRI scans. Moreover, by increasing the inference times, the model achieves\neffective interpolation, further improving the continuity and smoothness of the\ngenerated MRI sequences, representing one promising initial attempt for\ncost-effective medical imaging solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays\nare commonly used for its diagnosis due to their cost-effectiveness. Magnetic\nResonance Imaging (MRI), on the other hand, offers detailed soft tissue\nvisualization and has become a valuable supplementary diagnostic tool for KOA.\nUnfortunately, the high cost and limited accessibility of MRI hinder its\nwidespread use, leaving many patients with KOA reliant solely on X-ray imaging.\nIn this study, we introduce a novel diffusion-based Xray2MRI model capable of\ngenerating pseudo-MRI volumes from one single X-ray image. In addition to using\nX-rays as conditional input, our model integrates target depth, KOA probability\ndistribution, and image intensity distribution modules to guide the synthesis\nprocess, ensuring that the generated corresponding slices accurately correspond\nto the anatomical structures. Experimental results demonstrate that by\nintegrating information from X-rays with additional input data, our proposed\napproach is capable of generating pseudo-MRI sequences that approximate real\nMRI scans. Moreover, by increasing the inference times, the model achieves\neffective interpolation, further improving the continuity and smoothness of the\ngenerated MRI sequences, representing one promising initial attempt for\ncost-effective medical imaging solutions."
                },
                "authors": [
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Rachid Jennane"
                    },
                    {
                        "name": "Aladine Chetouani"
                    },
                    {
                        "name": "Mohamed Jarraya"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Jarraya"
                },
                "author": "Mohamed Jarraya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06992v1",
                "updated": "2024-10-09T15:38:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    38,
                    53,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:38:53Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    38,
                    53,
                    2,
                    283,
                    0
                ],
                "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Bench+: Enhanced Coding Benchmark for LLMs"
                },
                "summary": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues."
                },
                "authors": [
                    {
                        "name": "Reem Aleithan"
                    },
                    {
                        "name": "Haoran Xue"
                    },
                    {
                        "name": "Mohammad Mahdi Mohajer"
                    },
                    {
                        "name": "Elijah Nnorom"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Song Wang"
                    }
                ],
                "author_detail": {
                    "name": "Song Wang"
                },
                "author": "Song Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13053v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13053v3",
                "updated": "2024-10-09T15:33:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    33,
                    10,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-19T20:46:07Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    20,
                    46,
                    7,
                    6,
                    140,
                    0
                ],
                "title": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models"
                },
                "summary": "The pretrain+fine-tune paradigm is foundational for deploying large language\nmodels (LLMs) across various downstream applications. Within this framework,\nLow-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning\n(PEFT), producing numerous reusable task-specific LoRA adapters. However, this\napproach requires explicit task intention selection, posing challenges for\nautonomous task sensing and switching during inference with multiple existing\nLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA\n(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses\nmultiple task-specific LoRA adapters into the base LLM via a full-mode\nMixture-of-Experts (MoE) architecture. This framework also includes novel MoE\nforward acceleration strategies to address the efficiency challenges of\ntraditional MoE implementations. Our evaluation, using the LlaMA2-13B and\nLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,\ndemonstrates equivalent performance with the traditional PEFT method. Moreover,\nthe LLM equipped with MeteoRA achieves superior performance in handling\ncomposite tasks, effectively solving ten sequential problems in a single\ninference pass, thereby demonstrating the framework's enhanced capability for\ntimely adapter switching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pretrain+fine-tune paradigm is foundational for deploying large language\nmodels (LLMs) across various downstream applications. Within this framework,\nLow-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning\n(PEFT), producing numerous reusable task-specific LoRA adapters. However, this\napproach requires explicit task intention selection, posing challenges for\nautonomous task sensing and switching during inference with multiple existing\nLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA\n(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses\nmultiple task-specific LoRA adapters into the base LLM via a full-mode\nMixture-of-Experts (MoE) architecture. This framework also includes novel MoE\nforward acceleration strategies to address the efficiency challenges of\ntraditional MoE implementations. Our evaluation, using the LlaMA2-13B and\nLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,\ndemonstrates equivalent performance with the traditional PEFT method. Moreover,\nthe LLM equipped with MeteoRA achieves superior performance in handling\ncomposite tasks, effectively solving ten sequential problems in a single\ninference pass, thereby demonstrating the framework's enhanced capability for\ntimely adapter switching."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junyu Lai"
                    },
                    {
                        "name": "Yunpeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Huang"
                },
                "author": "Yunpeng Huang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13053v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13053v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06984v1",
                "updated": "2024-10-09T15:21:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    21,
                    43,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:21:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    21,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Observability rank conditions for analysing practical identifiability a\n  priori",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observability rank conditions for analysing practical identifiability a\n  priori"
                },
                "summary": "The concept of identifiability describes the possibility of inferring the\nparameters of a dynamic model by observing its output. It is common and useful\nto distinguish between structural and practical identifiability. The former\nproperty is fully determined by the model equations, while the latter is also\ninfluenced by the characteristics of the available experimental data.\nStructural identifiability can be determined by means of symbolic computations,\nwhich may be performed before collecting experimental data, and are hence\nsometimes called a priori analyses. Practical identifiability is typically\nassessed numerically, with methods that require simulations - and often also\noptimization - and are applied a posteriori. An approach to study structural\nlocal identifiability is to consider it as a particular case of observability,\nwhich is the possibility of inferring the internal state of a system from its\noutput. Thus, both properties can be analysed jointly, by building a\ngeneralized observability matrix and computing its rank. The aim of this paper\nis to investigate to which extent such observability-based methods can also\ninform about practical identifiability. To this end, we explore a number of\npossible extensions of the rank tests, and discuss the purposes for which they\ncan be informative as well as others for which they cannot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of identifiability describes the possibility of inferring the\nparameters of a dynamic model by observing its output. It is common and useful\nto distinguish between structural and practical identifiability. The former\nproperty is fully determined by the model equations, while the latter is also\ninfluenced by the characteristics of the available experimental data.\nStructural identifiability can be determined by means of symbolic computations,\nwhich may be performed before collecting experimental data, and are hence\nsometimes called a priori analyses. Practical identifiability is typically\nassessed numerically, with methods that require simulations - and often also\noptimization - and are applied a posteriori. An approach to study structural\nlocal identifiability is to consider it as a particular case of observability,\nwhich is the possibility of inferring the internal state of a system from its\noutput. Thus, both properties can be analysed jointly, by building a\ngeneralized observability matrix and computing its rank. The aim of this paper\nis to investigate to which extent such observability-based methods can also\ninform about practical identifiability. To this end, we explore a number of\npossible extensions of the rank tests, and discuss the purposes for which they\ncan be informative as well as others for which they cannot."
                },
                "authors": [
                    {
                        "name": "Alejandro F. Villaverde"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro F. Villaverde"
                },
                "author": "Alejandro F. Villaverde",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06981v1",
                "updated": "2024-10-09T15:18:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    18,
                    57,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:18:57Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    18,
                    57,
                    2,
                    283,
                    0
                ],
                "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models"
                },
                "summary": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality."
                },
                "authors": [
                    {
                        "name": "Michael Lan"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Austin Meek"
                    },
                    {
                        "name": "Ashkan Khakzar"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13895v2",
                "updated": "2024-10-09T15:13:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    13,
                    29,
                    2,
                    283,
                    0
                ],
                "published": "2024-08-25T17:17:52Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    17,
                    17,
                    52,
                    6,
                    238,
                    0
                ],
                "title": "ESG Rating Disagreement and Corporate Total Factor\n  Productivity:Inference and Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESG Rating Disagreement and Corporate Total Factor\n  Productivity:Inference and Prediction"
                },
                "summary": "This paper examines how ESG rating disagreement (Dis) affects corporate total\nfactor productivity (TFP) in China based on data of A-share listed companies\nfrom 2015 to 2022. We find that Dis reduces TFP, especially in state-owned,\nnon-capital-intensive, and low-pollution firms. Mechanism analysis shows that\ngreen innovation strengthens the dampening effect of Dis on TFP, and that Dis\nlowers corporate TFP by increasing financing constraints. Furthermore, XGBoost\nregression demonstrates that Dis plays a significant role in predicting TFP,\nwith SHAP showing that the dampening effect of ESG rating disagreement on TFP\nis still pronounced in firms with large Dis values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines how ESG rating disagreement (Dis) affects corporate total\nfactor productivity (TFP) in China based on data of A-share listed companies\nfrom 2015 to 2022. We find that Dis reduces TFP, especially in state-owned,\nnon-capital-intensive, and low-pollution firms. Mechanism analysis shows that\ngreen innovation strengthens the dampening effect of Dis on TFP, and that Dis\nlowers corporate TFP by increasing financing constraints. Furthermore, XGBoost\nregression demonstrates that Dis plays a significant role in predicting TFP,\nwith SHAP showing that the dampening effect of ESG rating disagreement on TFP\nis still pronounced in firms with large Dis values."
                },
                "authors": [
                    {
                        "name": "Zhanli Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhanli Li"
                },
                "author": "Zhanli Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15105v2",
                "updated": "2024-10-09T15:13:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    13,
                    23,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-22T10:48:12Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    10,
                    48,
                    12,
                    4,
                    82,
                    0
                ],
                "title": "SAGraph: A Large-scale Text-Rich Social Graph Dataset for Advertising\n  Campaigns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGraph: A Large-scale Text-Rich Social Graph Dataset for Advertising\n  Campaigns"
                },
                "summary": "Influencer selection in marketing involves choosing users with a strong\nonline presence to promote products or services, leveraging their credibility\nand audience reach. This process is vital for its direct impact on brand\nvisibility, consumer trust, and ultimately, sales conversion. Current research\nsimplifies complex elements like user attitudes, thought processes, and\nadvertising content into numerical values. This kind of approach risks missing\nthe dynamic and contextual nuances crucial for developing effective influencer\nmarketing strategies. To bridge this gap, we introduce a text-rich large Social\nAdvertisement Graph (SAGraph) dataset collected from Weibo, a real-world\ninfluencer advertising platform. Our dataset centers around the advertising\ncampaign for 6 products, consisting of 317,287 users, each with their profile\ninformation, and interaction data including 891,834 comments and 441,836\nreposts. By leveraging this rich interaction and textual content, one can gain\ndeeper insights into consumer behavior, refine influencer selection criteria,\nand develop more targeted and effective marketing strategies. We evaluated\nexisting influencer selection baselines and the latest LLMs on this dataset,\ndemonstrating the importance of textual content in advertising campaigns, as\nwell as the availability and significant potential of LLMs for enhancing\nadvertising strategies. We hope that this dataset will inspire further\nresearch: \\url{https://github.com/xiaoqzhwhu/SAGraph/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influencer selection in marketing involves choosing users with a strong\nonline presence to promote products or services, leveraging their credibility\nand audience reach. This process is vital for its direct impact on brand\nvisibility, consumer trust, and ultimately, sales conversion. Current research\nsimplifies complex elements like user attitudes, thought processes, and\nadvertising content into numerical values. This kind of approach risks missing\nthe dynamic and contextual nuances crucial for developing effective influencer\nmarketing strategies. To bridge this gap, we introduce a text-rich large Social\nAdvertisement Graph (SAGraph) dataset collected from Weibo, a real-world\ninfluencer advertising platform. Our dataset centers around the advertising\ncampaign for 6 products, consisting of 317,287 users, each with their profile\ninformation, and interaction data including 891,834 comments and 441,836\nreposts. By leveraging this rich interaction and textual content, one can gain\ndeeper insights into consumer behavior, refine influencer selection criteria,\nand develop more targeted and effective marketing strategies. We evaluated\nexisting influencer selection baselines and the latest LLMs on this dataset,\ndemonstrating the importance of textual content in advertising campaigns, as\nwell as the availability and significant potential of LLMs for enhancing\nadvertising strategies. We hope that this dataset will inspire further\nresearch: \\url{https://github.com/xiaoqzhwhu/SAGraph/}."
                },
                "authors": [
                    {
                        "name": "Xiaoqing Zhang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Zhenxing Hu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.13332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.13332v2",
                "updated": "2024-10-09T15:12:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    12,
                    2,
                    2,
                    283,
                    0
                ],
                "published": "2023-09-23T10:33:59Z",
                "published_parsed": [
                    2023,
                    9,
                    23,
                    10,
                    33,
                    59,
                    5,
                    266,
                    0
                ],
                "title": "Independent projections of diffusions: Gradient flows for variational\n  inference and optimal mean field approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Independent projections of diffusions: Gradient flows for variational\n  inference and optimal mean field approximations"
                },
                "summary": "What is the optimal way to approximate a high-dimensional diffusion process\nby one in which the coordinates are independent? This paper presents a\nconstruction, called the \\emph{independent projection}, which is optimal for\ntwo natural criteria. First, when the original diffusion is reversible with\ninvariant measure $\\rho_*$, the independent projection serves as the\nWasserstein gradient flow for the relative entropy $H(\\cdot\\,|\\,\\rho_*)$\nconstrained to the space of product measures. This is related to recent\nLangevin-based sampling schemes proposed in the statistical literature on mean\nfield variational inference. In addition, we provide both qualitative and\nquantitative results on the long-time convergence of the independent\nprojection, with quantitative results in the log-concave case derived via a new\nvariant of the logarithmic Sobolev inequality. Second, among all processes with\nindependent coordinates, the independent projection is shown to exhibit the\nslowest growth rate of path-space entropy relative to the original diffusion.\nThis sheds new light on the classical McKean-Vlasov equation and recent\nvariants proposed for non-exchangeable systems, which can be viewed as special\ncases of the independent projection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the optimal way to approximate a high-dimensional diffusion process\nby one in which the coordinates are independent? This paper presents a\nconstruction, called the \\emph{independent projection}, which is optimal for\ntwo natural criteria. First, when the original diffusion is reversible with\ninvariant measure $\\rho_*$, the independent projection serves as the\nWasserstein gradient flow for the relative entropy $H(\\cdot\\,|\\,\\rho_*)$\nconstrained to the space of product measures. This is related to recent\nLangevin-based sampling schemes proposed in the statistical literature on mean\nfield variational inference. In addition, we provide both qualitative and\nquantitative results on the long-time convergence of the independent\nprojection, with quantitative results in the log-concave case derived via a new\nvariant of the logarithmic Sobolev inequality. Second, among all processes with\nindependent coordinates, the independent projection is shown to exhibit the\nslowest growth rate of path-space entropy relative to the original diffusion.\nThis sheds new light on the classical McKean-Vlasov equation and recent\nvariants proposed for non-exchangeable systems, which can be viewed as special\ncases of the independent projection."
                },
                "authors": [
                    {
                        "name": "Daniel Lacker"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Lacker"
                },
                "author": "Daniel Lacker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.13332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.13332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06973v1",
                "updated": "2024-10-09T15:11:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    11,
                    13,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:11:13Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    11,
                    13,
                    2,
                    283,
                    0
                ],
                "title": "Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara"
                },
                "summary": "In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B."
                },
                "authors": [
                    {
                        "name": "Azree Nazri"
                    },
                    {
                        "name": "Olalekan Agbolade"
                    },
                    {
                        "name": "Faisal Aziz"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Aziz"
                },
                "author": "Faisal Aziz",
                "arxiv_comment": "20 pages, 5 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06972v1",
                "updated": "2024-10-09T15:10:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    10,
                    0,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:10:00Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    10,
                    0,
                    2,
                    283,
                    0
                ],
                "title": "Diamond of Thought: A Design Thinking-Based Framework for LLMs in\n  Wearable Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diamond of Thought: A Design Thinking-Based Framework for LLMs in\n  Wearable Design"
                },
                "summary": "Wearable design is an interdisciplinary field that balances technological\ninnovation, human factors, and human-computer interactions. Despite\ncontributions from various disciplines, many projects lack stable\ninterdisciplinary teams, which often leads to design failures. Large language\nmodels (LLMs) integrate diverse information and generate innovative solutions,\nmaking them a valuable tool for enhancing design processes. Thus, we have\nexplored the use of LLMs in wearable design by combining design-thinking\nprinciples with LLM capabilities. We have developed the \"Diamond of Thought\"\nframework and analysed 1,603 prototypes and 1,129 products from a body-centric\nperspective to create a comprehensive database. We employed retrieval-augmented\ngeneration to input database details into the LLMs, ensuring applicability to\nwearable design challenges and integration of embodied cognition into the\nprocess. Our LLM-based methodology for wearables has been experimentally\nvalidated, demonstrating the potential of LLMs for the advancement of design\npractices. This study offers new tools and methods for future wearable designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable design is an interdisciplinary field that balances technological\ninnovation, human factors, and human-computer interactions. Despite\ncontributions from various disciplines, many projects lack stable\ninterdisciplinary teams, which often leads to design failures. Large language\nmodels (LLMs) integrate diverse information and generate innovative solutions,\nmaking them a valuable tool for enhancing design processes. Thus, we have\nexplored the use of LLMs in wearable design by combining design-thinking\nprinciples with LLM capabilities. We have developed the \"Diamond of Thought\"\nframework and analysed 1,603 prototypes and 1,129 products from a body-centric\nperspective to create a comprehensive database. We employed retrieval-augmented\ngeneration to input database details into the LLMs, ensuring applicability to\nwearable design challenges and integration of embodied cognition into the\nprocess. Our LLM-based methodology for wearables has been experimentally\nvalidated, demonstrating the potential of LLMs for the advancement of design\npractices. This study offers new tools and methods for future wearable designs."
                },
                "authors": [
                    {
                        "name": "Qiyang Miao"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Zhihao Song"
                    },
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Yu Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cui"
                },
                "author": "Yu Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04417v2",
                "updated": "2024-10-09T15:04:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    4,
                    16,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-06T09:18:04Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    9,
                    18,
                    4,
                    6,
                    280,
                    0
                ],
                "title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language\n  Model Inference"
                },
                "summary": "In vision-language models (VLMs), visual tokens usually consume a significant\namount of computational overhead, despite their sparser information density\ncompared to text tokens. To address this, most existing methods learn a network\nto prune redundant visual tokens and require additional training data.\nDifferently, we propose an efficient training-free token optimization mechanism\ndubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,\ngiven that visual tokens complement text tokens in VLMs for linguistic\nreasoning, we select visual-relevant text tokens to rate the significance of\nvision tokens within the self-attention matrix extracted from the VLMs. Then we\nprogressively prune irrelevant tokens. To maximize sparsity while retaining\nessential information, we introduce a rank-based strategy to adaptively\ndetermine the sparsification ratio for each layer, alongside a token recycling\nmethod that compresses pruned tokens into more compact representations.\nExperimental results show that our SparseVLM improves the efficiency of various\nVLMs across a range of image and video understanding tasks. In particular,\nLLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio\nof 78% while maintaining 93% of the accuracy. Our code is available at\nhttps://github.com/Gumpest/SparseVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In vision-language models (VLMs), visual tokens usually consume a significant\namount of computational overhead, despite their sparser information density\ncompared to text tokens. To address this, most existing methods learn a network\nto prune redundant visual tokens and require additional training data.\nDifferently, we propose an efficient training-free token optimization mechanism\ndubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,\ngiven that visual tokens complement text tokens in VLMs for linguistic\nreasoning, we select visual-relevant text tokens to rate the significance of\nvision tokens within the self-attention matrix extracted from the VLMs. Then we\nprogressively prune irrelevant tokens. To maximize sparsity while retaining\nessential information, we introduce a rank-based strategy to adaptively\ndetermine the sparsification ratio for each layer, alongside a token recycling\nmethod that compresses pruned tokens into more compact representations.\nExperimental results show that our SparseVLM improves the efficiency of various\nVLMs across a range of image and video understanding tasks. In particular,\nLLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio\nof 78% while maintaining 93% of the accuracy. Our code is available at\nhttps://github.com/Gumpest/SparseVLMs."
                },
                "authors": [
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Chun-Kai Fan"
                    },
                    {
                        "name": "Junpeng Ma"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Kuan Cheng"
                    },
                    {
                        "name": "Denis Gudovskiy"
                    },
                    {
                        "name": "Tomoyuki Okuno"
                    },
                    {
                        "name": "Yohei Nakata"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09559v2",
                "updated": "2024-10-09T15:03:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    3,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-02T16:56:09Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    56,
                    9,
                    3,
                    123,
                    0
                ],
                "title": "KID-PPG: Knowledge Informed Deep Learning for Extracting Heart Rate from\n  a Smartwatch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KID-PPG: Knowledge Informed Deep Learning for Extracting Heart Rate from\n  a Smartwatch"
                },
                "summary": "Accurate extraction of heart rate from photoplethysmography (PPG) signals\nremains challenging due to motion artifacts and signal degradation. Although\ndeep learning methods trained as a data-driven inference problem offer\npromising solutions, they often underutilize existing knowledge from the\nmedical and signal processing community. In this paper, we address three\nshortcomings of deep learning models: motion artifact removal, degradation\nassessment, and physiologically plausible analysis of the PPG signal. We\npropose KID-PPG, a knowledge-informed deep learning model that integrates\nexpert knowledge through adaptive linear filtering, deep probabilistic\ninference, and data augmentation. We evaluate KID-PPG on the PPGDalia dataset,\nachieving an average mean absolute error of 2.85 beats per minute, surpassing\nexisting reproducible methods. Our results demonstrate a significant\nperformance improvement in heart rate tracking through the incorporation of\nprior knowledge into deep learning models. This approach shows promise in\nenhancing various biomedical applications by incorporating existing expert\nknowledge in deep learning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate extraction of heart rate from photoplethysmography (PPG) signals\nremains challenging due to motion artifacts and signal degradation. Although\ndeep learning methods trained as a data-driven inference problem offer\npromising solutions, they often underutilize existing knowledge from the\nmedical and signal processing community. In this paper, we address three\nshortcomings of deep learning models: motion artifact removal, degradation\nassessment, and physiologically plausible analysis of the PPG signal. We\npropose KID-PPG, a knowledge-informed deep learning model that integrates\nexpert knowledge through adaptive linear filtering, deep probabilistic\ninference, and data augmentation. We evaluate KID-PPG on the PPGDalia dataset,\nachieving an average mean absolute error of 2.85 beats per minute, surpassing\nexisting reproducible methods. Our results demonstrate a significant\nperformance improvement in heart rate tracking through the incorporation of\nprior knowledge into deep learning models. This approach shows promise in\nenhancing various biomedical applications by incorporating existing expert\nknowledge in deep learning models."
                },
                "authors": [
                    {
                        "name": "Christodoulos Kechris"
                    },
                    {
                        "name": "Jonathan Dan"
                    },
                    {
                        "name": "Jose Miranda"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05291v2",
                "updated": "2024-10-09T15:02:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    53,
                    2,
                    283,
                    0
                ],
                "published": "2024-04-08T08:29:00Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    8,
                    29,
                    0,
                    0,
                    99,
                    0
                ],
                "title": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models"
                },
                "summary": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nfor sketching a plan, a parameter calculator for predicting arguments in the\nplan, and a code generator to convert the plan into executable robot code. At\nthe low level, we adopt reinforcement learning to train a set of motion\nplanning and control skills to unleash the flexibility of quadrupeds for rich\nenvironment interactions. Our system is tested on long-horizon tasks that are\ninfeasible to complete with one single skill. Simulation and real-world\nexperiments show that it successfully figures out multi-step strategies and\ndemonstrates non-trivial behaviors, including building tools or notifying a\nhuman for help. Demos are available on our project page:\nhttps://sites.google.com/view/long-horizon-robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nfor sketching a plan, a parameter calculator for predicting arguments in the\nplan, and a code generator to convert the plan into executable robot code. At\nthe low level, we adopt reinforcement learning to train a set of motion\nplanning and control skills to unleash the flexibility of quadrupeds for rich\nenvironment interactions. Our system is tested on long-horizon tasks that are\ninfeasible to complete with one single skill. Simulation and real-world\nexperiments show that it successfully figures out multi-step strategies and\ndemonstrates non-trivial behaviors, including building tools or notifying a\nhuman for help. Demos are available on our project page:\nhttps://sites.google.com/view/long-horizon-robot."
                },
                "authors": [
                    {
                        "name": "Yutao Ouyang"
                    },
                    {
                        "name": "Jinhan Li"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06965v1",
                "updated": "2024-10-09T15:02:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:02:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Factor Level Preferences to Improve Human-Model Alignment"
                },
                "summary": "Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment."
                },
                "authors": [
                    {
                        "name": "Juhyun Oh"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Inha Cha"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06964v1",
                "updated": "2024-10-09T15:02:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:02:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically"
                },
                "summary": "The recent advancements in large-scale pre-training techniques have\nsignificantly enhanced the capabilities of vision foundation models, notably\nthe Segment Anything Model (SAM), which can generate precise masks based on\npoint and box prompts. Recent studies extend SAM to Few-shot Semantic\nSegmentation (FSS), focusing on prompt generation for SAM-based automatic\nsemantic segmentation. However, these methods struggle with selecting suitable\nprompts, require specific hyperparameter settings for different scenarios, and\nexperience prolonged one-shot inference times due to the overuse of SAM,\nresulting in low efficiency and limited automation ability. To address these\nissues, we propose a simple yet effective approach based on graph analysis. In\nparticular, a Positive-Negative Alignment module dynamically selects the point\nprompts for generating masks, especially uncovering the potential of the\nbackground context as the negative reference. Another subsequent Point-Mask\nClustering module aligns the granularity of masks and selected points as a\ndirected graph, based on mask coverage over points. These points are then\naggregated by decomposing the weakly connected components of the directed graph\nin an efficient manner, constructing distinct natural clusters. Finally, the\npositive and overshooting gating, benefiting from graph-based granularity\nalignment, aggregate high-confident masks and filter out the false-positive\nmasks for final prediction, reducing the usage of additional hyperparameters\nand redundant mask generation. Extensive experimental analysis across standard\nFSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the\neffectiveness and efficiency of the proposed approach, surpassing\nstate-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%\non LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large-scale pre-training techniques have\nsignificantly enhanced the capabilities of vision foundation models, notably\nthe Segment Anything Model (SAM), which can generate precise masks based on\npoint and box prompts. Recent studies extend SAM to Few-shot Semantic\nSegmentation (FSS), focusing on prompt generation for SAM-based automatic\nsemantic segmentation. However, these methods struggle with selecting suitable\nprompts, require specific hyperparameter settings for different scenarios, and\nexperience prolonged one-shot inference times due to the overuse of SAM,\nresulting in low efficiency and limited automation ability. To address these\nissues, we propose a simple yet effective approach based on graph analysis. In\nparticular, a Positive-Negative Alignment module dynamically selects the point\nprompts for generating masks, especially uncovering the potential of the\nbackground context as the negative reference. Another subsequent Point-Mask\nClustering module aligns the granularity of masks and selected points as a\ndirected graph, based on mask coverage over points. These points are then\naggregated by decomposing the weakly connected components of the directed graph\nin an efficient manner, constructing distinct natural clusters. Finally, the\npositive and overshooting gating, benefiting from graph-based granularity\nalignment, aggregate high-confident masks and filter out the false-positive\nmasks for final prediction, reducing the usage of additional hyperparameters\nand redundant mask generation. Extensive experimental analysis across standard\nFSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the\neffectiveness and efficiency of the proposed approach, surpassing\nstate-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%\non LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/."
                },
                "authors": [
                    {
                        "name": "Anqi Zhang"
                    },
                    {
                        "name": "Guangyu Gao"
                    },
                    {
                        "name": "Jianbo Jiao"
                    },
                    {
                        "name": "Chi Harold Liu"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "arxiv_comment": "Accepted to NeurIPS 2024 as Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06963v1",
                "updated": "2024-10-09T15:02:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    8,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:02:08Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    8,
                    2,
                    283,
                    0
                ],
                "title": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling"
                },
                "summary": "This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}"
                },
                "authors": [
                    {
                        "name": "Deok-Kyeong Jang"
                    },
                    {
                        "name": "Dongseok Yang"
                    },
                    {
                        "name": "Deok-Yun Jang"
                    },
                    {
                        "name": "Byeoli Choi"
                    },
                    {
                        "name": "Donghoon Shin"
                    },
                    {
                        "name": "Sung-hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sung-hee Lee"
                },
                "author": "Sung-hee Lee",
                "arxiv_doi": "10.1145/3687991",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3687991",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.06963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06961v1",
                "updated": "2024-10-09T14:57:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    57,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:57:31Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    57,
                    31,
                    2,
                    283,
                    0
                ],
                "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Boosting Large Language Models with Synthetic Preference Data"
                },
                "summary": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard."
                },
                "authors": [
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06954v1",
                "updated": "2024-10-09T14:51:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    51,
                    58,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:51:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    51,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "How Unique is Whose Web Browser? The role of demographics in browser\n  fingerprinting among US users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Unique is Whose Web Browser? The role of demographics in browser\n  fingerprinting among US users"
                },
                "summary": "Browser fingerprinting can be used to identify and track users across the\nWeb, even without cookies, by collecting attributes from users' devices to\ncreate unique \"fingerprints\". This technique and resulting privacy risks have\nbeen studied for over a decade. Yet further research is limited because prior\nstudies used data not publicly available. Additionally, data in prior studies\nlacked user demographics. Here we provide a first-of-its-kind dataset to enable\nfurther research. It includes browser attributes with users' demographics and\nsurvey responses, collected with informed consent from 8,400 US study\nparticipants. We use this dataset to demonstrate how fingerprinting risks\ndiffer across demographic groups. For example, we find lower income users are\nmore at risk, and find that as users' age increases, they are both more likely\nto be concerned about fingerprinting and at real risk of fingerprinting.\nFurthermore, we demonstrate an overlooked risk: user demographics, such as\ngender, age, income level and race, can be inferred from browser attributes\ncommonly used for fingerprinting, and we identify which browser attributes most\ncontribute to this risk. Our data collection process also conducted an\nexperiment to study what impacts users' likelihood to share browser data for\nopen research, in order to inform future data collection efforts, with\nresponses from 12,461 total participants. Female participants were\nsignificantly less likely to share their browser data, as were participants who\nwere shown the browser data we asked to collect. Overall, we show the important\nrole of user demographics in the ongoing work that intends to assess\nfingerprinting risks and improve user privacy, with findings to inform future\nprivacy enhancing browser developments. The dataset and data collection tool we\nprovide can be used to further study research questions not addressed in this\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Browser fingerprinting can be used to identify and track users across the\nWeb, even without cookies, by collecting attributes from users' devices to\ncreate unique \"fingerprints\". This technique and resulting privacy risks have\nbeen studied for over a decade. Yet further research is limited because prior\nstudies used data not publicly available. Additionally, data in prior studies\nlacked user demographics. Here we provide a first-of-its-kind dataset to enable\nfurther research. It includes browser attributes with users' demographics and\nsurvey responses, collected with informed consent from 8,400 US study\nparticipants. We use this dataset to demonstrate how fingerprinting risks\ndiffer across demographic groups. For example, we find lower income users are\nmore at risk, and find that as users' age increases, they are both more likely\nto be concerned about fingerprinting and at real risk of fingerprinting.\nFurthermore, we demonstrate an overlooked risk: user demographics, such as\ngender, age, income level and race, can be inferred from browser attributes\ncommonly used for fingerprinting, and we identify which browser attributes most\ncontribute to this risk. Our data collection process also conducted an\nexperiment to study what impacts users' likelihood to share browser data for\nopen research, in order to inform future data collection efforts, with\nresponses from 12,461 total participants. Female participants were\nsignificantly less likely to share their browser data, as were participants who\nwere shown the browser data we asked to collect. Overall, we show the important\nrole of user demographics in the ongoing work that intends to assess\nfingerprinting risks and improve user privacy, with findings to inform future\nprivacy enhancing browser developments. The dataset and data collection tool we\nprovide can be used to further study research questions not addressed in this\nwork."
                },
                "authors": [
                    {
                        "name": "Alex Berke"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Enrico Bacis"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Robin Lassonde"
                    },
                    {
                        "name": "Pasin Manurangsi"
                    },
                    {
                        "name": "Umar Syed"
                    }
                ],
                "author_detail": {
                    "name": "Umar Syed"
                },
                "author": "Umar Syed",
                "arxiv_comment": "Accepted to 25th Privacy Enhancing Technologies Symposium (PETS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06949v1",
                "updated": "2024-10-09T14:45:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    45,
                    45,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:45:45Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    45,
                    45,
                    2,
                    283,
                    0
                ],
                "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach"
                },
                "summary": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "26 pages, 7 figures. Submitted ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06943v1",
                "updated": "2024-10-09T14:38:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    38,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:38:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    38,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API\n  Request Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API\n  Request Generation"
                },
                "summary": "Large Language Models (LLMs) leverage external tools primarily through\ngenerating the API request to enhance task completion efficiency. The accuracy\nof API request generation significantly determines the capability of LLMs to\naccomplish tasks.\n  Due to the inherent hallucinations within the LLM, it is difficult to\nefficiently and accurately generate the correct API request.\n  Current research uses prompt-based feedback to facilitate the LLM-based API\nrequest generation. However, existing methods lack factual information and are\ninsufficiently detailed.\n  To address these issues, we propose AutoFeedback, an LLM-based framework for\nefficient and accurate API request generation, with a Static Scanning Component\n(SSC) and a Dynamic Analysis Component (DAC). SSC incorporates errors detected\nin the API requests as pseudo-facts into the feedback, enriching the factual\ninformation. DAC retrieves information from API documentation, enhancing the\nlevel of detail in feedback.\n  Based on this two components, Autofeedback implementes two feedback loops\nduring the process of generating API requests by the LLM.\n  Extensive experiments demonstrate that it significantly improves accuracy of\nAPI request generation and reduces the interaction cost. AutoFeedback achieves\nan accuracy of 100.00\\% on a real-world API dataset and reduces the cost of\ninteraction with GPT-3.5 Turbo by 23.44\\%, and GPT-4 Turbo by 11.85\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) leverage external tools primarily through\ngenerating the API request to enhance task completion efficiency. The accuracy\nof API request generation significantly determines the capability of LLMs to\naccomplish tasks.\n  Due to the inherent hallucinations within the LLM, it is difficult to\nefficiently and accurately generate the correct API request.\n  Current research uses prompt-based feedback to facilitate the LLM-based API\nrequest generation. However, existing methods lack factual information and are\ninsufficiently detailed.\n  To address these issues, we propose AutoFeedback, an LLM-based framework for\nefficient and accurate API request generation, with a Static Scanning Component\n(SSC) and a Dynamic Analysis Component (DAC). SSC incorporates errors detected\nin the API requests as pseudo-facts into the feedback, enriching the factual\ninformation. DAC retrieves information from API documentation, enhancing the\nlevel of detail in feedback.\n  Based on this two components, Autofeedback implementes two feedback loops\nduring the process of generating API requests by the LLM.\n  Extensive experiments demonstrate that it significantly improves accuracy of\nAPI request generation and reduces the interaction cost. AutoFeedback achieves\nan accuracy of 100.00\\% on a real-world API dataset and reduces the cost of\ninteraction with GPT-3.5 Turbo by 23.44\\%, and GPT-4 Turbo by 11.85\\%."
                },
                "authors": [
                    {
                        "name": "Huanxi Liu"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Dawei Feng"
                    },
                    {
                        "name": "Kele Xu"
                    },
                    {
                        "name": "Huaimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huaimin Wang"
                },
                "author": "Huaimin Wang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06932v1",
                "updated": "2024-10-09T14:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    26,
                    20,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:26:20Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    26,
                    20,
                    2,
                    283,
                    0
                ],
                "title": "Reproducing and Extending Experiments in Behavioral Strategy with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing and Extending Experiments in Behavioral Strategy with Large\n  Language Models"
                },
                "summary": "In this study, we propose LLM agents as a novel approach in behavioral\nstrategy research, complementing simulations and laboratory experiments to\nadvance our understanding of cognitive processes in decision-making.\nSpecifically, we reproduce a human laboratory experiment in behavioral strategy\nusing large language model (LLM) generated agents and investigate how LLM\nagents compare to observed human behavior. Our results show that LLM agents\neffectively reproduce search behavior and decision-making comparable to humans.\nExtending our experiment, we analyze LLM agents' simulated \"thoughts,\"\ndiscovering that more forward-looking thoughts correlate with favoring\nexploitation over exploration to maximize wealth. We show how this new approach\ncan be leveraged in behavioral strategy research and address limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose LLM agents as a novel approach in behavioral\nstrategy research, complementing simulations and laboratory experiments to\nadvance our understanding of cognitive processes in decision-making.\nSpecifically, we reproduce a human laboratory experiment in behavioral strategy\nusing large language model (LLM) generated agents and investigate how LLM\nagents compare to observed human behavior. Our results show that LLM agents\neffectively reproduce search behavior and decision-making comparable to humans.\nExtending our experiment, we analyze LLM agents' simulated \"thoughts,\"\ndiscovering that more forward-looking thoughts correlate with favoring\nexploitation over exploration to maximize wealth. We show how this new approach\ncan be leveraged in behavioral strategy research and address limitations."
                },
                "authors": [
                    {
                        "name": "Daniel Albert"
                    },
                    {
                        "name": "Stephan Billinger"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Billinger"
                },
                "author": "Stephan Billinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06916v1",
                "updated": "2024-10-09T14:15:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    15,
                    30,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:15:30Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    15,
                    30,
                    2,
                    283,
                    0
                ],
                "title": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration"
                },
                "summary": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by first employing a compact model to draft multiple tokens\nefficiently and then using the target LLM to verify them in parallel. While\nthis technique has achieved notable speedups, most existing approaches\nnecessitate either additional parameters or extensive training to construct\neffective draft models, thereby restricting their applicability across\ndifferent LLMs and tasks. To address this limitation, we explore a novel\nplug-and-play SD solution with layer-skipping, which skips intermediate layers\nof the target LLM as the compact draft model. Our analysis reveals that LLMs\nexhibit great potential for self-acceleration through layer sparsity and the\ntask-specific nature of this sparsity. Building on these insights, we introduce\nSWIFT, an on-the-fly self-speculative decoding algorithm that adaptively\nselects intermediate layers of LLMs to skip during inference. SWIFT does not\nrequire auxiliary models or additional training, making it a plug-and-play\nsolution for accelerating LLM inference across diverse input data streams. Our\nextensive experiments across a wide range of models and downstream tasks\ndemonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving\nthe original distribution of the generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by first employing a compact model to draft multiple tokens\nefficiently and then using the target LLM to verify them in parallel. While\nthis technique has achieved notable speedups, most existing approaches\nnecessitate either additional parameters or extensive training to construct\neffective draft models, thereby restricting their applicability across\ndifferent LLMs and tasks. To address this limitation, we explore a novel\nplug-and-play SD solution with layer-skipping, which skips intermediate layers\nof the target LLM as the compact draft model. Our analysis reveals that LLMs\nexhibit great potential for self-acceleration through layer sparsity and the\ntask-specific nature of this sparsity. Building on these insights, we introduce\nSWIFT, an on-the-fly self-speculative decoding algorithm that adaptively\nselects intermediate layers of LLMs to skip during inference. SWIFT does not\nrequire auxiliary models or additional training, making it a plug-and-play\nsolution for accelerating LLM inference across diverse input data streams. Our\nextensive experiments across a wide range of models and downstream tasks\ndemonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving\nthe original distribution of the generated text."
                },
                "authors": [
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12139v2",
                "updated": "2024-10-09T14:13:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    13,
                    0,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-20T16:01:01Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    16,
                    1,
                    1,
                    0,
                    141,
                    0
                ],
                "title": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on\n  LLM"
                },
                "summary": "Visual Language Tracking (VLT) enhances single object tracking (SOT) by\nintegrating natural language descriptions from a video, for the precise\ntracking of a specified object. By leveraging high-level semantic information,\nVLT guides object tracking, alleviating the constraints associated with relying\non a visual modality. Nevertheless, most VLT benchmarks are annotated in a\nsingle granularity and lack a coherent semantic framework to provide scientific\nguidance. Moreover, coordinating human annotators for high-quality annotations\nis laborious and time-consuming. To address these challenges, we introduce\nDTLLM-VLT, which automatically generates extensive and multi-granularity text\nto enhance environmental diversity. (1) DTLLM-VLT generates scientific and\nmulti-granularity text descriptions using a cohesive prompt framework. Its\nsuccinct and highly adaptable design allows seamless integration into various\nvisual tracking benchmarks. (2) We select three prominent benchmarks to deploy\nour approach: short-term tracking, long-term tracking, and global instance\ntracking. We offer four granularity combinations for these benchmarks,\nconsidering the extent and density of semantic information, thereby showcasing\nthe practicality and versatility of DTLLM-VLT. (3) We conduct comparative\nexperiments on VLT benchmarks with different text granularities, evaluating and\nanalyzing the impact of diverse text on tracking performance. Conclusionally,\nthis work leverages LLM to provide multi-granularity semantic information for\nVLT task from efficient and diverse perspectives, enabling fine-grained\nevaluation of multi-modal trackers. In the future, we believe this work can be\nextended to more datasets to support vision datasets understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Tracking (VLT) enhances single object tracking (SOT) by\nintegrating natural language descriptions from a video, for the precise\ntracking of a specified object. By leveraging high-level semantic information,\nVLT guides object tracking, alleviating the constraints associated with relying\non a visual modality. Nevertheless, most VLT benchmarks are annotated in a\nsingle granularity and lack a coherent semantic framework to provide scientific\nguidance. Moreover, coordinating human annotators for high-quality annotations\nis laborious and time-consuming. To address these challenges, we introduce\nDTLLM-VLT, which automatically generates extensive and multi-granularity text\nto enhance environmental diversity. (1) DTLLM-VLT generates scientific and\nmulti-granularity text descriptions using a cohesive prompt framework. Its\nsuccinct and highly adaptable design allows seamless integration into various\nvisual tracking benchmarks. (2) We select three prominent benchmarks to deploy\nour approach: short-term tracking, long-term tracking, and global instance\ntracking. We offer four granularity combinations for these benchmarks,\nconsidering the extent and density of semantic information, thereby showcasing\nthe practicality and versatility of DTLLM-VLT. (3) We conduct comparative\nexperiments on VLT benchmarks with different text granularities, evaluating and\nanalyzing the impact of diverse text on tracking performance. Conclusionally,\nthis work leverages LLM to provide multi-granularity semantic information for\nVLT task from efficient and diverse perspectives, enabling fine-grained\nevaluation of multi-modal trackers. In the future, we believe this work can be\nextended to more datasets to support vision datasets understanding."
                },
                "authors": [
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Dailing Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "arxiv_comment": "Accepted by CVPR Workshop 2024, Oral Presentation, Best Paper\n  Honorable Mention Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06913v1",
                "updated": "2024-10-09T14:12:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:12:51Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning"
                },
                "summary": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict emerges when the RAIT data is constructed solely\non correctness criteria, causing similar samples in the LLM's feature space to\nbe assigned different labels (original vs. modified \"I don't know\"). Dynamic\nconflict occurs due to the changes of LLM's knowledge state during fine-tuning,\nwhich transforms previous unknown questions into knowns, while the training\ndata, which is constructed based on the initial LLM, remains unchanged. These\nconflicts cause the trained LLM to misclassify known questions as unknown,\nresulting in over-refusal. To address this issue, we introduce Certainty\nRepresented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).\nCRaFT centers on two main contributions: First, we additionally incorporate\nresponse certainty to selectively filter and modify data, reducing static\nconflicts. Second, we implement preliminary rehearsal training to characterize\nchanges in the LLM's knowledge state, which helps mitigate dynamic conflicts\nduring the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Source code and training data will be released at Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict emerges when the RAIT data is constructed solely\non correctness criteria, causing similar samples in the LLM's feature space to\nbe assigned different labels (original vs. modified \"I don't know\"). Dynamic\nconflict occurs due to the changes of LLM's knowledge state during fine-tuning,\nwhich transforms previous unknown questions into knowns, while the training\ndata, which is constructed based on the initial LLM, remains unchanged. These\nconflicts cause the trained LLM to misclassify known questions as unknown,\nresulting in over-refusal. To address this issue, we introduce Certainty\nRepresented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).\nCRaFT centers on two main contributions: First, we additionally incorporate\nresponse certainty to selectively filter and modify data, reducing static\nconflicts. Second, we implement preliminary rehearsal training to characterize\nchanges in the LLM's knowledge state, which helps mitigate dynamic conflicts\nduring the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Source code and training data will be released at Github."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02492v2",
                "updated": "2024-10-09T14:07:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    7,
                    15,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-03T13:57:07Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    57,
                    7,
                    3,
                    277,
                    0
                ],
                "title": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM"
                },
                "summary": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/."
                },
                "authors": [
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Dailing Zhang"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "arxiv_comment": "Preprint, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06898v1",
                "updated": "2024-10-09T13:59:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    59,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:59:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    59,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "Generative Model for Less-Resourced Language with 1 billion parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Model for Less-Resourced Language with 1 billion parameters"
                },
                "summary": "Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model."
                },
                "authors": [
                    {
                        "name": "Domen Vreš"
                    },
                    {
                        "name": "Martin Božič"
                    },
                    {
                        "name": "Aljaž Potočnik"
                    },
                    {
                        "name": "Tomaž Martinčič"
                    },
                    {
                        "name": "Marko Robnik-Šikonja"
                    }
                ],
                "author_detail": {
                    "name": "Marko Robnik-Šikonja"
                },
                "author": "Marko Robnik-Šikonja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06886v1",
                "updated": "2024-10-09T13:47:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    47,
                    50,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:47:50Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    47,
                    50,
                    2,
                    283,
                    0
                ],
                "title": "FltLM: An Intergrated Long-Context Large Language Model for Effective\n  Context Filtering and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FltLM: An Intergrated Long-Context Large Language Model for Effective\n  Context Filtering and Understanding"
                },
                "summary": "The development of Long-Context Large Language Models (LLMs) has markedly\nadvanced natural language processing by facilitating the process of textual\ndata across long documents and multiple corpora. However, Long-Context LLMs\nstill face two critical challenges: The lost in the middle phenomenon, where\ncrucial middle-context information is likely to be missed, and the distraction\nissue that the models lose focus due to overly extended contexts. To address\nthese challenges, we propose the Context Filtering Language Model (FltLM), a\nnovel integrated Long-Context LLM which enhances the ability of the model on\nmulti-document question-answering (QA) tasks. Specifically, FltLM innovatively\nincorporates a context filter with a soft mask mechanism, identifying and\ndynamically excluding irrelevant content to concentrate on pertinent\ninformation for better comprehension and reasoning. Our approach not only\nmitigates these two challenges, but also enables the model to operate\nconveniently in a single forward pass. Experimental results demonstrate that\nFltLM significantly outperforms supervised fine-tuning and retrieval-based\nmethods in complex QA scenarios, suggesting a promising solution for more\naccurate and reliable long-context natural language understanding applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Long-Context Large Language Models (LLMs) has markedly\nadvanced natural language processing by facilitating the process of textual\ndata across long documents and multiple corpora. However, Long-Context LLMs\nstill face two critical challenges: The lost in the middle phenomenon, where\ncrucial middle-context information is likely to be missed, and the distraction\nissue that the models lose focus due to overly extended contexts. To address\nthese challenges, we propose the Context Filtering Language Model (FltLM), a\nnovel integrated Long-Context LLM which enhances the ability of the model on\nmulti-document question-answering (QA) tasks. Specifically, FltLM innovatively\nincorporates a context filter with a soft mask mechanism, identifying and\ndynamically excluding irrelevant content to concentrate on pertinent\ninformation for better comprehension and reasoning. Our approach not only\nmitigates these two challenges, but also enables the model to operate\nconveniently in a single forward pass. Experimental results demonstrate that\nFltLM significantly outperforms supervised fine-tuning and retrieval-based\nmethods in complex QA scenarios, suggesting a promising solution for more\naccurate and reliable long-context natural language understanding applications."
                },
                "authors": [
                    {
                        "name": "Jingyang Deng"
                    },
                    {
                        "name": "Zhengyang Shen"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Suqi Cheng"
                    },
                    {
                        "name": "Ying Nie"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "arxiv_comment": "Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI-2024), this is the full version of the paper including technical\n  appendices. This final version features enhanced formatting and corrections\n  to errors present in other online versions. We regret any inconvenience this\n  may have caused our readers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06885v1",
                "updated": "2024-10-09T13:46:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    46,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:46:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    46,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow\n  Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow\n  Matching"
                },
                "summary": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech\nsystem based on flow matching with Diffusion Transformer (DiT). Without\nrequiring complex designs such as duration model, text encoder, and phoneme\nalignment, the text input is simply padded with filler tokens to the same\nlength as input speech, and then the denoising is performed for speech\ngeneration, which was originally proved feasible by E2 TTS. However, the\noriginal design of E2 TTS makes it hard to follow due to its slow convergence\nand low robustness. To address these issues, we first model the input with\nConvNeXt to refine the text representation, making it easy to align with the\nspeech. We further propose an inference-time Sway Sampling strategy, which\nsignificantly improves our model's performance and efficiency. This sampling\nstrategy for flow step can be easily applied to existing flow matching based\nmodels without retraining. Our design allows faster training and achieves an\ninference RTF of 0.15, which is greatly improved compared to state-of-the-art\ndiffusion-based TTS models. Trained on a public 100K hours multilingual\ndataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching\n(F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless\ncode-switching capability, and speed control efficiency. Demo samples can be\nfound at https://SWivid.github.io/F5-TTS. We release all code and checkpoints\nto promote community development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech\nsystem based on flow matching with Diffusion Transformer (DiT). Without\nrequiring complex designs such as duration model, text encoder, and phoneme\nalignment, the text input is simply padded with filler tokens to the same\nlength as input speech, and then the denoising is performed for speech\ngeneration, which was originally proved feasible by E2 TTS. However, the\noriginal design of E2 TTS makes it hard to follow due to its slow convergence\nand low robustness. To address these issues, we first model the input with\nConvNeXt to refine the text representation, making it easy to align with the\nspeech. We further propose an inference-time Sway Sampling strategy, which\nsignificantly improves our model's performance and efficiency. This sampling\nstrategy for flow step can be easily applied to existing flow matching based\nmodels without retraining. Our design allows faster training and achieves an\ninference RTF of 0.15, which is greatly improved compared to state-of-the-art\ndiffusion-based TTS models. Trained on a public 100K hours multilingual\ndataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching\n(F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless\ncode-switching capability, and speed control efficiency. Demo samples can be\nfound at https://SWivid.github.io/F5-TTS. We release all code and checkpoints\nto promote community development."
                },
                "authors": [
                    {
                        "name": "Yushen Chen"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Chunhui Wang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02404v2",
                "updated": "2024-10-09T13:39:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    39,
                    54,
                    2,
                    283,
                    0
                ],
                "published": "2023-10-03T19:57:42Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    19,
                    57,
                    42,
                    1,
                    276,
                    0
                ],
                "title": "How to infer ocean freezing rates on icy satellites from measurements of\n  ice thickness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to infer ocean freezing rates on icy satellites from measurements of\n  ice thickness"
                },
                "summary": "Liquid-water oceans likely underlie the ice shells of Europa and Enceladus,\nbut ocean properties are challenging to measure due to the overlying ice. Here,\nwe consider gravity-driven flow of the ice shells of icy satellites and relate\nthis to ocean freeze and melt rates. We employ a first-principles approach\napplicable to conductive ice shells in a Cartesian geometry. We derive a\nscaling law under which ocean freeze/melt rates can be estimated from\nshell-thickness measurements. Under a steady-state assumption, ocean\nfreeze/melt rates can be inferred from measurements of ice thickness, given a\nbasal viscosity. Depending on a characteristic thickness scale and basal\nviscosity, characteristic freeze/melt rates range from around O(10$^{-1}$) to\nO(10$^{-5}$) mm/year. Our scaling is validated with ice-penetrating radar\nmeasurements of ice thickness and modelled snow accumulation for Roosevelt\nIsland, Antarctica. Our model, coupled with observations of shell thickness,\ncould help estimate the magnitudes of ocean freeze/melt rates on icy\nsatellites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid-water oceans likely underlie the ice shells of Europa and Enceladus,\nbut ocean properties are challenging to measure due to the overlying ice. Here,\nwe consider gravity-driven flow of the ice shells of icy satellites and relate\nthis to ocean freeze and melt rates. We employ a first-principles approach\napplicable to conductive ice shells in a Cartesian geometry. We derive a\nscaling law under which ocean freeze/melt rates can be estimated from\nshell-thickness measurements. Under a steady-state assumption, ocean\nfreeze/melt rates can be inferred from measurements of ice thickness, given a\nbasal viscosity. Depending on a characteristic thickness scale and basal\nviscosity, characteristic freeze/melt rates range from around O(10$^{-1}$) to\nO(10$^{-5}$) mm/year. Our scaling is validated with ice-penetrating radar\nmeasurements of ice thickness and modelled snow accumulation for Roosevelt\nIsland, Antarctica. Our model, coupled with observations of shell thickness,\ncould help estimate the magnitudes of ocean freeze/melt rates on icy\nsatellites."
                },
                "authors": [
                    {
                        "name": "Nicole C. Shibley"
                    },
                    {
                        "name": "Ching-Yao Lai"
                    },
                    {
                        "name": "Riley Culberg"
                    }
                ],
                "author_detail": {
                    "name": "Riley Culberg"
                },
                "author": "Riley Culberg",
                "arxiv_comment": "9 pages, 4 figures. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06927v3",
                "updated": "2024-10-09T13:39:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    39,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-11T00:56:02Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    0,
                    56,
                    2,
                    2,
                    255,
                    0
                ],
                "title": "Representation Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Tuning"
                },
                "summary": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f."
                },
                "authors": [
                    {
                        "name": "Christopher M. Ackerman"
                    }
                ],
                "author_detail": {
                    "name": "Christopher M. Ackerman"
                },
                "author": "Christopher M. Ackerman",
                "arxiv_comment": "9 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06854v1",
                "updated": "2024-10-09T13:17:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    17,
                    22,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:17:22Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    17,
                    22,
                    2,
                    283,
                    0
                ],
                "title": "Focal Surface Holographic Light Transport using Learned Spatially\n  Adaptive Convolutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focal Surface Holographic Light Transport using Learned Spatially\n  Adaptive Convolutions"
                },
                "summary": "Computer-Generated Holography (CGH) is a set of algorithmic methods for\nidentifying holograms that reconstruct Three-Dimensi-onal (3D) scenes in\nholographic displays. CGH algorithms decompose 3D scenes into multiplanes at\ndifferent depth levels and rely on simulations of light that propagated from a\nsource plane to a targeted plane. Thus, for n planes, CGH typically optimizes\nholograms using n plane-to-plane light transport simulations, leading to major\ntime and computational demands. Our work replaces multiple planes with a focal\nsurface and introduces a learned light transport model that could propagate a\nlight field from a source plane to the focal surface in a single inference. Our\nlearned light transport model leverages spatially adaptive convolution to\nachieve depth-varying propagation demanded by targeted focal surfaces. The\nproposed model reduces the hologram optimization process up to 1.5x, which\ncontributes to hologram dataset generation and the training of future learned\nCGH models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Generated Holography (CGH) is a set of algorithmic methods for\nidentifying holograms that reconstruct Three-Dimensi-onal (3D) scenes in\nholographic displays. CGH algorithms decompose 3D scenes into multiplanes at\ndifferent depth levels and rely on simulations of light that propagated from a\nsource plane to a targeted plane. Thus, for n planes, CGH typically optimizes\nholograms using n plane-to-plane light transport simulations, leading to major\ntime and computational demands. Our work replaces multiple planes with a focal\nsurface and introduces a learned light transport model that could propagate a\nlight field from a source plane to the focal surface in a single inference. Our\nlearned light transport model leverages spatially adaptive convolution to\nachieve depth-varying propagation demanded by targeted focal surfaces. The\nproposed model reduces the hologram optimization process up to 1.5x, which\ncontributes to hologram dataset generation and the training of future learned\nCGH models."
                },
                "authors": [
                    {
                        "name": "Chuanjun Zheng"
                    },
                    {
                        "name": "Yicheng Zhan"
                    },
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Ozan Cakmakci"
                    },
                    {
                        "name": "Kaan Akşit"
                    }
                ],
                "author_detail": {
                    "name": "Kaan Akşit"
                },
                "author": "Kaan Akşit",
                "arxiv_comment": "SIGGRAPH Asia 2024 Technical Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06853v1",
                "updated": "2024-10-09T13:17:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    17,
                    20,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:17:20Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    17,
                    20,
                    2,
                    283,
                    0
                ],
                "title": "Microlensing analysis of 14.5-year light curves in SDSS J1004+4112:\n  Quasar accretion disk size and intracluster stellar mass fraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microlensing analysis of 14.5-year light curves in SDSS J1004+4112:\n  Quasar accretion disk size and intracluster stellar mass fraction"
                },
                "summary": "Context. The gravitational lens system SDSS J1004+4112 was the first known\nexample of a quasar lensed by a galaxy cluster. The interest in this system has\nbeen renewed following the publication of r-band light curves spanning 14.5\nyears and the determination of the time delays between the four brightest\nquasar images.\n  Aims. We constrained the quasar accretion disk size and the fraction of the\nlens mass in stars using the signature of microlensing in the quasar image\nlight curves.\n  Methods. We built the six possible histograms of microlensing magnitude\ndifferences between the four quasar images and compared them with simulated\nmodel histograms, using a $\\chi^2$ test to infer the model parameters.\n  Results. We infer a quasar disk half-light radius of $R_{1/2}=(0.70\\pm0.04)\\,\nR_E=(6.4\\pm0.4) \\sqrt{M/0.3M_{\\odot}}$ light-days at 2407\\r{A} in the rest\nframe and stellar mass fractions at the quasar image positions of\n$\\alpha_A>0.059$, $\\alpha_B=0.056^{+0.021}_{-0.027}$,\n$\\alpha_C=0.030^{+0.031}_{-0.021}$, and $\\alpha_D=0.072^{+0.034}_{-0.016}$.\n  Conclusions. The inferred disk size is broadly compatible with most previous\nestimates, and the stellar mass fractions are within the expected ranges for\ngalaxy clusters. In the region where image C lies, the stellar mass fraction is\ncompatible with a stellar contribution from the brightest cluster galaxy,\ngalaxy cluster members, and intracluster light, but the values at images B, D,\nand especially A are slightly larger, possibly suggesting the presence of extra\nstellar components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The gravitational lens system SDSS J1004+4112 was the first known\nexample of a quasar lensed by a galaxy cluster. The interest in this system has\nbeen renewed following the publication of r-band light curves spanning 14.5\nyears and the determination of the time delays between the four brightest\nquasar images.\n  Aims. We constrained the quasar accretion disk size and the fraction of the\nlens mass in stars using the signature of microlensing in the quasar image\nlight curves.\n  Methods. We built the six possible histograms of microlensing magnitude\ndifferences between the four quasar images and compared them with simulated\nmodel histograms, using a $\\chi^2$ test to infer the model parameters.\n  Results. We infer a quasar disk half-light radius of $R_{1/2}=(0.70\\pm0.04)\\,\nR_E=(6.4\\pm0.4) \\sqrt{M/0.3M_{\\odot}}$ light-days at 2407\\r{A} in the rest\nframe and stellar mass fractions at the quasar image positions of\n$\\alpha_A>0.059$, $\\alpha_B=0.056^{+0.021}_{-0.027}$,\n$\\alpha_C=0.030^{+0.031}_{-0.021}$, and $\\alpha_D=0.072^{+0.034}_{-0.016}$.\n  Conclusions. The inferred disk size is broadly compatible with most previous\nestimates, and the stellar mass fractions are within the expected ranges for\ngalaxy clusters. In the region where image C lies, the stellar mass fraction is\ncompatible with a stellar contribution from the brightest cluster galaxy,\ngalaxy cluster members, and intracluster light, but the values at images B, D,\nand especially A are slightly larger, possibly suggesting the presence of extra\nstellar components."
                },
                "authors": [
                    {
                        "name": "R. Forés-Toribio"
                    },
                    {
                        "name": "J. A. Muñoz"
                    },
                    {
                        "name": "C. Fian"
                    },
                    {
                        "name": "J. Jiménez-Vicente"
                    },
                    {
                        "name": "E. Mediavilla"
                    }
                ],
                "author_detail": {
                    "name": "E. Mediavilla"
                },
                "author": "E. Mediavilla",
                "arxiv_comment": "7 pages, 6 figures, 2 tables, A&A in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.10652v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.10652v4",
                "updated": "2024-10-09T13:06:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    6,
                    48,
                    2,
                    283,
                    0
                ],
                "published": "2023-05-18T02:19:05Z",
                "published_parsed": [
                    2023,
                    5,
                    18,
                    2,
                    19,
                    5,
                    3,
                    138,
                    0
                ],
                "title": "Speech Separation based on Contrastive Learning and Deep Modularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Separation based on Contrastive Learning and Deep Modularization"
                },
                "summary": "The current monaural state of the art tools for speech separation relies on\nsupervised learning. This means that they must deal with permutation problem,\nthey are impacted by the mismatch on the number of speakers used in training\nand inference. Moreover, their performance heavily relies on the presence of\nhigh-quality labelled data. These problems can be effectively addressed by\nemploying a fully unsupervised technique for speech separation. In this paper,\nwe use contrastive learning to establish the representations of frames then use\nthe learned representations in the downstream deep modularization task.\nConcretely, we demonstrate experimentally that in speech separation, different\nframes of a speaker can be viewed as augmentations of a given hidden standard\nframe of that speaker. The frames of a speaker contain enough prosodic\ninformation overlap which is key in speech separation. Based on this, we\nimplement a self-supervised learning to learn to minimize the distance between\nframes belonging to a given speaker. The learned representations are used in a\ndownstream deep modularization task to cluster frames based on speaker\nidentity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix\nshows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively\nin WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7\nrespectively in WSJ0-2mix. Its greatest strength being that as the number of\nspeakers increase, its performance does not degrade significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current monaural state of the art tools for speech separation relies on\nsupervised learning. This means that they must deal with permutation problem,\nthey are impacted by the mismatch on the number of speakers used in training\nand inference. Moreover, their performance heavily relies on the presence of\nhigh-quality labelled data. These problems can be effectively addressed by\nemploying a fully unsupervised technique for speech separation. In this paper,\nwe use contrastive learning to establish the representations of frames then use\nthe learned representations in the downstream deep modularization task.\nConcretely, we demonstrate experimentally that in speech separation, different\nframes of a speaker can be viewed as augmentations of a given hidden standard\nframe of that speaker. The frames of a speaker contain enough prosodic\ninformation overlap which is key in speech separation. Based on this, we\nimplement a self-supervised learning to learn to minimize the distance between\nframes belonging to a given speaker. The learned representations are used in a\ndownstream deep modularization task to cluster frames based on speaker\nidentity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix\nshows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively\nin WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7\nrespectively in WSJ0-2mix. Its greatest strength being that as the number of\nspeakers increase, its performance does not degrade significantly."
                },
                "authors": [
                    {
                        "name": "Peter Ochieng"
                    }
                ],
                "author_detail": {
                    "name": "Peter Ochieng"
                },
                "author": "Peter Ochieng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2212.00369",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.10652v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.10652v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06846v1",
                "updated": "2024-10-09T13:06:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    6,
                    43,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:06:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    6,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity"
                },
                "summary": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12239v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12239v3",
                "updated": "2024-10-09T12:59:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    59,
                    51,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-17T01:11:20Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    1,
                    11,
                    20,
                    2,
                    199,
                    0
                ],
                "title": "Motion and Structure from Event-based Normal Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion and Structure from Event-based Normal Flow"
                },
                "summary": "Recovering the camera motion and scene geometry from visual data is a\nfundamental problem in the field of computer vision. Its success in standard\nvision is attributed to the maturity of feature extraction, data association\nand multi-view geometry. The recent emergence of neuromorphic event-based\ncameras places great demands on approaches that use raw event data as input to\nsolve this fundamental problem. Existing state-of-the-art solutions typically\ninfer implicitly data association by iteratively reversing the event data\ngeneration process. However, the nonlinear nature of these methods limits their\napplicability in real-time tasks, and the constant-motion assumption leads to\nunstable results under agile motion. To this end, we rethink the problem\nformulation in a way that aligns better with the differential working principle\nof event cameras. We show that the event-based normal flow can be used, via the\nproposed geometric error term, as an alternative to the full flow in solving a\nfamily of geometric problems that involve instantaneous first-order kinematics\nand scene geometry. Furthermore, we develop a fast linear solver and a\ncontinuous-time nonlinear solver on top of the proposed geometric error term.\nExperiments on both synthetic and real data show the superiority of our linear\nsolver in terms of accuracy and efficiency, and indicate its complementary\nfeature as an initialization method for existing nonlinear solvers. Besides,\nour continuous-time non-linear solver exhibits exceptional capability in\naccommodating sudden variations in motion since it does not rely on the\nconstant-motion assumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering the camera motion and scene geometry from visual data is a\nfundamental problem in the field of computer vision. Its success in standard\nvision is attributed to the maturity of feature extraction, data association\nand multi-view geometry. The recent emergence of neuromorphic event-based\ncameras places great demands on approaches that use raw event data as input to\nsolve this fundamental problem. Existing state-of-the-art solutions typically\ninfer implicitly data association by iteratively reversing the event data\ngeneration process. However, the nonlinear nature of these methods limits their\napplicability in real-time tasks, and the constant-motion assumption leads to\nunstable results under agile motion. To this end, we rethink the problem\nformulation in a way that aligns better with the differential working principle\nof event cameras. We show that the event-based normal flow can be used, via the\nproposed geometric error term, as an alternative to the full flow in solving a\nfamily of geometric problems that involve instantaneous first-order kinematics\nand scene geometry. Furthermore, we develop a fast linear solver and a\ncontinuous-time nonlinear solver on top of the proposed geometric error term.\nExperiments on both synthetic and real data show the superiority of our linear\nsolver in terms of accuracy and efficiency, and indicate its complementary\nfeature as an initialization method for existing nonlinear solvers. Besides,\nour continuous-time non-linear solver exhibits exceptional capability in\naccommodating sudden variations in motion since it does not rely on the\nconstant-motion assumption."
                },
                "authors": [
                    {
                        "name": "Zhongyang Ren"
                    },
                    {
                        "name": "Bangyan Liao"
                    },
                    {
                        "name": "Delei Kong"
                    },
                    {
                        "name": "Jinghang Li"
                    },
                    {
                        "name": "Peidong Liu"
                    },
                    {
                        "name": "Laurent Kneip"
                    },
                    {
                        "name": "Guillermo Gallego"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "arxiv_comment": "This paper has been accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12239v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12239v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06841v1",
                "updated": "2024-10-09T12:57:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    57,
                    45,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T12:57:45Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    57,
                    45,
                    2,
                    283,
                    0
                ],
                "title": "Boosting Few-Shot Detection with Large Language Models and\n  Layout-to-Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Few-Shot Detection with Large Language Models and\n  Layout-to-Image Synthesis"
                },
                "summary": "Recent advancements in diffusion models have enabled a wide range of works\nexploiting their ability to generate high-volume, high-quality data for use in\nvarious downstream tasks. One subclass of such models, dubbed Layout-to-Image\nSynthesis (LIS), learns to generate images conditioned on a spatial layout\n(bounding boxes, masks, poses, etc.) and has shown a promising ability to\ngenerate realistic images, albeit with limited layout-adherence. Moreover, the\nquestion of how to effectively transfer those models for scalable augmentation\nof few-shot detection data remains unanswered. Thus, we propose a collaborative\nframework employing a Large Language Model (LLM) and an LIS model for enhancing\nfew-shot detection beyond state-of-the-art generative augmentation approaches.\nWe leverage LLM's reasoning ability to extrapolate the spatial prior of the\nannotation space by generating new bounding boxes given only a few example\nannotations. Additionally, we introduce our novel layout-aware CLIP score for\nsample ranking, enabling tight coupling between generated layouts and images.\nSignificant improvements on COCO few-shot benchmarks are observed. With our\napproach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on\nthe COCO 5-,10-, and 30-shot settings, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion models have enabled a wide range of works\nexploiting their ability to generate high-volume, high-quality data for use in\nvarious downstream tasks. One subclass of such models, dubbed Layout-to-Image\nSynthesis (LIS), learns to generate images conditioned on a spatial layout\n(bounding boxes, masks, poses, etc.) and has shown a promising ability to\ngenerate realistic images, albeit with limited layout-adherence. Moreover, the\nquestion of how to effectively transfer those models for scalable augmentation\nof few-shot detection data remains unanswered. Thus, we propose a collaborative\nframework employing a Large Language Model (LLM) and an LIS model for enhancing\nfew-shot detection beyond state-of-the-art generative augmentation approaches.\nWe leverage LLM's reasoning ability to extrapolate the spatial prior of the\nannotation space by generating new bounding boxes given only a few example\nannotations. Additionally, we introduce our novel layout-aware CLIP score for\nsample ranking, enabling tight coupling between generated layouts and images.\nSignificant improvements on COCO few-shot benchmarks are observed. With our\napproach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on\nthe COCO 5-,10-, and 30-shot settings, respectively."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdullah"
                    },
                    {
                        "name": "Nikolas Ebert"
                    },
                    {
                        "name": "Oliver Wasenmüller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Wasenmüller"
                },
                "author": "Oliver Wasenmüller",
                "arxiv_comment": "This paper has been accepted at the Asian Conference on Computer\n  Vision (ACCV), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02966v3",
                "updated": "2024-10-09T12:46:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    46,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-05T13:43:58Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    13,
                    43,
                    58,
                    1,
                    65,
                    0
                ],
                "title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering"
                },
                "summary": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary."
                },
                "authors": [
                    {
                        "name": "Sungho Ko"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17264v2",
                "updated": "2024-10-09T12:34:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    34,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-27T15:22:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    15,
                    22,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "On the Noise Robustness of In-Context Learning for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Noise Robustness of In-Context Learning for Text Generation"
                },
                "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations."
                },
                "authors": [
                    {
                        "name": "Hongfu Gao"
                    },
                    {
                        "name": "Feipeng Zhang"
                    },
                    {
                        "name": "Wenyu Jiang"
                    },
                    {
                        "name": "Jun Shu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v4",
                "updated": "2024-10-09T12:29:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    29,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.00214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.00214v2",
                "updated": "2024-10-09T12:26:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    26,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2023-07-01T03:44:05Z",
                "published_parsed": [
                    2023,
                    7,
                    1,
                    3,
                    44,
                    5,
                    5,
                    182,
                    0
                ],
                "title": "Utilizing a Capture-Recapture Strategy to Accelerate Infectious Disease\n  Surveillance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing a Capture-Recapture Strategy to Accelerate Infectious Disease\n  Surveillance"
                },
                "summary": "Monitoring key elements of disease dynamics (e.g., prevalence, case counts)\nis of great importance in infectious disease prevention and control, as\nemphasized during the COVID-19 pandemic. To facilitate this effort, we propose\na new capture-recapture (CRC) analysis strategy that takes misclassification\ninto account from easily-administered, imperfect diagnostic test kits, such as\nthe Rapid Antigen Test-kits or saliva tests. Our method is based on a recently\nproposed \"anchor stream\" design, whereby an existing voluntary surveillance\ndata stream is augmented by a smaller and judiciously drawn random sample. It\nincorporates manufacturer-specified sensitivity and specificity parameters to\naccount for imperfect diagnostic results in one or both data streams. For\ninference to accompany case count estimation, we improve upon traditional\nWald-type confidence intervals by developing an adapted Bayesian credible\ninterval for the CRC estimator that yields favorable frequentist coverage\nproperties. When feasible, the proposed design and analytic strategy provides a\nmore efficient solution than traditional CRC methods or random sampling-based\nbiased-corrected estimation to monitor disease prevalence while accounting for\nmisclassification. We demonstrate the benefits of this approach through\nsimulation studies that underscore its potential utility in practice for\neconomical disease monitoring among a registered closed population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring key elements of disease dynamics (e.g., prevalence, case counts)\nis of great importance in infectious disease prevention and control, as\nemphasized during the COVID-19 pandemic. To facilitate this effort, we propose\na new capture-recapture (CRC) analysis strategy that takes misclassification\ninto account from easily-administered, imperfect diagnostic test kits, such as\nthe Rapid Antigen Test-kits or saliva tests. Our method is based on a recently\nproposed \"anchor stream\" design, whereby an existing voluntary surveillance\ndata stream is augmented by a smaller and judiciously drawn random sample. It\nincorporates manufacturer-specified sensitivity and specificity parameters to\naccount for imperfect diagnostic results in one or both data streams. For\ninference to accompany case count estimation, we improve upon traditional\nWald-type confidence intervals by developing an adapted Bayesian credible\ninterval for the CRC estimator that yields favorable frequentist coverage\nproperties. When feasible, the proposed design and analytic strategy provides a\nmore efficient solution than traditional CRC methods or random sampling-based\nbiased-corrected estimation to monitor disease prevalence while accounting for\nmisclassification. We demonstrate the benefits of this approach through\nsimulation studies that underscore its potential utility in practice for\neconomical disease monitoring among a registered closed population."
                },
                "authors": [
                    {
                        "name": "Lin Ge"
                    },
                    {
                        "name": "Yuzi Zhang"
                    },
                    {
                        "name": "Lance A. Waller"
                    },
                    {
                        "name": "Robert H. Lyles"
                    }
                ],
                "author_detail": {
                    "name": "Robert H. Lyles"
                },
                "author": "Robert H. Lyles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.00214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.00214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19846v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19846v5",
                "updated": "2024-10-09T12:14:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    14,
                    22,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-30T08:50:55Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    8,
                    50,
                    55,
                    3,
                    151,
                    0
                ],
                "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model"
                },
                "summary": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes."
                },
                "authors": [
                    {
                        "name": "Chaochen Gao"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Qi Fu"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19846v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19846v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06814v1",
                "updated": "2024-10-09T12:13:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    13,
                    49,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T12:13:49Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    13,
                    49,
                    2,
                    283,
                    0
                ],
                "title": "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning"
                },
                "summary": "Over-parameterized models are typically vulnerable to membership inference\nattacks, which aim to determine whether a specific sample is included in the\ntraining of a given model. Previous Weight regularizations (e.g., L1\nregularization) typically impose uniform penalties on all parameters, leading\nto a suboptimal tradeoff between model utility and privacy. In this work, we\nfirst show that only a small fraction of parameters substantially impact the\nprivacy risk. In light of this, we propose Privacy-aware Sparsity Tuning\n(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties\nto different parameters. Our key idea behind PAST is to promote sparsity in\nparameters that significantly contribute to privacy leakage. In particular, we\nconstruct the adaptive weight for each parameter based on its privacy\nsensitivity, i.e., the gradient of the loss gap with respect to the parameter.\nUsing PAST, the network shrinks the loss gap between members and non-members,\nleading to strong resistance to privacy attacks. Extensive experiments\ndemonstrate the superiority of PAST, achieving a state-of-the-art balance in\nthe privacy-utility trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-parameterized models are typically vulnerable to membership inference\nattacks, which aim to determine whether a specific sample is included in the\ntraining of a given model. Previous Weight regularizations (e.g., L1\nregularization) typically impose uniform penalties on all parameters, leading\nto a suboptimal tradeoff between model utility and privacy. In this work, we\nfirst show that only a small fraction of parameters substantially impact the\nprivacy risk. In light of this, we propose Privacy-aware Sparsity Tuning\n(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties\nto different parameters. Our key idea behind PAST is to promote sparsity in\nparameters that significantly contribute to privacy leakage. In particular, we\nconstruct the adaptive weight for each parameter based on its privacy\nsensitivity, i.e., the gradient of the loss gap with respect to the parameter.\nUsing PAST, the network shrinks the loss gap between members and non-members,\nleading to strong resistance to privacy attacks. Extensive experiments\ndemonstrate the superiority of PAST, achieving a state-of-the-art balance in\nthe privacy-utility trade-off."
                },
                "authors": [
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Hengxiang Zhang"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18231v2",
                "updated": "2024-10-09T12:11:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    11,
                    15,
                    2,
                    283,
                    0
                ],
                "published": "2024-04-28T15:56:41Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    15,
                    56,
                    41,
                    6,
                    119,
                    0
                ],
                "title": "From Persona to Personalization: A Survey on Role-Playing Language\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Persona to Personalization: A Survey on Role-Playing Language\n  Agents"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony."
                },
                "authors": [
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Nianqi Li"
                    },
                    {
                        "name": "Lida Chen"
                    },
                    {
                        "name": "Caiyu Hu"
                    },
                    {
                        "name": "Siye Wu"
                    },
                    {
                        "name": "Scott Ren"
                    },
                    {
                        "name": "Ziquan Fu"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Accepted to TMLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01121v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01121v4",
                "updated": "2024-10-09T12:10:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    10,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-02T08:05:03Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    8,
                    5,
                    3,
                    5,
                    62,
                    0
                ],
                "title": "OpenGraph: Towards Open Graph Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenGraph: Towards Open Graph Foundation Models"
                },
                "summary": "Graph learning has become essential in various domains, including\nrecommendation systems and social network analysis. Graph Neural Networks\n(GNNs) have emerged as promising techniques for encoding structural information\nand improving performance in tasks like link prediction and node\nclassification. However, a key challenge remains: the difficulty of\ngeneralizing to unseen graph data with different properties. In this work, we\npropose a novel graph foundation model, called OpenGraph, to address this\nchallenge. Our approach tackles several technical obstacles. Firstly, we\nenhance data augmentation using a large language model (LLM) to overcome data\nscarcity in real-world scenarios. Secondly, we introduce a unified graph\ntokenizer that enables the model to generalize effectively to diverse graph\ndata, even when encountering unseen properties during training. Thirdly, our\ndeveloped scalable graph transformer captures node-wise dependencies within the\nglobal topological context. Extensive experiments validate the effectiveness of\nour framework. By adapting OpenGraph to new graph characteristics and\ncomprehending diverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings. We release the model\nimplementation at https://github.com/HKUDS/OpenGraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph learning has become essential in various domains, including\nrecommendation systems and social network analysis. Graph Neural Networks\n(GNNs) have emerged as promising techniques for encoding structural information\nand improving performance in tasks like link prediction and node\nclassification. However, a key challenge remains: the difficulty of\ngeneralizing to unseen graph data with different properties. In this work, we\npropose a novel graph foundation model, called OpenGraph, to address this\nchallenge. Our approach tackles several technical obstacles. Firstly, we\nenhance data augmentation using a large language model (LLM) to overcome data\nscarcity in real-world scenarios. Secondly, we introduce a unified graph\ntokenizer that enables the model to generalize effectively to diverse graph\ndata, even when encountering unseen properties during training. Thirdly, our\ndeveloped scalable graph transformer captures node-wise dependencies within the\nglobal topological context. Extensive experiments validate the effectiveness of\nour framework. By adapting OpenGraph to new graph characteristics and\ncomprehending diverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings. We release the model\nimplementation at https://github.com/HKUDS/OpenGraph."
                },
                "authors": [
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Ben Kao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted by EMNLP'2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01121v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01121v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06809v1",
                "updated": "2024-10-09T12:09:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    9,
                    30,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T12:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    9,
                    30,
                    2,
                    283,
                    0
                ],
                "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level"
                },
                "summary": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xinyi Zeng"
                    },
                    {
                        "name": "Yuying Shang"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03278v2",
                "updated": "2024-10-09T12:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    7,
                    8,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-04T09:50:45Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    50,
                    45,
                    4,
                    278,
                    0
                ],
                "title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What do Large Language Models Need for Machine Translation Evaluation?"
                },
                "summary": "Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility."
                },
                "authors": [
                    {
                        "name": "Shenbin Qian"
                    },
                    {
                        "name": "Archchana Sindhujan"
                    },
                    {
                        "name": "Minnie Kabra"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Constantin Orăsan"
                    },
                    {
                        "name": "Tharindu Ranasinghe"
                    },
                    {
                        "name": "Frédéric Blain"
                    }
                ],
                "author_detail": {
                    "name": "Frédéric Blain"
                },
                "author": "Frédéric Blain",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05127v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05127v3",
                "updated": "2024-10-09T12:01:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    1,
                    24,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-07T17:55:43Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    17,
                    55,
                    43,
                    4,
                    159,
                    0
                ],
                "title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Semantic Equivalence of Tokenization in Multimodal LLM"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/."
                },
                "authors": [
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "Technical Report. The project page:\n  https://chocowu.github.io/SeTok-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05127v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06800v1",
                "updated": "2024-10-09T11:54:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    54,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T11:54:33Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    54,
                    33,
                    2,
                    283,
                    0
                ],
                "title": "Efficient Weight-Space Laplace-Gaussian Filtering and Smoothing for\n  Sequential Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Weight-Space Laplace-Gaussian Filtering and Smoothing for\n  Sequential Deep Learning"
                },
                "summary": "Efficiently learning a sequence of related tasks, such as in continual\nlearning, poses a significant challenge for neural nets due to the delicate\ntrade-off between catastrophic forgetting and loss of plasticity. We address\nthis challenge with a grounded framework for sequentially learning related\ntasks based on Bayesian inference. Specifically, we treat the model's\nparameters as a nonlinear Gaussian state-space model and perform efficient\ninference using Gaussian filtering and smoothing. This general formalism\nsubsumes existing continual learning approaches, while also offering a clearer\nconceptual understanding of its components. Leveraging Laplace approximations\nduring filtering, we construct Gaussian posterior measures on the weight space\nof a neural network for each task. We use it as an efficient regularizer by\nexploiting the structure of the generalized Gauss-Newton matrix (GGN) to\nconstruct diagonal plus low-rank approximations. The dynamics model allows\ntargeted control of the learning process and the incorporation of\ndomain-specific knowledge, such as modeling the type of shift between tasks.\nAdditionally, using Bayesian approximate smoothing can enhance the performance\nof task-specific models without needing to re-access any data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently learning a sequence of related tasks, such as in continual\nlearning, poses a significant challenge for neural nets due to the delicate\ntrade-off between catastrophic forgetting and loss of plasticity. We address\nthis challenge with a grounded framework for sequentially learning related\ntasks based on Bayesian inference. Specifically, we treat the model's\nparameters as a nonlinear Gaussian state-space model and perform efficient\ninference using Gaussian filtering and smoothing. This general formalism\nsubsumes existing continual learning approaches, while also offering a clearer\nconceptual understanding of its components. Leveraging Laplace approximations\nduring filtering, we construct Gaussian posterior measures on the weight space\nof a neural network for each task. We use it as an efficient regularizer by\nexploiting the structure of the generalized Gauss-Newton matrix (GGN) to\nconstruct diagonal plus low-rank approximations. The dynamics model allows\ntargeted control of the learning process and the incorporation of\ndomain-specific knowledge, such as modeling the type of shift between tasks.\nAdditionally, using Bayesian approximate smoothing can enhance the performance\nof task-specific models without needing to re-access any data."
                },
                "authors": [
                    {
                        "name": "Joanna Sliwa"
                    },
                    {
                        "name": "Frank Schneider"
                    },
                    {
                        "name": "Nathanael Bosch"
                    },
                    {
                        "name": "Agustinus Kristiadi"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "arxiv_comment": "20 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12518v2",
                "updated": "2024-10-09T11:48:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    48,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-19T07:18:41Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    18,
                    41,
                    3,
                    263,
                    0
                ],
                "title": "Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical\n  Gaussian Splatting"
                },
                "summary": "We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a\nnovel hierarchical categorical representation, which enables accurate global 3D\nsemantic mapping, scaling-up capability, and explicit semantic label prediction\nin the 3D world. The parameter usage in semantic SLAM systems increases\nsignificantly with the growing complexity of the environment, making it\nparticularly challenging and costly for scene understanding. To address this\nproblem, we introduce a novel hierarchical representation that encodes semantic\ninformation in a compact form into 3D Gaussian Splatting, leveraging the\ncapabilities of large language models (LLMs). We further introduce a novel\nsemantic loss designed to optimize hierarchical semantic information through\nboth inter-level and cross-level optimization. Furthermore, we enhance the\nwhole SLAM system, resulting in improved tracking and mapping performance. Our\nHi-SLAM outperforms existing dense SLAM methods in both mapping and tracking\naccuracy, while achieving a 2x operation speed-up. Additionally, it exhibits\ncompetitive performance in rendering semantic segmentation in small synthetic\nscenes, with significantly reduced storage and training time requirements.\nRendering FPS impressively reaches 2,000 with semantic information and 3,000\nwithout it. Most notably, it showcases the capability of handling the complex\nreal-world scene with more than 500 semantic classes, highlighting its valuable\nscaling-up capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a\nnovel hierarchical categorical representation, which enables accurate global 3D\nsemantic mapping, scaling-up capability, and explicit semantic label prediction\nin the 3D world. The parameter usage in semantic SLAM systems increases\nsignificantly with the growing complexity of the environment, making it\nparticularly challenging and costly for scene understanding. To address this\nproblem, we introduce a novel hierarchical representation that encodes semantic\ninformation in a compact form into 3D Gaussian Splatting, leveraging the\ncapabilities of large language models (LLMs). We further introduce a novel\nsemantic loss designed to optimize hierarchical semantic information through\nboth inter-level and cross-level optimization. Furthermore, we enhance the\nwhole SLAM system, resulting in improved tracking and mapping performance. Our\nHi-SLAM outperforms existing dense SLAM methods in both mapping and tracking\naccuracy, while achieving a 2x operation speed-up. Additionally, it exhibits\ncompetitive performance in rendering semantic segmentation in small synthetic\nscenes, with significantly reduced storage and training time requirements.\nRendering FPS impressively reaches 2,000 with semantic information and 3,000\nwithout it. Most notably, it showcases the capability of handling the complex\nreal-world scene with more than 500 semantic classes, highlighting its valuable\nscaling-up capability."
                },
                "authors": [
                    {
                        "name": "Boying Li"
                    },
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Rezatofighi"
                },
                "author": "Hamid Rezatofighi",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06917v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06917v3",
                "updated": "2024-10-09T11:17:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    17,
                    46,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-09T14:52:52Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    14,
                    52,
                    52,
                    1,
                    191,
                    0
                ],
                "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Accepted to EMNLP Main 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06917v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06917v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06777v1",
                "updated": "2024-10-09T11:14:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    14,
                    7,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T11:14:07Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    14,
                    7,
                    2,
                    283,
                    0
                ],
                "title": "HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric\n  Understanding"
                },
                "summary": "The significant advancements in visual understanding and instruction\nfollowing from Multimodal Large Language Models (MLLMs) have opened up more\npossibilities for broader applications in diverse and universal human-centric\nscenarios. However, existing image-text data may not support the precise\nmodality alignment and integration of multi-grained information, which is\ncrucial for human-centric visual understanding. In this paper, we introduce\nHERM-Bench, a benchmark for evaluating the human-centric understanding\ncapabilities of MLLMs. Our work reveals the limitations of existing MLLMs in\nunderstanding complex human-centric scenarios. To address these challenges, we\npresent HERM-100K, a comprehensive dataset with multi-level human-centric\nannotations, aimed at enhancing MLLMs' training. Furthermore, we develop\nHERM-7B, a MLLM that leverages enhanced training data from HERM-100K.\nEvaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms\nexisting MLLMs across various human-centric dimensions, reflecting the current\ninadequacy of data annotations used in MLLM training for human-centric visual\nunderstanding. This research emphasizes the importance of specialized datasets\nand benchmarks in advancing the MLLMs' capabilities for human-centric\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements in visual understanding and instruction\nfollowing from Multimodal Large Language Models (MLLMs) have opened up more\npossibilities for broader applications in diverse and universal human-centric\nscenarios. However, existing image-text data may not support the precise\nmodality alignment and integration of multi-grained information, which is\ncrucial for human-centric visual understanding. In this paper, we introduce\nHERM-Bench, a benchmark for evaluating the human-centric understanding\ncapabilities of MLLMs. Our work reveals the limitations of existing MLLMs in\nunderstanding complex human-centric scenarios. To address these challenges, we\npresent HERM-100K, a comprehensive dataset with multi-level human-centric\nannotations, aimed at enhancing MLLMs' training. Furthermore, we develop\nHERM-7B, a MLLM that leverages enhanced training data from HERM-100K.\nEvaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms\nexisting MLLMs across various human-centric dimensions, reflecting the current\ninadequacy of data annotations used in MLLM training for human-centric visual\nunderstanding. This research emphasizes the importance of specialized datasets\nand benchmarks in advancing the MLLMs' capabilities for human-centric\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Keliang Li"
                    },
                    {
                        "name": "Zaifei Yang"
                    },
                    {
                        "name": "Jiahe Zhao"
                    },
                    {
                        "name": "Hongze Shen"
                    },
                    {
                        "name": "Ruibing Hou"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03553v2",
                "updated": "2024-10-09T10:49:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    49,
                    8,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-04T16:02:50Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    16,
                    2,
                    50,
                    4,
                    278,
                    0
                ],
                "title": "Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding"
                },
                "summary": "Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Mingze Yin"
                    },
                    {
                        "name": "Yiheng Zhu"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06742v1",
                "updated": "2024-10-09T10:20:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    54,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T10:20:54Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    54,
                    2,
                    283,
                    0
                ],
                "title": "Inference over Unseen Entities, Relations and Literals on Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference over Unseen Entities, Relations and Literals on Knowledge\n  Graphs"
                },
                "summary": "In recent years, knowledge graph embedding models have been successfully\napplied in the transductive setting to tackle various challenging tasks\nincluding link prediction, and query answering. Yet, the transductive setting\ndoes not allow for reasoning over unseen entities, relations, let alone\nnumerical or non-numerical literals. Although increasing efforts are put into\nexploring inductive scenarios, inference over unseen entities, relations, and\nliterals has yet to come. This limitation prohibits the existing methods from\nhandling real-world dynamic knowledge graphs involving heterogeneous\ninformation about the world. Here, we propose a remedy to this limitation. We\npropose the attentive byte-pair encoding layer (BytE) to construct a triple\nembedding from a sequence of byte-pair encoded subword units of entities and\nrelations. Compared to the conventional setting, BytE leads to massive feature\nreuse via weight tying, since it forces a knowledge graph embedding model to\nlearn embeddings for subword units instead of entities and relations directly.\nConsequently, the size of the embedding matrices are not anymore bound to the\nunique number of entities and relations of a knowledge graph. Experimental\nresults show that BytE improves the link prediction performance of 4 knowledge\ngraph embedding models on datasets where the syntactic representations of\ntriples are semantically meaningful. However, benefits of training a knowledge\ngraph embedding model with BytE dissipate on knowledge graphs where entities\nand relations are represented with plain numbers or URIs. We provide an open\nsource implementation of BytE to foster reproducible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, knowledge graph embedding models have been successfully\napplied in the transductive setting to tackle various challenging tasks\nincluding link prediction, and query answering. Yet, the transductive setting\ndoes not allow for reasoning over unseen entities, relations, let alone\nnumerical or non-numerical literals. Although increasing efforts are put into\nexploring inductive scenarios, inference over unseen entities, relations, and\nliterals has yet to come. This limitation prohibits the existing methods from\nhandling real-world dynamic knowledge graphs involving heterogeneous\ninformation about the world. Here, we propose a remedy to this limitation. We\npropose the attentive byte-pair encoding layer (BytE) to construct a triple\nembedding from a sequence of byte-pair encoded subword units of entities and\nrelations. Compared to the conventional setting, BytE leads to massive feature\nreuse via weight tying, since it forces a knowledge graph embedding model to\nlearn embeddings for subword units instead of entities and relations directly.\nConsequently, the size of the embedding matrices are not anymore bound to the\nunique number of entities and relations of a knowledge graph. Experimental\nresults show that BytE improves the link prediction performance of 4 knowledge\ngraph embedding models on datasets where the syntactic representations of\ntriples are semantically meaningful. However, benefits of training a knowledge\ngraph embedding model with BytE dissipate on knowledge graphs where entities\nand relations are represented with plain numbers or URIs. We provide an open\nsource implementation of BytE to foster reproducible research."
                },
                "authors": [
                    {
                        "name": "Caglar Demir"
                    },
                    {
                        "name": "N'Dah Jean Kouagou"
                    },
                    {
                        "name": "Arnab Sharma"
                    },
                    {
                        "name": "Axel-Cyrille Ngonga Ngomo"
                    }
                ],
                "author_detail": {
                    "name": "Axel-Cyrille Ngonga Ngomo"
                },
                "author": "Axel-Cyrille Ngonga Ngomo",
                "arxiv_comment": "8 pages, 4 figures, ECAI 2024 Workshops (CompAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06741v1",
                "updated": "2024-10-09T10:20:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    32,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T10:20:32Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    32,
                    2,
                    283,
                    0
                ],
                "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models"
                },
                "summary": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder."
                },
                "authors": [
                    {
                        "name": "Zi Gong"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Cong Liao"
                    },
                    {
                        "name": "Bingchang Liu"
                    },
                    {
                        "name": "Chaoyu Chen"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "15 pages, main conference of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.07177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07177v1",
                "updated": "2024-10-09T17:59:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Ego: Towards Building Egocentric Multimodal LLMs"
                },
                "summary": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding."
                },
                "authors": [
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Jiasen Lu"
                    },
                    {
                        "name": "Yinfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Yang"
                },
                "author": "Yinfei Yang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07176v1",
                "updated": "2024-10-09T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    58,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Xingchen Wan"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Sercan Ö. Arık"
                    }
                ],
                "author_detail": {
                    "name": "Sercan Ö. Arık"
                },
                "author": "Sercan Ö. Arık",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07174v1",
                "updated": "2024-10-09T17:59:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    45,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:45Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    45,
                    2,
                    283,
                    0
                ],
                "title": "Neural Circuit Architectural Priors for Quadruped Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Circuit Architectural Priors for Quadruped Locomotion"
                },
                "summary": "Learning-based approaches to quadruped locomotion commonly adopt generic\npolicy architectures like fully connected MLPs. As such architectures contain\nfew inductive biases, it is common in practice to incorporate priors in the\nform of rewards, training curricula, imitation data, or trajectory generators.\nIn nature, animals are born with priors in the form of their nervous system's\narchitecture, which has been shaped by evolution to confer innate ability and\nefficient learning. For instance, a horse can walk within hours of birth and\ncan quickly improve with practice. Such architectural priors can also be useful\nin ANN architectures for AI. In this work, we explore the advantages of a\nbiologically inspired ANN architecture for quadruped locomotion based on neural\ncircuits in the limbs and spinal cord of mammals. Our architecture achieves\ngood initial performance and comparable final performance to MLPs, while using\nless data and orders of magnitude fewer parameters. Our architecture also\nexhibits better generalization to task variations, even admitting deployment on\na physical robot without standard sim-to-real methods. This work shows that\nneural circuits can provide valuable architectural priors for locomotion and\nencourages future work in other sensorimotor skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based approaches to quadruped locomotion commonly adopt generic\npolicy architectures like fully connected MLPs. As such architectures contain\nfew inductive biases, it is common in practice to incorporate priors in the\nform of rewards, training curricula, imitation data, or trajectory generators.\nIn nature, animals are born with priors in the form of their nervous system's\narchitecture, which has been shaped by evolution to confer innate ability and\nefficient learning. For instance, a horse can walk within hours of birth and\ncan quickly improve with practice. Such architectural priors can also be useful\nin ANN architectures for AI. In this work, we explore the advantages of a\nbiologically inspired ANN architecture for quadruped locomotion based on neural\ncircuits in the limbs and spinal cord of mammals. Our architecture achieves\ngood initial performance and comparable final performance to MLPs, while using\nless data and orders of magnitude fewer parameters. Our architecture also\nexhibits better generalization to task variations, even admitting deployment on\na physical robot without standard sim-to-real methods. This work shows that\nneural circuits can provide valuable architectural priors for locomotion and\nencourages future work in other sensorimotor skills."
                },
                "authors": [
                    {
                        "name": "Nikhil X. Bhattasali"
                    },
                    {
                        "name": "Venkatesh Pattabiraman"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Grace W. Lindsay"
                    }
                ],
                "author_detail": {
                    "name": "Grace W. Lindsay"
                },
                "author": "Grace W. Lindsay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07173v1",
                "updated": "2024-10-09T17:59:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:33Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    33,
                    2,
                    283,
                    0
                ],
                "title": "Do better language models have crisper vision?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do better language models have crisper vision?"
                },
                "summary": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released."
                },
                "authors": [
                    {
                        "name": "Jona Ruthardt"
                    },
                    {
                        "name": "Gertjan J. Burghouts"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07172v1",
                "updated": "2024-10-09T17:59:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    14,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:14Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    14,
                    2,
                    283,
                    0
                ],
                "title": "Glider: Global and Local Instruction-Driven Expert Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glider: Global and Local Instruction-Driven Expert Router"
                },
                "summary": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to particular domains. This\nhas enabled the creation of powerful and adaptive routing-based \"Model\nMoErging\" methods with the goal of using expert modules to create an aggregate\nsystem with improved performance or generalization. However, existing MoErging\nmethods often prioritize generalization to unseen tasks at the expense of\nperformance on held-in tasks, which limits its practical applicability in\nreal-world deployment scenarios. We observe that current token-level routing\nmechanisms neglect the global semantic context of the input task. This\ntoken-wise independence hinders effective expert selection for held-in tasks,\nas routing decisions fail to incorporate the semantic properties of the task.\nTo address this, we propose, Global and Local Instruction Driven Expert Router\n(GLIDER) that integrates a multi-scale routing mechanism, encompassing a\nsemantic global router and a learned local router. The global router leverages\nLLM's advanced reasoning capabilities for semantic-related contexts to enhance\nexpert selection. Given the input query and LLM, the router generates semantic\ntask instructions that guide the retrieval of the most relevant experts across\nall layers. This global guidance is complemented by a local router that\nfacilitates token-level routing decisions within each module, enabling finer\ncontrol and enhanced performance on unseen tasks. Our experiments using\nT5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves\nsubstantially improved held-in performance while maintaining strong\ngeneralization on held-out tasks. We also perform ablations experiments to dive\ndeeper into the components of GLIDER. Our experiments highlight the importance\nof our multi-scale routing that leverages LLM-driven semantic reasoning for\nMoErging methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to particular domains. This\nhas enabled the creation of powerful and adaptive routing-based \"Model\nMoErging\" methods with the goal of using expert modules to create an aggregate\nsystem with improved performance or generalization. However, existing MoErging\nmethods often prioritize generalization to unseen tasks at the expense of\nperformance on held-in tasks, which limits its practical applicability in\nreal-world deployment scenarios. We observe that current token-level routing\nmechanisms neglect the global semantic context of the input task. This\ntoken-wise independence hinders effective expert selection for held-in tasks,\nas routing decisions fail to incorporate the semantic properties of the task.\nTo address this, we propose, Global and Local Instruction Driven Expert Router\n(GLIDER) that integrates a multi-scale routing mechanism, encompassing a\nsemantic global router and a learned local router. The global router leverages\nLLM's advanced reasoning capabilities for semantic-related contexts to enhance\nexpert selection. Given the input query and LLM, the router generates semantic\ntask instructions that guide the retrieval of the most relevant experts across\nall layers. This global guidance is complemented by a local router that\nfacilitates token-level routing decisions within each module, enabling finer\ncontrol and enhanced performance on unseen tasks. Our experiments using\nT5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves\nsubstantially improved held-in performance while maintaining strong\ngeneralization on held-out tasks. We also perform ablations experiments to dive\ndeeper into the components of GLIDER. Our experiments highlight the importance\nof our multi-scale routing that leverages LLM-driven semantic reasoning for\nMoErging methods."
                },
                "authors": [
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Prateek Yadav"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Yi-Lin Sung"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "Our code is available at https://github.com/UNITES-Lab/glider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07167v1",
                "updated": "2024-10-09T17:59:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    4,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:04Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    4,
                    2,
                    283,
                    0
                ],
                "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate"
                },
                "summary": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate."
                },
                "authors": [
                    {
                        "name": "Qidong Huang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "Project page: https://github.com/shikiw/Modality-Integration-Rate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07166v1",
                "updated": "2024-10-09T17:59:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    0,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:59:00Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    59,
                    0,
                    2,
                    283,
                    0
                ],
                "title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making"
                },
                "summary": "We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making."
                },
                "authors": [
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Qineng Wang"
                    },
                    {
                        "name": "Kangrui Wang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Sanjana Srivastava"
                    },
                    {
                        "name": "Cem Gokmen"
                    },
                    {
                        "name": "Tony Lee"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Ruohan Zhang"
                    },
                    {
                        "name": "Weiyu Liu"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Li Fei-Fei"
                    },
                    {
                        "name": "Jiayuan Mao"
                    },
                    {
                        "name": "Jiajun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Wu"
                },
                "author": "Jiajun Wu",
                "arxiv_comment": "Accepted for oral presentation at NeurIPS 2024 in the Datasets and\n  Benchmarks track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07164v1",
                "updated": "2024-10-09T17:58:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    56,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:58:56Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    56,
                    2,
                    283,
                    0
                ],
                "title": "AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation"
                },
                "summary": "Recent advancements in diffusion models have led to significant improvements\nin the generation and animation of 4D full-body human-object interactions\n(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion\ngeneration, which is limited by the scarcity of realistic large-scale\ninteraction data. This constraint affects their ability to create everyday HOI\nscenes. This paper addresses this challenge using a zero-shot approach with a\npre-trained diffusion model. Despite this potential, achieving our goals is\ndifficult due to the diffusion model's lack of understanding of ''where'' and\n''how'' objects interact with the human body. To tackle these issues, we\nintroduce AvatarGO, a novel framework designed to generate animatable 4D HOI\nscenes directly from textual inputs. Specifically, 1) for the ''where''\nchallenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to\nidentify the contact body part from text prompts, ensuring precise\nrepresentation of human-object spatial relations. 2) For the ''how'' challenge,\nwe introduce correspondence-aware motion optimization that constructs motion\nfields for both human and object models using the linear blend skinning\nfunction from SMPL-X. Our framework not only generates coherent compositional\nmotions, but also exhibits greater robustness in handling penetration issues.\nExtensive experiments with existing methods validate AvatarGO's superior\ngeneration and animation capabilities on a variety of human-object pairs and\ndiverse poses. As the first attempt to synthesize 4D avatars with object\ninteractions, we hope AvatarGO could open new doors for human-centric 4D\ncontent creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion models have led to significant improvements\nin the generation and animation of 4D full-body human-object interactions\n(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion\ngeneration, which is limited by the scarcity of realistic large-scale\ninteraction data. This constraint affects their ability to create everyday HOI\nscenes. This paper addresses this challenge using a zero-shot approach with a\npre-trained diffusion model. Despite this potential, achieving our goals is\ndifficult due to the diffusion model's lack of understanding of ''where'' and\n''how'' objects interact with the human body. To tackle these issues, we\nintroduce AvatarGO, a novel framework designed to generate animatable 4D HOI\nscenes directly from textual inputs. Specifically, 1) for the ''where''\nchallenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to\nidentify the contact body part from text prompts, ensuring precise\nrepresentation of human-object spatial relations. 2) For the ''how'' challenge,\nwe introduce correspondence-aware motion optimization that constructs motion\nfields for both human and object models using the linear blend skinning\nfunction from SMPL-X. Our framework not only generates coherent compositional\nmotions, but also exhibits greater robustness in handling penetration issues.\nExtensive experiments with existing methods validate AvatarGO's superior\ngeneration and animation capabilities on a variety of human-object pairs and\ndiverse poses. As the first attempt to synthesize 4D avatars with object\ninteractions, we hope AvatarGO could open new doors for human-centric 4D\ncontent creation."
                },
                "authors": [
                    {
                        "name": "Yukang Cao"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Project page: https://yukangcao.github.io/AvatarGO/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07163v1",
                "updated": "2024-10-09T17:58:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    12,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning"
                },
                "summary": "In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple."
                },
                "authors": [
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Jiancheng Liu"
                    },
                    {
                        "name": "Licong Lin"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06813v2",
                "updated": "2024-10-09T17:57:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    57,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-09T12:37:54Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    12,
                    37,
                    54,
                    1,
                    191,
                    0
                ],
                "title": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy"
                },
                "summary": "Diplomacy is one of the most sophisticated activities in human society. The\ncomplex interactions among multiple parties/ agents involve various abilities\nlike social reasoning, negotiation arts, and long-term strategy planning.\nPrevious AI agents surely have proved their capability of handling multi-step\ngames and larger action spaces on tasks involving multiple agents. However,\ndiplomacy involves a staggering magnitude of decision spaces, especially\nconsidering the negotiation stage required. Recently, LLM agents have shown\ntheir potential for extending the boundary of previous agents on a couple of\napplications, however, it is still not enough to handle a very long planning\nperiod in a complex multi-agent environment. Empowered with cutting-edge LLM\ntechnology, we make the first stab to explore AI's upper bound towards a\nhuman-like agent for such a highly comprehensive multi-agent mission by\ncombining three core and essential capabilities for stronger LLM-based societal\nagents: 1) strategic planner with memory and reflection; 2) goal-oriented\nnegotiate with social reasoning; 3) augmenting memory by self-play games to\nself-evolving without any human in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diplomacy is one of the most sophisticated activities in human society. The\ncomplex interactions among multiple parties/ agents involve various abilities\nlike social reasoning, negotiation arts, and long-term strategy planning.\nPrevious AI agents surely have proved their capability of handling multi-step\ngames and larger action spaces on tasks involving multiple agents. However,\ndiplomacy involves a staggering magnitude of decision spaces, especially\nconsidering the negotiation stage required. Recently, LLM agents have shown\ntheir potential for extending the boundary of previous agents on a couple of\napplications, however, it is still not enough to handle a very long planning\nperiod in a complex multi-agent environment. Empowered with cutting-edge LLM\ntechnology, we make the first stab to explore AI's upper bound towards a\nhuman-like agent for such a highly comprehensive multi-agent mission by\ncombining three core and essential capabilities for stronger LLM-based societal\nagents: 1) strategic planner with memory and reflection; 2) goal-oriented\nnegotiate with social reasoning; 3) augmenting memory by self-play games to\nself-evolving without any human in the loop."
                },
                "authors": [
                    {
                        "name": "Zhenyu Guan"
                    },
                    {
                        "name": "Xiangyu Kong"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_journal_ref": "NuerIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07137v1",
                "updated": "2024-10-09T17:53:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    53,
                    6,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:53:06Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    53,
                    6,
                    2,
                    283,
                    0
                ],
                "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates"
                },
                "summary": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks."
                },
                "authors": [
                    {
                        "name": "Xiaosen Zheng"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07129v1",
                "updated": "2024-10-09T17:51:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    55,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:51:55Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    55,
                    2,
                    283,
                    0
                ],
                "title": "Mental Disorders Detection in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental Disorders Detection in the Era of Large Language Models"
                },
                "summary": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications."
                },
                "authors": [
                    {
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "name": "Petr Strepetov"
                    },
                    {
                        "name": "Maksim Stankevich"
                    },
                    {
                        "name": "Ivan Smirnov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12877v2",
                "updated": "2024-10-09T17:51:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    51,
                    44,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-16T08:25:26Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    25,
                    26,
                    1,
                    198,
                    0
                ],
                "title": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models"
                },
                "summary": "Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code ."
                },
                "authors": [
                    {
                        "name": "Yaswanth Narsupalli"
                    },
                    {
                        "name": "Abhranil Chandra"
                    },
                    {
                        "name": "Sreevatsa Muppirala"
                    },
                    {
                        "name": "Manish Gupta"
                    },
                    {
                        "name": "Pawan Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Goyal"
                },
                "author": "Pawan Goyal",
                "arxiv_comment": "Paper Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07109v1",
                "updated": "2024-10-09T17:45:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    47,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:45:47Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    47,
                    2,
                    283,
                    0
                ],
                "title": "I Want to Break Free! Anti-Social Behavior and Persuasion Ability of\n  LLMs in Multi-Agent Settings with Social Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Want to Break Free! Anti-Social Behavior and Persuasion Ability of\n  LLMs in Multi-Agent Settings with Social Hierarchy"
                },
                "summary": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact."
                },
                "authors": [
                    {
                        "name": "Gian Maria Campedelli"
                    },
                    {
                        "name": "Nicolò Penzo"
                    },
                    {
                        "name": "Massimo Stefan"
                    },
                    {
                        "name": "Roberto Dessì"
                    },
                    {
                        "name": "Marco Guerini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12108v2",
                "updated": "2024-10-09T17:45:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    7,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-16T18:28:40Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    28,
                    40,
                    1,
                    198,
                    0
                ],
                "title": "Private prediction for large-scale synthetic text generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private prediction for large-scale synthetic text generation"
                },
                "summary": "We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data."
                },
                "authors": [
                    {
                        "name": "Kareem Amin"
                    },
                    {
                        "name": "Alex Bie"
                    },
                    {
                        "name": "Weiwei Kong"
                    },
                    {
                        "name": "Alexey Kurakin"
                    },
                    {
                        "name": "Natalia Ponomareva"
                    },
                    {
                        "name": "Umar Syed"
                    },
                    {
                        "name": "Andreas Terzis"
                    },
                    {
                        "name": "Sergei Vassilvitskii"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Vassilvitskii"
                },
                "author": "Sergei Vassilvitskii",
                "arxiv_comment": "20 pages; updated figure + some new experiments from EMNLP 2024\n  findings camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07103v1",
                "updated": "2024-10-09T17:41:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    41,
                    53,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:41:53Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    41,
                    53,
                    2,
                    283,
                    0
                ],
                "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models\n  through Repetition of Misordered Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Multi-Hop Reasoning Potential in Large Language Models\n  through Repetition of Misordered Context"
                },
                "summary": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning."
                },
                "authors": [
                    {
                        "name": "Sangwon Yu"
                    },
                    {
                        "name": "Ik-hwan Kim"
                    },
                    {
                        "name": "Jongyoon Song"
                    },
                    {
                        "name": "Saehyung Lee"
                    },
                    {
                        "name": "Junsung Park"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00260v2",
                "updated": "2024-10-09T17:39:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    39,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-30T22:15:58Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    15,
                    58,
                    0,
                    274,
                    0
                ],
                "title": "DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data\n  Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data\n  Mining"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline."
                },
                "authors": [
                    {
                        "name": "Vinayak Arannil"
                    },
                    {
                        "name": "Neha Narwal"
                    },
                    {
                        "name": "Sourav Sanjukta Bhabesh"
                    },
                    {
                        "name": "Sai Nikhil Thirandas"
                    },
                    {
                        "name": "Darren Yow-Bang Wang"
                    },
                    {
                        "name": "Graham Horwood"
                    },
                    {
                        "name": "Alex Anto Chirayath"
                    },
                    {
                        "name": "Gouri Pandeshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gouri Pandeshwar"
                },
                "author": "Gouri Pandeshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07087v2",
                "updated": "2024-10-10T05:02:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    2,
                    4,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-09T17:29:01Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    29,
                    1,
                    2,
                    283,
                    0
                ],
                "title": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark,\n  and Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark,\n  and Methodology"
                },
                "summary": "Developing agents capable of navigating to a target location based on\nlanguage instructions and visual information, known as vision-language\nnavigation (VLN), has attracted widespread interest. Most research has focused\non ground-based agents, while UAV-based VLN remains relatively underexplored.\nRecent efforts in UAV vision-language navigation predominantly adopt\nground-based VLN settings, relying on predefined discrete action spaces and\nneglecting the inherent disparities in agent movement dynamics and the\ncomplexity of navigation tasks between ground and aerial environments. To\naddress these disparities and challenges, we propose solutions from three\nperspectives: platform, benchmark, and methodology. To enable realistic UAV\ntrajectory simulation in VLN tasks, we propose the OpenUAV platform, which\nfeatures diverse environments, realistic flight control, and extensive\nalgorithmic support. We further construct a target-oriented VLN dataset\nconsisting of approximately 12k trajectories on this platform, serving as the\nfirst dataset specifically designed for realistic UAV VLN tasks. To tackle the\nchallenges posed by complex aerial environments, we propose an assistant-guided\nUAV object search benchmark called UAV-Need-Help, which provides varying levels\nof guidance information to help UAVs better accomplish realistic VLN tasks. We\nalso propose a UAV navigation LLM that, given multi-view images, task\ndescriptions, and assistant instructions, leverages the multimodal\nunderstanding capabilities of the MLLM to jointly process visual and textual\ninformation, and performs hierarchical trajectory generation. The evaluation\nresults of our method significantly outperform the baseline models, while there\nremains a considerable gap between our results and those achieved by human\noperators, underscoring the challenge presented by the UAV-Need-Help task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing agents capable of navigating to a target location based on\nlanguage instructions and visual information, known as vision-language\nnavigation (VLN), has attracted widespread interest. Most research has focused\non ground-based agents, while UAV-based VLN remains relatively underexplored.\nRecent efforts in UAV vision-language navigation predominantly adopt\nground-based VLN settings, relying on predefined discrete action spaces and\nneglecting the inherent disparities in agent movement dynamics and the\ncomplexity of navigation tasks between ground and aerial environments. To\naddress these disparities and challenges, we propose solutions from three\nperspectives: platform, benchmark, and methodology. To enable realistic UAV\ntrajectory simulation in VLN tasks, we propose the OpenUAV platform, which\nfeatures diverse environments, realistic flight control, and extensive\nalgorithmic support. We further construct a target-oriented VLN dataset\nconsisting of approximately 12k trajectories on this platform, serving as the\nfirst dataset specifically designed for realistic UAV VLN tasks. To tackle the\nchallenges posed by complex aerial environments, we propose an assistant-guided\nUAV object search benchmark called UAV-Need-Help, which provides varying levels\nof guidance information to help UAVs better accomplish realistic VLN tasks. We\nalso propose a UAV navigation LLM that, given multi-view images, task\ndescriptions, and assistant instructions, leverages the multimodal\nunderstanding capabilities of the MLLM to jointly process visual and textual\ninformation, and performs hierarchical trajectory generation. The evaluation\nresults of our method significantly outperform the baseline models, while there\nremains a considerable gap between our results and those achieved by human\noperators, underscoring the challenge presented by the UAV-Need-Help task."
                },
                "authors": [
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Donglin Yang"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Hohin Kwan"
                    },
                    {
                        "name": "Jinyu Chen"
                    },
                    {
                        "name": "Wenjun Wu"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07083v1",
                "updated": "2024-10-09T17:24:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    24,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:24:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    24,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stanceformer: Target-Aware Transformer for Stance Detection"
                },
                "summary": "The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}"
                },
                "authors": [
                    {
                        "name": "Krishna Garg"
                    },
                    {
                        "name": "Cornelia Caragea"
                    }
                ],
                "author_detail": {
                    "name": "Cornelia Caragea"
                },
                "author": "Cornelia Caragea",
                "arxiv_comment": "16 pages, 2 figures, 14 tables including Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07076v1",
                "updated": "2024-10-09T17:19:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    58,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:19:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses"
                },
                "summary": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations."
                },
                "authors": [
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Wanhao Liu"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "Code and Benchmark are available at\n  https://github.com/ZonglinY/MOOSE-Chem.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14279v2",
                "updated": "2024-10-09T17:19:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    47,
                    2,
                    283,
                    0
                ],
                "published": "2024-01-25T16:10:33Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    10,
                    33,
                    3,
                    25,
                    0
                ],
                "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs"
                },
                "summary": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1."
                },
                "authors": [
                    {
                        "name": "Azmain Kabir"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Muhammad Asaduzzaman"
                    },
                    {
                        "name": "Wenbin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Zhang"
                },
                "author": "Wenbin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07074v1",
                "updated": "2024-10-09T17:19:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:19:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning"
                },
                "summary": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Yichuan Li"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Kyumin Lee"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07073v1",
                "updated": "2024-10-09T17:16:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    16,
                    22,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:16:22Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    16,
                    22,
                    2,
                    283,
                    0
                ],
                "title": "Pixtral 12B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixtral 12B"
                },
                "summary": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "Pravesh Agrawal"
                    },
                    {
                        "name": "Szymon Antoniak"
                    },
                    {
                        "name": "Emma Bou Hanna"
                    },
                    {
                        "name": "Devendra Chaplot"
                    },
                    {
                        "name": "Jessica Chudnovsky"
                    },
                    {
                        "name": "Saurabh Garg"
                    },
                    {
                        "name": "Theophile Gervet"
                    },
                    {
                        "name": "Soham Ghosh"
                    },
                    {
                        "name": "Amélie Héliou"
                    },
                    {
                        "name": "Paul Jacob"
                    },
                    {
                        "name": "Albert Q. Jiang"
                    },
                    {
                        "name": "Timothée Lacroix"
                    },
                    {
                        "name": "Guillaume Lample"
                    },
                    {
                        "name": "Diego Las Casas"
                    },
                    {
                        "name": "Thibaut Lavril"
                    },
                    {
                        "name": "Teven Le Scao"
                    },
                    {
                        "name": "Andy Lo"
                    },
                    {
                        "name": "William Marshall"
                    },
                    {
                        "name": "Louis Martin"
                    },
                    {
                        "name": "Arthur Mensch"
                    },
                    {
                        "name": "Pavankumar Muddireddy"
                    },
                    {
                        "name": "Valera Nemychnikova"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Patrick Von Platen"
                    },
                    {
                        "name": "Nikhil Raghuraman"
                    },
                    {
                        "name": "Baptiste Rozière"
                    },
                    {
                        "name": "Alexandre Sablayrolles"
                    },
                    {
                        "name": "Lucile Saulnier"
                    },
                    {
                        "name": "Romain Sauvestre"
                    },
                    {
                        "name": "Wendy Shang"
                    },
                    {
                        "name": "Roman Soletskyi"
                    },
                    {
                        "name": "Lawrence Stewart"
                    },
                    {
                        "name": "Pierre Stock"
                    },
                    {
                        "name": "Joachim Studnia"
                    },
                    {
                        "name": "Sandeep Subramanian"
                    },
                    {
                        "name": "Sagar Vaze"
                    },
                    {
                        "name": "Thomas Wang"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wang"
                },
                "author": "Thomas Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06809v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06809v3",
                "updated": "2024-10-09T17:16:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    16,
                    15,
                    2,
                    283,
                    0
                ],
                "published": "2024-04-10T07:56:26Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    7,
                    56,
                    26,
                    2,
                    101,
                    0
                ],
                "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation"
                },
                "summary": "The rapid development of large language models has led to the widespread\nadoption of Retrieval-Augmented Generation (RAG), which integrates external\nknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.\nHowever, the existing RAG paradigm inevitably suffers from the impact of flawed\ninformation introduced during the retrieval phrase, thereby diminishing the\nreliability and correctness of the generated outcomes. In this paper, we\npropose Credibility-aware Generation (CAG), a universally applicable framework\ndesigned to mitigate the impact of flawed information in RAG. At its core, CAG\naims to equip models with the ability to discern and process information based\non its credibility. To this end, we propose an innovative data transformation\nframework that generates data based on credibility, thereby effectively\nendowing models with the capability of CAG. Furthermore, to accurately evaluate\nthe models' capabilities of CAG, we construct a comprehensive benchmark\ncovering three critical real-world scenarios. Experimental results demonstrate\nthat our model can effectively understand and utilize credibility for\ngeneration, significantly outperform other models with retrieval augmentation,\nand exhibit resilience against the disruption caused by noisy documents,\nthereby maintaining robust performance. Moreover, our model supports customized\ncredibility, offering a wide range of potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models has led to the widespread\nadoption of Retrieval-Augmented Generation (RAG), which integrates external\nknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.\nHowever, the existing RAG paradigm inevitably suffers from the impact of flawed\ninformation introduced during the retrieval phrase, thereby diminishing the\nreliability and correctness of the generated outcomes. In this paper, we\npropose Credibility-aware Generation (CAG), a universally applicable framework\ndesigned to mitigate the impact of flawed information in RAG. At its core, CAG\naims to equip models with the ability to discern and process information based\non its credibility. To this end, we propose an innovative data transformation\nframework that generates data based on credibility, thereby effectively\nendowing models with the capability of CAG. Furthermore, to accurately evaluate\nthe models' capabilities of CAG, we construct a comprehensive benchmark\ncovering three critical real-world scenarios. Experimental results demonstrate\nthat our model can effectively understand and utilize credibility for\ngeneration, significantly outperform other models with retrieval augmentation,\nand exhibit resilience against the disruption caused by noisy documents,\nthereby maintaining robust performance. Moreover, our model supports customized\ncredibility, offering a wide range of potential applications."
                },
                "authors": [
                    {
                        "name": "Ruotong Pan"
                    },
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Sirui Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Our code, benchmark, and\n  models are available at https://github.com/panruotong/CAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06809v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06809v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07069v1",
                "updated": "2024-10-09T17:14:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    14,
                    50,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:14:50Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    14,
                    50,
                    2,
                    283,
                    0
                ],
                "title": "ReIFE: Re-evaluating Instruction-Following Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReIFE: Re-evaluating Instruction-Following Evaluation"
                },
                "summary": "The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Kejian Shi"
                    },
                    {
                        "name": "Alexander R. Fabbri"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result\n  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07062v1",
                "updated": "2024-10-09T17:03:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    3,
                    49,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T17:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    3,
                    49,
                    2,
                    283,
                    0
                ],
                "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection"
                },
                "summary": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo"
                },
                "authors": [
                    {
                        "name": "Cristian Gutierrez"
                    }
                ],
                "author_detail": {
                    "name": "Cristian Gutierrez"
                },
                "author": "Cristian Gutierrez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07054v1",
                "updated": "2024-10-09T16:51:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    21,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:51:21Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    21,
                    2,
                    283,
                    0
                ],
                "title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing"
                },
                "summary": "Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases."
                },
                "authors": [
                    {
                        "name": "Weichuan Wang"
                    },
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Chen Ma"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "arxiv_comment": "20 pages, EMNLP'2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07053v1",
                "updated": "2024-10-09T16:51:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    10,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:51:10Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    51,
                    10,
                    2,
                    283,
                    0
                ],
                "title": "Robots in the Middle: Evaluating LLMs in Dispute Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots in the Middle: Evaluating LLMs in Dispute Resolution"
                },
                "summary": "Mediation is a dispute resolution method featuring a neutral third-party\n(mediator) who intervenes to help the individuals resolve their dispute. In\nthis paper, we investigate to which extent large language models (LLMs) are\nable to act as mediators. We investigate whether LLMs are able to analyze\ndispute conversations, select suitable intervention types, and generate\nappropriate intervention messages. Using a novel, manually created dataset of\n50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human\nannotators across several key metrics. Overall, the LLMs showed strong\nperformance, even outperforming our human annotators across dimensions.\nSpecifically, in 62% of the cases, the LLMs chose intervention types that were\nrated as better than or equivalent to those chosen by humans. Moreover, in 84%\nof the cases, the intervention messages generated by the LLMs were rated as\nbetter than or equal to the intervention messages written by humans. LLMs\nlikewise performed favourably on metrics such as impartiality, understanding\nand contextualization. Our results demonstrate the potential of integrating AI\nin online dispute resolution (ODR) platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediation is a dispute resolution method featuring a neutral third-party\n(mediator) who intervenes to help the individuals resolve their dispute. In\nthis paper, we investigate to which extent large language models (LLMs) are\nable to act as mediators. We investigate whether LLMs are able to analyze\ndispute conversations, select suitable intervention types, and generate\nappropriate intervention messages. Using a novel, manually created dataset of\n50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human\nannotators across several key metrics. Overall, the LLMs showed strong\nperformance, even outperforming our human annotators across dimensions.\nSpecifically, in 62% of the cases, the LLMs chose intervention types that were\nrated as better than or equivalent to those chosen by humans. Moreover, in 84%\nof the cases, the intervention messages generated by the LLMs were rated as\nbetter than or equal to the intervention messages written by humans. LLMs\nlikewise performed favourably on metrics such as impartiality, understanding\nand contextualization. Our results demonstrate the potential of integrating AI\nin online dispute resolution (ODR) platforms."
                },
                "authors": [
                    {
                        "name": "Jinzhe Tan"
                    },
                    {
                        "name": "Hannes Westermann"
                    },
                    {
                        "name": "Nikhil Reddy Pottanigari"
                    },
                    {
                        "name": "Jaromír Šavelka"
                    },
                    {
                        "name": "Sébastien Meeùs"
                    },
                    {
                        "name": "Mia Godet"
                    },
                    {
                        "name": "Karim Benyekhlef"
                    }
                ],
                "author_detail": {
                    "name": "Karim Benyekhlef"
                },
                "author": "Karim Benyekhlef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07035v1",
                "updated": "2024-10-09T16:15:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    15,
                    36,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:15:36Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    15,
                    36,
                    2,
                    283,
                    0
                ],
                "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Feiyu Duan"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "arxiv_comment": "39 pages. CP-Bench and LenCtrl-Bench are available in\n  https://huggingface.co/datasets/ZenMoore/CP-Bench and\n  https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07030v1",
                "updated": "2024-10-09T16:13:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    13,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:13:19Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    13,
                    19,
                    2,
                    283,
                    0
                ],
                "title": "Clean Evaluations on Contaminated Visual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clean Evaluations on Contaminated Visual Language Models"
                },
                "summary": "How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication."
                },
                "authors": [
                    {
                        "name": "Hongyuan Lu"
                    },
                    {
                        "name": "Shujie Miao"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07025v1",
                "updated": "2024-10-09T16:07:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    7,
                    11,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:07:11Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    7,
                    11,
                    2,
                    283,
                    0
                ],
                "title": "Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation\n  Models Without Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation\n  Models Without Human Feedback"
                },
                "summary": "Radiologists play a crucial role by translating medical images into medical\nreports. However, the field faces staffing shortages and increasing workloads.\nWhile automated approaches using vision-language models (VLMs) show promise as\nassistants, they require exceptionally high accuracy. Most current VLMs in\nradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the\ngeneral domain, additional preference fine-tuning has become standard practice.\nThe challenge in radiology lies in the prohibitive cost of obtaining\nradiologist feedback. We propose a scalable automated preference alignment\ntechnique for VLMs in radiology, focusing on chest X-ray (CXR) report\ngeneration. Our method leverages publicly available datasets with an\nLLM-as-a-Judge mechanism, eliminating the need for additional expert\nradiologist feedback. We evaluate and benchmark five direct alignment\nalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN\nscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in\nan average across six metrics (domain specific and general), compared to the\nSFT baseline. We study reward overoptimization via length exploitation, with\nreports lengthening by up to 3.2x. To assess a potential alignment tax, we\nbenchmark on six additional diverse tasks, finding no significant degradations.\nA reader study involving four board-certified radiologists indicates win rates\nof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.\nOur analysis provides actionable insights for the development of VLMs in\nhigh-stakes fields like radiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiologists play a crucial role by translating medical images into medical\nreports. However, the field faces staffing shortages and increasing workloads.\nWhile automated approaches using vision-language models (VLMs) show promise as\nassistants, they require exceptionally high accuracy. Most current VLMs in\nradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the\ngeneral domain, additional preference fine-tuning has become standard practice.\nThe challenge in radiology lies in the prohibitive cost of obtaining\nradiologist feedback. We propose a scalable automated preference alignment\ntechnique for VLMs in radiology, focusing on chest X-ray (CXR) report\ngeneration. Our method leverages publicly available datasets with an\nLLM-as-a-Judge mechanism, eliminating the need for additional expert\nradiologist feedback. We evaluate and benchmark five direct alignment\nalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN\nscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in\nan average across six metrics (domain specific and general), compared to the\nSFT baseline. We study reward overoptimization via length exploitation, with\nreports lengthening by up to 3.2x. To assess a potential alignment tax, we\nbenchmark on six additional diverse tasks, finding no significant degradations.\nA reader study involving four board-certified radiologists indicates win rates\nof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.\nOur analysis provides actionable insights for the development of VLMs in\nhigh-stakes fields like radiology."
                },
                "authors": [
                    {
                        "name": "Dennis Hein"
                    },
                    {
                        "name": "Zhihong Chen"
                    },
                    {
                        "name": "Sophie Ostmeier"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Maya Varma"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Arne Edward Michalson"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Hyun Joo Shin"
                    },
                    {
                        "name": "Curtis Langlotz"
                    },
                    {
                        "name": "Akshay S Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Akshay S Chaudhari"
                },
                "author": "Akshay S Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12746v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12746v5",
                "updated": "2024-10-09T16:04:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    4,
                    39,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-18T16:06:38Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    16,
                    6,
                    38,
                    1,
                    170,
                    0
                ],
                "title": "Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies\n  for Zero-shot Knowledge-based VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies\n  for Zero-shot Knowledge-based VQA"
                },
                "summary": "Knowledge-based Visual Question-answering (K-VQA) often requires the use of\nbackground knowledge beyond the image. However, we discover that a single\nknowledge generation strategy is often insufficient for all K-VQA questions. To\nthis end, we propose Diversification, Evidence Truncation, and Combination for\nKnowledge-based Elucidation (DietCoke), which utilizes a bundle of\ncomplementary question-answering tactics and aggregates their answers using\ntextual rationales. DietCoke comprises of three stages: diversification,\nrationalization, and ensemble. The diversification stage generates three\ndistinctive decision contexts, each leading to its own answer candidate. The\nrationalization stage generates two rationales, the automatic rationale and the\nmechanistic rationale, for each answer candidate using decorrelated techniques.\nFinally, in the ensemble stage, an LLM informed by the rationales selects one\nanswer from the three candidates. Experiments show that DietCoke significantly\noutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on\nA-OKVOA and that the strategies in the ensembles are highly complementary. Code\nis available at: https://github.com/limiaoyu/DietCoke",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Visual Question-answering (K-VQA) often requires the use of\nbackground knowledge beyond the image. However, we discover that a single\nknowledge generation strategy is often insufficient for all K-VQA questions. To\nthis end, we propose Diversification, Evidence Truncation, and Combination for\nKnowledge-based Elucidation (DietCoke), which utilizes a bundle of\ncomplementary question-answering tactics and aggregates their answers using\ntextual rationales. DietCoke comprises of three stages: diversification,\nrationalization, and ensemble. The diversification stage generates three\ndistinctive decision contexts, each leading to its own answer candidate. The\nrationalization stage generates two rationales, the automatic rationale and the\nmechanistic rationale, for each answer candidate using decorrelated techniques.\nFinally, in the ensemble stage, an LLM informed by the rationales selects one\nanswer from the three candidates. Experiments show that DietCoke significantly\noutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on\nA-OKVOA and that the strategies in the ensembles are highly complementary. Code\nis available at: https://github.com/limiaoyu/DietCoke"
                },
                "authors": [
                    {
                        "name": "Miaoyu Li"
                    },
                    {
                        "name": "Haoxin Li"
                    },
                    {
                        "name": "Zilin Du"
                    },
                    {
                        "name": "Boyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Boyang Li"
                },
                "author": "Boyang Li",
                "arxiv_comment": "Accepted to Findings of EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12746v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12746v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00795v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00795v4",
                "updated": "2024-10-09T16:02:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    2,
                    13,
                    2,
                    283,
                    0
                ],
                "published": "2024-02-01T17:28:10Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    17,
                    28,
                    10,
                    3,
                    32,
                    0
                ],
                "title": "LLMs learn governing principles of dynamical systems, revealing an\n  in-context neural scaling law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs learn governing principles of dynamical systems, revealing an\n  in-context neural scaling law"
                },
                "summary": "Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. We study LLMs' ability to\nextrapolate the behavior of dynamical systems whose evolution is governed by\nprinciples of physical interest. Our results show that LLaMA 2, a language\nmodel trained primarily on texts, achieves accurate predictions of dynamical\nsystem time series without fine-tuning or prompt engineering. Moreover, the\naccuracy of the learned physical rules increases with the length of the input\ncontext window, revealing an in-context version of neural scaling law. Along\nthe way, we present a flexible and efficient algorithm for extracting\nprobability density functions of multi-digit numbers directly from LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. We study LLMs' ability to\nextrapolate the behavior of dynamical systems whose evolution is governed by\nprinciples of physical interest. Our results show that LLaMA 2, a language\nmodel trained primarily on texts, achieves accurate predictions of dynamical\nsystem time series without fine-tuning or prompt engineering. Moreover, the\naccuracy of the learned physical rules increases with the length of the input\ncontext window, revealing an in-context version of neural scaling law. Along\nthe way, we present a flexible and efficient algorithm for extracting\nprobability density functions of multi-digit numbers directly from LLMs."
                },
                "authors": [
                    {
                        "name": "Toni J. B. Liu"
                    },
                    {
                        "name": "Nicolas Boullé"
                    },
                    {
                        "name": "Raphaël Sarfati"
                    },
                    {
                        "name": "Christopher J. Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J. Earls"
                },
                "author": "Christopher J. Earls",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00795v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00795v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07018v1",
                "updated": "2024-10-09T16:00:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    0,
                    21,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T16:00:21Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    0,
                    21,
                    2,
                    283,
                    0
                ],
                "title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series\n  OOD Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series\n  OOD Generalization"
                },
                "summary": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning\narea of study. Its primary goal is to enhance the adaptability and resilience\nof machine learning models when faced with new, unseen, and potentially\nadversarial data that significantly diverges from their original training\ndatasets. In this paper, we investigate time series OOD generalization via\npre-trained Large Language Models (LLMs). We first propose a novel\n\\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries\n\\textbf{O}OD generalization, termed TTSO, which considers both sample-level and\ngroup-level uncertainties. This formula offers a fresh theoretic perspective\nfor formulating and analyzing OOD generalization problem. In addition, we\nprovide a theoretical analysis to justify this method is well motivated. We\nthen develop a stratified localization algorithm tailored for this tri-level\noptimization problem, theoretically demonstrating the guaranteed convergence of\nthe proposed algorithm. Our analysis also reveals that the iteration complexity\nto obtain an $\\epsilon$-stationary point is bounded by\nO($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have\nbeen conducted to elucidate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning\narea of study. Its primary goal is to enhance the adaptability and resilience\nof machine learning models when faced with new, unseen, and potentially\nadversarial data that significantly diverges from their original training\ndatasets. In this paper, we investigate time series OOD generalization via\npre-trained Large Language Models (LLMs). We first propose a novel\n\\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries\n\\textbf{O}OD generalization, termed TTSO, which considers both sample-level and\ngroup-level uncertainties. This formula offers a fresh theoretic perspective\nfor formulating and analyzing OOD generalization problem. In addition, we\nprovide a theoretical analysis to justify this method is well motivated. We\nthen develop a stratified localization algorithm tailored for this tri-level\noptimization problem, theoretically demonstrating the guaranteed convergence of\nthe proposed algorithm. Our analysis also reveals that the iteration complexity\nto obtain an $\\epsilon$-stationary point is bounded by\nO($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have\nbeen conducted to elucidate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Chengtao Jian"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yang Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Jiao"
                },
                "author": "Yang Jiao",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07009v1",
                "updated": "2024-10-09T15:52:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    52,
                    48,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:52:48Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    52,
                    48,
                    2,
                    283,
                    0
                ],
                "title": "Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based\n  Outline-guided Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based\n  Outline-guided Generation"
                },
                "summary": "The patent domain is gaining attention in natural language processing\nresearch, offering practical applications in streamlining the patenting process\nand providing challenging benchmarks for large language models (LLMs). However,\nthe generation of the description sections of patents, which constitute more\nthan 90% of the patent document, has not been studied to date. We address this\ngap by introducing the task of outline-guided paper-to-patent generation, where\nan academic paper provides the technical specification of the invention and an\noutline conveys the desired patent structure. We present PAP2PAT, a new\nchallenging benchmark of 1.8k patent-paper pairs with document outlines,\ncollected using heuristics that reflect typical research lab practices. Our\nexperiments with current open-weight LLMs and outline-guided chunk-based\ngeneration show that they can effectively use information from the paper but\nstruggle with repetitions, likely due to the inherent repetitiveness of patent\nlanguage. We release our data and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The patent domain is gaining attention in natural language processing\nresearch, offering practical applications in streamlining the patenting process\nand providing challenging benchmarks for large language models (LLMs). However,\nthe generation of the description sections of patents, which constitute more\nthan 90% of the patent document, has not been studied to date. We address this\ngap by introducing the task of outline-guided paper-to-patent generation, where\nan academic paper provides the technical specification of the invention and an\noutline conveys the desired patent structure. We present PAP2PAT, a new\nchallenging benchmark of 1.8k patent-paper pairs with document outlines,\ncollected using heuristics that reflect typical research lab practices. Our\nexperiments with current open-weight LLMs and outline-guided chunk-based\ngeneration show that they can effectively use information from the paper but\nstruggle with repetitions, likely due to the inherent repetitiveness of patent\nlanguage. We release our data and code."
                },
                "authors": [
                    {
                        "name": "Valentin Knappich"
                    },
                    {
                        "name": "Simon Razniewski"
                    },
                    {
                        "name": "Anna Hätty"
                    },
                    {
                        "name": "Annemarie Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Annemarie Friedrich"
                },
                "author": "Annemarie Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10054v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10054v3",
                "updated": "2024-10-09T15:44:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    44,
                    36,
                    2,
                    283,
                    0
                ],
                "published": "2023-11-16T17:48:55Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    17,
                    48,
                    55,
                    3,
                    320,
                    0
                ],
                "title": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models"
                },
                "summary": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles."
                },
                "authors": [
                    {
                        "name": "Mingqian Zheng"
                    },
                    {
                        "name": "Jiaxin Pei"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "arxiv_comment": "Accepted by Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10054v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06996v1",
                "updated": "2024-10-09T15:44:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    44,
                    29,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:44:29Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    44,
                    29,
                    2,
                    283,
                    0
                ],
                "title": "Enhancing the sensing power of bike-sharing system for urban environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the sensing power of bike-sharing system for urban environment"
                },
                "summary": "The development of smart cities requires innovative sensing solutions for\nefficient and low-cost urban environment monitoring. Bike-sharing systems, with\ntheir wide coverage, flexible mobility, and dense urban distribution, present a\npromising platform for pervasive sensing. At a relative early stage, research\non bike-based sensing focuses on the application of data collected via passive\nsensing, without consideration of the optimization of data collection through\nsensor deployment or vehicle scheduling. To address this gap, this study\nintegrates a binomial probability model with a mixed-integer linear programming\nmodel to optimize sensor allocation across bike stands. Additionally, an active\nscheduling strategy guides user bike selection to enhance the efficacy of data\ncollection. A case study in Manhattan validates the proposed strategy, showing\nthat equipping sensors on just 1\\% of the bikes covers approximately 70\\% of\nroad segments in a day, highlighting the significant potential of bike-sharing\nsystems for urban sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of smart cities requires innovative sensing solutions for\nefficient and low-cost urban environment monitoring. Bike-sharing systems, with\ntheir wide coverage, flexible mobility, and dense urban distribution, present a\npromising platform for pervasive sensing. At a relative early stage, research\non bike-based sensing focuses on the application of data collected via passive\nsensing, without consideration of the optimization of data collection through\nsensor deployment or vehicle scheduling. To address this gap, this study\nintegrates a binomial probability model with a mixed-integer linear programming\nmodel to optimize sensor allocation across bike stands. Additionally, an active\nscheduling strategy guides user bike selection to enhance the efficacy of data\ncollection. A case study in Manhattan validates the proposed strategy, showing\nthat equipping sensors on just 1\\% of the bikes covers approximately 70\\% of\nroad segments in a day, highlighting the significant potential of bike-sharing\nsystems for urban sensing."
                },
                "authors": [
                    {
                        "name": "Wen Ji"
                    },
                    {
                        "name": "Ke Han"
                    },
                    {
                        "name": "Qi Hao"
                    },
                    {
                        "name": "Qian Ge"
                    },
                    {
                        "name": "Ying Long"
                    }
                ],
                "author_detail": {
                    "name": "Ying Long"
                },
                "author": "Ying Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06992v1",
                "updated": "2024-10-09T15:38:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    38,
                    53,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:38:53Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    38,
                    53,
                    2,
                    283,
                    0
                ],
                "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Bench+: Enhanced Coding Benchmark for LLMs"
                },
                "summary": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues."
                },
                "authors": [
                    {
                        "name": "Reem Aleithan"
                    },
                    {
                        "name": "Haoran Xue"
                    },
                    {
                        "name": "Mohammad Mahdi Mohajer"
                    },
                    {
                        "name": "Elijah Nnorom"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Song Wang"
                    }
                ],
                "author_detail": {
                    "name": "Song Wang"
                },
                "author": "Song Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13053v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13053v3",
                "updated": "2024-10-09T15:33:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    33,
                    10,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-19T20:46:07Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    20,
                    46,
                    7,
                    6,
                    140,
                    0
                ],
                "title": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models"
                },
                "summary": "The pretrain+fine-tune paradigm is foundational for deploying large language\nmodels (LLMs) across various downstream applications. Within this framework,\nLow-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning\n(PEFT), producing numerous reusable task-specific LoRA adapters. However, this\napproach requires explicit task intention selection, posing challenges for\nautonomous task sensing and switching during inference with multiple existing\nLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA\n(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses\nmultiple task-specific LoRA adapters into the base LLM via a full-mode\nMixture-of-Experts (MoE) architecture. This framework also includes novel MoE\nforward acceleration strategies to address the efficiency challenges of\ntraditional MoE implementations. Our evaluation, using the LlaMA2-13B and\nLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,\ndemonstrates equivalent performance with the traditional PEFT method. Moreover,\nthe LLM equipped with MeteoRA achieves superior performance in handling\ncomposite tasks, effectively solving ten sequential problems in a single\ninference pass, thereby demonstrating the framework's enhanced capability for\ntimely adapter switching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pretrain+fine-tune paradigm is foundational for deploying large language\nmodels (LLMs) across various downstream applications. Within this framework,\nLow-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning\n(PEFT), producing numerous reusable task-specific LoRA adapters. However, this\napproach requires explicit task intention selection, posing challenges for\nautonomous task sensing and switching during inference with multiple existing\nLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA\n(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses\nmultiple task-specific LoRA adapters into the base LLM via a full-mode\nMixture-of-Experts (MoE) architecture. This framework also includes novel MoE\nforward acceleration strategies to address the efficiency challenges of\ntraditional MoE implementations. Our evaluation, using the LlaMA2-13B and\nLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,\ndemonstrates equivalent performance with the traditional PEFT method. Moreover,\nthe LLM equipped with MeteoRA achieves superior performance in handling\ncomposite tasks, effectively solving ten sequential problems in a single\ninference pass, thereby demonstrating the framework's enhanced capability for\ntimely adapter switching."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junyu Lai"
                    },
                    {
                        "name": "Yunpeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Huang"
                },
                "author": "Yunpeng Huang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13053v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13053v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06981v1",
                "updated": "2024-10-09T15:18:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    18,
                    57,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:18:57Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    18,
                    57,
                    2,
                    283,
                    0
                ],
                "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models"
                },
                "summary": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality."
                },
                "authors": [
                    {
                        "name": "Michael Lan"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Austin Meek"
                    },
                    {
                        "name": "Ashkan Khakzar"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15105v2",
                "updated": "2024-10-09T15:13:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    13,
                    23,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-22T10:48:12Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    10,
                    48,
                    12,
                    4,
                    82,
                    0
                ],
                "title": "SAGraph: A Large-scale Text-Rich Social Graph Dataset for Advertising\n  Campaigns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGraph: A Large-scale Text-Rich Social Graph Dataset for Advertising\n  Campaigns"
                },
                "summary": "Influencer selection in marketing involves choosing users with a strong\nonline presence to promote products or services, leveraging their credibility\nand audience reach. This process is vital for its direct impact on brand\nvisibility, consumer trust, and ultimately, sales conversion. Current research\nsimplifies complex elements like user attitudes, thought processes, and\nadvertising content into numerical values. This kind of approach risks missing\nthe dynamic and contextual nuances crucial for developing effective influencer\nmarketing strategies. To bridge this gap, we introduce a text-rich large Social\nAdvertisement Graph (SAGraph) dataset collected from Weibo, a real-world\ninfluencer advertising platform. Our dataset centers around the advertising\ncampaign for 6 products, consisting of 317,287 users, each with their profile\ninformation, and interaction data including 891,834 comments and 441,836\nreposts. By leveraging this rich interaction and textual content, one can gain\ndeeper insights into consumer behavior, refine influencer selection criteria,\nand develop more targeted and effective marketing strategies. We evaluated\nexisting influencer selection baselines and the latest LLMs on this dataset,\ndemonstrating the importance of textual content in advertising campaigns, as\nwell as the availability and significant potential of LLMs for enhancing\nadvertising strategies. We hope that this dataset will inspire further\nresearch: \\url{https://github.com/xiaoqzhwhu/SAGraph/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influencer selection in marketing involves choosing users with a strong\nonline presence to promote products or services, leveraging their credibility\nand audience reach. This process is vital for its direct impact on brand\nvisibility, consumer trust, and ultimately, sales conversion. Current research\nsimplifies complex elements like user attitudes, thought processes, and\nadvertising content into numerical values. This kind of approach risks missing\nthe dynamic and contextual nuances crucial for developing effective influencer\nmarketing strategies. To bridge this gap, we introduce a text-rich large Social\nAdvertisement Graph (SAGraph) dataset collected from Weibo, a real-world\ninfluencer advertising platform. Our dataset centers around the advertising\ncampaign for 6 products, consisting of 317,287 users, each with their profile\ninformation, and interaction data including 891,834 comments and 441,836\nreposts. By leveraging this rich interaction and textual content, one can gain\ndeeper insights into consumer behavior, refine influencer selection criteria,\nand develop more targeted and effective marketing strategies. We evaluated\nexisting influencer selection baselines and the latest LLMs on this dataset,\ndemonstrating the importance of textual content in advertising campaigns, as\nwell as the availability and significant potential of LLMs for enhancing\nadvertising strategies. We hope that this dataset will inspire further\nresearch: \\url{https://github.com/xiaoqzhwhu/SAGraph/}."
                },
                "authors": [
                    {
                        "name": "Xiaoqing Zhang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Zhenxing Hu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06973v1",
                "updated": "2024-10-09T15:11:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    11,
                    13,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:11:13Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    11,
                    13,
                    2,
                    283,
                    0
                ],
                "title": "Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara"
                },
                "summary": "In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B."
                },
                "authors": [
                    {
                        "name": "Azree Nazri"
                    },
                    {
                        "name": "Olalekan Agbolade"
                    },
                    {
                        "name": "Faisal Aziz"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Aziz"
                },
                "author": "Faisal Aziz",
                "arxiv_comment": "20 pages, 5 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06972v1",
                "updated": "2024-10-09T15:10:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    10,
                    0,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:10:00Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    10,
                    0,
                    2,
                    283,
                    0
                ],
                "title": "Diamond of Thought: A Design Thinking-Based Framework for LLMs in\n  Wearable Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diamond of Thought: A Design Thinking-Based Framework for LLMs in\n  Wearable Design"
                },
                "summary": "Wearable design is an interdisciplinary field that balances technological\ninnovation, human factors, and human-computer interactions. Despite\ncontributions from various disciplines, many projects lack stable\ninterdisciplinary teams, which often leads to design failures. Large language\nmodels (LLMs) integrate diverse information and generate innovative solutions,\nmaking them a valuable tool for enhancing design processes. Thus, we have\nexplored the use of LLMs in wearable design by combining design-thinking\nprinciples with LLM capabilities. We have developed the \"Diamond of Thought\"\nframework and analysed 1,603 prototypes and 1,129 products from a body-centric\nperspective to create a comprehensive database. We employed retrieval-augmented\ngeneration to input database details into the LLMs, ensuring applicability to\nwearable design challenges and integration of embodied cognition into the\nprocess. Our LLM-based methodology for wearables has been experimentally\nvalidated, demonstrating the potential of LLMs for the advancement of design\npractices. This study offers new tools and methods for future wearable designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable design is an interdisciplinary field that balances technological\ninnovation, human factors, and human-computer interactions. Despite\ncontributions from various disciplines, many projects lack stable\ninterdisciplinary teams, which often leads to design failures. Large language\nmodels (LLMs) integrate diverse information and generate innovative solutions,\nmaking them a valuable tool for enhancing design processes. Thus, we have\nexplored the use of LLMs in wearable design by combining design-thinking\nprinciples with LLM capabilities. We have developed the \"Diamond of Thought\"\nframework and analysed 1,603 prototypes and 1,129 products from a body-centric\nperspective to create a comprehensive database. We employed retrieval-augmented\ngeneration to input database details into the LLMs, ensuring applicability to\nwearable design challenges and integration of embodied cognition into the\nprocess. Our LLM-based methodology for wearables has been experimentally\nvalidated, demonstrating the potential of LLMs for the advancement of design\npractices. This study offers new tools and methods for future wearable designs."
                },
                "authors": [
                    {
                        "name": "Qiyang Miao"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Zhihao Song"
                    },
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Yu Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cui"
                },
                "author": "Yu Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05291v2",
                "updated": "2024-10-09T15:02:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    53,
                    2,
                    283,
                    0
                ],
                "published": "2024-04-08T08:29:00Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    8,
                    29,
                    0,
                    0,
                    99,
                    0
                ],
                "title": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models"
                },
                "summary": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nfor sketching a plan, a parameter calculator for predicting arguments in the\nplan, and a code generator to convert the plan into executable robot code. At\nthe low level, we adopt reinforcement learning to train a set of motion\nplanning and control skills to unleash the flexibility of quadrupeds for rich\nenvironment interactions. Our system is tested on long-horizon tasks that are\ninfeasible to complete with one single skill. Simulation and real-world\nexperiments show that it successfully figures out multi-step strategies and\ndemonstrates non-trivial behaviors, including building tools or notifying a\nhuman for help. Demos are available on our project page:\nhttps://sites.google.com/view/long-horizon-robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nfor sketching a plan, a parameter calculator for predicting arguments in the\nplan, and a code generator to convert the plan into executable robot code. At\nthe low level, we adopt reinforcement learning to train a set of motion\nplanning and control skills to unleash the flexibility of quadrupeds for rich\nenvironment interactions. Our system is tested on long-horizon tasks that are\ninfeasible to complete with one single skill. Simulation and real-world\nexperiments show that it successfully figures out multi-step strategies and\ndemonstrates non-trivial behaviors, including building tools or notifying a\nhuman for help. Demos are available on our project page:\nhttps://sites.google.com/view/long-horizon-robot."
                },
                "authors": [
                    {
                        "name": "Yutao Ouyang"
                    },
                    {
                        "name": "Jinhan Li"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06965v1",
                "updated": "2024-10-09T15:02:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T15:02:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    2,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Factor Level Preferences to Improve Human-Model Alignment"
                },
                "summary": "Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment."
                },
                "authors": [
                    {
                        "name": "Juhyun Oh"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Inha Cha"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06961v1",
                "updated": "2024-10-09T14:57:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    57,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:57:31Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    57,
                    31,
                    2,
                    283,
                    0
                ],
                "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Boosting Large Language Models with Synthetic Preference Data"
                },
                "summary": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard."
                },
                "authors": [
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06949v1",
                "updated": "2024-10-09T14:45:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    45,
                    45,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:45:45Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    45,
                    45,
                    2,
                    283,
                    0
                ],
                "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach"
                },
                "summary": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "26 pages, 7 figures. Submitted ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06943v1",
                "updated": "2024-10-09T14:38:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    38,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:38:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    38,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API\n  Request Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API\n  Request Generation"
                },
                "summary": "Large Language Models (LLMs) leverage external tools primarily through\ngenerating the API request to enhance task completion efficiency. The accuracy\nof API request generation significantly determines the capability of LLMs to\naccomplish tasks.\n  Due to the inherent hallucinations within the LLM, it is difficult to\nefficiently and accurately generate the correct API request.\n  Current research uses prompt-based feedback to facilitate the LLM-based API\nrequest generation. However, existing methods lack factual information and are\ninsufficiently detailed.\n  To address these issues, we propose AutoFeedback, an LLM-based framework for\nefficient and accurate API request generation, with a Static Scanning Component\n(SSC) and a Dynamic Analysis Component (DAC). SSC incorporates errors detected\nin the API requests as pseudo-facts into the feedback, enriching the factual\ninformation. DAC retrieves information from API documentation, enhancing the\nlevel of detail in feedback.\n  Based on this two components, Autofeedback implementes two feedback loops\nduring the process of generating API requests by the LLM.\n  Extensive experiments demonstrate that it significantly improves accuracy of\nAPI request generation and reduces the interaction cost. AutoFeedback achieves\nan accuracy of 100.00\\% on a real-world API dataset and reduces the cost of\ninteraction with GPT-3.5 Turbo by 23.44\\%, and GPT-4 Turbo by 11.85\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) leverage external tools primarily through\ngenerating the API request to enhance task completion efficiency. The accuracy\nof API request generation significantly determines the capability of LLMs to\naccomplish tasks.\n  Due to the inherent hallucinations within the LLM, it is difficult to\nefficiently and accurately generate the correct API request.\n  Current research uses prompt-based feedback to facilitate the LLM-based API\nrequest generation. However, existing methods lack factual information and are\ninsufficiently detailed.\n  To address these issues, we propose AutoFeedback, an LLM-based framework for\nefficient and accurate API request generation, with a Static Scanning Component\n(SSC) and a Dynamic Analysis Component (DAC). SSC incorporates errors detected\nin the API requests as pseudo-facts into the feedback, enriching the factual\ninformation. DAC retrieves information from API documentation, enhancing the\nlevel of detail in feedback.\n  Based on this two components, Autofeedback implementes two feedback loops\nduring the process of generating API requests by the LLM.\n  Extensive experiments demonstrate that it significantly improves accuracy of\nAPI request generation and reduces the interaction cost. AutoFeedback achieves\nan accuracy of 100.00\\% on a real-world API dataset and reduces the cost of\ninteraction with GPT-3.5 Turbo by 23.44\\%, and GPT-4 Turbo by 11.85\\%."
                },
                "authors": [
                    {
                        "name": "Huanxi Liu"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Dawei Feng"
                    },
                    {
                        "name": "Kele Xu"
                    },
                    {
                        "name": "Huaimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huaimin Wang"
                },
                "author": "Huaimin Wang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06938v1",
                "updated": "2024-10-09T14:32:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    32,
                    1,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:32:01Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    32,
                    1,
                    2,
                    283,
                    0
                ],
                "title": "Optimized Resource Allocation for Cloud-Native 6G Networks: Zero-Touch\n  ML Models in Microservices-based VNF Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Resource Allocation for Cloud-Native 6G Networks: Zero-Touch\n  ML Models in Microservices-based VNF Deployments"
                },
                "summary": "6G, the next generation of mobile networks, is set to offer even higher data\nrates, ultra-reliability, and lower latency than 5G. New 6G services will\nincrease the load and dynamism of the network. Network Function Virtualization\n(NFV) aids with this increased load and dynamism by eliminating hardware\ndependency. It aims to boost the flexibility and scalability of network\ndeployment services by separating network functions from their specific\nproprietary forms so that they can run as virtual network functions (VNFs) on\ncommodity hardware. It is essential to design an NFV orchestration and\nmanagement framework to support these services. However, deploying bulky\nmonolithic VNFs on the network is difficult, especially when underlying\nresources are scarce, resulting in ineffective resource management. To address\nthis, microservices-based NFV approaches are proposed. In this approach,\nmonolithic VNFs are decomposed into micro VNFs, increasing the likelihood of\ntheir successful placement and resulting in more efficient resource management.\nThis article discusses the proposed framework for resource allocation for\nmicroservices-based services to provide end-to-end Quality of Service (QoS)\nusing the Double Deep Q Learning (DDQL) approach. Furthermore, to enhance this\nresource allocation approach, we discussed and addressed two crucial\nsub-problems: the need for a dynamic priority technique and the presence of the\nlow-priority starvation problem. Using the Deep Deterministic Policy Gradient\n(DDPG) model, an Adaptive Scheduling model is developed that effectively\nmitigates the starvation problem. Additionally, the impact of incorporating\ntraffic load considerations into deployment and scheduling is thoroughly\ninvestigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G, the next generation of mobile networks, is set to offer even higher data\nrates, ultra-reliability, and lower latency than 5G. New 6G services will\nincrease the load and dynamism of the network. Network Function Virtualization\n(NFV) aids with this increased load and dynamism by eliminating hardware\ndependency. It aims to boost the flexibility and scalability of network\ndeployment services by separating network functions from their specific\nproprietary forms so that they can run as virtual network functions (VNFs) on\ncommodity hardware. It is essential to design an NFV orchestration and\nmanagement framework to support these services. However, deploying bulky\nmonolithic VNFs on the network is difficult, especially when underlying\nresources are scarce, resulting in ineffective resource management. To address\nthis, microservices-based NFV approaches are proposed. In this approach,\nmonolithic VNFs are decomposed into micro VNFs, increasing the likelihood of\ntheir successful placement and resulting in more efficient resource management.\nThis article discusses the proposed framework for resource allocation for\nmicroservices-based services to provide end-to-end Quality of Service (QoS)\nusing the Double Deep Q Learning (DDQL) approach. Furthermore, to enhance this\nresource allocation approach, we discussed and addressed two crucial\nsub-problems: the need for a dynamic priority technique and the presence of the\nlow-priority starvation problem. Using the Deep Deterministic Policy Gradient\n(DDPG) model, an Adaptive Scheduling model is developed that effectively\nmitigates the starvation problem. Additionally, the impact of incorporating\ntraffic load considerations into deployment and scheduling is thoroughly\ninvestigated."
                },
                "authors": [
                    {
                        "name": "Swarna Bindu Chetty"
                    },
                    {
                        "name": "Avishek Nag"
                    },
                    {
                        "name": "Ahmed Al-Tahmeesschi"
                    },
                    {
                        "name": "Qiao Wang"
                    },
                    {
                        "name": "Berk Canberk"
                    },
                    {
                        "name": "Johann Marquez-Barja"
                    },
                    {
                        "name": "Hamed Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Ahmadi"
                },
                "author": "Hamed Ahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06932v1",
                "updated": "2024-10-09T14:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    26,
                    20,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:26:20Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    26,
                    20,
                    2,
                    283,
                    0
                ],
                "title": "Reproducing and Extending Experiments in Behavioral Strategy with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing and Extending Experiments in Behavioral Strategy with Large\n  Language Models"
                },
                "summary": "In this study, we propose LLM agents as a novel approach in behavioral\nstrategy research, complementing simulations and laboratory experiments to\nadvance our understanding of cognitive processes in decision-making.\nSpecifically, we reproduce a human laboratory experiment in behavioral strategy\nusing large language model (LLM) generated agents and investigate how LLM\nagents compare to observed human behavior. Our results show that LLM agents\neffectively reproduce search behavior and decision-making comparable to humans.\nExtending our experiment, we analyze LLM agents' simulated \"thoughts,\"\ndiscovering that more forward-looking thoughts correlate with favoring\nexploitation over exploration to maximize wealth. We show how this new approach\ncan be leveraged in behavioral strategy research and address limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose LLM agents as a novel approach in behavioral\nstrategy research, complementing simulations and laboratory experiments to\nadvance our understanding of cognitive processes in decision-making.\nSpecifically, we reproduce a human laboratory experiment in behavioral strategy\nusing large language model (LLM) generated agents and investigate how LLM\nagents compare to observed human behavior. Our results show that LLM agents\neffectively reproduce search behavior and decision-making comparable to humans.\nExtending our experiment, we analyze LLM agents' simulated \"thoughts,\"\ndiscovering that more forward-looking thoughts correlate with favoring\nexploitation over exploration to maximize wealth. We show how this new approach\ncan be leveraged in behavioral strategy research and address limitations."
                },
                "authors": [
                    {
                        "name": "Daniel Albert"
                    },
                    {
                        "name": "Stephan Billinger"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Billinger"
                },
                "author": "Stephan Billinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06916v1",
                "updated": "2024-10-09T14:15:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    15,
                    30,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:15:30Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    15,
                    30,
                    2,
                    283,
                    0
                ],
                "title": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration"
                },
                "summary": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by first employing a compact model to draft multiple tokens\nefficiently and then using the target LLM to verify them in parallel. While\nthis technique has achieved notable speedups, most existing approaches\nnecessitate either additional parameters or extensive training to construct\neffective draft models, thereby restricting their applicability across\ndifferent LLMs and tasks. To address this limitation, we explore a novel\nplug-and-play SD solution with layer-skipping, which skips intermediate layers\nof the target LLM as the compact draft model. Our analysis reveals that LLMs\nexhibit great potential for self-acceleration through layer sparsity and the\ntask-specific nature of this sparsity. Building on these insights, we introduce\nSWIFT, an on-the-fly self-speculative decoding algorithm that adaptively\nselects intermediate layers of LLMs to skip during inference. SWIFT does not\nrequire auxiliary models or additional training, making it a plug-and-play\nsolution for accelerating LLM inference across diverse input data streams. Our\nextensive experiments across a wide range of models and downstream tasks\ndemonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving\nthe original distribution of the generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by first employing a compact model to draft multiple tokens\nefficiently and then using the target LLM to verify them in parallel. While\nthis technique has achieved notable speedups, most existing approaches\nnecessitate either additional parameters or extensive training to construct\neffective draft models, thereby restricting their applicability across\ndifferent LLMs and tasks. To address this limitation, we explore a novel\nplug-and-play SD solution with layer-skipping, which skips intermediate layers\nof the target LLM as the compact draft model. Our analysis reveals that LLMs\nexhibit great potential for self-acceleration through layer sparsity and the\ntask-specific nature of this sparsity. Building on these insights, we introduce\nSWIFT, an on-the-fly self-speculative decoding algorithm that adaptively\nselects intermediate layers of LLMs to skip during inference. SWIFT does not\nrequire auxiliary models or additional training, making it a plug-and-play\nsolution for accelerating LLM inference across diverse input data streams. Our\nextensive experiments across a wide range of models and downstream tasks\ndemonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving\nthe original distribution of the generated text."
                },
                "authors": [
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12139v2",
                "updated": "2024-10-09T14:13:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    13,
                    0,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-20T16:01:01Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    16,
                    1,
                    1,
                    0,
                    141,
                    0
                ],
                "title": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on\n  LLM"
                },
                "summary": "Visual Language Tracking (VLT) enhances single object tracking (SOT) by\nintegrating natural language descriptions from a video, for the precise\ntracking of a specified object. By leveraging high-level semantic information,\nVLT guides object tracking, alleviating the constraints associated with relying\non a visual modality. Nevertheless, most VLT benchmarks are annotated in a\nsingle granularity and lack a coherent semantic framework to provide scientific\nguidance. Moreover, coordinating human annotators for high-quality annotations\nis laborious and time-consuming. To address these challenges, we introduce\nDTLLM-VLT, which automatically generates extensive and multi-granularity text\nto enhance environmental diversity. (1) DTLLM-VLT generates scientific and\nmulti-granularity text descriptions using a cohesive prompt framework. Its\nsuccinct and highly adaptable design allows seamless integration into various\nvisual tracking benchmarks. (2) We select three prominent benchmarks to deploy\nour approach: short-term tracking, long-term tracking, and global instance\ntracking. We offer four granularity combinations for these benchmarks,\nconsidering the extent and density of semantic information, thereby showcasing\nthe practicality and versatility of DTLLM-VLT. (3) We conduct comparative\nexperiments on VLT benchmarks with different text granularities, evaluating and\nanalyzing the impact of diverse text on tracking performance. Conclusionally,\nthis work leverages LLM to provide multi-granularity semantic information for\nVLT task from efficient and diverse perspectives, enabling fine-grained\nevaluation of multi-modal trackers. In the future, we believe this work can be\nextended to more datasets to support vision datasets understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Tracking (VLT) enhances single object tracking (SOT) by\nintegrating natural language descriptions from a video, for the precise\ntracking of a specified object. By leveraging high-level semantic information,\nVLT guides object tracking, alleviating the constraints associated with relying\non a visual modality. Nevertheless, most VLT benchmarks are annotated in a\nsingle granularity and lack a coherent semantic framework to provide scientific\nguidance. Moreover, coordinating human annotators for high-quality annotations\nis laborious and time-consuming. To address these challenges, we introduce\nDTLLM-VLT, which automatically generates extensive and multi-granularity text\nto enhance environmental diversity. (1) DTLLM-VLT generates scientific and\nmulti-granularity text descriptions using a cohesive prompt framework. Its\nsuccinct and highly adaptable design allows seamless integration into various\nvisual tracking benchmarks. (2) We select three prominent benchmarks to deploy\nour approach: short-term tracking, long-term tracking, and global instance\ntracking. We offer four granularity combinations for these benchmarks,\nconsidering the extent and density of semantic information, thereby showcasing\nthe practicality and versatility of DTLLM-VLT. (3) We conduct comparative\nexperiments on VLT benchmarks with different text granularities, evaluating and\nanalyzing the impact of diverse text on tracking performance. Conclusionally,\nthis work leverages LLM to provide multi-granularity semantic information for\nVLT task from efficient and diverse perspectives, enabling fine-grained\nevaluation of multi-modal trackers. In the future, we believe this work can be\nextended to more datasets to support vision datasets understanding."
                },
                "authors": [
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Dailing Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "arxiv_comment": "Accepted by CVPR Workshop 2024, Oral Presentation, Best Paper\n  Honorable Mention Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06913v1",
                "updated": "2024-10-09T14:12:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:12:51Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning"
                },
                "summary": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict emerges when the RAIT data is constructed solely\non correctness criteria, causing similar samples in the LLM's feature space to\nbe assigned different labels (original vs. modified \"I don't know\"). Dynamic\nconflict occurs due to the changes of LLM's knowledge state during fine-tuning,\nwhich transforms previous unknown questions into knowns, while the training\ndata, which is constructed based on the initial LLM, remains unchanged. These\nconflicts cause the trained LLM to misclassify known questions as unknown,\nresulting in over-refusal. To address this issue, we introduce Certainty\nRepresented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).\nCRaFT centers on two main contributions: First, we additionally incorporate\nresponse certainty to selectively filter and modify data, reducing static\nconflicts. Second, we implement preliminary rehearsal training to characterize\nchanges in the LLM's knowledge state, which helps mitigate dynamic conflicts\nduring the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Source code and training data will be released at Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict emerges when the RAIT data is constructed solely\non correctness criteria, causing similar samples in the LLM's feature space to\nbe assigned different labels (original vs. modified \"I don't know\"). Dynamic\nconflict occurs due to the changes of LLM's knowledge state during fine-tuning,\nwhich transforms previous unknown questions into knowns, while the training\ndata, which is constructed based on the initial LLM, remains unchanged. These\nconflicts cause the trained LLM to misclassify known questions as unknown,\nresulting in over-refusal. To address this issue, we introduce Certainty\nRepresented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).\nCRaFT centers on two main contributions: First, we additionally incorporate\nresponse certainty to selectively filter and modify data, reducing static\nconflicts. Second, we implement preliminary rehearsal training to characterize\nchanges in the LLM's knowledge state, which helps mitigate dynamic conflicts\nduring the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Source code and training data will be released at Github."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06911v1",
                "updated": "2024-10-09T14:12:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:12:28Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    28,
                    2,
                    283,
                    0
                ],
                "title": "Combining Planning and Diffusion for Mobility with Unknown Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Planning and Diffusion for Mobility with Unknown Dynamics"
                },
                "summary": "Manipulation of large objects over long horizons (such as carts in a\nwarehouse) is an essential skill for deployable robotic systems. Large objects\nrequire mobile manipulation which involves simultaneous manipulation,\nnavigation, and movement with the object in tow. In many real-world situations,\nobject dynamics are incredibly complex, such as the interaction of an office\nchair (with a rotating base and five caster wheels) and the ground. We present\na hierarchical algorithm for long-horizon robot manipulation problems in which\nthe dynamics are partially unknown. We observe that diffusion-based behavior\ncloning is highly effective for short-horizon problems with unknown dynamics,\nso we decompose the problem into an abstract high-level, obstacle-aware\nmotion-planning problem that produces a waypoint sequence. We use a\nshort-horizon, relative-motion diffusion policy to achieve the waypoints in\nsequence. We train mobile manipulation policies on a Spot robot that has to\npush and pull an office chair. Our hierarchical manipulation policy performs\nconsistently better, especially when the horizon increases, compared to a\ndiffusion policy trained on long-horizon demonstrations or motion planning\nassuming a rigidly-attached object (success rate of 8 (versus 0 and 5\nrespectively) out of 10 runs). Importantly, our learned policy generalizes to\nnew layouts, grasps, chairs, and flooring that induces more friction, without\nany further training, showing promise for other complex mobile manipulation\nproblems. Project Page: https://yravan.github.io/plannerorderedpolicy/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manipulation of large objects over long horizons (such as carts in a\nwarehouse) is an essential skill for deployable robotic systems. Large objects\nrequire mobile manipulation which involves simultaneous manipulation,\nnavigation, and movement with the object in tow. In many real-world situations,\nobject dynamics are incredibly complex, such as the interaction of an office\nchair (with a rotating base and five caster wheels) and the ground. We present\na hierarchical algorithm for long-horizon robot manipulation problems in which\nthe dynamics are partially unknown. We observe that diffusion-based behavior\ncloning is highly effective for short-horizon problems with unknown dynamics,\nso we decompose the problem into an abstract high-level, obstacle-aware\nmotion-planning problem that produces a waypoint sequence. We use a\nshort-horizon, relative-motion diffusion policy to achieve the waypoints in\nsequence. We train mobile manipulation policies on a Spot robot that has to\npush and pull an office chair. Our hierarchical manipulation policy performs\nconsistently better, especially when the horizon increases, compared to a\ndiffusion policy trained on long-horizon demonstrations or motion planning\nassuming a rigidly-attached object (success rate of 8 (versus 0 and 5\nrespectively) out of 10 runs). Importantly, our learned policy generalizes to\nnew layouts, grasps, chairs, and flooring that induces more friction, without\nany further training, showing promise for other complex mobile manipulation\nproblems. Project Page: https://yravan.github.io/plannerorderedpolicy/"
                },
                "authors": [
                    {
                        "name": "Yajvan Ravan"
                    },
                    {
                        "name": "Zhutian Yang"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Tomás Lozano-Pérez"
                    },
                    {
                        "name": "Leslie Pack Kaelbling"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Pack Kaelbling"
                },
                "author": "Leslie Pack Kaelbling",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02492v2",
                "updated": "2024-10-09T14:07:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    7,
                    15,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-03T13:57:07Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    57,
                    7,
                    3,
                    277,
                    0
                ],
                "title": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM"
                },
                "summary": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/."
                },
                "authors": [
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Dailing Zhang"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "arxiv_comment": "Preprint, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06898v1",
                "updated": "2024-10-09T13:59:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    59,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:59:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    59,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "Generative Model for Less-Resourced Language with 1 billion parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Model for Less-Resourced Language with 1 billion parameters"
                },
                "summary": "Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model."
                },
                "authors": [
                    {
                        "name": "Domen Vreš"
                    },
                    {
                        "name": "Martin Božič"
                    },
                    {
                        "name": "Aljaž Potočnik"
                    },
                    {
                        "name": "Tomaž Martinčič"
                    },
                    {
                        "name": "Marko Robnik-Šikonja"
                    }
                ],
                "author_detail": {
                    "name": "Marko Robnik-Šikonja"
                },
                "author": "Marko Robnik-Šikonja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06886v1",
                "updated": "2024-10-09T13:47:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    47,
                    50,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:47:50Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    47,
                    50,
                    2,
                    283,
                    0
                ],
                "title": "FltLM: An Intergrated Long-Context Large Language Model for Effective\n  Context Filtering and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FltLM: An Intergrated Long-Context Large Language Model for Effective\n  Context Filtering and Understanding"
                },
                "summary": "The development of Long-Context Large Language Models (LLMs) has markedly\nadvanced natural language processing by facilitating the process of textual\ndata across long documents and multiple corpora. However, Long-Context LLMs\nstill face two critical challenges: The lost in the middle phenomenon, where\ncrucial middle-context information is likely to be missed, and the distraction\nissue that the models lose focus due to overly extended contexts. To address\nthese challenges, we propose the Context Filtering Language Model (FltLM), a\nnovel integrated Long-Context LLM which enhances the ability of the model on\nmulti-document question-answering (QA) tasks. Specifically, FltLM innovatively\nincorporates a context filter with a soft mask mechanism, identifying and\ndynamically excluding irrelevant content to concentrate on pertinent\ninformation for better comprehension and reasoning. Our approach not only\nmitigates these two challenges, but also enables the model to operate\nconveniently in a single forward pass. Experimental results demonstrate that\nFltLM significantly outperforms supervised fine-tuning and retrieval-based\nmethods in complex QA scenarios, suggesting a promising solution for more\naccurate and reliable long-context natural language understanding applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Long-Context Large Language Models (LLMs) has markedly\nadvanced natural language processing by facilitating the process of textual\ndata across long documents and multiple corpora. However, Long-Context LLMs\nstill face two critical challenges: The lost in the middle phenomenon, where\ncrucial middle-context information is likely to be missed, and the distraction\nissue that the models lose focus due to overly extended contexts. To address\nthese challenges, we propose the Context Filtering Language Model (FltLM), a\nnovel integrated Long-Context LLM which enhances the ability of the model on\nmulti-document question-answering (QA) tasks. Specifically, FltLM innovatively\nincorporates a context filter with a soft mask mechanism, identifying and\ndynamically excluding irrelevant content to concentrate on pertinent\ninformation for better comprehension and reasoning. Our approach not only\nmitigates these two challenges, but also enables the model to operate\nconveniently in a single forward pass. Experimental results demonstrate that\nFltLM significantly outperforms supervised fine-tuning and retrieval-based\nmethods in complex QA scenarios, suggesting a promising solution for more\naccurate and reliable long-context natural language understanding applications."
                },
                "authors": [
                    {
                        "name": "Jingyang Deng"
                    },
                    {
                        "name": "Zhengyang Shen"
                    },
                    {
                        "name": "Boyang Wang"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Suqi Cheng"
                    },
                    {
                        "name": "Ying Nie"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "arxiv_comment": "Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI-2024), this is the full version of the paper including technical\n  appendices. This final version features enhanced formatting and corrections\n  to errors present in other online versions. We regret any inconvenience this\n  may have caused our readers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06880v1",
                "updated": "2024-10-09T13:43:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    43,
                    35,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T13:43:35Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    43,
                    35,
                    2,
                    283,
                    0
                ],
                "title": "Cooperative UAV-Relay based Satellite Aerial Ground Integrated Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative UAV-Relay based Satellite Aerial Ground Integrated Networks"
                },
                "summary": "In the post-fifth generation (5G) era, escalating user quality of service\n(QoS) strains terrestrial network capacity, especially in urban areas with\ndynamic traffic distributions. This paper introduces a novel cooperative\nunmanned aerial vehicle relay-based deployment (CUD) framework in satellite\nair-ground integrated networks (SAGIN). The CUD strategy deploys an unmanned\naerial vehicle-based relay (UAVr) in an amplify-andforward (AF) mode to enhance\nuser QoS when terrestrial base stations fall short of network capacity. By\ncombining low earth orbit (LEO) satellite and UAVr signals using cooperative\ndiversity, the CUD framework enhances the signal to noise ratio (SNR) at the\nuser. Comparative evaluations against existing frameworks reveal performance\nimprovements, demonstrating the effectiveness of the CUD framework in\naddressing the evolving demands of next-generation networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the post-fifth generation (5G) era, escalating user quality of service\n(QoS) strains terrestrial network capacity, especially in urban areas with\ndynamic traffic distributions. This paper introduces a novel cooperative\nunmanned aerial vehicle relay-based deployment (CUD) framework in satellite\nair-ground integrated networks (SAGIN). The CUD strategy deploys an unmanned\naerial vehicle-based relay (UAVr) in an amplify-andforward (AF) mode to enhance\nuser QoS when terrestrial base stations fall short of network capacity. By\ncombining low earth orbit (LEO) satellite and UAVr signals using cooperative\ndiversity, the CUD framework enhances the signal to noise ratio (SNR) at the\nuser. Comparative evaluations against existing frameworks reveal performance\nimprovements, demonstrating the effectiveness of the CUD framework in\naddressing the evolving demands of next-generation networks."
                },
                "authors": [
                    {
                        "name": "Bhola"
                    },
                    {
                        "name": "Yu-Jia Chen"
                    },
                    {
                        "name": "Ashutosh Balakrishnan"
                    },
                    {
                        "name": "Swades De"
                    },
                    {
                        "name": "Li-Chun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Li-Chun Wang"
                },
                "author": "Li-Chun Wang",
                "arxiv_comment": "5 pages, 3 figures, to appear in IEEE 100th Vehicular Technology\n  Conference (VTC2024-Fall)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06927v3",
                "updated": "2024-10-09T13:39:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    39,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-11T00:56:02Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    0,
                    56,
                    2,
                    2,
                    255,
                    0
                ],
                "title": "Representation Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Tuning"
                },
                "summary": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f."
                },
                "authors": [
                    {
                        "name": "Christopher M. Ackerman"
                    }
                ],
                "author_detail": {
                    "name": "Christopher M. Ackerman"
                },
                "author": "Christopher M. Ackerman",
                "arxiv_comment": "9 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06841v1",
                "updated": "2024-10-09T12:57:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    57,
                    45,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T12:57:45Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    57,
                    45,
                    2,
                    283,
                    0
                ],
                "title": "Boosting Few-Shot Detection with Large Language Models and\n  Layout-to-Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Few-Shot Detection with Large Language Models and\n  Layout-to-Image Synthesis"
                },
                "summary": "Recent advancements in diffusion models have enabled a wide range of works\nexploiting their ability to generate high-volume, high-quality data for use in\nvarious downstream tasks. One subclass of such models, dubbed Layout-to-Image\nSynthesis (LIS), learns to generate images conditioned on a spatial layout\n(bounding boxes, masks, poses, etc.) and has shown a promising ability to\ngenerate realistic images, albeit with limited layout-adherence. Moreover, the\nquestion of how to effectively transfer those models for scalable augmentation\nof few-shot detection data remains unanswered. Thus, we propose a collaborative\nframework employing a Large Language Model (LLM) and an LIS model for enhancing\nfew-shot detection beyond state-of-the-art generative augmentation approaches.\nWe leverage LLM's reasoning ability to extrapolate the spatial prior of the\nannotation space by generating new bounding boxes given only a few example\nannotations. Additionally, we introduce our novel layout-aware CLIP score for\nsample ranking, enabling tight coupling between generated layouts and images.\nSignificant improvements on COCO few-shot benchmarks are observed. With our\napproach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on\nthe COCO 5-,10-, and 30-shot settings, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion models have enabled a wide range of works\nexploiting their ability to generate high-volume, high-quality data for use in\nvarious downstream tasks. One subclass of such models, dubbed Layout-to-Image\nSynthesis (LIS), learns to generate images conditioned on a spatial layout\n(bounding boxes, masks, poses, etc.) and has shown a promising ability to\ngenerate realistic images, albeit with limited layout-adherence. Moreover, the\nquestion of how to effectively transfer those models for scalable augmentation\nof few-shot detection data remains unanswered. Thus, we propose a collaborative\nframework employing a Large Language Model (LLM) and an LIS model for enhancing\nfew-shot detection beyond state-of-the-art generative augmentation approaches.\nWe leverage LLM's reasoning ability to extrapolate the spatial prior of the\nannotation space by generating new bounding boxes given only a few example\nannotations. Additionally, we introduce our novel layout-aware CLIP score for\nsample ranking, enabling tight coupling between generated layouts and images.\nSignificant improvements on COCO few-shot benchmarks are observed. With our\napproach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on\nthe COCO 5-,10-, and 30-shot settings, respectively."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdullah"
                    },
                    {
                        "name": "Nikolas Ebert"
                    },
                    {
                        "name": "Oliver Wasenmüller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Wasenmüller"
                },
                "author": "Oliver Wasenmüller",
                "arxiv_comment": "This paper has been accepted at the Asian Conference on Computer\n  Vision (ACCV), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02966v3",
                "updated": "2024-10-09T12:46:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    46,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-05T13:43:58Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    13,
                    43,
                    58,
                    1,
                    65,
                    0
                ],
                "title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering"
                },
                "summary": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary."
                },
                "authors": [
                    {
                        "name": "Sungho Ko"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17264v2",
                "updated": "2024-10-09T12:34:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    34,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-27T15:22:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    15,
                    22,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "On the Noise Robustness of In-Context Learning for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Noise Robustness of In-Context Learning for Text Generation"
                },
                "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations."
                },
                "authors": [
                    {
                        "name": "Hongfu Gao"
                    },
                    {
                        "name": "Feipeng Zhang"
                    },
                    {
                        "name": "Wenyu Jiang"
                    },
                    {
                        "name": "Jun Shu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v4",
                "updated": "2024-10-09T12:29:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    29,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19846v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19846v5",
                "updated": "2024-10-09T12:14:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    14,
                    22,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-30T08:50:55Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    8,
                    50,
                    55,
                    3,
                    151,
                    0
                ],
                "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model"
                },
                "summary": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes."
                },
                "authors": [
                    {
                        "name": "Chaochen Gao"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Qi Fu"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19846v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19846v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18231v2",
                "updated": "2024-10-09T12:11:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    11,
                    15,
                    2,
                    283,
                    0
                ],
                "published": "2024-04-28T15:56:41Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    15,
                    56,
                    41,
                    6,
                    119,
                    0
                ],
                "title": "From Persona to Personalization: A Survey on Role-Playing Language\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Persona to Personalization: A Survey on Role-Playing Language\n  Agents"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony."
                },
                "authors": [
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Nianqi Li"
                    },
                    {
                        "name": "Lida Chen"
                    },
                    {
                        "name": "Caiyu Hu"
                    },
                    {
                        "name": "Siye Wu"
                    },
                    {
                        "name": "Scott Ren"
                    },
                    {
                        "name": "Ziquan Fu"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Accepted to TMLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01121v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01121v4",
                "updated": "2024-10-09T12:10:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    10,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-02T08:05:03Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    8,
                    5,
                    3,
                    5,
                    62,
                    0
                ],
                "title": "OpenGraph: Towards Open Graph Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenGraph: Towards Open Graph Foundation Models"
                },
                "summary": "Graph learning has become essential in various domains, including\nrecommendation systems and social network analysis. Graph Neural Networks\n(GNNs) have emerged as promising techniques for encoding structural information\nand improving performance in tasks like link prediction and node\nclassification. However, a key challenge remains: the difficulty of\ngeneralizing to unseen graph data with different properties. In this work, we\npropose a novel graph foundation model, called OpenGraph, to address this\nchallenge. Our approach tackles several technical obstacles. Firstly, we\nenhance data augmentation using a large language model (LLM) to overcome data\nscarcity in real-world scenarios. Secondly, we introduce a unified graph\ntokenizer that enables the model to generalize effectively to diverse graph\ndata, even when encountering unseen properties during training. Thirdly, our\ndeveloped scalable graph transformer captures node-wise dependencies within the\nglobal topological context. Extensive experiments validate the effectiveness of\nour framework. By adapting OpenGraph to new graph characteristics and\ncomprehending diverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings. We release the model\nimplementation at https://github.com/HKUDS/OpenGraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph learning has become essential in various domains, including\nrecommendation systems and social network analysis. Graph Neural Networks\n(GNNs) have emerged as promising techniques for encoding structural information\nand improving performance in tasks like link prediction and node\nclassification. However, a key challenge remains: the difficulty of\ngeneralizing to unseen graph data with different properties. In this work, we\npropose a novel graph foundation model, called OpenGraph, to address this\nchallenge. Our approach tackles several technical obstacles. Firstly, we\nenhance data augmentation using a large language model (LLM) to overcome data\nscarcity in real-world scenarios. Secondly, we introduce a unified graph\ntokenizer that enables the model to generalize effectively to diverse graph\ndata, even when encountering unseen properties during training. Thirdly, our\ndeveloped scalable graph transformer captures node-wise dependencies within the\nglobal topological context. Extensive experiments validate the effectiveness of\nour framework. By adapting OpenGraph to new graph characteristics and\ncomprehending diverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings. We release the model\nimplementation at https://github.com/HKUDS/OpenGraph."
                },
                "authors": [
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Ben Kao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted by EMNLP'2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01121v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01121v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06809v1",
                "updated": "2024-10-09T12:09:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    9,
                    30,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T12:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    9,
                    30,
                    2,
                    283,
                    0
                ],
                "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level"
                },
                "summary": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xinyi Zeng"
                    },
                    {
                        "name": "Yuying Shang"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03278v2",
                "updated": "2024-10-09T12:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    7,
                    8,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-04T09:50:45Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    50,
                    45,
                    4,
                    278,
                    0
                ],
                "title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What do Large Language Models Need for Machine Translation Evaluation?"
                },
                "summary": "Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility."
                },
                "authors": [
                    {
                        "name": "Shenbin Qian"
                    },
                    {
                        "name": "Archchana Sindhujan"
                    },
                    {
                        "name": "Minnie Kabra"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Constantin Orăsan"
                    },
                    {
                        "name": "Tharindu Ranasinghe"
                    },
                    {
                        "name": "Frédéric Blain"
                    }
                ],
                "author_detail": {
                    "name": "Frédéric Blain"
                },
                "author": "Frédéric Blain",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05127v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05127v3",
                "updated": "2024-10-09T12:01:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    1,
                    24,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-07T17:55:43Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    17,
                    55,
                    43,
                    4,
                    159,
                    0
                ],
                "title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Semantic Equivalence of Tokenization in Multimodal LLM"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/."
                },
                "authors": [
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "Technical Report. The project page:\n  https://chocowu.github.io/SeTok-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05127v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12518v2",
                "updated": "2024-10-09T11:48:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    48,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-09-19T07:18:41Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    18,
                    41,
                    3,
                    263,
                    0
                ],
                "title": "Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical\n  Gaussian Splatting"
                },
                "summary": "We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a\nnovel hierarchical categorical representation, which enables accurate global 3D\nsemantic mapping, scaling-up capability, and explicit semantic label prediction\nin the 3D world. The parameter usage in semantic SLAM systems increases\nsignificantly with the growing complexity of the environment, making it\nparticularly challenging and costly for scene understanding. To address this\nproblem, we introduce a novel hierarchical representation that encodes semantic\ninformation in a compact form into 3D Gaussian Splatting, leveraging the\ncapabilities of large language models (LLMs). We further introduce a novel\nsemantic loss designed to optimize hierarchical semantic information through\nboth inter-level and cross-level optimization. Furthermore, we enhance the\nwhole SLAM system, resulting in improved tracking and mapping performance. Our\nHi-SLAM outperforms existing dense SLAM methods in both mapping and tracking\naccuracy, while achieving a 2x operation speed-up. Additionally, it exhibits\ncompetitive performance in rendering semantic segmentation in small synthetic\nscenes, with significantly reduced storage and training time requirements.\nRendering FPS impressively reaches 2,000 with semantic information and 3,000\nwithout it. Most notably, it showcases the capability of handling the complex\nreal-world scene with more than 500 semantic classes, highlighting its valuable\nscaling-up capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a\nnovel hierarchical categorical representation, which enables accurate global 3D\nsemantic mapping, scaling-up capability, and explicit semantic label prediction\nin the 3D world. The parameter usage in semantic SLAM systems increases\nsignificantly with the growing complexity of the environment, making it\nparticularly challenging and costly for scene understanding. To address this\nproblem, we introduce a novel hierarchical representation that encodes semantic\ninformation in a compact form into 3D Gaussian Splatting, leveraging the\ncapabilities of large language models (LLMs). We further introduce a novel\nsemantic loss designed to optimize hierarchical semantic information through\nboth inter-level and cross-level optimization. Furthermore, we enhance the\nwhole SLAM system, resulting in improved tracking and mapping performance. Our\nHi-SLAM outperforms existing dense SLAM methods in both mapping and tracking\naccuracy, while achieving a 2x operation speed-up. Additionally, it exhibits\ncompetitive performance in rendering semantic segmentation in small synthetic\nscenes, with significantly reduced storage and training time requirements.\nRendering FPS impressively reaches 2,000 with semantic information and 3,000\nwithout it. Most notably, it showcases the capability of handling the complex\nreal-world scene with more than 500 semantic classes, highlighting its valuable\nscaling-up capability."
                },
                "authors": [
                    {
                        "name": "Boying Li"
                    },
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Rezatofighi"
                },
                "author": "Hamid Rezatofighi",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06917v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06917v3",
                "updated": "2024-10-09T11:17:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    17,
                    46,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-09T14:52:52Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    14,
                    52,
                    52,
                    1,
                    191,
                    0
                ],
                "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Accepted to EMNLP Main 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06917v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06917v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06777v1",
                "updated": "2024-10-09T11:14:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    14,
                    7,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T11:14:07Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    14,
                    7,
                    2,
                    283,
                    0
                ],
                "title": "HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric\n  Understanding"
                },
                "summary": "The significant advancements in visual understanding and instruction\nfollowing from Multimodal Large Language Models (MLLMs) have opened up more\npossibilities for broader applications in diverse and universal human-centric\nscenarios. However, existing image-text data may not support the precise\nmodality alignment and integration of multi-grained information, which is\ncrucial for human-centric visual understanding. In this paper, we introduce\nHERM-Bench, a benchmark for evaluating the human-centric understanding\ncapabilities of MLLMs. Our work reveals the limitations of existing MLLMs in\nunderstanding complex human-centric scenarios. To address these challenges, we\npresent HERM-100K, a comprehensive dataset with multi-level human-centric\nannotations, aimed at enhancing MLLMs' training. Furthermore, we develop\nHERM-7B, a MLLM that leverages enhanced training data from HERM-100K.\nEvaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms\nexisting MLLMs across various human-centric dimensions, reflecting the current\ninadequacy of data annotations used in MLLM training for human-centric visual\nunderstanding. This research emphasizes the importance of specialized datasets\nand benchmarks in advancing the MLLMs' capabilities for human-centric\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements in visual understanding and instruction\nfollowing from Multimodal Large Language Models (MLLMs) have opened up more\npossibilities for broader applications in diverse and universal human-centric\nscenarios. However, existing image-text data may not support the precise\nmodality alignment and integration of multi-grained information, which is\ncrucial for human-centric visual understanding. In this paper, we introduce\nHERM-Bench, a benchmark for evaluating the human-centric understanding\ncapabilities of MLLMs. Our work reveals the limitations of existing MLLMs in\nunderstanding complex human-centric scenarios. To address these challenges, we\npresent HERM-100K, a comprehensive dataset with multi-level human-centric\nannotations, aimed at enhancing MLLMs' training. Furthermore, we develop\nHERM-7B, a MLLM that leverages enhanced training data from HERM-100K.\nEvaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms\nexisting MLLMs across various human-centric dimensions, reflecting the current\ninadequacy of data annotations used in MLLM training for human-centric visual\nunderstanding. This research emphasizes the importance of specialized datasets\nand benchmarks in advancing the MLLMs' capabilities for human-centric\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Keliang Li"
                    },
                    {
                        "name": "Zaifei Yang"
                    },
                    {
                        "name": "Jiahe Zhao"
                    },
                    {
                        "name": "Hongze Shen"
                    },
                    {
                        "name": "Ruibing Hou"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03553v2",
                "updated": "2024-10-09T10:49:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    49,
                    8,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-04T16:02:50Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    16,
                    2,
                    50,
                    4,
                    278,
                    0
                ],
                "title": "Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding"
                },
                "summary": "Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Mingze Yin"
                    },
                    {
                        "name": "Yiheng Zhu"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06741v1",
                "updated": "2024-10-09T10:20:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    32,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T10:20:32Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    32,
                    2,
                    283,
                    0
                ],
                "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models"
                },
                "summary": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder."
                },
                "authors": [
                    {
                        "name": "Zi Gong"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Cong Liao"
                    },
                    {
                        "name": "Bingchang Liu"
                    },
                    {
                        "name": "Chaoyu Chen"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "15 pages, main conference of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06735v1",
                "updated": "2024-10-09T10:13:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    13,
                    13,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T10:13:13Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    13,
                    13,
                    2,
                    283,
                    0
                ],
                "title": "Which Programming Language and What Features at Pre-training Stage\n  Affect Downstream Logical Inference Performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Programming Language and What Features at Pre-training Stage\n  Affect Downstream Logical Inference Performance?"
                },
                "summary": "Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Fumiya Uchiyama"
                    },
                    {
                        "name": "Takeshi Kojima"
                    },
                    {
                        "name": "Andrew Gambardella"
                    },
                    {
                        "name": "Qi Cao"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Yutaka Matsuo"
                },
                "author": "Yutaka Matsuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06733v1",
                "updated": "2024-10-09T10:09:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    9,
                    11,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T10:09:11Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    9,
                    11,
                    2,
                    283,
                    0
                ],
                "title": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with\n  Situation Puzzles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with\n  Situation Puzzles"
                },
                "summary": "While advancements in NLP have significantly improved the performance of\nLarge Language Models (LLMs) on tasks requiring vertical thinking, their\nlateral thinking capabilities remain under-explored and challenging to measure\ndue to the complexity of assessing creative thought processes and the scarcity\nof relevant data. To address these challenges, we introduce SPLAT, a benchmark\nleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.\nThis benchmark, containing 975 graded situation puzzles across three difficulty\nlevels, employs a new multi-turn player-judge framework instead of the\ntraditional model-based evaluation, which often necessitates a stronger\nevaluation model. This framework simulates an interactive game where the model\n(player) asks the evaluation model (judge) questions about an incomplete story\nto infer the full scenario. The judge answers based on a detailed reference\nscenario or evaluates if the player's predictions align with the reference one.\nThis approach lessens dependence on more robust evaluation models, enabling the\nassessment of state-of-the-art LLMs. The experiments demonstrate that a robust\nevaluation model, such as WizardLM-2, closely matches human judgements in both\nintermediate question-answering and final scenario accuracy, achieving over 80%\nagreement-similar to the agreement levels among humans. Furthermore, applying\ndata and reasoning processes from our benchmark to other lateral\nthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to\nperformance enhancements. This suggests that our benchmark effectively\nevaluates and elicits the lateral thinking abilities of LLMs. Code is available\nat: https://github.com/chenqi008/LateralThinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While advancements in NLP have significantly improved the performance of\nLarge Language Models (LLMs) on tasks requiring vertical thinking, their\nlateral thinking capabilities remain under-explored and challenging to measure\ndue to the complexity of assessing creative thought processes and the scarcity\nof relevant data. To address these challenges, we introduce SPLAT, a benchmark\nleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.\nThis benchmark, containing 975 graded situation puzzles across three difficulty\nlevels, employs a new multi-turn player-judge framework instead of the\ntraditional model-based evaluation, which often necessitates a stronger\nevaluation model. This framework simulates an interactive game where the model\n(player) asks the evaluation model (judge) questions about an incomplete story\nto infer the full scenario. The judge answers based on a detailed reference\nscenario or evaluates if the player's predictions align with the reference one.\nThis approach lessens dependence on more robust evaluation models, enabling the\nassessment of state-of-the-art LLMs. The experiments demonstrate that a robust\nevaluation model, such as WizardLM-2, closely matches human judgements in both\nintermediate question-answering and final scenario accuracy, achieving over 80%\nagreement-similar to the agreement levels among humans. Furthermore, applying\ndata and reasoning processes from our benchmark to other lateral\nthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to\nperformance enhancements. This suggests that our benchmark effectively\nevaluates and elicits the lateral thinking abilities of LLMs. Code is available\nat: https://github.com/chenqi008/LateralThinking."
                },
                "authors": [
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Qi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wu"
                },
                "author": "Qi Wu",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06722v1",
                "updated": "2024-10-09T09:45:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    45,
                    1,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:45:01Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    45,
                    1,
                    2,
                    283,
                    0
                ],
                "title": "Scaling Laws for Mixed quantization in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Mixed quantization in Large Language Models"
                },
                "summary": "Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the computational requirements for running inference on\nthese models. In this study, we focus on a straightforward question: When\naiming for a specific accuracy or perplexity target for low-precision\nquantization, how many high-precision numbers or calculations are required to\npreserve as we scale LLMs to larger sizes? We first introduce a critical metric\nnamed the quantization ratio, which compares the number of parameters quantized\nto low-precision arithmetic against the total parameter count. Through\nextensive and carefully controlled experiments across different model families,\narithmetic types, and quantization granularities (e.g. layer-wise,\nmatmul-wise), we identify two central phenomenons. 1) The larger the models,\nthe better they can preserve performance with an increased quantization ratio,\nas measured by perplexity in pre-training tasks or accuracy in downstream\ntasks. 2) The finer the granularity of mixed-precision quantization (e.g.,\nmatmul-wise), the more the model can increase the quantization ratio. We\nbelieve these observed phenomena offer valuable insights for future AI hardware\ndesign and the development of advanced Efficient AI algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the computational requirements for running inference on\nthese models. In this study, we focus on a straightforward question: When\naiming for a specific accuracy or perplexity target for low-precision\nquantization, how many high-precision numbers or calculations are required to\npreserve as we scale LLMs to larger sizes? We first introduce a critical metric\nnamed the quantization ratio, which compares the number of parameters quantized\nto low-precision arithmetic against the total parameter count. Through\nextensive and carefully controlled experiments across different model families,\narithmetic types, and quantization granularities (e.g. layer-wise,\nmatmul-wise), we identify two central phenomenons. 1) The larger the models,\nthe better they can preserve performance with an increased quantization ratio,\nas measured by perplexity in pre-training tasks or accuracy in downstream\ntasks. 2) The finer the granularity of mixed-precision quantization (e.g.,\nmatmul-wise), the more the model can increase the quantization ratio. We\nbelieve these observed phenomena offer valuable insights for future AI hardware\ndesign and the development of advanced Efficient AI algorithms."
                },
                "authors": [
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06721v1",
                "updated": "2024-10-09T09:44:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    44,
                    51,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    44,
                    51,
                    2,
                    283,
                    0
                ],
                "title": "Orchestrating the Execution of Serverless Functions in Hybrid Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orchestrating the Execution of Serverless Functions in Hybrid Clouds"
                },
                "summary": "In recent years, serverless computing, especially Function as a Service\n(FaaS), is rapidly growing in popularity as a cloud programming model. The\nserverless computing model provides an intuitive interface for developing\ncloud-based applications, where the development and deployment of scalable\nmicroservices has become easier and cost-effective. An increasing number of\nbatch-processing applications are deployed as pipelines that comprise a\nsequence of functions that must meet their deadline targets to be practical. In\nthis paper, we present our Hybrid Cloud Scheduler (HCS) for orchestrating the\nexecution of serverless batch-processing pipelines deployed over heterogeneous\ninfrastructures. Our framework enables developers to (i) automatically schedule\nand execute batch-processing applications in heterogeneous environments such as\nthe private edge and public cloud serverless infrastructures, (ii) benefit from\ncost reduction through the utilization of their own resources in a private\ncluster, and (iii) significantly improves the probability of meeting the\ndeadline requirements of their applications. Our experimental evaluation\ndemonstrates the efficiency and benefits of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, serverless computing, especially Function as a Service\n(FaaS), is rapidly growing in popularity as a cloud programming model. The\nserverless computing model provides an intuitive interface for developing\ncloud-based applications, where the development and deployment of scalable\nmicroservices has become easier and cost-effective. An increasing number of\nbatch-processing applications are deployed as pipelines that comprise a\nsequence of functions that must meet their deadline targets to be practical. In\nthis paper, we present our Hybrid Cloud Scheduler (HCS) for orchestrating the\nexecution of serverless batch-processing pipelines deployed over heterogeneous\ninfrastructures. Our framework enables developers to (i) automatically schedule\nand execute batch-processing applications in heterogeneous environments such as\nthe private edge and public cloud serverless infrastructures, (ii) benefit from\ncost reduction through the utilization of their own resources in a private\ncluster, and (iii) significantly improves the probability of meeting the\ndeadline requirements of their applications. Our experimental evaluation\ndemonstrates the efficiency and benefits of our approach."
                },
                "authors": [
                    {
                        "name": "Aristotelis Peri"
                    },
                    {
                        "name": "Michail Tsenos"
                    },
                    {
                        "name": "Vana Kalogeraki"
                    }
                ],
                "author_detail": {
                    "name": "Vana Kalogeraki"
                },
                "author": "Vana Kalogeraki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06718v1",
                "updated": "2024-10-09T09:41:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    41,
                    34,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:41:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    41,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "MatMamba: A Matryoshka State Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatMamba: A Matryoshka State Space Model"
                },
                "summary": "State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}"
                },
                "authors": [
                    {
                        "name": "Abhinav Shukla"
                    },
                    {
                        "name": "Sai Vemprala"
                    },
                    {
                        "name": "Aditya Kusupati"
                    },
                    {
                        "name": "Ashish Kapoor"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Kapoor"
                },
                "author": "Ashish Kapoor",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06716v1",
                "updated": "2024-10-09T09:39:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    39,
                    55,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:39:55Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    39,
                    55,
                    2,
                    283,
                    0
                ],
                "title": "Guaranteed Generation from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guaranteed Generation from Large Language Models"
                },
                "summary": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities."
                },
                "authors": [
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Hwaran Lee"
                    },
                    {
                        "name": "Kyomin Jung"
                    },
                    {
                        "name": "Marc Dymetman"
                    }
                ],
                "author_detail": {
                    "name": "Marc Dymetman"
                },
                "author": "Marc Dymetman",
                "arxiv_comment": "22 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06707v1",
                "updated": "2024-10-09T09:20:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    20,
                    24,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:20:24Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    20,
                    24,
                    2,
                    283,
                    0
                ],
                "title": "Calibrating Verbalized Probabilities for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Verbalized Probabilities for Large Language Models"
                },
                "summary": "Calibrating verbalized probabilities presents a novel approach for reliably\nassessing and leveraging outputs from black-box Large Language Models (LLMs).\nRecent methods have demonstrated improved calibration by applying techniques\nlike Platt scaling or temperature scaling to the confidence scores generated by\nLLMs. In this paper, we explore the calibration of verbalized probability\ndistributions for discriminative tasks. First, we investigate the capability of\nLLMs to generate probability distributions over categorical labels. We\ntheoretically and empirically identify the issue of re-softmax arising from the\nscaling of verbalized probabilities, and propose using the invert softmax trick\nto approximate the \"logit\" by inverting verbalized probabilities. Through\nextensive evaluation on three public datasets, we demonstrate: (1) the robust\ncapability of LLMs in generating class distributions, and (2) the effectiveness\nof the invert softmax trick in estimating logits, which, in turn, facilitates\npost-calibration adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating verbalized probabilities presents a novel approach for reliably\nassessing and leveraging outputs from black-box Large Language Models (LLMs).\nRecent methods have demonstrated improved calibration by applying techniques\nlike Platt scaling or temperature scaling to the confidence scores generated by\nLLMs. In this paper, we explore the calibration of verbalized probability\ndistributions for discriminative tasks. First, we investigate the capability of\nLLMs to generate probability distributions over categorical labels. We\ntheoretically and empirically identify the issue of re-softmax arising from the\nscaling of verbalized probabilities, and propose using the invert softmax trick\nto approximate the \"logit\" by inverting verbalized probabilities. Through\nextensive evaluation on three public datasets, we demonstrate: (1) the robust\ncapability of LLMs in generating class distributions, and (2) the effectiveness\nof the invert softmax trick in estimating logits, which, in turn, facilitates\npost-calibration adjustments."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Gyuri Szarvas"
                    },
                    {
                        "name": "Georges Balazs"
                    },
                    {
                        "name": "Pavel Danchenko"
                    },
                    {
                        "name": "Patrick Ernst"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Ernst"
                },
                "author": "Patrick Ernst",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06704v1",
                "updated": "2024-10-09T09:16:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    16,
                    25,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:16:25Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    16,
                    25,
                    2,
                    283,
                    0
                ],
                "title": "PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs"
                },
                "summary": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Krishna Kanth Nakka"
                    },
                    {
                        "name": "Ahmed Frikha"
                    },
                    {
                        "name": "Ricardo Mendes"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Xuebing Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Zhou"
                },
                "author": "Xuebing Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06703v1",
                "updated": "2024-10-09T09:13:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    13,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:13:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    13,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness\n  in Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness\n  in Web Agents"
                },
                "summary": "Recent advancements in LLM-based web agents have introduced novel\narchitectures and benchmarks showcasing progress in autonomous web navigation\nand interaction. However, most existing benchmarks prioritize effectiveness and\naccuracy, overlooking crucial factors like safety and trustworthiness which are\nessential for deploying web agents in enterprise settings. The risks of unsafe\nweb agent behavior, such as accidentally deleting user accounts or performing\nunintended actions in critical business operations, pose significant barriers\nto widespread adoption.In this paper, we present ST-WebAgentBench, a new online\nbenchmark specifically designed to evaluate the safety and trustworthiness of\nweb agents in enterprise contexts. This benchmark is grounded in a detailed\nframework that defines safe and trustworthy (ST) agent behavior, outlines how\nST policies should be structured and introduces the Completion under Policies\nmetric to assess agent performance. Our evaluation reveals that current SOTA\nagents struggle with policy adherence and cannot yet be relied upon for\ncritical business applications. Additionally, we propose architectural\nprinciples aimed at improving policy awareness and compliance in web agents. We\nopen-source this benchmark and invite the community to contribute, with the\ngoal of fostering a new generation of safer, more trustworthy AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-based web agents have introduced novel\narchitectures and benchmarks showcasing progress in autonomous web navigation\nand interaction. However, most existing benchmarks prioritize effectiveness and\naccuracy, overlooking crucial factors like safety and trustworthiness which are\nessential for deploying web agents in enterprise settings. The risks of unsafe\nweb agent behavior, such as accidentally deleting user accounts or performing\nunintended actions in critical business operations, pose significant barriers\nto widespread adoption.In this paper, we present ST-WebAgentBench, a new online\nbenchmark specifically designed to evaluate the safety and trustworthiness of\nweb agents in enterprise contexts. This benchmark is grounded in a detailed\nframework that defines safe and trustworthy (ST) agent behavior, outlines how\nST policies should be structured and introduces the Completion under Policies\nmetric to assess agent performance. Our evaluation reveals that current SOTA\nagents struggle with policy adherence and cannot yet be relied upon for\ncritical business applications. Additionally, we propose architectural\nprinciples aimed at improving policy awareness and compliance in web agents. We\nopen-source this benchmark and invite the community to contribute, with the\ngoal of fostering a new generation of safer, more trustworthy AI agents."
                },
                "authors": [
                    {
                        "name": "Ido Levy"
                    },
                    {
                        "name": "Ben Wiesel"
                    },
                    {
                        "name": "Sami Marreed"
                    },
                    {
                        "name": "Alon Oved"
                    },
                    {
                        "name": "Avi Yaeli"
                    },
                    {
                        "name": "Segev Shlomov"
                    }
                ],
                "author_detail": {
                    "name": "Segev Shlomov"
                },
                "author": "Segev Shlomov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06700v1",
                "updated": "2024-10-09T09:09:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    9,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:09:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    9,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "Optimizing Integrated Terrestrial and Non-Terrestrial Networks\n  Performance with Traffic-Aware Resource Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Integrated Terrestrial and Non-Terrestrial Networks\n  Performance with Traffic-Aware Resource Management"
                },
                "summary": "To address an ever-increasing demand for ubiquitous high-speed connectivity,\nmobile networks have intensified their deployment process. However, achieving\nthis target has proven to be a challenge and has led to a surge in overall\nenergy consumption. In recent years, non-terrestrial networks (NTNs) have been\nendorsed as a potential solution to these problems by complementing the\ncoverage of the terrestrial network in areas with limited network deployment.\nTo this end, this paper proposes an integrated terrestrial and non-terrestrial\nnetwork (TN-NTN) that utilises the overall available communication resources to\nexpand coverage and meet Quality of Service (QoS) requirements during\nhigh-traffic hours in any deployment scenario. Importantly, our framework\nallows to drastically reduce the terrestrial network energy consumption during\nlow-traffic hours. Specifically, we introduce a novel radio resource management\nalgorithm, BLASTER (Bandwidth SpLit, User ASsociation, and PowEr ContRol),\nwhich integrates bandwidth allocation, user equipment (UE) association, power\ncontrol, and base station activation within the TN-NTN. This algorithm aims to\noptimize network resource allocation fairness and energy consumption\ndynamically, demonstrating new opportunities in deploying satellite networks in\nlegacy cellular systems. Our study offers a comprehensive analysis of the\nintegrated network model, emphasizing the effective balance between energy\nsaving and QoS, and proposing practical solutions to meet the fluctuating\ntraffic demands of cellular networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address an ever-increasing demand for ubiquitous high-speed connectivity,\nmobile networks have intensified their deployment process. However, achieving\nthis target has proven to be a challenge and has led to a surge in overall\nenergy consumption. In recent years, non-terrestrial networks (NTNs) have been\nendorsed as a potential solution to these problems by complementing the\ncoverage of the terrestrial network in areas with limited network deployment.\nTo this end, this paper proposes an integrated terrestrial and non-terrestrial\nnetwork (TN-NTN) that utilises the overall available communication resources to\nexpand coverage and meet Quality of Service (QoS) requirements during\nhigh-traffic hours in any deployment scenario. Importantly, our framework\nallows to drastically reduce the terrestrial network energy consumption during\nlow-traffic hours. Specifically, we introduce a novel radio resource management\nalgorithm, BLASTER (Bandwidth SpLit, User ASsociation, and PowEr ContRol),\nwhich integrates bandwidth allocation, user equipment (UE) association, power\ncontrol, and base station activation within the TN-NTN. This algorithm aims to\noptimize network resource allocation fairness and energy consumption\ndynamically, demonstrating new opportunities in deploying satellite networks in\nlegacy cellular systems. Our study offers a comprehensive analysis of the\nintegrated network model, emphasizing the effective balance between energy\nsaving and QoS, and proposing practical solutions to meet the fluctuating\ntraffic demands of cellular networks."
                },
                "authors": [
                    {
                        "name": "Henri Alam"
                    },
                    {
                        "name": "Antonio de Domenico"
                    },
                    {
                        "name": "David López-Pérez"
                    },
                    {
                        "name": "Florian Kaltenberger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Kaltenberger"
                },
                "author": "Florian Kaltenberger",
                "arxiv_comment": "Submitted to IEEE Transactions on Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04360v2",
                "updated": "2024-10-09T09:03:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    3,
                    48,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-06T05:02:23Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    5,
                    2,
                    23,
                    6,
                    280,
                    0
                ],
                "title": "GenSim: A General Social Simulation Platform with Large Language Model\n  based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenSim: A General Social Simulation Platform with Large Language Model\n  based Agents"
                },
                "summary": "With the rapid advancement of large language models (LLMs), recent years have\nwitnessed many promising studies on leveraging LLM-based agents to simulate\nhuman social behavior. While prior work has demonstrated significant potential\nacross various domains, much of it has focused on specific scenarios involving\na limited number of agents and has lacked the ability to adapt when errors\noccur during simulation. To overcome these limitations, we propose a novel\nLLM-agent-based simulation platform called \\textit{GenSim}, which: (1)\n\\textbf{Abstracts a set of general functions} to simplify the simulation of\ncustomized social scenarios; (2) \\textbf{Supports one hundred thousand agents}\nto better simulate large-scale populations in real-world contexts; (3)\n\\textbf{Incorporates error-correction mechanisms} to ensure more reliable and\nlong-term simulations. To evaluate our platform, we assess both the efficiency\nof large-scale agent simulations and the effectiveness of the error-correction\nmechanisms. To our knowledge, GenSim represents an initial step toward a\ngeneral, large-scale, and correctable social simulation platform based on LLM\nagents, promising to further advance the field of social science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), recent years have\nwitnessed many promising studies on leveraging LLM-based agents to simulate\nhuman social behavior. While prior work has demonstrated significant potential\nacross various domains, much of it has focused on specific scenarios involving\na limited number of agents and has lacked the ability to adapt when errors\noccur during simulation. To overcome these limitations, we propose a novel\nLLM-agent-based simulation platform called \\textit{GenSim}, which: (1)\n\\textbf{Abstracts a set of general functions} to simplify the simulation of\ncustomized social scenarios; (2) \\textbf{Supports one hundred thousand agents}\nto better simulate large-scale populations in real-world contexts; (3)\n\\textbf{Incorporates error-correction mechanisms} to ensure more reliable and\nlong-term simulations. To evaluate our platform, we assess both the efficiency\nof large-scale agent simulations and the effectiveness of the error-correction\nmechanisms. To our knowledge, GenSim represents an initial step toward a\ngeneral, large-scale, and correctable social simulation platform based on LLM\nagents, promising to further advance the field of social science."
                },
                "authors": [
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Heyang Gao"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Haoran Tan"
                    },
                    {
                        "name": "Dawei Gao"
                    },
                    {
                        "name": "Yushuo Chen"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06695v1",
                "updated": "2024-10-09T09:02:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    2,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T09:02:31Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    2,
                    31,
                    2,
                    283,
                    0
                ],
                "title": "Energy Efficient Scheduling for Serverless Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Efficient Scheduling for Serverless Systems"
                },
                "summary": "Serverless computing, also referred to as Function-as-a-Service (FaaS), is a\ncloud computing model that has attracted significant attention and has been\nwidely adopted in recent years. The serverless computing model offers an\nintuitive, event-based interface that makes the development and deployment of\nscalable cloud-based applications easier and cost-effective. An important\naspect that has not been examined in these systems is their energy consumption\nduring the application execution. One way to deal with this issue is to\nschedule the function invocations in an energy-efficient way. However,\nefficient scheduling of applications in a multi-tenant environment, like FaaS\nsystems, poses significant challenges. The trade-off between the server's\nenergy usage and the hosted functions' performance requirements needs to be\ntaken into consideration. In this work, we propose an Energy Efficient\nScheduler for orchestrating the execution of serverless functions so that it\nminimizes energy consumption while it satisfies the applications' performance\ndemands. Our approach considers real-time performance measurements and\nhistorical data and applies a novel DVFS technique to minimize energy\nconsumption. Our detailed experimental evaluation using realistic workloads on\nour local cluster illustrates the working and benefits of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing, also referred to as Function-as-a-Service (FaaS), is a\ncloud computing model that has attracted significant attention and has been\nwidely adopted in recent years. The serverless computing model offers an\nintuitive, event-based interface that makes the development and deployment of\nscalable cloud-based applications easier and cost-effective. An important\naspect that has not been examined in these systems is their energy consumption\nduring the application execution. One way to deal with this issue is to\nschedule the function invocations in an energy-efficient way. However,\nefficient scheduling of applications in a multi-tenant environment, like FaaS\nsystems, poses significant challenges. The trade-off between the server's\nenergy usage and the hosted functions' performance requirements needs to be\ntaken into consideration. In this work, we propose an Energy Efficient\nScheduler for orchestrating the execution of serverless functions so that it\nminimizes energy consumption while it satisfies the applications' performance\ndemands. Our approach considers real-time performance measurements and\nhistorical data and applies a novel DVFS technique to minimize energy\nconsumption. Our detailed experimental evaluation using realistic workloads on\nour local cluster illustrates the working and benefits of our approach."
                },
                "authors": [
                    {
                        "name": "Michail Tsenos"
                    },
                    {
                        "name": "Aristotelis Peri"
                    },
                    {
                        "name": "Vana Kalogeraki"
                    }
                ],
                "author_detail": {
                    "name": "Vana Kalogeraki"
                },
                "author": "Vana Kalogeraki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06693v1",
                "updated": "2024-10-09T08:59:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    59,
                    57,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T08:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    59,
                    57,
                    2,
                    283,
                    0
                ],
                "title": "Autonomous localization of multiple ionizing radiation sources using\n  miniature single-layer Compton cameras onboard a group of micro aerial\n  vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous localization of multiple ionizing radiation sources using\n  miniature single-layer Compton cameras onboard a group of micro aerial\n  vehicles"
                },
                "summary": "A novel method for autonomous localization of multiple sources of gamma\nradiation using a group of Micro Aerial Vehicles (MAVs) is presented in this\npaper. The method utilizes an extremely lightweight (44 g) Compton camera\nMiniPIX TPX3. The compact size of the detector allows for deployment onboard\nsafe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed\nradiation mapping approach fuses measurements from multiple distributed Compton\ncamera sensors to accurately estimate the positions of multiple radioactive\nsources in real time. Unlike commonly used intensity-based detectors, the\nCompton camera reconstructs the set of possible directions towards a radiation\nsource from just a single ionizing particle. Therefore, the proposed approach\ncan localize radiation sources without having to estimate the gradient of a\nradiation field or contour lines, which require longer measurements. The\ninstant estimation is able to fully exploit the potential of highly mobile\nMAVs. The radiation mapping method is combined with an active search strategy,\nwhich coordinates the future actions of the MAVs in order to improve the\nquality of the estimate of the sources' positions, as well as to explore the\narea of interest faster. The proposed solution is evaluated in simulation and\nreal world experiments with multiple Cesium-137 radiation sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel method for autonomous localization of multiple sources of gamma\nradiation using a group of Micro Aerial Vehicles (MAVs) is presented in this\npaper. The method utilizes an extremely lightweight (44 g) Compton camera\nMiniPIX TPX3. The compact size of the detector allows for deployment onboard\nsafe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed\nradiation mapping approach fuses measurements from multiple distributed Compton\ncamera sensors to accurately estimate the positions of multiple radioactive\nsources in real time. Unlike commonly used intensity-based detectors, the\nCompton camera reconstructs the set of possible directions towards a radiation\nsource from just a single ionizing particle. Therefore, the proposed approach\ncan localize radiation sources without having to estimate the gradient of a\nradiation field or contour lines, which require longer measurements. The\ninstant estimation is able to fully exploit the potential of highly mobile\nMAVs. The radiation mapping method is combined with an active search strategy,\nwhich coordinates the future actions of the MAVs in order to improve the\nquality of the estimate of the sources' positions, as well as to explore the\narea of interest faster. The proposed solution is evaluated in simulation and\nreal world experiments with multiple Cesium-137 radiation sources."
                },
                "authors": [
                    {
                        "name": "Michal Werner"
                    },
                    {
                        "name": "Tomáš Báča"
                    },
                    {
                        "name": "Petr Štibinger"
                    },
                    {
                        "name": "Daniela Doubravová"
                    },
                    {
                        "name": "Jaroslav Šolc"
                    },
                    {
                        "name": "Jan Rusňák"
                    },
                    {
                        "name": "Martin Saska"
                    }
                ],
                "author_detail": {
                    "name": "Martin Saska"
                },
                "author": "Martin Saska",
                "arxiv_comment": "International Conference on Intelligent Robots and Systems (IROS)\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17011v2",
                "updated": "2024-10-09T08:58:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    58,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-24T05:26:52Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    5,
                    26,
                    52,
                    2,
                    206,
                    0
                ],
                "title": "Unveiling In-Context Learning: A Coordinate System to Understand Its\n  Working Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling In-Context Learning: A Coordinate System to Understand Its\n  Working Mechanism"
                },
                "summary": "Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. The other attributes it to LLMs'\ninherent ability of task recognition, deeming label correctness and shot\nnumbers of demonstrations as not crucial. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether similar examples are presented in the demonstrations\n(perception) and whether LLMs can recognize the task (cognition). We propose\nthe peak inverse rank metric to detect the task recognition ability of LLMs and\nstudy LLMs' reactions to different definitions of similarity. Based on these,\nwe conduct extensive experiments to elucidate how ICL functions across each\nquadrant on multiple representative classification tasks. Finally, we extend\nour analyses to generation tasks, showing that our coordinate system can also\nbe used to interpret ICL for generation tasks effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. The other attributes it to LLMs'\ninherent ability of task recognition, deeming label correctness and shot\nnumbers of demonstrations as not crucial. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether similar examples are presented in the demonstrations\n(perception) and whether LLMs can recognize the task (cognition). We propose\nthe peak inverse rank metric to detect the task recognition ability of LLMs and\nstudy LLMs' reactions to different definitions of similarity. Based on these,\nwe conduct extensive experiments to elucidate how ICL functions across each\nquadrant on multiple representative classification tasks. Finally, we extend\nour analyses to generation tasks, showing that our coordinate system can also\nbe used to interpret ICL for generation tasks effectively."
                },
                "authors": [
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00965v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00965v4",
                "updated": "2024-10-10T02:36:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    2,
                    36,
                    53,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T03:38:56Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    3,
                    38,
                    56,
                    0,
                    155,
                    0
                ],
                "title": "HBTP: Heuristic Behavior Tree Planning with Large Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HBTP: Heuristic Behavior Tree Planning with Large Language Model\n  Reasoning"
                },
                "summary": "Behavior Trees (BTs) are increasingly becoming a popular control structure in\nrobotics due to their modularity, reactivity, and robustness. In terms of BT\ngeneration methods, BT planning shows promise for generating reliable BTs.\nHowever, the scalability of BT planning is often constrained by prolonged\nplanning times in complex scenarios, largely due to a lack of domain knowledge.\nIn contrast, pre-trained Large Language Models (LLMs) have demonstrated task\nreasoning capabilities across various domains, though the correctness and\nsafety of their planning remain uncertain. This paper proposes integrating BT\nplanning with LLM reasoning, introducing Heuristic Behavior Tree Planning\n(HBTP)-a reliable and efficient framework for BT generation. The key idea in\nHBTP is to leverage LLMs for task-specific reasoning to generate a heuristic\npath, which BT planning can then follow to expand efficiently. We first\nintroduce the heuristic BT expansion process, along with two heuristic variants\ndesigned for optimal planning and satisficing planning, respectively. Then, we\npropose methods to address the inaccuracies of LLM reasoning, including action\nspace pruning and reflective feedback, to further enhance both reasoning\naccuracy and planning efficiency. Experiments demonstrate the theoretical\nbounds of HBTP, and results from four datasets confirm its practical\neffectiveness in everyday service robot applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior Trees (BTs) are increasingly becoming a popular control structure in\nrobotics due to their modularity, reactivity, and robustness. In terms of BT\ngeneration methods, BT planning shows promise for generating reliable BTs.\nHowever, the scalability of BT planning is often constrained by prolonged\nplanning times in complex scenarios, largely due to a lack of domain knowledge.\nIn contrast, pre-trained Large Language Models (LLMs) have demonstrated task\nreasoning capabilities across various domains, though the correctness and\nsafety of their planning remain uncertain. This paper proposes integrating BT\nplanning with LLM reasoning, introducing Heuristic Behavior Tree Planning\n(HBTP)-a reliable and efficient framework for BT generation. The key idea in\nHBTP is to leverage LLMs for task-specific reasoning to generate a heuristic\npath, which BT planning can then follow to expand efficiently. We first\nintroduce the heuristic BT expansion process, along with two heuristic variants\ndesigned for optimal planning and satisficing planning, respectively. Then, we\npropose methods to address the inaccuracies of LLM reasoning, including action\nspace pruning and reflective feedback, to further enhance both reasoning\naccuracy and planning efficiency. Experiments demonstrate the theoretical\nbounds of HBTP, and results from four datasets confirm its practical\neffectiveness in everyday service robot applications."
                },
                "authors": [
                    {
                        "name": "Yishuai Cai"
                    },
                    {
                        "name": "Xinglin Chen"
                    },
                    {
                        "name": "Yunxin Mao"
                    },
                    {
                        "name": "Minglong Li"
                    },
                    {
                        "name": "Shaowu Yang"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Ji Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wang"
                },
                "author": "Ji Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00965v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00965v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00469v2",
                "updated": "2024-10-09T08:52:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    52,
                    33,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-01T12:12:59Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    12,
                    12,
                    59,
                    2,
                    122,
                    0
                ],
                "title": "Exploiting Positional Bias for Query-Agnostic Generative Content in\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Positional Bias for Query-Agnostic Generative Content in\n  Search"
                },
                "summary": "In recent years, neural ranking models (NRMs) have been shown to\nsubstantially outperform their lexical counterparts in text retrieval. In\ntraditional search pipelines, a combination of features leads to well-defined\nbehaviour. However, as neural approaches become increasingly prevalent as the\nfinal scoring component of engines or as standalone systems, their robustness\nto malicious text and, more generally, semantic perturbation needs to be better\nunderstood. We posit that the transformer attention mechanism can induce\nexploitable defects through positional bias in search models, leading to an\nattack that could generalise beyond a single query or topic. We demonstrate\nsuch defects by showing that non-relevant text--such as promotional\ncontent--can be easily injected into a document without adversely affecting its\nposition in search results. Unlike previous gradient-based attacks, we\ndemonstrate these biases in a query-agnostic fashion. In doing so, without the\nknowledge of topicality, we can still reduce the negative effects of\nnon-relevant content injection by controlling injection position. Our\nexperiments are conducted with simulated on-topic promotional text\nautomatically generated by prompting LLMs with topical context from target\ndocuments. We find that contextualisation of a non-relevant text further\nreduces negative effects whilst likely circumventing existing content filtering\nmechanisms. In contrast, lexical models are found to be more resilient to such\ncontent injection attacks. We then investigate a simple yet effective\ncompensation for the weaknesses of the NRMs in search, validating our\nhypotheses regarding transformer bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, neural ranking models (NRMs) have been shown to\nsubstantially outperform their lexical counterparts in text retrieval. In\ntraditional search pipelines, a combination of features leads to well-defined\nbehaviour. However, as neural approaches become increasingly prevalent as the\nfinal scoring component of engines or as standalone systems, their robustness\nto malicious text and, more generally, semantic perturbation needs to be better\nunderstood. We posit that the transformer attention mechanism can induce\nexploitable defects through positional bias in search models, leading to an\nattack that could generalise beyond a single query or topic. We demonstrate\nsuch defects by showing that non-relevant text--such as promotional\ncontent--can be easily injected into a document without adversely affecting its\nposition in search results. Unlike previous gradient-based attacks, we\ndemonstrate these biases in a query-agnostic fashion. In doing so, without the\nknowledge of topicality, we can still reduce the negative effects of\nnon-relevant content injection by controlling injection position. Our\nexperiments are conducted with simulated on-topic promotional text\nautomatically generated by prompting LLMs with topical context from target\ndocuments. We find that contextualisation of a non-relevant text further\nreduces negative effects whilst likely circumventing existing content filtering\nmechanisms. In contrast, lexical models are found to be more resilient to such\ncontent injection attacks. We then investigate a simple yet effective\ncompensation for the weaknesses of the NRMs in search, validating our\nhypotheses regarding transformer bias."
                },
                "authors": [
                    {
                        "name": "Andrew Parry"
                    },
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Debasis Ganguly"
                    }
                ],
                "author_detail": {
                    "name": "Debasis Ganguly"
                },
                "author": "Debasis Ganguly",
                "arxiv_doi": "10.18653/v1/2024.findings-acl.656",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.findings-acl.656",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 4 main figures, 7 appendix pages, 2 appendix figures,\n  Accepted to ACL 2024 Findings",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06682v1",
                "updated": "2024-10-09T08:44:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    44,
                    47,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T08:44:47Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    44,
                    47,
                    2,
                    283,
                    0
                ],
                "title": "Enhancing Multimodal LLM for Detailed and Accurate Video Captioning\n  using Multi-Round Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multimodal LLM for Detailed and Accurate Video Captioning\n  using Multi-Round Preference Optimization"
                },
                "summary": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimization (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimized using DPO. To further improve training, we\nintroduce a novel multi-round DPO (mrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initializing the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilize the\nprocess. To address potential catastrophic forgetting of non-captioning\nabilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO\nLLM by using the captions generated by the mrDPO-trained model as supervised\nlabels. Experiments show that mrDPO significantly enhances video-SALMONN 2's\ncaptioning accuracy, reducing global and local error rates by 40\\% and 20\\%,\nrespectively, while decreasing the repetition rate by 35\\%. The final\nvideo-SALMONN 2 model, with just 7 billion parameters, surpasses leading models\nsuch as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining\ncompetitive performance to the state-of-the-art on widely used video\nquestion-answering benchmark among models of similar size. Upon acceptance, we\nwill release the code, model checkpoints, and training and test data. Demos are\navailable at\n\\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimization (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimized using DPO. To further improve training, we\nintroduce a novel multi-round DPO (mrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initializing the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilize the\nprocess. To address potential catastrophic forgetting of non-captioning\nabilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO\nLLM by using the captions generated by the mrDPO-trained model as supervised\nlabels. Experiments show that mrDPO significantly enhances video-SALMONN 2's\ncaptioning accuracy, reducing global and local error rates by 40\\% and 20\\%,\nrespectively, while decreasing the repetition rate by 35\\%. The final\nvideo-SALMONN 2 model, with just 7 billion parameters, surpasses leading models\nsuch as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining\ncompetitive performance to the state-of-the-art on widely used video\nquestion-answering benchmark among models of similar size. Upon acceptance, we\nwill release the code, model checkpoints, and training and test data. Demos are\navailable at\n\\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}."
                },
                "authors": [
                    {
                        "name": "Changli Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zujun Ma"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07482v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07482v3",
                "updated": "2024-10-09T08:43:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    43,
                    25,
                    2,
                    283,
                    0
                ],
                "published": "2024-08-14T11:55:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice."
                },
                "authors": [
                    {
                        "name": "Ning Lu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Jiantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Ma"
                },
                "author": "Jiantao Ma",
                "arxiv_comment": "To be published in: IEEE International Symposium on Software\n  Reliability Engineering (ISSRE2024) workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07482v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07482v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06667v2",
                "updated": "2024-10-10T05:12:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    12,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-09T08:23:22Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    23,
                    22,
                    2,
                    283,
                    0
                ],
                "title": "Large Language Models as Code Executors: An Exploratory Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Code Executors: An Exploratory Study"
                },
                "summary": "The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks."
                },
                "authors": [
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Lecheng Yan"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Wenxi Li"
                    },
                    {
                        "name": "Younes Samih"
                    },
                    {
                        "name": "Tianbo Ji"
                    },
                    {
                        "name": "Longyue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Longyue Wang"
                },
                "author": "Longyue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03951v2",
                "updated": "2024-10-09T08:16:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    16,
                    18,
                    2,
                    283,
                    0
                ],
                "published": "2024-07-04T14:08:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    14,
                    8,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "Uncertainty-Guided Optimization on Large Language Model Search Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Guided Optimization on Large Language Model Search Trees"
                },
                "summary": "Tree search algorithms such as greedy and beam search are the standard when\nit comes to finding sequences of maximum likelihood in the decoding processes\nof large language models (LLMs). However, they are myopic since they do not\ntake the complete root-to-leaf path into account. Moreover, they are agnostic\nto prior knowledge available about the process: For example, it does not\nconsider that the objective being maximized is a probability and thereby has\nspecific properties like being bound in the unit interval. Taking a\nprobabilistic approach, we define prior beliefs over LLMs' transition\nprobabilities and obtain posterior beliefs over the most promising paths in\neach iteration. These beliefs are useful for defining a sample-based,\nnon-myopic acquisition function that allows for a more data-efficient\nexploration scheme than standard search algorithms on LLMs. Crucially, unlike\nexpensive simulation-based non-myopic methods like the Monte Carlo tree search,\nour method only requires samples from the beliefs. Our formulation thus views\nLLM decoding as Bayesian optimization on trees. We discuss how to select the\nprior and the acquisition function, and demonstrate in experiments with various\nLLMs that our method achieves higher efficiency than recent baselines: Our\nmethod achieves the same or a higher likelihood while expanding fewer nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree search algorithms such as greedy and beam search are the standard when\nit comes to finding sequences of maximum likelihood in the decoding processes\nof large language models (LLMs). However, they are myopic since they do not\ntake the complete root-to-leaf path into account. Moreover, they are agnostic\nto prior knowledge available about the process: For example, it does not\nconsider that the objective being maximized is a probability and thereby has\nspecific properties like being bound in the unit interval. Taking a\nprobabilistic approach, we define prior beliefs over LLMs' transition\nprobabilities and obtain posterior beliefs over the most promising paths in\neach iteration. These beliefs are useful for defining a sample-based,\nnon-myopic acquisition function that allows for a more data-efficient\nexploration scheme than standard search algorithms on LLMs. Crucially, unlike\nexpensive simulation-based non-myopic methods like the Monte Carlo tree search,\nour method only requires samples from the beliefs. Our formulation thus views\nLLM decoding as Bayesian optimization on trees. We discuss how to select the\nprior and the acquisition function, and demonstrate in experiments with various\nLLMs that our method achieves higher efficiency than recent baselines: Our\nmethod achieves the same or a higher likelihood while expanding fewer nodes."
                },
                "authors": [
                    {
                        "name": "Julia Grosse"
                    },
                    {
                        "name": "Ruotian Wu"
                    },
                    {
                        "name": "Ahmad Rashid"
                    },
                    {
                        "name": "Philipp Hennig"
                    },
                    {
                        "name": "Pascal Poupart"
                    },
                    {
                        "name": "Agustinus Kristiadi"
                    }
                ],
                "author_detail": {
                    "name": "Agustinus Kristiadi"
                },
                "author": "Agustinus Kristiadi",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12815v2",
                "updated": "2024-10-09T07:58:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    58,
                    37,
                    2,
                    283,
                    0
                ],
                "published": "2024-08-23T03:21:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    21,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and\n  Long-Range Dependencies for Structural Crack Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and\n  Long-Range Dependencies for Structural Crack Segmentation"
                },
                "summary": "Detecting cracks with pixel-level precision for key structures is a\nsignificant challenge, as existing methods struggle to effectively integrate\nlocal textures and pixel dependencies of cracks. Furthermore, these methods\noften possess numerous parameters and substantial computational requirements,\ncomplicating deployment on edge control devices. In this paper, we propose a\nstaircase cascaded fusion crack segmentation network (CrackSCF) that generates\nhigh-quality crack segmentation maps using minimal computational resources. We\nconstructed a staircase cascaded fusion module that effectively captures local\npatterns of cracks and long-range dependencies of pixels, and it can suppress\nbackground noise well. To reduce the computational resources required by the\nmodel, we introduced a lightweight convolution block, which replaces all\nconvolution operations in the network, significantly reducing the required\ncomputation and parameters without affecting the network's performance. To\nevaluate our method, we created a challenging benchmark dataset called TUT and\nconducted experiments on this dataset and five other public datasets. The\nexperimental results indicate that our method offers significant advantages\nover existing methods, especially in handling background noise interference and\ndetailed crack segmentation. The F1 and mIoU scores on the TUT dataset are\n0.8382 and 0.8473, respectively, achieving state-of-the-art (SOTA) performance\nwhile requiring the least computational resources. The code and dataset is\navailable at https://github.com/Karl1109/CrackSCF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting cracks with pixel-level precision for key structures is a\nsignificant challenge, as existing methods struggle to effectively integrate\nlocal textures and pixel dependencies of cracks. Furthermore, these methods\noften possess numerous parameters and substantial computational requirements,\ncomplicating deployment on edge control devices. In this paper, we propose a\nstaircase cascaded fusion crack segmentation network (CrackSCF) that generates\nhigh-quality crack segmentation maps using minimal computational resources. We\nconstructed a staircase cascaded fusion module that effectively captures local\npatterns of cracks and long-range dependencies of pixels, and it can suppress\nbackground noise well. To reduce the computational resources required by the\nmodel, we introduced a lightweight convolution block, which replaces all\nconvolution operations in the network, significantly reducing the required\ncomputation and parameters without affecting the network's performance. To\nevaluate our method, we created a challenging benchmark dataset called TUT and\nconducted experiments on this dataset and five other public datasets. The\nexperimental results indicate that our method offers significant advantages\nover existing methods, especially in handling background noise interference and\ndetailed crack segmentation. The F1 and mIoU scores on the TUT dataset are\n0.8382 and 0.8473, respectively, achieving state-of-the-art (SOTA) performance\nwhile requiring the least computational resources. The code and dataset is\navailable at https://github.com/Karl1109/CrackSCF."
                },
                "authors": [
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Chen Jia"
                    },
                    {
                        "name": "Fan Shi"
                    },
                    {
                        "name": "Xu Cheng"
                    },
                    {
                        "name": "Mianzhao Wang"
                    },
                    {
                        "name": "Shengyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shengyong Chen"
                },
                "author": "Shengyong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06638v1",
                "updated": "2024-10-09T07:43:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    43,
                    38,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:43:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    43,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing"
                },
                "summary": "Large Language Models (LLMs) have exhibited strong mathematical reasoning and\ncomputational prowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle errors, such\nas miscalculations or incorrect substitutions, limit the models' full\nmathematical potential. Existing studies to improve mathematical ability\ntypically involve distilling reasoning skills from stronger LLMs or applying\npreference learning to step-wise response pairs. Although these methods\nleverage samples of varying granularity to mitigate reasoning errors, they\noverlook the frequently occurring subtle errors. A major reason is that sampled\npreference pairs involve differences unrelated to the errors, which may\ndistract the model from focusing on subtle errors. In this work, we propose a\nnovel preference learning framework called eRror-Injected Self-Editing (RISE),\nwhich injects predefined subtle errors into partial tokens of correct solutions\nto construct hard pairs for error mitigation. In detail, RISE uses the model\nitself to edit a small number of tokens in the solution, injecting designed\nsubtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective to focus on predefined errors and their tokens, without\nrequiring fine-grained sampling or preference annotation. Extensive experiments\nvalidate the effectiveness of RISE, with preference learning on\nQwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on\nMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited strong mathematical reasoning and\ncomputational prowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle errors, such\nas miscalculations or incorrect substitutions, limit the models' full\nmathematical potential. Existing studies to improve mathematical ability\ntypically involve distilling reasoning skills from stronger LLMs or applying\npreference learning to step-wise response pairs. Although these methods\nleverage samples of varying granularity to mitigate reasoning errors, they\noverlook the frequently occurring subtle errors. A major reason is that sampled\npreference pairs involve differences unrelated to the errors, which may\ndistract the model from focusing on subtle errors. In this work, we propose a\nnovel preference learning framework called eRror-Injected Self-Editing (RISE),\nwhich injects predefined subtle errors into partial tokens of correct solutions\nto construct hard pairs for error mitigation. In detail, RISE uses the model\nitself to edit a small number of tokens in the solution, injecting designed\nsubtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective to focus on predefined errors and their tokens, without\nrequiring fine-grained sampling or preference annotation. Extensive experiments\nvalidate the effectiveness of RISE, with preference learning on\nQwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on\nMATH."
                },
                "authors": [
                    {
                        "name": "Kaishuai Xu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Wenjun Hou"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16563v2",
                "updated": "2024-10-09T07:39:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    39,
                    29,
                    2,
                    283,
                    0
                ],
                "published": "2024-04-25T12:24:37Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    12,
                    24,
                    37,
                    3,
                    116,
                    0
                ],
                "title": "Evaluating Large Language Models on Time Series Feature Understanding: A\n  Comprehensive Taxonomy and Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models on Time Series Feature Understanding: A\n  Comprehensive Taxonomy and Benchmark"
                },
                "summary": "Large Language Models (LLMs) offer the potential for automatic time series\nanalysis and reporting, which is a critical task across many domains, spanning\nhealthcare, finance, climate, energy, and many more. In this paper, we propose\na framework for rigorously evaluating the capabilities of LLMs on time series\nunderstanding, encompassing both univariate and multivariate forms. We\nintroduce a comprehensive taxonomy of time series features, a critical\nframework that delineates various characteristics inherent in time series data.\nLeveraging this taxonomy, we have systematically designed and synthesized a\ndiverse dataset of time series, embodying the different outlined features, each\naccompanied by textual descriptions. This dataset acts as a solid foundation\nfor assessing the proficiency of LLMs in comprehending time series. Our\nexperiments shed light on the strengths and limitations of state-of-the-art\nLLMs in time series understanding, revealing which features these models\nreadily comprehend effectively and where they falter. In addition, we uncover\nthe sensitivity of LLMs to factors including the formatting of the data, the\nposition of points queried within a series and the overall time series length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer the potential for automatic time series\nanalysis and reporting, which is a critical task across many domains, spanning\nhealthcare, finance, climate, energy, and many more. In this paper, we propose\na framework for rigorously evaluating the capabilities of LLMs on time series\nunderstanding, encompassing both univariate and multivariate forms. We\nintroduce a comprehensive taxonomy of time series features, a critical\nframework that delineates various characteristics inherent in time series data.\nLeveraging this taxonomy, we have systematically designed and synthesized a\ndiverse dataset of time series, embodying the different outlined features, each\naccompanied by textual descriptions. This dataset acts as a solid foundation\nfor assessing the proficiency of LLMs in comprehending time series. Our\nexperiments shed light on the strengths and limitations of state-of-the-art\nLLMs in time series understanding, revealing which features these models\nreadily comprehend effectively and where they falter. In addition, we uncover\nthe sensitivity of LLMs to factors including the formatting of the data, the\nposition of points queried within a series and the overall time series length."
                },
                "authors": [
                    {
                        "name": "Elizabeth Fons"
                    },
                    {
                        "name": "Rachneet Kaur"
                    },
                    {
                        "name": "Soham Palande"
                    },
                    {
                        "name": "Zhen Zeng"
                    },
                    {
                        "name": "Tucker Balch"
                    },
                    {
                        "name": "Manuela Veloso"
                    },
                    {
                        "name": "Svitlana Vyetrenko"
                    }
                ],
                "author_detail": {
                    "name": "Svitlana Vyetrenko"
                },
                "author": "Svitlana Vyetrenko",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06634v1",
                "updated": "2024-10-09T07:35:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    35,
                    46,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:35:46Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    35,
                    46,
                    2,
                    283,
                    0
                ],
                "title": "Tree of Problems: Improving structured problem solving with\n  compositionality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Problems: Improving structured problem solving with\n  compositionality"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nmultiple tasks through in-context learning. For complex reasoning tasks that\nrequire step-by-step thinking, Chain-of-Thought (CoT) prompting has given\nimpressive results, especially when combined with self-consistency.\nNonetheless, some tasks remain particularly difficult for LLMs to solve. Tree\nof Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing\nthe complex problem into paths of subproblems. In this paper, we propose Tree\nof Problems (ToP), a simpler version of ToT, which we hypothesise can work\nbetter for complex tasks that can be divided into identical subtasks. Our\nempirical results show that our approach outperforms ToT and GoT, and in\naddition performs better than CoT on complex reasoning tasks. All code for this\npaper is publicly available here:\nhttps://github.com/ArmelRandy/tree-of-problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\nmultiple tasks through in-context learning. For complex reasoning tasks that\nrequire step-by-step thinking, Chain-of-Thought (CoT) prompting has given\nimpressive results, especially when combined with self-consistency.\nNonetheless, some tasks remain particularly difficult for LLMs to solve. Tree\nof Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing\nthe complex problem into paths of subproblems. In this paper, we propose Tree\nof Problems (ToP), a simpler version of ToT, which we hypothesise can work\nbetter for complex tasks that can be divided into identical subtasks. Our\nempirical results show that our approach outperforms ToT and GoT, and in\naddition performs better than CoT on complex reasoning tasks. All code for this\npaper is publicly available here:\nhttps://github.com/ArmelRandy/tree-of-problems."
                },
                "authors": [
                    {
                        "name": "Armel Zebaze"
                    },
                    {
                        "name": "Benoît Sagot"
                    },
                    {
                        "name": "Rachel Bawden"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Bawden"
                },
                "author": "Rachel Bawden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02599v2",
                "updated": "2024-10-09T07:31:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    31,
                    18,
                    2,
                    283,
                    0
                ],
                "published": "2024-08-05T16:21:17Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    16,
                    21,
                    17,
                    0,
                    218,
                    0
                ],
                "title": "Progressively Label Enhancement for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressively Label Enhancement for Large Language Model Alignment"
                },
                "summary": "Large Language Models (LLM) alignment aims to prevent models from producing\ncontent that misaligns with human expectations, which can lead to ethical and\nlegal concerns. In the last few years, Reinforcement Learning from Human\nFeedback (RLHF) has been the most prominent method for achieving alignment. Due\nto challenges in stability and scalability with RLHF stages, which arise from\nthe complex interactions between multiple models, researchers are exploring\nalternative methods to achieve effects comparable to those of RLHF. However,\nthese methods often rely on large high-quality datasets. Despite some methods\nconsidering the generation of additional data to expand datasets, they often\ntreat model training and data generation as separate and static processes,\noverlooking the fact that these processes are highly interdependent, leading to\ninefficient utilization of the generated data. To deal with this problem, we\npropose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a\nframework that dynamically adjusts the model's training process based on the\nevolving quality of the generated data. Specifically, we prompt the model to\ngenerate responses for both the original query and the query guided by a set of\ncarefully designed principles, and then utilize a dynamic threshold to\ndetermine the appropriate training approach for both responses based on their\ncorresponding reward scores. Experimental results demonstrate the effectiveness\nof PLE compared to existing LLM alignment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) alignment aims to prevent models from producing\ncontent that misaligns with human expectations, which can lead to ethical and\nlegal concerns. In the last few years, Reinforcement Learning from Human\nFeedback (RLHF) has been the most prominent method for achieving alignment. Due\nto challenges in stability and scalability with RLHF stages, which arise from\nthe complex interactions between multiple models, researchers are exploring\nalternative methods to achieve effects comparable to those of RLHF. However,\nthese methods often rely on large high-quality datasets. Despite some methods\nconsidering the generation of additional data to expand datasets, they often\ntreat model training and data generation as separate and static processes,\noverlooking the fact that these processes are highly interdependent, leading to\ninefficient utilization of the generated data. To deal with this problem, we\npropose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a\nframework that dynamically adjusts the model's training process based on the\nevolving quality of the generated data. Specifically, we prompt the model to\ngenerate responses for both the original query and the query guided by a set of\ncarefully designed principles, and then utilize a dynamic threshold to\ndetermine the appropriate training approach for both responses based on their\ncorresponding reward scores. Experimental results demonstrate the effectiveness\nof PLE compared to existing LLM alignment methods."
                },
                "authors": [
                    {
                        "name": "Biao Liu"
                    },
                    {
                        "name": "Ning Xu"
                    },
                    {
                        "name": "Xin Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Geng"
                },
                "author": "Xin Geng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]