[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v1",
                "updated": "2025-06-25T07:26:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v1",
                "updated": "2025-06-24T14:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From memories to maps: Mechanisms of in context reinforcement learning\n  in transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From memories to maps: Mechanisms of in context reinforcement learning\n  in transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v2",
                "updated": "2025-06-20T16:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    59,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Suraj Kothawade"
                    },
                    {
                        "name": "Pacal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pacal Poupart"
                },
                "author": "Pacal Poupart",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v3",
                "updated": "2025-06-20T12:59:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    59,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thévenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thévenet"
                },
                "author": "M. Thévenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v1",
                "updated": "2025-06-18T17:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rúben Adão"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "João Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v1",
                "updated": "2025-06-15T07:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v1",
                "updated": "2025-06-14T13:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v2",
                "updated": "2025-06-12T00:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    0,
                    25,
                    14,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v3",
                "updated": "2025-06-11T22:50:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    22,
                    50,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v2",
                "updated": "2025-06-11T21:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    21,
                    59,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10100v1",
                "updated": "2025-06-11T18:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T18:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark."
                },
                "authors": [
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Luo Zhongwei"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09282v1",
                "updated": "2025-06-10T22:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T22:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs"
                },
                "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements."
                },
                "authors": [
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IC3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v2",
                "updated": "2025-06-10T22:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    1,
                    14,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v1",
                "updated": "2025-06-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08842v1",
                "updated": "2025-06-10T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T14:29:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."
                },
                "authors": [
                    {
                        "name": "Kainan Wang"
                    },
                    {
                        "name": "Chengyi Yang"
                    },
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Yee Sin Ang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v4",
                "updated": "2025-06-10T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    50,
                    34,
                    1,
                    161,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08529v1",
                "updated": "2025-06-10T07:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T07:49:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s"
                },
                "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Project page: https://kopperx.github.io/projects/liftvsr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v1",
                "updated": "2025-06-10T02:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v1",
                "updated": "2025-06-09T19:13:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08009v1",
                "updated": "2025-06-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion"
                },
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project website: http://self-forcing.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v2",
                "updated": "2025-06-09T15:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    31,
                    53,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.20666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20666v1",
                "updated": "2025-06-25T17:58:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    58,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:58:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    58,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "Inside you are many wolves: Using cognitive models to interpret value\n  trade-offs in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside you are many wolves: Using cognitive models to interpret value\n  trade-offs in LLMs"
                },
                "summary": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training."
                },
                "authors": [
                    {
                        "name": "Sonia K. Murthy"
                    },
                    {
                        "name": "Rosie Zhao"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Peng Qian"
                    },
                    {
                        "name": "Tomer Ullman"
                    }
                ],
                "author_detail": {
                    "name": "Tomer Ullman"
                },
                "author": "Tomer Ullman",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20664v1",
                "updated": "2025-06-25T17:55:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    55,
                    27,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:55:27Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    55,
                    27,
                    2,
                    176,
                    0
                ],
                "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind"
                },
                "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents."
                },
                "authors": [
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Timon Willi"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "arxiv_comment": "41 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03023v3",
                "updated": "2025-06-25T17:54:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    54,
                    32,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-03T15:59:29Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    59,
                    29,
                    1,
                    154,
                    0
                ],
                "title": "TDCOSMO 2025: Cosmological constraints from strong lensing time delays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDCOSMO 2025: Cosmological constraints from strong lensing time delays"
                },
                "summary": "We present cosmological constraints from 8 strongly lensed quasars\n(hereafter, the TDCOSMO-2025 sample). Building on previous work, our analysis\nincorporated new deflector stellar velocity dispersions measured from spectra\nobtained with the James Webb Space Telescope (JWST), the Keck Telescopes, and\nthe Very Large Telescope (VLT), utilizing improved methods. We used integrated\nJWST stellar kinematics for 5 lenses, VLT-MUSE for 2, and resolved kinematics\nfrom Keck and JWST for RX J1131-1231. We also considered two samples of\nnon-time-delay lenses: 11 from the Sloan Lens ACS (SLACS) sample with Keck-KCWI\nresolved kinematics; and 4 from the Strong Lenses in the Legacy Survey (SL2S)\nsample. We improved our analysis of line-of-sight effects, the surface\nbrightness profile of the lens galaxies, and orbital anisotropy, and corrected\nfor projection effects in the dynamics. Our uncertainties are maximally\nconservative by accounting for the mass-sheet degeneracy in the deflectors'\nmass density profiles. The analysis was blinded to prevent experimenter bias.\nOur primary result is based on the TDCOSMO-2025 sample, in combination with\n$\\Omega_{\\rm m}$ constraints from the Pantheon+ Type Ia supernovae (SN)\ndataset. In the flat $\\Lambda$ Cold Dark Matter (CDM), we find\n$H_0=71.6^{+3.9}_{-3.3}$ km s$^{-1}$ Mpc$^{-1}$. The SLACS and SL2S samples are\nin excellent agreement with the TDCOSMO-2025 sample, improving the precision on\n$H_0$ in flat $\\Lambda$CDM to 4.6%. Using the Dark Energy Survey SN Year-5\ndataset (DES-SN5YR) or DESI-DR2 baryonic acoustic oscillations (BAO)\nlikelihoods instead of Pantheon+ yields very similar results. We also present\nconstraints in the open $\\Lambda$CDM, $w$CDM, $w_0w_a$CDM, and $w_{\\phi}$CDM\ncosmologies. The TDCOSMO $H_0$ inference is robust and consistent across all\npresented cosmological models, and our cosmological constraints in them agree\nwith those from the BAO and SN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present cosmological constraints from 8 strongly lensed quasars\n(hereafter, the TDCOSMO-2025 sample). Building on previous work, our analysis\nincorporated new deflector stellar velocity dispersions measured from spectra\nobtained with the James Webb Space Telescope (JWST), the Keck Telescopes, and\nthe Very Large Telescope (VLT), utilizing improved methods. We used integrated\nJWST stellar kinematics for 5 lenses, VLT-MUSE for 2, and resolved kinematics\nfrom Keck and JWST for RX J1131-1231. We also considered two samples of\nnon-time-delay lenses: 11 from the Sloan Lens ACS (SLACS) sample with Keck-KCWI\nresolved kinematics; and 4 from the Strong Lenses in the Legacy Survey (SL2S)\nsample. We improved our analysis of line-of-sight effects, the surface\nbrightness profile of the lens galaxies, and orbital anisotropy, and corrected\nfor projection effects in the dynamics. Our uncertainties are maximally\nconservative by accounting for the mass-sheet degeneracy in the deflectors'\nmass density profiles. The analysis was blinded to prevent experimenter bias.\nOur primary result is based on the TDCOSMO-2025 sample, in combination with\n$\\Omega_{\\rm m}$ constraints from the Pantheon+ Type Ia supernovae (SN)\ndataset. In the flat $\\Lambda$ Cold Dark Matter (CDM), we find\n$H_0=71.6^{+3.9}_{-3.3}$ km s$^{-1}$ Mpc$^{-1}$. The SLACS and SL2S samples are\nin excellent agreement with the TDCOSMO-2025 sample, improving the precision on\n$H_0$ in flat $\\Lambda$CDM to 4.6%. Using the Dark Energy Survey SN Year-5\ndataset (DES-SN5YR) or DESI-DR2 baryonic acoustic oscillations (BAO)\nlikelihoods instead of Pantheon+ yields very similar results. We also present\nconstraints in the open $\\Lambda$CDM, $w$CDM, $w_0w_a$CDM, and $w_{\\phi}$CDM\ncosmologies. The TDCOSMO $H_0$ inference is robust and consistent across all\npresented cosmological models, and our cosmological constraints in them agree\nwith those from the BAO and SN."
                },
                "authors": [
                    {
                        "name": "TDCOSMO Collaboration"
                    },
                    {
                        "name": "Simon Birrer"
                    },
                    {
                        "name": "Elizabeth J. Buckley-Geer"
                    },
                    {
                        "name": "Michele Cappellari"
                    },
                    {
                        "name": "Frédéric Courbin"
                    },
                    {
                        "name": "Frédéric Dux"
                    },
                    {
                        "name": "Christopher D. Fassnacht"
                    },
                    {
                        "name": "Joshua A. Frieman"
                    },
                    {
                        "name": "Aymeric Galan"
                    },
                    {
                        "name": "Daniel Gilman"
                    },
                    {
                        "name": "Xiang-Yu Huang"
                    },
                    {
                        "name": "Shawn Knabel"
                    },
                    {
                        "name": "Danial Langeroodi"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Martin Millon"
                    },
                    {
                        "name": "Takahiro Morishita"
                    },
                    {
                        "name": "Veronica Motta"
                    },
                    {
                        "name": "Pritom Mozumdar"
                    },
                    {
                        "name": "Eric Paic"
                    },
                    {
                        "name": "Anowar J. Shajib"
                    },
                    {
                        "name": "William Sheu"
                    },
                    {
                        "name": "Dominique Sluse"
                    },
                    {
                        "name": "Alessandro Sonnenfeld"
                    },
                    {
                        "name": "Chiara Spiniello"
                    },
                    {
                        "name": "Massimo Stiavelli"
                    },
                    {
                        "name": "Sherry H. Suyu"
                    },
                    {
                        "name": "Chin Yi Tan"
                    },
                    {
                        "name": "Tommaso Treu"
                    },
                    {
                        "name": "Lyne Van de Vyvere"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Patrick Wells"
                    },
                    {
                        "name": "Devon M. Williams"
                    },
                    {
                        "name": "Kenneth C. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth C. Wong"
                },
                "author": "Kenneth C. Wong",
                "arxiv_comment": "34 pages, 17 figures, 8 tables, (this version: minor changes in\n  numerical values due to post-blinding discovery of coding error; conclusions\n  and text otherwise unaffected). The CosmoVerse Seminar on this paper given on\n  June 12, 2025 can be watched at https://www.youtube.com/watch?v=sr0Ft6O4VBg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20657v1",
                "updated": "2025-06-25T17:52:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    52,
                    26,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:52:26Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    52,
                    26,
                    2,
                    176,
                    0
                ],
                "title": "SuperSONIC: Cloud-Native Infrastructure for ML Inferencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperSONIC: Cloud-Native Infrastructure for ML Inferencing"
                },
                "summary": "The increasing computational demand from growing data rates and complex\nmachine learning (ML) algorithms in large-scale scientific experiments has\ndriven the adoption of the Services for Optimized Network Inference on\nCoprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it\nto local or remote coprocessors to optimize resource utilization. Leveraging\nits portability to different types of coprocessors, SONIC enhances data\nprocessing and model deployment efficiency for cutting-edge research in high\nenergy physics (HEP) and multi-messenger astrophysics (MMA). We developed the\nSuperSONIC project, a scalable server infrastructure for SONIC, enabling the\ndeployment of computationally intensive tasks to Kubernetes clusters equipped\nwith graphics processing units (GPUs). Using NVIDIA Triton Inference Server,\nSuperSONIC decouples client workflows from server infrastructure, standardizing\ncommunication, optimizing throughput, load balancing, and monitoring.\nSuperSONIC has been successfully deployed for the CMS and ATLAS experiments at\nthe CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory\n(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)\nand tested on Kubernetes clusters at Purdue University, the National Research\nPlatform (NRP), and the University of Chicago. SuperSONIC addresses the\nchallenges of the Cloud-native era by providing a reusable, configurable\nframework that enhances the efficiency of accelerator-based inference\ndeployment across diverse scientific domains and industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing computational demand from growing data rates and complex\nmachine learning (ML) algorithms in large-scale scientific experiments has\ndriven the adoption of the Services for Optimized Network Inference on\nCoprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it\nto local or remote coprocessors to optimize resource utilization. Leveraging\nits portability to different types of coprocessors, SONIC enhances data\nprocessing and model deployment efficiency for cutting-edge research in high\nenergy physics (HEP) and multi-messenger astrophysics (MMA). We developed the\nSuperSONIC project, a scalable server infrastructure for SONIC, enabling the\ndeployment of computationally intensive tasks to Kubernetes clusters equipped\nwith graphics processing units (GPUs). Using NVIDIA Triton Inference Server,\nSuperSONIC decouples client workflows from server infrastructure, standardizing\ncommunication, optimizing throughput, load balancing, and monitoring.\nSuperSONIC has been successfully deployed for the CMS and ATLAS experiments at\nthe CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory\n(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)\nand tested on Kubernetes clusters at Purdue University, the National Research\nPlatform (NRP), and the University of Chicago. SuperSONIC addresses the\nchallenges of the Cloud-native era by providing a reusable, configurable\nframework that enhances the efficiency of accelerator-based inference\ndeployment across diverse scientific domains and industries."
                },
                "authors": [
                    {
                        "name": "Dmitry Kondratyev"
                    },
                    {
                        "name": "Benedikt Riedel"
                    },
                    {
                        "name": "Yuan-Tang Chou"
                    },
                    {
                        "name": "Miles Cochran-Branson"
                    },
                    {
                        "name": "Noah Paladino"
                    },
                    {
                        "name": "David Schultz"
                    },
                    {
                        "name": "Mia Liu"
                    },
                    {
                        "name": "Javier Duarte"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Shih-Chieh Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Chieh Hsu"
                },
                "author": "Shih-Chieh Hsu",
                "arxiv_doi": "10.1145/3708035.3736049",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708035.3736049",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.20657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Submission to PEARC25 Conference",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20642v1",
                "updated": "2025-06-25T17:37:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    37,
                    59,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:37:59Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    37,
                    59,
                    2,
                    176,
                    0
                ],
                "title": "Memento: Note-Taking for Your Future Self",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memento: Note-Taking for Your Future Self"
                },
                "summary": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when\nreasoning must be tightly coupled with retrieval, as in multi-hop question\nanswering. To overcome these limitations, we introduce a prompting strategy\nthat first decomposes a complex question into smaller steps, then dynamically\nconstructs a database of facts using LLMs, and finally pieces these facts\ntogether to solve the question. We show how this three-stage strategy, which we\ncall Memento, can boost the performance of existing prompting strategies across\ndiverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the\nperformance of chain-of-thought (CoT) when all information is provided in\ncontext. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento\nimproves over vanilla CoT-RAG by more than 20 F1 percentage points and over the\nmulti-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the\nchallenging MuSiQue dataset, Memento improves ReAct by more than 3 F1\npercentage points, demonstrating its utility in agentic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when\nreasoning must be tightly coupled with retrieval, as in multi-hop question\nanswering. To overcome these limitations, we introduce a prompting strategy\nthat first decomposes a complex question into smaller steps, then dynamically\nconstructs a database of facts using LLMs, and finally pieces these facts\ntogether to solve the question. We show how this three-stage strategy, which we\ncall Memento, can boost the performance of existing prompting strategies across\ndiverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the\nperformance of chain-of-thought (CoT) when all information is provided in\ncontext. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento\nimproves over vanilla CoT-RAG by more than 20 F1 percentage points and over the\nmulti-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the\nchallenging MuSiQue dataset, Memento improves ReAct by more than 3 F1\npercentage points, demonstrating its utility in agentic settings."
                },
                "authors": [
                    {
                        "name": "Chao Wan"
                    },
                    {
                        "name": "Albert Gong"
                    },
                    {
                        "name": "Mihir Mishra"
                    },
                    {
                        "name": "Carl-Leander Henneking"
                    },
                    {
                        "name": "Claas Beger"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    }
                ],
                "author_detail": {
                    "name": "Kilian Q. Weinberger"
                },
                "author": "Kilian Q. Weinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20639v1",
                "updated": "2025-06-25T17:35:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    35,
                    47,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:35:47Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    35,
                    47,
                    2,
                    176,
                    0
                ],
                "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation"
                },
                "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder."
                },
                "authors": [
                    {
                        "name": "Shansan Gong"
                    },
                    {
                        "name": "Ruixiang Zhang"
                    },
                    {
                        "name": "Huangjie Zheng"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20608v1",
                "updated": "2025-06-25T17:00:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    5,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:00:05Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    5,
                    2,
                    176,
                    0
                ],
                "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base"
                },
                "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery."
                },
                "authors": [
                    {
                        "name": "Barry Smith"
                    },
                    {
                        "name": "Junchao Zhang"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Lois Curfman McInnes"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Archit Vasan"
                    },
                    {
                        "name": "Satish Balay"
                    },
                    {
                        "name": "Toby Isaac"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20606v1",
                "updated": "2025-06-25T16:51:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    51,
                    51,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:51:51Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    51,
                    51,
                    2,
                    176,
                    0
                ],
                "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior\n  Toward Beneficence or Harm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior\n  Toward Beneficence or Harm"
                },
                "summary": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing."
                },
                "authors": [
                    {
                        "name": "Baixiang Huang"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Huan Liu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "arxiv_comment": "Main paper: 9 pages; total: 18 pages (including appendix). Code,\n  data, results, and additional resources are available at:\n  https://model-editing.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09978v2",
                "updated": "2025-06-25T16:49:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    49,
                    52,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-13T02:27:26Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    2,
                    27,
                    26,
                    3,
                    72,
                    0
                ],
                "title": "A Conditional Point Cloud Diffusion Model for Deformable Liver Motion\n  Tracking Via a Single Arbitrarily-Angled X-ray Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Conditional Point Cloud Diffusion Model for Deformable Liver Motion\n  Tracking Via a Single Arbitrarily-Angled X-ray Projection"
                },
                "summary": "Deformable liver motion tracking using a single X-ray projection enables\nreal-time motion monitoring and treatment intervention. We introduce a\nconditional point cloud diffusion model-based framework for accurate and robust\nliver motion tracking from arbitrarily angled single X-ray projections\n(PCD-Liver), which estimates volumetric liver motion by solving deformable\nvector fields (DVFs) of a prior liver surface point cloud based on a single\nX-ray image. The model is patient-specific and consists of two main components:\na rigid alignment model to estimate the liver's overall shifts and a\nconditional point cloud diffusion model that further corrects for liver surface\ndeformations. Conditioned on motion-encoded features extracted from a single\nX-ray projection via a geometry-informed feature pooling layer, the diffusion\nmodel iteratively solves detailed liver surface DVFs in a projection\nangle-agnostic manner. The liver surface motion estimated by PCD-Liver serves\nas a boundary condition for a U-Net-based biomechanical model to infer internal\nliver motion and localize liver tumors. A dataset of ten liver cancer patients\nwas used for evaluation. The accuracy of liver point cloud motion estimation\nwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorff\ndistance (HD95), while liver tumor localization error was quantified using\ncenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COME\nof the prior liver or tumor before motion estimation were 8.82(3.58) mm,\n10.84(4.55) mm, and 9.72(4.34) mm, respectively. After PCD-Liver motion\nestimation, the corresponding values improved to 3.63(1.88) mm, 4.29(1.75) mm,\nand 3.46(2.15) mm. Under highly noisy conditions, PCD-Liver maintained stable\nperformance. This study presents an accurate and robust framework for\ndeformable liver motion estimation and tumor localization in image-guided\nradiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deformable liver motion tracking using a single X-ray projection enables\nreal-time motion monitoring and treatment intervention. We introduce a\nconditional point cloud diffusion model-based framework for accurate and robust\nliver motion tracking from arbitrarily angled single X-ray projections\n(PCD-Liver), which estimates volumetric liver motion by solving deformable\nvector fields (DVFs) of a prior liver surface point cloud based on a single\nX-ray image. The model is patient-specific and consists of two main components:\na rigid alignment model to estimate the liver's overall shifts and a\nconditional point cloud diffusion model that further corrects for liver surface\ndeformations. Conditioned on motion-encoded features extracted from a single\nX-ray projection via a geometry-informed feature pooling layer, the diffusion\nmodel iteratively solves detailed liver surface DVFs in a projection\nangle-agnostic manner. The liver surface motion estimated by PCD-Liver serves\nas a boundary condition for a U-Net-based biomechanical model to infer internal\nliver motion and localize liver tumors. A dataset of ten liver cancer patients\nwas used for evaluation. The accuracy of liver point cloud motion estimation\nwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorff\ndistance (HD95), while liver tumor localization error was quantified using\ncenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COME\nof the prior liver or tumor before motion estimation were 8.82(3.58) mm,\n10.84(4.55) mm, and 9.72(4.34) mm, respectively. After PCD-Liver motion\nestimation, the corresponding values improved to 3.63(1.88) mm, 4.29(1.75) mm,\nand 3.46(2.15) mm. Under highly noisy conditions, PCD-Liver maintained stable\nperformance. This study presents an accurate and robust framework for\ndeformable liver motion estimation and tumor localization in image-guided\nradiotherapy."
                },
                "authors": [
                    {
                        "name": "Jiacheng Xie"
                    },
                    {
                        "name": "Hua-Chieh Shao"
                    },
                    {
                        "name": "Yunxiang Li"
                    },
                    {
                        "name": "Shunyu Yan"
                    },
                    {
                        "name": "Chenyang Shen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "You Zhang"
                    }
                ],
                "author_detail": {
                    "name": "You Zhang"
                },
                "author": "You Zhang",
                "arxiv_doi": "10.1088/1361-6560/addf0e",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1361-6560/addf0e",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "26 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06300v3",
                "updated": "2025-06-25T16:48:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    48,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T09:10:31Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    9,
                    10,
                    31,
                    0,
                    139,
                    0
                ],
                "title": "LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network\n  for Boundary-focused Engineering Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network\n  for Boundary-focused Engineering Optimization"
                },
                "summary": "Physics-informed neural networks (PINNs) have emerged as a powerful meshless\ntool for topology optimization, capable of simultaneously determining optimal\ntopologies and physical solutions. However, conventional PINNs rely on\ndensity-based topology descriptions, which necessitate manual interpolation and\nlimit their applicability to complex geometries. To address this, we propose\nLagrangian topology-conscious PINNs (LT-PINNs), a novel framework for\nboundary-focused engineering optimization. By parameterizing the control\nvariables of topology boundary curves as learnable parameters, LT-PINNs\neliminate the need for manual interpolation and enable precise boundary\ndetermination. We further introduce specialized boundary condition loss\nfunction and topology loss function to ensure sharp and accurate boundary\nrepresentations, even for intricate topologies. The accuracy and robustness of\nLT-PINNs are validated via two types of partial differential equations (PDEs),\nincluding elastic equation with Dirichlet boundary conditions and Laplace's\nequation with Neumann boundary conditions. Furthermore, we demonstrate\neffectiveness of LT-PINNs on more complex time-dependent and time-independent\nflow problems without relying on measurement data, and showcase their\nengineering application potential in flow velocity rearrangement, transforming\na uniform upstream velocity into a sine-shaped downstream profile. The results\ndemonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors\ncompared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)\nLT-PINNs can handle arbitrary boundary conditions, making them suitable for a\nwide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries\nwithout manual interpolation, especially for complex topologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed neural networks (PINNs) have emerged as a powerful meshless\ntool for topology optimization, capable of simultaneously determining optimal\ntopologies and physical solutions. However, conventional PINNs rely on\ndensity-based topology descriptions, which necessitate manual interpolation and\nlimit their applicability to complex geometries. To address this, we propose\nLagrangian topology-conscious PINNs (LT-PINNs), a novel framework for\nboundary-focused engineering optimization. By parameterizing the control\nvariables of topology boundary curves as learnable parameters, LT-PINNs\neliminate the need for manual interpolation and enable precise boundary\ndetermination. We further introduce specialized boundary condition loss\nfunction and topology loss function to ensure sharp and accurate boundary\nrepresentations, even for intricate topologies. The accuracy and robustness of\nLT-PINNs are validated via two types of partial differential equations (PDEs),\nincluding elastic equation with Dirichlet boundary conditions and Laplace's\nequation with Neumann boundary conditions. Furthermore, we demonstrate\neffectiveness of LT-PINNs on more complex time-dependent and time-independent\nflow problems without relying on measurement data, and showcase their\nengineering application potential in flow velocity rearrangement, transforming\na uniform upstream velocity into a sine-shaped downstream profile. The results\ndemonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors\ncompared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)\nLT-PINNs can handle arbitrary boundary conditions, making them suitable for a\nwide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries\nwithout manual interpolation, especially for complex topologies."
                },
                "authors": [
                    {
                        "name": "Yuanye Zhou"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Hui Tang"
                    },
                    {
                        "name": "Xiaofan Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Li"
                },
                "author": "Xiaofan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11707v2",
                "updated": "2025-06-25T16:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    48,
                    16,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-17T11:46:46Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    46,
                    46,
                    0,
                    48,
                    0
                ],
                "title": "Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating\n  Large Language Models"
                },
                "summary": "This study utilizes the game Codenames as a benchmarking tool to evaluate\nlarge language models (LLMs) with respect to specific linguistic and cognitive\nskills. LLMs play each side of the game, where one side generates a clue word\ncovering several target words and the other guesses those target words. We\ndesigned various experiments by controlling the choice of words (abstract vs.\nconcrete words, ambiguous vs. monosemic) or the opponent (programmed to be\nfaster or slower in revealing words). Recent commercial and open-weight models\nwere compared side-by-side to find out factors affecting their performance. The\nevaluation reveals details about their strategies, challenging cases, and\nlimitations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study utilizes the game Codenames as a benchmarking tool to evaluate\nlarge language models (LLMs) with respect to specific linguistic and cognitive\nskills. LLMs play each side of the game, where one side generates a clue word\ncovering several target words and the other guesses those target words. We\ndesigned various experiments by controlling the choice of words (abstract vs.\nconcrete words, ambiguous vs. monosemic) or the opponent (programmed to be\nfaster or slower in revealing words). Recent commercial and open-weight models\nwere compared side-by-side to find out factors affecting their performance. The\nevaluation reveals details about their strategies, challenging cases, and\nlimitations of LLMs."
                },
                "authors": [
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "Lara Pfennigschmidt"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "Accepted at GemBench workshop co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20601v1",
                "updated": "2025-06-25T16:40:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    40,
                    17,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:40:17Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    40,
                    17,
                    2,
                    176,
                    0
                ],
                "title": "Video Perception Models for 3D Scene Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Perception Models for 3D Scene Synthesis"
                },
                "summary": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released."
                },
                "authors": [
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Zuria Bauer"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Gao Huang"
                    },
                    {
                        "name": "Francis Engelmann"
                    }
                ],
                "author_detail": {
                    "name": "Francis Engelmann"
                },
                "author": "Francis Engelmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20598v1",
                "updated": "2025-06-25T16:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    37,
                    46,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    37,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of\n  Multi-Agent AI for Addressing Sustainable Protein Production Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of\n  Multi-Agent AI for Addressing Sustainable Protein Production Challenges"
                },
                "summary": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities"
                },
                "authors": [
                    {
                        "name": "Alexander D. Kalian"
                    },
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Stefan P. Johannesson"
                    },
                    {
                        "name": "Lennart Otte"
                    },
                    {
                        "name": "Christer Hogstrand"
                    },
                    {
                        "name": "Miao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Miao Guo"
                },
                "author": "Miao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20596v1",
                "updated": "2025-06-25T16:36:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    36,
                    2,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:36:02Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    36,
                    2,
                    2,
                    176,
                    0
                ],
                "title": "Inference for Error-Prone Count Data: Estimation under a Binomial\n  Convolution Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Error-Prone Count Data: Estimation under a Binomial\n  Convolution Framework"
                },
                "summary": "Measurement error in count data is common but underexplored in the\nliterature, particularly in contexts where observed scores are bounded and\narise from discrete scoring processes. Motivated by applications in oral\nreading fluency assessment, we propose a binomial convolution framework that\nextends binary misclassification models to settings where only the aggregate\nnumber of correct responses is observed, and errors may involve both\novercounting and undercounting the number of events. The model accommodates\ndistinct true positive and true negative accuracy rates and preserves the\nbounded nature of the data.\n  Assuming the availability of both contaminated and error-free scores on a\nsubset of items, we develop and compare three estimation strategies: maximum\nlikelihood estimation (MLE), linear regression, and generalized method of\nmoments (GMM). Extensive simulations show that MLE is most accurate when the\nmodel is correctly specified but is computationally intensive and less robust\nto misspecification. Regression is simple and stable but less precise, while\nGMM offers a compromise in model dependence, though it is sensitive to\noutliers.\n  In practice, this framework supports improved inference in unsupervised\nsettings where contaminated scores serve as inputs to downstream analyses. By\nquantifying accuracy rates, the model enables score corrections even when no\nspecific outcome is yet defined. We demonstrate its utility using real oral\nreading fluency data, comparing human and AI-generated scores. Findings\nhighlight the practical implications of estimator choice and underscore the\nimportance of explicitly modeling asymmetric measurement error in count data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement error in count data is common but underexplored in the\nliterature, particularly in contexts where observed scores are bounded and\narise from discrete scoring processes. Motivated by applications in oral\nreading fluency assessment, we propose a binomial convolution framework that\nextends binary misclassification models to settings where only the aggregate\nnumber of correct responses is observed, and errors may involve both\novercounting and undercounting the number of events. The model accommodates\ndistinct true positive and true negative accuracy rates and preserves the\nbounded nature of the data.\n  Assuming the availability of both contaminated and error-free scores on a\nsubset of items, we develop and compare three estimation strategies: maximum\nlikelihood estimation (MLE), linear regression, and generalized method of\nmoments (GMM). Extensive simulations show that MLE is most accurate when the\nmodel is correctly specified but is computationally intensive and less robust\nto misspecification. Regression is simple and stable but less precise, while\nGMM offers a compromise in model dependence, though it is sensitive to\noutliers.\n  In practice, this framework supports improved inference in unsupervised\nsettings where contaminated scores serve as inputs to downstream analyses. By\nquantifying accuracy rates, the model enables score corrections even when no\nspecific outcome is yet defined. We demonstrate its utility using real oral\nreading fluency data, comparing human and AI-generated scores. Findings\nhighlight the practical implications of estimator choice and underscore the\nimportance of explicitly modeling asymmetric measurement error in count data."
                },
                "authors": [
                    {
                        "name": "Yuqiu Yang"
                    },
                    {
                        "name": "Christina Vu"
                    },
                    {
                        "name": "Cornelis J. Potgieter"
                    },
                    {
                        "name": "Xinlei Wang"
                    },
                    {
                        "name": "Akihito Kamata"
                    }
                ],
                "author_detail": {
                    "name": "Akihito Kamata"
                },
                "author": "Akihito Kamata",
                "arxiv_comment": "40 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20595v1",
                "updated": "2025-06-25T16:34:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    34,
                    9,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:34:09Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    34,
                    9,
                    2,
                    176,
                    0
                ],
                "title": "AI in the Writing Process: How Purposeful AI Support Fosters Student\n  Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in the Writing Process: How Purposeful AI Support Fosters Student\n  Writing"
                },
                "summary": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content."
                },
                "authors": [
                    {
                        "name": "Momin N. Siddiqui"
                    },
                    {
                        "name": "Roy Pea"
                    },
                    {
                        "name": "Hari Subramonyam"
                    }
                ],
                "author_detail": {
                    "name": "Hari Subramonyam"
                },
                "author": "Hari Subramonyam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02280v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02280v3",
                "updated": "2025-06-26T01:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    1,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-02T21:39:40Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    21,
                    39,
                    40,
                    0,
                    153,
                    0
                ],
                "title": "The State of Large Language Models for African Languages: Progress and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The State of Large Language Models for African Languages: Progress and\n  Challenges"
                },
                "summary": "Large Language Models (LLMs) are transforming Natural Language Processing\n(NLP), but their benefits are largely absent for Africa's 2,000 low-resource\nlanguages. This paper comparatively analyzes African language coverage across\nsix LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).\nThe evaluation covers language coverage, training sets, technical limitations,\nscript problems, and language modelling roadmaps. The work identifies 42\nsupported African languages and 23 available public data sets, and it shows a\nbig gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are\nalways treated while there is over 98\\% of unsupported African languages.\nMoreover, the review shows that just Latin, Arabic, and Ge'ez scripts are\nidentified while 20 active scripts are neglected. Some of the primary\nchallenges are lack of data, tokenization biases, computational costs being\nvery high, and evaluation issues. These issues demand language standardization,\ncorpus development by the community, and effective adaptation methods for\nAfrican languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming Natural Language Processing\n(NLP), but their benefits are largely absent for Africa's 2,000 low-resource\nlanguages. This paper comparatively analyzes African language coverage across\nsix LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).\nThe evaluation covers language coverage, training sets, technical limitations,\nscript problems, and language modelling roadmaps. The work identifies 42\nsupported African languages and 23 available public data sets, and it shows a\nbig gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are\nalways treated while there is over 98\\% of unsupported African languages.\nMoreover, the review shows that just Latin, Arabic, and Ge'ez scripts are\nidentified while 20 active scripts are neglected. Some of the primary\nchallenges are lack of data, tokenization biases, computational costs being\nvery high, and evaluation issues. These issues demand language standardization,\ncorpus development by the community, and effective adaptation methods for\nAfrican languages."
                },
                "authors": [
                    {
                        "name": "Kedir Yassin Hussen"
                    },
                    {
                        "name": "Walelign Tewabe Sewunetie"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "Sukairaj Hafiz Imam"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    }
                ],
                "author_detail": {
                    "name": "Seid Muhie Yimam"
                },
                "author": "Seid Muhie Yimam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02280v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02280v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20586v1",
                "updated": "2025-06-25T16:26:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    26,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:26:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    26,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Learning-Based Distance Estimation for 360° Single-Sensor Setups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Based Distance Estimation for 360° Single-Sensor Setups"
                },
                "summary": "Accurate distance estimation is a fundamental challenge in robotic\nperception, particularly in omnidirectional imaging, where traditional\ngeometric methods struggle with lens distortions and environmental variability.\nIn this work, we propose a neural network-based approach for monocular distance\nestimation using a single 360{\\deg} fisheye lens camera. Unlike classical\ntrigonometric techniques that rely on precise lens calibration, our method\ndirectly learns and infers the distance of objects from raw omnidirectional\ninputs, offering greater robustness and adaptability across diverse conditions.\nWe evaluate our approach on three 360{\\deg} datasets (LOAF, ULM360, and a newly\ncaptured dataset Boat360), each representing distinct environmental and sensor\nsetups. Our experimental results demonstrate that the proposed learning-based\nmodel outperforms traditional geometry-based methods and other learning\nbaselines in both accuracy and robustness. These findings highlight the\npotential of deep learning for real-time omnidirectional distance estimation,\nmaking our approach particularly well-suited for low-cost applications in\nrobotics, autonomous navigation, and surveillance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate distance estimation is a fundamental challenge in robotic\nperception, particularly in omnidirectional imaging, where traditional\ngeometric methods struggle with lens distortions and environmental variability.\nIn this work, we propose a neural network-based approach for monocular distance\nestimation using a single 360{\\deg} fisheye lens camera. Unlike classical\ntrigonometric techniques that rely on precise lens calibration, our method\ndirectly learns and infers the distance of objects from raw omnidirectional\ninputs, offering greater robustness and adaptability across diverse conditions.\nWe evaluate our approach on three 360{\\deg} datasets (LOAF, ULM360, and a newly\ncaptured dataset Boat360), each representing distinct environmental and sensor\nsetups. Our experimental results demonstrate that the proposed learning-based\nmodel outperforms traditional geometry-based methods and other learning\nbaselines in both accuracy and robustness. These findings highlight the\npotential of deep learning for real-time omnidirectional distance estimation,\nmaking our approach particularly well-suited for low-cost applications in\nrobotics, autonomous navigation, and surveillance."
                },
                "authors": [
                    {
                        "name": "Yitong Quan"
                    },
                    {
                        "name": "Benjamin Kiefer"
                    },
                    {
                        "name": "Martin Messmer"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "arxiv_comment": "Submitted to ECMR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20585v1",
                "updated": "2025-06-25T16:26:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    26,
                    1,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:26:01Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    26,
                    1,
                    2,
                    176,
                    0
                ],
                "title": "On the Impact of Sybil-based Attacks on Mobile Crowdsensing for\n  Transportation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Sybil-based Attacks on Mobile Crowdsensing for\n  Transportation"
                },
                "summary": "Mobile Crowd-Sensing (MCS) enables users with personal mobile devices (PMDs)\nto gain information on their surroundings. Users collect and contribute data on\ndifferent phenomena using their PMD sensors, and the MCS system processes this\ndata to extract valuable information for end users. Navigation MCS-based\napplications (N-MCS) are prevalent and important for transportation: users\nshare their location and speed while driving and, in return, find efficient\nroutes to their destinations. However, N-MCS are currently vulnerable to\nmalicious contributors, often termed Sybils: submitting falsified data,\nseemingly from many devices that are not truly present on target roads, falsely\nreporting congestion when there is none, thus changing the road status the\nN-MCS infers. The attack effect is that the N-MCS returns suboptimal routes to\nusers, causing late arrival and, overall, deteriorating road traffic flow. We\ninvestigate exactly the impact of Sybil-based attacks on N-MCS: we design an\nN-MCS system that offers efficient routing on top of the vehicular simulator\nSUMO, using the InTAS road network as our scenario. We design experiments\nattacking an individual N-MCS user as well as a larger population of users,\nselecting the adversary targets based on graph-theoretical arguments. Our\nexperiments show that the resources required for a successful attack depend on\nthe location of the attack (i.e., the surrounding road network and traffic) and\nthe extent of Sybil contributed data for the targeted road(s). We demonstrate\nthat Sybil attacks can alter the route of N-MCS users, increasing average\ntravel time by 20% with Sybils 3% of the N-MCS user population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Crowd-Sensing (MCS) enables users with personal mobile devices (PMDs)\nto gain information on their surroundings. Users collect and contribute data on\ndifferent phenomena using their PMD sensors, and the MCS system processes this\ndata to extract valuable information for end users. Navigation MCS-based\napplications (N-MCS) are prevalent and important for transportation: users\nshare their location and speed while driving and, in return, find efficient\nroutes to their destinations. However, N-MCS are currently vulnerable to\nmalicious contributors, often termed Sybils: submitting falsified data,\nseemingly from many devices that are not truly present on target roads, falsely\nreporting congestion when there is none, thus changing the road status the\nN-MCS infers. The attack effect is that the N-MCS returns suboptimal routes to\nusers, causing late arrival and, overall, deteriorating road traffic flow. We\ninvestigate exactly the impact of Sybil-based attacks on N-MCS: we design an\nN-MCS system that offers efficient routing on top of the vehicular simulator\nSUMO, using the InTAS road network as our scenario. We design experiments\nattacking an individual N-MCS user as well as a larger population of users,\nselecting the adversary targets based on graph-theoretical arguments. Our\nexperiments show that the resources required for a successful attack depend on\nthe location of the attack (i.e., the surrounding road network and traffic) and\nthe extent of Sybil contributed data for the targeted road(s). We demonstrate\nthat Sybil attacks can alter the route of N-MCS users, increasing average\ntravel time by 20% with Sybils 3% of the N-MCS user population."
                },
                "authors": [
                    {
                        "name": "Alexander Söderhäll"
                    },
                    {
                        "name": "Zahra Alimadadi"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_doi": "10.1109/PerComWorkshops65533.2025.00113",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PerComWorkshops65533.2025.00113",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.20585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 5 figures, 2 tables, TrustSense workshop of PerCom 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03905v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03905v3",
                "updated": "2025-06-25T16:21:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    21,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2024-12-05T06:21:31Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    21,
                    31,
                    3,
                    340,
                    0
                ],
                "title": "Integrating Various Software Artifacts for Better LLM-based Bug\n  Localization and Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Various Software Artifacts for Better LLM-based Bug\n  Localization and Program Repair"
                },
                "summary": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nFurthermore, we re-implemented and evaluated our framework, demonstrating its\neffectiveness in its effectiveness in resolving 9 unique issues compared to\nother state-of-the-art frameworks using the same or more advanced models on\nSWE-bench Lite.We also discussed whether a leading framework for Python code\ncan be directly applied to Java code, or vice versa. The source code and\nexperimental results of this work for replication are available at\nhttps://github.com/XYZboom/DEVLoRe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nFurthermore, we re-implemented and evaluated our framework, demonstrating its\neffectiveness in its effectiveness in resolving 9 unique issues compared to\nother state-of-the-art frameworks using the same or more advanced models on\nSWE-bench Lite.We also discussed whether a leading framework for Python code\ncan be directly applied to Java code, or vice versa. The source code and\nexperimental results of this work for replication are available at\nhttps://github.com/XYZboom/DEVLoRe."
                },
                "authors": [
                    {
                        "name": "Qiong Feng"
                    },
                    {
                        "name": "Xiaotian Ma"
                    },
                    {
                        "name": "Jiayi Sheng"
                    },
                    {
                        "name": "Ziyuan Feng"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Peng Liang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liang"
                },
                "author": "Peng Liang",
                "arxiv_comment": "25 pages, 12 images, 10 tables, Manuscript revision submitted to a\n  journal (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03905v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03905v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06256v2",
                "updated": "2025-06-25T16:21:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    21,
                    31,
                    2,
                    176,
                    0
                ],
                "published": "2025-01-09T09:45:05Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    45,
                    5,
                    3,
                    9,
                    0
                ],
                "title": "Unlocking In-Context Learning for Natural Datasets Beyond Language\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking In-Context Learning for Natural Datasets Beyond Language\n  Modelling"
                },
                "summary": "Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables\nthe model to perform new tasks conditioning only on the examples provided in\nthe context without updating the model's weights. While ICL offers fast\nadaptation across natural language tasks and domains, its emergence is less\nstraightforward for modalities beyond text. In this work, we systematically\nuncover properties present in LLMs that support the emergence of ICL for\nautoregressive models and various modalities by promoting the learning of the\nneeded mechanisms for ICL. We identify exact token repetitions in the training\ndata sequences as an important factor for ICL. Such repetitions further improve\nstability and reduce transiency in ICL performance. Moreover, we emphasise the\nsignificance of training task difficulty for the emergence of ICL. Finally, by\napplying our novel insights on ICL emergence, we unlock ICL capabilities for\nvarious visual datasets and a more challenging EEG classification task in a\nfew-shot learning regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables\nthe model to perform new tasks conditioning only on the examples provided in\nthe context without updating the model's weights. While ICL offers fast\nadaptation across natural language tasks and domains, its emergence is less\nstraightforward for modalities beyond text. In this work, we systematically\nuncover properties present in LLMs that support the emergence of ICL for\nautoregressive models and various modalities by promoting the learning of the\nneeded mechanisms for ICL. We identify exact token repetitions in the training\ndata sequences as an important factor for ICL. Such repetitions further improve\nstability and reduce transiency in ICL performance. Moreover, we emphasise the\nsignificance of training task difficulty for the emergence of ICL. Finally, by\napplying our novel insights on ICL emergence, we unlock ICL capabilities for\nvarious visual datasets and a more challenging EEG classification task in a\nfew-shot learning regime."
                },
                "authors": [
                    {
                        "name": "Jelena Bratulić"
                    },
                    {
                        "name": "Sudhanshu Mittal"
                    },
                    {
                        "name": "David T. Hoffmann"
                    },
                    {
                        "name": "Samuel Böhm"
                    },
                    {
                        "name": "Robin Tibor Schirrmeister"
                    },
                    {
                        "name": "Tonio Ball"
                    },
                    {
                        "name": "Christian Rupprecht"
                    },
                    {
                        "name": "Thomas Brox"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Brox"
                },
                "author": "Thomas Brox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20579v1",
                "updated": "2025-06-25T16:14:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    14,
                    17,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:14:17Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    14,
                    17,
                    2,
                    176,
                    0
                ],
                "title": "Communication-Aware Map Compression for Online Path-Planning: A\n  Rate-Distortion Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-Aware Map Compression for Online Path-Planning: A\n  Rate-Distortion Approach"
                },
                "summary": "This paper addresses the problem of collaborative navigation in an unknown\nenvironment, where two robots, referred to in the sequel as the Seeker and the\nSupporter, traverse the space simultaneously. The Supporter assists the Seeker\nby transmitting a compressed representation of its local map under bandwidth\nconstraints to support the Seeker's path-planning task. We introduce a bit-rate\nmetric based on the expected binary codeword length to quantify communication\ncost. Using this metric, we formulate the compression design problem as a\nrate-distortion optimization problem that determines when to communicate, which\nregions of the map should be included in the compressed representation, and at\nwhat resolution (i.e., quantization level) they should be encoded. Our\nformulation allows different map regions to be encoded at varying quantization\nlevels based on their relevance to the Seeker's path-planning task. We\ndemonstrate that the resulting optimization problem is convex, and admits a\nclosed-form solution known in the information theory literature as reverse\nwater-filling, enabling efficient, low-computation, and real-time\nimplementation. Additionally, we show that the Seeker can infer the compression\ndecisions of the Supporter independently, requiring only the encoded map\ncontent and not the encoding policy itself to be transmitted, thereby reducing\ncommunication overhead. Simulation results indicate that our method effectively\nconstructs compressed, task-relevant map representations, both in content and\nresolution, that guide the Seeker's planning decisions even under tight\nbandwidth limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of collaborative navigation in an unknown\nenvironment, where two robots, referred to in the sequel as the Seeker and the\nSupporter, traverse the space simultaneously. The Supporter assists the Seeker\nby transmitting a compressed representation of its local map under bandwidth\nconstraints to support the Seeker's path-planning task. We introduce a bit-rate\nmetric based on the expected binary codeword length to quantify communication\ncost. Using this metric, we formulate the compression design problem as a\nrate-distortion optimization problem that determines when to communicate, which\nregions of the map should be included in the compressed representation, and at\nwhat resolution (i.e., quantization level) they should be encoded. Our\nformulation allows different map regions to be encoded at varying quantization\nlevels based on their relevance to the Seeker's path-planning task. We\ndemonstrate that the resulting optimization problem is convex, and admits a\nclosed-form solution known in the information theory literature as reverse\nwater-filling, enabling efficient, low-computation, and real-time\nimplementation. Additionally, we show that the Seeker can infer the compression\ndecisions of the Supporter independently, requiring only the encoded map\ncontent and not the encoding policy itself to be transmitted, thereby reducing\ncommunication overhead. Simulation results indicate that our method effectively\nconstructs compressed, task-relevant map representations, both in content and\nresolution, that guide the Seeker's planning decisions even under tight\nbandwidth limitations."
                },
                "authors": [
                    {
                        "name": "Ali Reza Pedram"
                    },
                    {
                        "name": "Evangelos Psomiadis"
                    },
                    {
                        "name": "Dipankar Maity"
                    },
                    {
                        "name": "Panagiotis Tsiotras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Tsiotras"
                },
                "author": "Panagiotis Tsiotras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19677v2",
                "updated": "2025-06-25T16:13:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    13,
                    14,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-24T14:44:33Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    33,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees"
                },
                "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving."
                },
                "authors": [
                    {
                        "name": "Shi Chang"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Hanan Lutfiyya"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20573v1",
                "updated": "2025-06-25T16:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    7,
                    59,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    7,
                    59,
                    2,
                    176,
                    0
                ],
                "title": "LARP: Learner-Agnostic Robust Data Prefiltering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARP: Learner-Agnostic Robust Data Prefiltering"
                },
                "summary": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets."
                },
                "authors": [
                    {
                        "name": "Kristian Minchev"
                    },
                    {
                        "name": "Dimitar Iliev Dimitrov"
                    },
                    {
                        "name": "Nikola Konstantinov"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Konstantinov"
                },
                "author": "Nikola Konstantinov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20558v1",
                "updated": "2025-06-25T15:56:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    56,
                    7,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:56:07Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    56,
                    7,
                    2,
                    176,
                    0
                ],
                "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment\n  Inconsistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment\n  Inconsistency"
                },
                "summary": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability."
                },
                "authors": [
                    {
                        "name": "Renyi Zhong"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Wenwei Gu"
                    },
                    {
                        "name": "Jinxi Kuang"
                    },
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Guangba Yu"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "This manuscript is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20551v1",
                "updated": "2025-06-25T15:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    50,
                    34,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:50:34Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    50,
                    34,
                    2,
                    176,
                    0
                ],
                "title": "Large Language Model-Driven Code Compliance Checking in Building\n  Information Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Code Compliance Checking in Building\n  Information Modeling"
                },
                "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects."
                },
                "authors": [
                    {
                        "name": "Soumya Madireddy"
                    },
                    {
                        "name": "Lu Gao"
                    },
                    {
                        "name": "Zia Din"
                    },
                    {
                        "name": "Kinam Kim"
                    },
                    {
                        "name": "Ahmed Senouci"
                    },
                    {
                        "name": "Zhe Han"
                    },
                    {
                        "name": "Yunpeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Zhang"
                },
                "author": "Yunpeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20550v1",
                "updated": "2025-06-25T15:49:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    49,
                    7,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:49:07Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    49,
                    7,
                    2,
                    176,
                    0
                ],
                "title": "Lightweight Multi-Frame Integration for Robust YOLO Object Detection in\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Multi-Frame Integration for Robust YOLO Object Detection in\n  Videos"
                },
                "summary": "Modern image-based object detection models, such as YOLOv7, primarily process\nindividual frames independently, thus ignoring valuable temporal context\nnaturally present in videos. Meanwhile, existing video-based detection methods\noften introduce complex temporal modules, significantly increasing model size\nand computational complexity. In practical applications such as surveillance\nand autonomous driving, transient challenges including motion blur, occlusions,\nand abrupt appearance changes can severely degrade single-frame detection\nperformance. To address these issues, we propose a straightforward yet highly\neffective strategy: stacking multiple consecutive frames as input to a\nYOLO-based detector while supervising only the output corresponding to a single\ntarget frame. This approach leverages temporal information with minimal\nmodifications to existing architectures, preserving simplicity, computational\nefficiency, and real-time inference capability. Extensive experiments on the\nchallenging MOT20Det and our BOAT360 datasets demonstrate that our method\nimproves detection robustness, especially for lightweight models, effectively\nnarrowing the gap between compact and heavy detection networks. Additionally,\nwe contribute the BOAT360 benchmark dataset, comprising annotated fisheye video\nsequences captured from a boat, to support future research in multi-frame video\nobject detection in challenging real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern image-based object detection models, such as YOLOv7, primarily process\nindividual frames independently, thus ignoring valuable temporal context\nnaturally present in videos. Meanwhile, existing video-based detection methods\noften introduce complex temporal modules, significantly increasing model size\nand computational complexity. In practical applications such as surveillance\nand autonomous driving, transient challenges including motion blur, occlusions,\nand abrupt appearance changes can severely degrade single-frame detection\nperformance. To address these issues, we propose a straightforward yet highly\neffective strategy: stacking multiple consecutive frames as input to a\nYOLO-based detector while supervising only the output corresponding to a single\ntarget frame. This approach leverages temporal information with minimal\nmodifications to existing architectures, preserving simplicity, computational\nefficiency, and real-time inference capability. Extensive experiments on the\nchallenging MOT20Det and our BOAT360 datasets demonstrate that our method\nimproves detection robustness, especially for lightweight models, effectively\nnarrowing the gap between compact and heavy detection networks. Additionally,\nwe contribute the BOAT360 benchmark dataset, comprising annotated fisheye video\nsequences captured from a boat, to support future research in multi-frame video\nobject detection in challenging real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Yitong Quan"
                    },
                    {
                        "name": "Benjamin Kiefer"
                    },
                    {
                        "name": "Martin Messmer"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "arxiv_comment": "Submitted to ECMR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20549v1",
                "updated": "2025-06-25T15:47:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    47,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:47:23Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    47,
                    23,
                    2,
                    176,
                    0
                ],
                "title": "Causal Inference for Latent Outcomes Learned with Factor Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference for Latent Outcomes Learned with Factor Models"
                },
                "summary": "In many fields$\\unicode{x2013}$including genomics, epidemiology, natural\nlanguage processing, social and behavioral sciences, and\neconomics$\\unicode{x2013}$it is increasingly important to address causal\nquestions in the context of factor models or representation learning. In this\nwork, we investigate causal effects on $\\textit{latent outcomes}$ derived from\nhigh-dimensional observed data using nonnegative matrix factorization. To the\nbest of our knowledge, this is the first study to formally address causal\ninference in this setting. A central challenge is that estimating a latent\nfactor model can cause an individual's learned latent outcome to depend on\nother individuals' treatments, thereby violating the standard causal inference\nassumption of no interference. We formalize this issue as\n$\\textit{learning-induced interference}$ and distinguish it from interference\npresent in a data-generating process. To address this, we propose a novel,\nintuitive, and theoretically grounded algorithm to estimate causal effects on\nlatent outcomes while mitigating learning-induced interference and improving\nestimation efficiency. We establish theoretical guarantees for the consistency\nof our estimator and demonstrate its practical utility through simulation\nstudies and an application to cancer mutational signature analysis. All\nbaseline and proposed methods are available in our open-source R package, ${\\tt\ncausalLFO}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many fields$\\unicode{x2013}$including genomics, epidemiology, natural\nlanguage processing, social and behavioral sciences, and\neconomics$\\unicode{x2013}$it is increasingly important to address causal\nquestions in the context of factor models or representation learning. In this\nwork, we investigate causal effects on $\\textit{latent outcomes}$ derived from\nhigh-dimensional observed data using nonnegative matrix factorization. To the\nbest of our knowledge, this is the first study to formally address causal\ninference in this setting. A central challenge is that estimating a latent\nfactor model can cause an individual's learned latent outcome to depend on\nother individuals' treatments, thereby violating the standard causal inference\nassumption of no interference. We formalize this issue as\n$\\textit{learning-induced interference}$ and distinguish it from interference\npresent in a data-generating process. To address this, we propose a novel,\nintuitive, and theoretically grounded algorithm to estimate causal effects on\nlatent outcomes while mitigating learning-induced interference and improving\nestimation efficiency. We establish theoretical guarantees for the consistency\nof our estimator and demonstrate its practical utility through simulation\nstudies and an application to cancer mutational signature analysis. All\nbaseline and proposed methods are available in our open-source R package, ${\\tt\ncausalLFO}$."
                },
                "authors": [
                    {
                        "name": "Jenna M. Landy"
                    },
                    {
                        "name": "Dafne Zorzetto"
                    },
                    {
                        "name": "Roberta De Vito"
                    },
                    {
                        "name": "Giovanni Parmigiani"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Parmigiani"
                },
                "author": "Giovanni Parmigiani",
                "arxiv_comment": "18 pages, 7 figures, 1 table (+ references and supplement). For\n  open-source R software package, see https://github.com/jennalandy/causalLFO.\n  For all code used in the simulation studies and data application, see\n  https://github.com/jennalandy/causalLFO_PAPER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20544v1",
                "updated": "2025-06-25T15:37:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    37,
                    53,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:37:53Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    37,
                    53,
                    2,
                    176,
                    0
                ],
                "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages."
                },
                "authors": [
                    {
                        "name": "Ammar Khairi"
                    },
                    {
                        "name": "Daniel D'souza"
                    },
                    {
                        "name": "Ye Shen"
                    },
                    {
                        "name": "Julia Kreutzer"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01633v2",
                "updated": "2025-06-25T15:31:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    31,
                    17,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-03T18:59:01Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    1,
                    0,
                    34,
                    0
                ],
                "title": "Adversarial Reasoning at Jailbreaking Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Reasoning at Jailbreaking Time"
                },
                "summary": "As large language models (LLMs) are becoming more capable and widespread, the\nstudy of their failure cases is becoming increasingly important. Recent\nadvances in standardizing, measuring, and scaling test-time compute suggest new\nmethodologies for optimizing models to achieve high performance on hard tasks.\nIn this paper, we apply these advances to the task of model jailbreaking:\neliciting harmful responses from aligned LLMs. We develop an adversarial\nreasoning approach to automatic jailbreaking that leverages a loss signal to\nguide the test-time compute, achieving SOTA attack success rates against many\naligned LLMs, even those that aim to trade inference-time compute for\nadversarial robustness. Our approach introduces a new paradigm in understanding\nLLM vulnerabilities, laying the foundation for the development of more robust\nand trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are becoming more capable and widespread, the\nstudy of their failure cases is becoming increasingly important. Recent\nadvances in standardizing, measuring, and scaling test-time compute suggest new\nmethodologies for optimizing models to achieve high performance on hard tasks.\nIn this paper, we apply these advances to the task of model jailbreaking:\neliciting harmful responses from aligned LLMs. We develop an adversarial\nreasoning approach to automatic jailbreaking that leverages a loss signal to\nguide the test-time compute, achieving SOTA attack success rates against many\naligned LLMs, even those that aim to trade inference-time compute for\nadversarial robustness. Our approach introduces a new paradigm in understanding\nLLM vulnerabilities, laying the foundation for the development of more robust\nand trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Mahdi Sabbaghi"
                    },
                    {
                        "name": "Paul Kassianik"
                    },
                    {
                        "name": "George Pappas"
                    },
                    {
                        "name": "Yaron Singer"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Hamed Hassani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Hassani"
                },
                "author": "Hamed Hassani",
                "arxiv_comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20537v1",
                "updated": "2025-06-25T15:25:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    25,
                    1,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:25:01Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    25,
                    1,
                    2,
                    176,
                    0
                ],
                "title": "Physics-Informed Machine Learning Regulated by Finite Element Analysis\n  for Simulation Acceleration of Laser Powder Bed Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Machine Learning Regulated by Finite Element Analysis\n  for Simulation Acceleration of Laser Powder Bed Fusion"
                },
                "summary": "Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process\nprediction due to the lasting issue of high computation cost using traditional\nnumerical methods such as finite element analysis (FEA). This study presents an\nefficient modeling framework termed FEA-Regulated Physics-Informed Neural\nNetwork (FEA-PINN) to accelerate the thermal field prediction in a LPBF process\nwhile maintaining the FEA accuracy. A novel dynamic material updating strategy\nis developed to capture the dynamic phase change of powder-liquid-solid in the\nPINN model. The PINN model incorporates temperature-dependent material\nproperties and phase change behavior using the apparent heat capacity method.\nWhile the PINN model demonstrates high accuracy with a small training data and\nenables generalization of new process parameters via transfer learning, it\nfaces the challenge of high computation cost in time-dependent problems due to\nthe residual accumulation. To overcome this issue, the FEA-PINN framework\nintegrates corrective FEA simulations during inference to enforce physical\nconsistency and reduce error drift. A comparative analysis shows that FEA-PINN\nachieves equivalent accuracy to FEA while significantly reducing computational\ncost. The framework has been validated using the benchmark FEA data and\ndemonstrated through single-track scanning in LPBF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process\nprediction due to the lasting issue of high computation cost using traditional\nnumerical methods such as finite element analysis (FEA). This study presents an\nefficient modeling framework termed FEA-Regulated Physics-Informed Neural\nNetwork (FEA-PINN) to accelerate the thermal field prediction in a LPBF process\nwhile maintaining the FEA accuracy. A novel dynamic material updating strategy\nis developed to capture the dynamic phase change of powder-liquid-solid in the\nPINN model. The PINN model incorporates temperature-dependent material\nproperties and phase change behavior using the apparent heat capacity method.\nWhile the PINN model demonstrates high accuracy with a small training data and\nenables generalization of new process parameters via transfer learning, it\nfaces the challenge of high computation cost in time-dependent problems due to\nthe residual accumulation. To overcome this issue, the FEA-PINN framework\nintegrates corrective FEA simulations during inference to enforce physical\nconsistency and reduce error drift. A comparative analysis shows that FEA-PINN\nachieves equivalent accuracy to FEA while significantly reducing computational\ncost. The framework has been validated using the benchmark FEA data and\ndemonstrated through single-track scanning in LPBF."
                },
                "authors": [
                    {
                        "name": "R. Sharma"
                    },
                    {
                        "name": "M. Raissi"
                    },
                    {
                        "name": "Y. B. Guo"
                    }
                ],
                "author_detail": {
                    "name": "Y. B. Guo"
                },
                "author": "Y. B. Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20535v1",
                "updated": "2025-06-25T15:24:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    24,
                    45,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:24:45Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    24,
                    45,
                    2,
                    176,
                    0
                ],
                "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads"
                },
                "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI."
                },
                "authors": [
                    {
                        "name": "Hongzhen Huang"
                    },
                    {
                        "name": "Kunming Zhang"
                    },
                    {
                        "name": "Hanlong Liao"
                    },
                    {
                        "name": "Kui Wu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "arxiv_comment": "11 pages, 7 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20531v1",
                "updated": "2025-06-25T15:19:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    19,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:19:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    19,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Case-based Reasoning Augmented Large Language Model Framework for\n  Decision Making in Realistic Safety-Critical Driving Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case-based Reasoning Augmented Large Language Model Framework for\n  Decision Making in Realistic Safety-Critical Driving Scenarios"
                },
                "summary": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems."
                },
                "authors": [
                    {
                        "name": "Wenbin Gan"
                    },
                    {
                        "name": "Minh-Son Dao"
                    },
                    {
                        "name": "Koji Zettsu"
                    }
                ],
                "author_detail": {
                    "name": "Koji Zettsu"
                },
                "author": "Koji Zettsu",
                "arxiv_comment": "12 pages, 10 figures, under-review conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02767v3",
                "updated": "2025-06-25T15:18:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    18,
                    56,
                    2,
                    176,
                    0
                ],
                "published": "2024-12-03T19:09:48Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    19,
                    9,
                    48,
                    1,
                    338,
                    0
                ],
                "title": "Endogenous Heteroskedasticity in Linear Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogenous Heteroskedasticity in Linear Models"
                },
                "summary": "Linear regressions with endogeneity are widely used to estimate causal\neffects. This paper studies a framework that involves two common issues:\nendogeneity of the regressors and heteroskedasticity that depends on endogenous\nregressors-i.e., endogenous heteroskedasticity. We show that the presence of\nendogenous heteroskedasticity in the structural regression renders the\ntwo-stage least squares estimator inconsistent. To address this issue, we\npropose sufficient conditions and a control function approach to identify and\nestimate the causal parameters of interest. We establish the limiting\nproperties of the estimator--namely, consistency and asymptotic normality--and\npropose inference procedures. Monte Carlo simulations provide evidence on the\nfinite-sample performance of the proposed methods and evaluate different\nimplementation strategies. We revisit an empirical application on job training\nto illustrate the methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear regressions with endogeneity are widely used to estimate causal\neffects. This paper studies a framework that involves two common issues:\nendogeneity of the regressors and heteroskedasticity that depends on endogenous\nregressors-i.e., endogenous heteroskedasticity. We show that the presence of\nendogenous heteroskedasticity in the structural regression renders the\ntwo-stage least squares estimator inconsistent. To address this issue, we\npropose sufficient conditions and a control function approach to identify and\nestimate the causal parameters of interest. We establish the limiting\nproperties of the estimator--namely, consistency and asymptotic normality--and\npropose inference procedures. Monte Carlo simulations provide evidence on the\nfinite-sample performance of the proposed methods and evaluate different\nimplementation strategies. We revisit an empirical application on job training\nto illustrate the methods."
                },
                "authors": [
                    {
                        "name": "Javier Alejo"
                    },
                    {
                        "name": "Antonio F. Galvao"
                    },
                    {
                        "name": "Julian Martinez-Iriarte"
                    },
                    {
                        "name": "Gabriel Montes-Rojas"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Montes-Rojas"
                },
                "author": "Gabriel Montes-Rojas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08745v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08745v4",
                "updated": "2025-06-25T15:16:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    16,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2024-11-13T16:26:19Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    26,
                    19,
                    2,
                    318,
                    0
                ],
                "title": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers"
                },
                "summary": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word-translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean\nrepresentation of a concept across different languages does not affect the\nmodels' ability to translate it, but instead improves it. Finally, we\ngeneralize to multi-token generation and demonstrate that the model can\ngenerate natural language description of those mean representations. Our\nresults provide evidence for the existence of language-agnostic concept\nrepresentations within the investigated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word-translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean\nrepresentation of a concept across different languages does not affect the\nmodels' ability to translate it, but instead improves it. Finally, we\ngeneralize to multi-token generation and demonstrate that the model can\ngenerate natural language description of those mean representations. Our\nresults provide evidence for the existence of language-agnostic concept\nrepresentations within the investigated models."
                },
                "authors": [
                    {
                        "name": "Clément Dumas"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "20 pages, 14 figures, previous version published under the title \"How\n  Do Llamas Process Multilingual Text? A Latent Exploration through Activation\n  Patching\" at the ICML 2024 mechanistic interpretability workshop at\n  https://openreview.net/forum?id=0ku2hIm4BS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08745v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08745v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20523v1",
                "updated": "2025-06-25T15:09:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    9,
                    3,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:09:03Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    9,
                    3,
                    2,
                    176,
                    0
                ],
                "title": "Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment\n  and Balanced Power",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment\n  and Balanced Power"
                },
                "summary": "Adaptive experiments have become central to modern experimental design,\nenabling researchers to efficiently identify optimal treatments, improve\nstatistical power, and maximize respondent welfare. However, adaptive\nexperiments compromise valid inference and leave sub-optimal treatments\nunderpowered. We introduce two methodological advances for adaptive\nexperimentation. First, covariate-adjusted Mixture Adaptive Design (MADCovar)\nachieves substantial improvements in average treatment effect (ATE) precision\nby incorporating covariate adjustment within an anytime-valid inference\nframework. Second, power-modified MAD (MADMod) reallocates sample to\nunderpowered treatment arms, improving statistical power across all treatments\nwhile maintaining error control. Both methods provide anytime-valid guarantees,\nenabling continuous monitoring without inflating Type 1 error rates. Simulation\nstudies and empirical analyses demonstrate that MADCovar delivers significant\nprecision gains and that MADMod ensures robust inference even for suboptimal\ntreatments. Together, these methods address key limitations of adaptive\nexperiments and equip researchers with practical tools for precise and reliable\ncausal inference. Our proposed methods are implemented through an open-source\nsoftware package.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive experiments have become central to modern experimental design,\nenabling researchers to efficiently identify optimal treatments, improve\nstatistical power, and maximize respondent welfare. However, adaptive\nexperiments compromise valid inference and leave sub-optimal treatments\nunderpowered. We introduce two methodological advances for adaptive\nexperimentation. First, covariate-adjusted Mixture Adaptive Design (MADCovar)\nachieves substantial improvements in average treatment effect (ATE) precision\nby incorporating covariate adjustment within an anytime-valid inference\nframework. Second, power-modified MAD (MADMod) reallocates sample to\nunderpowered treatment arms, improving statistical power across all treatments\nwhile maintaining error control. Both methods provide anytime-valid guarantees,\nenabling continuous monitoring without inflating Type 1 error rates. Simulation\nstudies and empirical analyses demonstrate that MADCovar delivers significant\nprecision gains and that MADMod ensures robust inference even for suboptimal\ntreatments. Together, these methods address key limitations of adaptive\nexperiments and equip researchers with practical tools for precise and reliable\ncausal inference. Our proposed methods are implemented through an open-source\nsoftware package."
                },
                "authors": [
                    {
                        "name": "Daniel Molitor"
                    },
                    {
                        "name": "Samantha Gold"
                    }
                ],
                "author_detail": {
                    "name": "Samantha Gold"
                },
                "author": "Samantha Gold",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20520v1",
                "updated": "2025-06-25T15:07:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    7,
                    16,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:07:16Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    7,
                    16,
                    2,
                    176,
                    0
                ],
                "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing\n  positive and negative rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing\n  positive and negative rewards"
                },
                "summary": "Reinforcement learning (RL) is increasingly used to align large language\nmodels (LLMs). Off-policy methods offer greater implementation simplicity and\ndata efficiency than on-policy techniques, but often result in suboptimal\nperformance. In this work, we study the intermediate range of algorithms\nbetween off-policy RL and supervised fine-tuning by analyzing a simple\noff-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with\n$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$\nemphasizes high-reward samples, while raising it penalizes low-reward ones more\nheavily. We first provide a theoretical analysis of this off-policy REINFORCE\nalgorithm, showing that when the baseline $V$ lower-bounds the expected reward,\nthe algorithm enjoys a policy improvement guarantee. Our analysis reveals that\nwhile on-policy updates can safely leverage both positive and negative signals,\noff-policy updates benefit from focusing more on positive rewards than on\nnegative ones. We validate our findings experimentally in a controlled\nstochastic bandit setting and through fine-tuning state-of-the-art LLMs on\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is increasingly used to align large language\nmodels (LLMs). Off-policy methods offer greater implementation simplicity and\ndata efficiency than on-policy techniques, but often result in suboptimal\nperformance. In this work, we study the intermediate range of algorithms\nbetween off-policy RL and supervised fine-tuning by analyzing a simple\noff-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with\n$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$\nemphasizes high-reward samples, while raising it penalizes low-reward ones more\nheavily. We first provide a theoretical analysis of this off-policy REINFORCE\nalgorithm, showing that when the baseline $V$ lower-bounds the expected reward,\nthe algorithm enjoys a policy improvement guarantee. Our analysis reveals that\nwhile on-policy updates can safely leverage both positive and negative signals,\noff-policy updates benefit from focusing more on positive rewards than on\nnegative ones. We validate our findings experimentally in a controlled\nstochastic bandit setting and through fine-tuning state-of-the-art LLMs on\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Charles Arnal"
                    },
                    {
                        "name": "Gaëtan Narozniak"
                    },
                    {
                        "name": "Vivien Cabannes"
                    },
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Remi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Remi Munos"
                },
                "author": "Remi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19777v2",
                "updated": "2025-06-25T14:53:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    53,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-25T15:47:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    47,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation"
                },
                "summary": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS"
                },
                "authors": [
                    {
                        "name": "Vladan Stojnić"
                    },
                    {
                        "name": "Yannis Kalantidis"
                    },
                    {
                        "name": "Jiří Matas"
                    },
                    {
                        "name": "Giorgos Tolias"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Tolias"
                },
                "author": "Giorgos Tolias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20503v1",
                "updated": "2025-06-25T14:49:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    49,
                    28,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:49:28Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    49,
                    28,
                    2,
                    176,
                    0
                ],
                "title": "BotHash: Efficient and Training-Free Bot Detection Through Approximate\n  Nearest Neighbor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BotHash: Efficient and Training-Free Bot Detection Through Approximate\n  Nearest Neighbor"
                },
                "summary": "Online Social Networks (OSNs) are a cornerstone in modern society, serving as\nplatforms for diverse content consumption by millions of users each day.\nHowever, the challenge of ensuring the accuracy of information shared on these\nplatforms remains significant, especially with the widespread dissemination of\ndisinformation. Social bots -- automated accounts designed to mimic human\nbehavior, frequently spreading misinformation -- represent one of the critical\nproblems of OSNs. The advent of Large Language Models (LLMs) has further\ncomplicated bot behaviors, making detection increasingly difficult. This paper\npresents BotHash, an innovative, training-free approach to social bot\ndetection. BotHash leverages a simplified user representation that enables\napproximate nearest-neighbor search to detect bots, avoiding the complexities\nof Deep-Learning model training and large dataset creation. We demonstrate that\nBotHash effectively differentiates between human and bot accounts, even when\nstate-of-the-art LLMs are employed to generate posts' content. BotHash offers\nseveral advantages over existing methods, including its independence from a\ntraining phase, robust performance with minimal ground-truth data, and early\ndetection capabilities, showing promising results across various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Social Networks (OSNs) are a cornerstone in modern society, serving as\nplatforms for diverse content consumption by millions of users each day.\nHowever, the challenge of ensuring the accuracy of information shared on these\nplatforms remains significant, especially with the widespread dissemination of\ndisinformation. Social bots -- automated accounts designed to mimic human\nbehavior, frequently spreading misinformation -- represent one of the critical\nproblems of OSNs. The advent of Large Language Models (LLMs) has further\ncomplicated bot behaviors, making detection increasingly difficult. This paper\npresents BotHash, an innovative, training-free approach to social bot\ndetection. BotHash leverages a simplified user representation that enables\napproximate nearest-neighbor search to detect bots, avoiding the complexities\nof Deep-Learning model training and large dataset creation. We demonstrate that\nBotHash effectively differentiates between human and bot accounts, even when\nstate-of-the-art LLMs are employed to generate posts' content. BotHash offers\nseveral advantages over existing methods, including its independence from a\ntraining phase, robust performance with minimal ground-truth data, and early\ndetection capabilities, showing promising results across various datasets."
                },
                "authors": [
                    {
                        "name": "Edoardo Di Paolo"
                    },
                    {
                        "name": "Fabio De Gaspari"
                    },
                    {
                        "name": "Angelo Spognardi"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Spognardi"
                },
                "author": "Angelo Spognardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08727v3",
                "updated": "2025-06-25T14:45:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    45,
                    56,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-11T01:07:57Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    1,
                    7,
                    57,
                    1,
                    70,
                    0
                ],
                "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation"
                },
                "summary": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG."
                },
                "authors": [
                    {
                        "name": "Lucas Caccia"
                    },
                    {
                        "name": "Alan Ansell"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Ivan Vulić"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sordoni"
                },
                "author": "Alessandro Sordoni",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04030v2",
                "updated": "2025-06-25T14:44:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    44,
                    30,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-06T12:47:25Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    47,
                    25,
                    3,
                    37,
                    0
                ],
                "title": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging"
                },
                "summary": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps."
                },
                "authors": [
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20495v1",
                "updated": "2025-06-25T14:41:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    41,
                    13,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:41:13Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    41,
                    13,
                    2,
                    176,
                    0
                ],
                "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCode: Updating Code API Knowledge with Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode."
                },
                "authors": [
                    {
                        "name": "Haoze Wu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20488v1",
                "updated": "2025-06-25T14:36:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    36,
                    31,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:36:31Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    36,
                    31,
                    2,
                    176,
                    0
                ],
                "title": "Generative AI for Vulnerability Detection in 6G Wireless Networks:\n  Advances, Case Study, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Vulnerability Detection in 6G Wireless Networks:\n  Advances, Case Study, and Future Directions"
                },
                "summary": "The rapid advancement of 6G wireless networks, IoT, and edge computing has\nsignificantly expanded the cyberattack surface, necessitating more intelligent\nand adaptive vulnerability detection mechanisms. Traditional security methods,\nwhile foundational, struggle with zero-day exploits, adversarial threats, and\ncontext-dependent vulnerabilities in highly dynamic network environments.\nGenerative AI (GAI) emerges as a transformative solution, leveraging synthetic\ndata generation, multimodal reasoning, and adaptive learning to enhance\nsecurity frameworks. This paper explores the integration of GAI-powered\nvulnerability detection in 6G wireless networks, focusing on code auditing,\nprotocol security, cloud-edge defenses, and hardware protection. We introduce a\nthree-layer framework comprising the Technology Layer, Capability Layer, and\nApplication Layer to systematically analyze the role of VAEs, GANs, LLMs, and\nGDMs in securing next-generation wireless ecosystems. To demonstrate practical\nimplementation, we present a case study on LLM-driven code vulnerability\ndetection, highlighting its effectiveness, performance, and challenges.\nFinally, we outline future research directions, including lightweight models,\nhigh-authenticity data generation, external knowledge integration, and\nprivacy-preserving technologies. By synthesizing current advancements and open\nchallenges, this work provides a roadmap for researchers and practitioners to\nharness GAI for building resilient and adaptive security solutions in 6G\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of 6G wireless networks, IoT, and edge computing has\nsignificantly expanded the cyberattack surface, necessitating more intelligent\nand adaptive vulnerability detection mechanisms. Traditional security methods,\nwhile foundational, struggle with zero-day exploits, adversarial threats, and\ncontext-dependent vulnerabilities in highly dynamic network environments.\nGenerative AI (GAI) emerges as a transformative solution, leveraging synthetic\ndata generation, multimodal reasoning, and adaptive learning to enhance\nsecurity frameworks. This paper explores the integration of GAI-powered\nvulnerability detection in 6G wireless networks, focusing on code auditing,\nprotocol security, cloud-edge defenses, and hardware protection. We introduce a\nthree-layer framework comprising the Technology Layer, Capability Layer, and\nApplication Layer to systematically analyze the role of VAEs, GANs, LLMs, and\nGDMs in securing next-generation wireless ecosystems. To demonstrate practical\nimplementation, we present a case study on LLM-driven code vulnerability\ndetection, highlighting its effectiveness, performance, and challenges.\nFinally, we outline future research directions, including lightweight models,\nhigh-authenticity data generation, external knowledge integration, and\nprivacy-preserving technologies. By synthesizing current advancements and open\nchallenges, this work provides a roadmap for researchers and practitioners to\nharness GAI for building resilient and adaptive security solutions in 6G\nnetworks."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Danyang Song"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    }
                ],
                "author_detail": {
                    "name": "Edith C. H. Ngai"
                },
                "author": "Edith C. H. Ngai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19841v2",
                "updated": "2025-06-25T14:25:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    25,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-04-28T14:41:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    41,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "Inference with few treated units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with few treated units"
                },
                "summary": "In many causal inference applications, only one or a few units (or clusters\nof units) are treated. An important challenge in such settings is that standard\ninference methods that rely on asymptotic theory may be unreliable, even when\nthe total number of units is large. This survey reviews and categorizes\ninference methods that are designed to accommodate few treated units,\nconsidering both cross-sectional and panel data methods. We discuss trade-offs\nand connections between different approaches. In doing so, we propose slight\nmodifications to improve the finite-sample validity of some methods, and we\nalso provide theoretical justifications for existing heuristic approaches that\nhave been proposed in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many causal inference applications, only one or a few units (or clusters\nof units) are treated. An important challenge in such settings is that standard\ninference methods that rely on asymptotic theory may be unreliable, even when\nthe total number of units is large. This survey reviews and categorizes\ninference methods that are designed to accommodate few treated units,\nconsidering both cross-sectional and panel data methods. We discuss trade-offs\nand connections between different approaches. In doing so, we propose slight\nmodifications to improve the finite-sample validity of some methods, and we\nalso provide theoretical justifications for existing heuristic approaches that\nhave been proposed in the literature."
                },
                "authors": [
                    {
                        "name": "Luis Alvarez"
                    },
                    {
                        "name": "Bruno Ferman"
                    },
                    {
                        "name": "Kaspar Wüthrich"
                    }
                ],
                "author_detail": {
                    "name": "Kaspar Wüthrich"
                },
                "author": "Kaspar Wüthrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20480v1",
                "updated": "2025-06-25T14:24:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    24,
                    59,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:24:59Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    24,
                    59,
                    2,
                    176,
                    0
                ],
                "title": "GPTailor: Large Language Model Pruning Through Layer Cutting and\n  Stitching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTailor: Large Language Model Pruning Through Layer Cutting and\n  Stitching"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\ndeployment and inference. While structured pruning of model parameters offers a\npromising way to reduce computational costs at deployment time, current methods\nprimarily focus on single model pruning. In this work, we develop a novel\nstrategy to compress models by strategically combining or merging layers from\nfinetuned model variants, which preserves the original model's abilities by\naggregating capabilities accentuated in different finetunes. We pose the\noptimal tailoring of these LLMs as a zero-order optimization problem, adopting\na search space that supports three different operations: (1) Layer removal, (2)\nLayer selection from different candidate models, and (3) Layer merging. Our\nexperiments demonstrate that this approach leads to competitive model pruning,\nfor example, for the Llama2-13B model families, our compressed models maintain\napproximately 97.3\\% of the original performance while removing $\\sim25\\%$ of\nparameters, significantly outperforming previous state-of-the-art methods. The\ncode is available at https://github.com/Guinan-Su/auto-merge-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\ndeployment and inference. While structured pruning of model parameters offers a\npromising way to reduce computational costs at deployment time, current methods\nprimarily focus on single model pruning. In this work, we develop a novel\nstrategy to compress models by strategically combining or merging layers from\nfinetuned model variants, which preserves the original model's abilities by\naggregating capabilities accentuated in different finetunes. We pose the\noptimal tailoring of these LLMs as a zero-order optimization problem, adopting\na search space that supports three different operations: (1) Layer removal, (2)\nLayer selection from different candidate models, and (3) Layer merging. Our\nexperiments demonstrate that this approach leads to competitive model pruning,\nfor example, for the Llama2-13B model families, our compressed models maintain\napproximately 97.3\\% of the original performance while removing $\\sim25\\%$ of\nparameters, significantly outperforming previous state-of-the-art methods. The\ncode is available at https://github.com/Guinan-Su/auto-merge-llm."
                },
                "authors": [
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Yanwu Yang"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19494v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19494v3",
                "updated": "2025-06-25T14:24:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    24,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2024-10-25T11:51:37Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models"
                },
                "summary": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel."
                },
                "authors": [
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19494v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19494v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03906v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03906v3",
                "updated": "2025-06-25T14:22:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    22,
                    4,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-06T18:22:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    22,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "MARCO: Multi-Agent Code Optimization with Real-Time Knowledge\n  Integration for High-Performance Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCO: Multi-Agent Code Optimization with Real-Time Knowledge\n  Integration for High-Performance Computing"
                },
                "summary": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6\\% average runtime reduction\ncompared to Claude 3.5 Sonnet alone, while the integration of the web-search\ncomponent yields a 30.9\\% performance improvement over the base MARCO system.\nThese results highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6\\% average runtime reduction\ncompared to Claude 3.5 Sonnet alone, while the integration of the web-search\ncomponent yields a 30.9\\% performance improvement over the base MARCO system.\nThese results highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Asif Rahman"
                    },
                    {
                        "name": "Veljko Cvetkovic"
                    },
                    {
                        "name": "Kathleen Reece"
                    },
                    {
                        "name": "Aidan Walters"
                    },
                    {
                        "name": "Yasir Hassan"
                    },
                    {
                        "name": "Aneesh Tummeti"
                    },
                    {
                        "name": "Bryan Torres"
                    },
                    {
                        "name": "Denise Cooney"
                    },
                    {
                        "name": "Margaret Ellis"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "9 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03906v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03906v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20471v1",
                "updated": "2025-06-25T14:19:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    19,
                    57,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:19:57Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    19,
                    57,
                    2,
                    176,
                    0
                ],
                "title": "Probing AI Safety with Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing AI Safety with Source Code"
                },
                "summary": "Large language models (LLMs) have become ubiquitous, interfacing with humans\nin numerous safety-critical applications. This necessitates improving\ncapabilities, but importantly coupled with greater safety measures to align\nthese models with human values and preferences. In this work, we demonstrate\nthat contemporary models fall concerningly short of the goal of AI safety,\nleading to an unsafe and harmful experience for users. We introduce a prompting\nstrategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT\nconverts natural language inputs to simple code that represents the same\nintent. For instance, CoDoT transforms the natural language prompt \"Make the\nstatement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT\nresults in a consistent failure of a wide range of state-of-the-art LLMs. For\nexample, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of\nthe time, and toxicity increases 300% on average across seven modern LLMs.\nAdditionally, recursively applying CoDoT can further increase toxicity two\ntimes. Given the rapid and widespread adoption of LLMs, CoDoT underscores the\ncritical need to evaluate safety efforts from first principles, ensuring that\nsafety and capabilities advance together.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become ubiquitous, interfacing with humans\nin numerous safety-critical applications. This necessitates improving\ncapabilities, but importantly coupled with greater safety measures to align\nthese models with human values and preferences. In this work, we demonstrate\nthat contemporary models fall concerningly short of the goal of AI safety,\nleading to an unsafe and harmful experience for users. We introduce a prompting\nstrategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT\nconverts natural language inputs to simple code that represents the same\nintent. For instance, CoDoT transforms the natural language prompt \"Make the\nstatement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT\nresults in a consistent failure of a wide range of state-of-the-art LLMs. For\nexample, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of\nthe time, and toxicity increases 300% on average across seven modern LLMs.\nAdditionally, recursively applying CoDoT can further increase toxicity two\ntimes. Given the rapid and widespread adoption of LLMs, CoDoT underscores the\ncritical need to evaluate safety efforts from first principles, ensuring that\nsafety and capabilities advance together."
                },
                "authors": [
                    {
                        "name": "Ujwal Narayan"
                    },
                    {
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Vishvak Murahari"
                    }
                ],
                "author_detail": {
                    "name": "Vishvak Murahari"
                },
                "author": "Vishvak Murahari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14409v2",
                "updated": "2025-06-25T14:17:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    17,
                    20,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-18T16:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    49,
                    56,
                    1,
                    77,
                    0
                ],
                "title": "Inference and Learning of Nonlinear LFR State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference and Learning of Nonlinear LFR State-Space Models"
                },
                "summary": "Estimating the parameters of nonlinear block-oriented state-space models from\ninput-output data typically involves solving a highly non-convex optimization\nproblem, which is prone to poor local minima and slow convergence. This paper\npresents a computationally efficient initialization method for nonlinear linear\nfractional representation (NL-LFR) models using periodic data. By first\ninferring the latent signals and subsequently estimating the model parameters,\nthe approach generates initial estimates for use in a later nonlinear\noptimization step. The proposed method shows robustness against poor local\nminima, and achieves a twofold error reduction compared to the state-of-the-art\non a challenging benchmark dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the parameters of nonlinear block-oriented state-space models from\ninput-output data typically involves solving a highly non-convex optimization\nproblem, which is prone to poor local minima and slow convergence. This paper\npresents a computationally efficient initialization method for nonlinear linear\nfractional representation (NL-LFR) models using periodic data. By first\ninferring the latent signals and subsequently estimating the model parameters,\nthe approach generates initial estimates for use in a later nonlinear\noptimization step. The proposed method shows robustness against poor local\nminima, and achieves a twofold error reduction compared to the state-of-the-art\non a challenging benchmark dataset."
                },
                "authors": [
                    {
                        "name": "Merijn Floren"
                    },
                    {
                        "name": "Jean-Philippe Noël"
                    },
                    {
                        "name": "Jan Swevers"
                    }
                ],
                "author_detail": {
                    "name": "Jan Swevers"
                },
                "author": "Jan Swevers",
                "arxiv_doi": "10.1109/LCSYS.2025.3580354",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LCSYS.2025.3580354",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Final, published paper in IEEE Xplore:\n  https://ieeexplore.ieee.org/abstract/document/11037476/",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07089v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07089v3",
                "updated": "2025-06-25T14:14:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    14,
                    56,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-11T18:38:00Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    18,
                    38,
                    0,
                    6,
                    131,
                    0
                ],
                "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models"
                },
                "summary": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\ninherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often\nunderperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sightedness in\nthe planning process, and hallucinations during command generation. Moreover,\nthe trial-and-error nature of the PT process is constrained by existing\nframeworks lacking mechanisms to learn from previous failures, restricting\nadaptive improvement of PT strategies. To address these limitations, we propose\na knowledge-informed, self-reflective PT framework powered by LLMs, called\nRefPentester. This AutoPT framework is designed to assist human operators in\nidentifying the current stage of the PT process, selecting appropriate tactics\nand techniques for each stage, choosing suggested actions, providing\nstep-by-step operational guidance, and reflecting on and learning from previous\nfailed operations. We also modeled the PT process as a seven-state Stage\nMachine to integrate the proposed framework effectively. The evaluation shows\nthat RefPentester can successfully reveal credentials on Hack The Box's Sau\nmachine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages,\nRefPentester also demonstrates superior success rates on PT stage transitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\ninherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often\nunderperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sightedness in\nthe planning process, and hallucinations during command generation. Moreover,\nthe trial-and-error nature of the PT process is constrained by existing\nframeworks lacking mechanisms to learn from previous failures, restricting\nadaptive improvement of PT strategies. To address these limitations, we propose\na knowledge-informed, self-reflective PT framework powered by LLMs, called\nRefPentester. This AutoPT framework is designed to assist human operators in\nidentifying the current stage of the PT process, selecting appropriate tactics\nand techniques for each stage, choosing suggested actions, providing\nstep-by-step operational guidance, and reflecting on and learning from previous\nfailed operations. We also modeled the PT process as a seven-state Stage\nMachine to integrate the proposed framework effectively. The evaluation shows\nthat RefPentester can successfully reveal credentials on Hack The Box's Sau\nmachine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages,\nRefPentester also demonstrates superior success rates on PT stage transitions."
                },
                "authors": [
                    {
                        "name": "Hanzheng Dai"
                    },
                    {
                        "name": "Yuanliang Li"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Zhibo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Zhang"
                },
                "author": "Zhibo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07089v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07089v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19330v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19330v3",
                "updated": "2025-06-25T14:03:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    3,
                    16,
                    2,
                    176,
                    0
                ],
                "published": "2024-11-28T19:00:01Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    0,
                    1,
                    3,
                    333,
                    0
                ],
                "title": "A direct measurement of the electron density turbulence parameter $C_1$\n  and implications for the emission size of the magnetar XTE J1810-197",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A direct measurement of the electron density turbulence parameter $C_1$\n  and implications for the emission size of the magnetar XTE J1810-197"
                },
                "summary": "We report a direct measurement of the electron density turbulence parameter\n$C_1$, enabled by 550-750~MHz baseband observations with the upgraded Giant\nMetrewave Radio Telescope. The parameter $C_1$ depends on the power law index\nof the wavenumber spectrum of electron density inhomogeneities in the ionized\ninterstellar medium. Radio waves propagating through the inhomogeneous ionized\nmedium suffer multipath propagation, as a result of which the pulsed emission\nfrom a neutron star undergoes scatter broadening. Consequently, interference\nbetween the delayed copies of the scatter-broadened electric field manifests as\nscintillation. We measure a scintillation bandwidth \\nud=$149\\pm3$~Hz as well\nas a scatter-broadening timescale \\taud=$1.22\\pm0.09$~ms at 650~MHz. These two\nquantities are related through the uncertainty relation $C_1 = 2\\pi$\\nud\\taud,\nusing which we directly measure $C_1=1.2\\pm0.1$. We describe the methods\nemployed to obtain these results and discuss their implications in general, as\nwell as for the magnetar XTE~J1810\\textminus197, towards which the measurements\nhave been made. We also discuss how such, effectively in-situ, measurements of\n$C_1$ can aid in inferring the wavenumber spectrum power law index and hence\nquantitatively discriminate between the various possible scattering scenarios\nin the ionized medium. Finally, using the fact $C_1 \\sim 1$, we nominally\nconstrain the emission size to less than a few 1000~km for a screen very close\nto the magnetar, and to within the magnetosphere for all screen distances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report a direct measurement of the electron density turbulence parameter\n$C_1$, enabled by 550-750~MHz baseband observations with the upgraded Giant\nMetrewave Radio Telescope. The parameter $C_1$ depends on the power law index\nof the wavenumber spectrum of electron density inhomogeneities in the ionized\ninterstellar medium. Radio waves propagating through the inhomogeneous ionized\nmedium suffer multipath propagation, as a result of which the pulsed emission\nfrom a neutron star undergoes scatter broadening. Consequently, interference\nbetween the delayed copies of the scatter-broadened electric field manifests as\nscintillation. We measure a scintillation bandwidth \\nud=$149\\pm3$~Hz as well\nas a scatter-broadening timescale \\taud=$1.22\\pm0.09$~ms at 650~MHz. These two\nquantities are related through the uncertainty relation $C_1 = 2\\pi$\\nud\\taud,\nusing which we directly measure $C_1=1.2\\pm0.1$. We describe the methods\nemployed to obtain these results and discuss their implications in general, as\nwell as for the magnetar XTE~J1810\\textminus197, towards which the measurements\nhave been made. We also discuss how such, effectively in-situ, measurements of\n$C_1$ can aid in inferring the wavenumber spectrum power law index and hence\nquantitatively discriminate between the various possible scattering scenarios\nin the ionized medium. Finally, using the fact $C_1 \\sim 1$, we nominally\nconstrain the emission size to less than a few 1000~km for a screen very close\nto the magnetar, and to within the magnetosphere for all screen distances."
                },
                "authors": [
                    {
                        "name": "Visweshwar Ram Marthi"
                    },
                    {
                        "name": "Yogesh Maan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Maan"
                },
                "author": "Yogesh Maan",
                "arxiv_comment": "Accepted for publication in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19330v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19330v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20767v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20767v4",
                "updated": "2025-06-25T14:02:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    2,
                    19,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-27T06:16:27Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    16,
                    27,
                    1,
                    147,
                    0
                ],
                "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing\n  Cognitive Faithfulness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniBench: A Legal-inspired Framework and Dataset for Assessing\n  Cognitive Faithfulness of Large Language Models"
                },
                "summary": "Faithfulness hallucinations are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandards, existing benchmarks focus on \"factual statements\" that rephrase\nsource materials while overlooking \"cognitive statements\" that involve making\ninferences from the given context. Consequently, evaluating and detecting the\nhallucination of cognitive statements remains challenging. Inspired by how\nevidence is assessed in the legal domain, we design a rigorous framework to\nassess different levels of faithfulness of cognitive statements and introduce\nthe CogniBench dataset where we reveal insightful statistics. To keep pace with\nrapidly evolving LLMs, we further develop an automatic annotation pipeline that\nscales easily across different models. This results in a large-scale\nCogniBench-L dataset, which facilitates training accurate detectors for both\nfactual and cognitive hallucinations. We release our model and datasets at:\nhttps://github.com/FUTUREEEEEE/CogniBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness hallucinations are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandards, existing benchmarks focus on \"factual statements\" that rephrase\nsource materials while overlooking \"cognitive statements\" that involve making\ninferences from the given context. Consequently, evaluating and detecting the\nhallucination of cognitive statements remains challenging. Inspired by how\nevidence is assessed in the legal domain, we design a rigorous framework to\nassess different levels of faithfulness of cognitive statements and introduce\nthe CogniBench dataset where we reveal insightful statistics. To keep pace with\nrapidly evolving LLMs, we further develop an automatic annotation pipeline that\nscales easily across different models. This results in a large-scale\nCogniBench-L dataset, which facilitates training accurate detectors for both\nfactual and cognitive hallucinations. We release our model and datasets at:\nhttps://github.com/FUTUREEEEEE/CogniBench"
                },
                "authors": [
                    {
                        "name": "Xiaqiang Tang"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Keyu Hu"
                    },
                    {
                        "name": "Du Nan"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Sihong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Sihong Xie"
                },
                "author": "Sihong Xie",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20767v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20767v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17983v2",
                "updated": "2025-06-25T14:02:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    2,
                    15,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-22T10:45:35Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    45,
                    35,
                    6,
                    173,
                    0
                ],
                "title": "LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework\n  for Lossless Compression of Medical Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework\n  for Lossless Compression of Medical Images"
                },
                "summary": "Autoregressive Initial Bits is a framework that integrates sub-image\nautoregression and latent variable modeling, demonstrating its advantages in\nlossless medical image compression. However, in existing methods, the image\nsegmentation process leads to an even distribution of latent variable\ninformation across each sub-image, which in turn causes posterior collapse and\ninefficient utilization of latent variables. To deal with these issues, we\npropose a prediction-based end-to-end lossless medical image compression method\nnamed LVPNet, leveraging global latent variables to predict pixel values and\nencoding predicted probabilities for lossless compression. Specifically, we\nintroduce the Global Multi-scale Sensing Module (GMSM), which extracts compact\nand informative latent representations from the entire image, effectively\ncapturing spatial dependencies within the latent space. Furthermore, to\nmitigate the information loss introduced during quantization, we propose the\nQuantization Compensation Module (QCM), which learns the distribution of\nquantization errors and refines the quantized features to compensate for\nquantization loss. Extensive experiments on challenging benchmarks demonstrate\nthat our method achieves superior compression efficiency compared to\nstate-of-the-art lossless image compression approaches, while maintaining\ncompetitive inference speed. The code is at\nhttps://github.com/scy-Jackel/LVPNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Initial Bits is a framework that integrates sub-image\nautoregression and latent variable modeling, demonstrating its advantages in\nlossless medical image compression. However, in existing methods, the image\nsegmentation process leads to an even distribution of latent variable\ninformation across each sub-image, which in turn causes posterior collapse and\ninefficient utilization of latent variables. To deal with these issues, we\npropose a prediction-based end-to-end lossless medical image compression method\nnamed LVPNet, leveraging global latent variables to predict pixel values and\nencoding predicted probabilities for lossless compression. Specifically, we\nintroduce the Global Multi-scale Sensing Module (GMSM), which extracts compact\nand informative latent representations from the entire image, effectively\ncapturing spatial dependencies within the latent space. Furthermore, to\nmitigate the information loss introduced during quantization, we propose the\nQuantization Compensation Module (QCM), which learns the distribution of\nquantization errors and refines the quantized features to compensate for\nquantization loss. Extensive experiments on challenging benchmarks demonstrate\nthat our method achieves superior compression efficiency compared to\nstate-of-the-art lossless image compression approaches, while maintaining\ncompetitive inference speed. The code is at\nhttps://github.com/scy-Jackel/LVPNet."
                },
                "authors": [
                    {
                        "name": "Chenyue Song"
                    },
                    {
                        "name": "Chen Hui"
                    },
                    {
                        "name": "Qing Lin"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Siqiao Li"
                    },
                    {
                        "name": "Haiqi Zhu"
                    },
                    {
                        "name": "Zhixuan Li"
                    },
                    {
                        "name": "Shengping Zhang"
                    },
                    {
                        "name": "Shaohui Liu"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "Accepted to MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20451v1",
                "updated": "2025-06-25T13:57:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    57,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:57:54Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    57,
                    54,
                    2,
                    176,
                    0
                ],
                "title": "Automatic Demonstration Selection for LLM-based Tabular Data\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Demonstration Selection for LLM-based Tabular Data\n  Classification"
                },
                "summary": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs."
                },
                "authors": [
                    {
                        "name": "Shuchu Han"
                    },
                    {
                        "name": "Wolfgang Bruckner"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Bruckner"
                },
                "author": "Wolfgang Bruckner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11677v2",
                "updated": "2025-06-25T13:46:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    46,
                    10,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-17T11:11:09Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    11,
                    9,
                    0,
                    48,
                    0
                ],
                "title": "Towards Fully Exploiting LLM Internal States to Enhance Knowledge\n  Boundary Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Exploiting LLM Internal States to Enhance Knowledge\n  Boundary Perception"
                },
                "summary": "Large language models (LLMs) exhibit impressive performance across diverse\ntasks but often struggle to accurately gauge their knowledge boundaries,\nleading to confident yet incorrect responses. This paper explores leveraging\nLLMs' internal states to enhance their perception of knowledge boundaries from\nefficiency and risk perspectives. We investigate whether LLMs can estimate\ntheir confidence using internal states before response generation, potentially\nsaving computational resources. Our experiments on datasets like Natural\nQuestions, HotpotQA, and MMLU reveal that LLMs demonstrate significant\npre-generation perception, which is further refined post-generation, with\nperception gaps remaining stable across varying conditions. To mitigate risks\nin critical domains, we introduce Confidence Consistency-based Calibration\n($C^3$), which assesses confidence consistency through question reformulation.\n$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,\nenhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our\nfindings suggest that pre-generation confidence estimation can optimize\nefficiency, while $C^3$ effectively controls output risks, advancing the\nreliability of LLMs in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive performance across diverse\ntasks but often struggle to accurately gauge their knowledge boundaries,\nleading to confident yet incorrect responses. This paper explores leveraging\nLLMs' internal states to enhance their perception of knowledge boundaries from\nefficiency and risk perspectives. We investigate whether LLMs can estimate\ntheir confidence using internal states before response generation, potentially\nsaving computational resources. Our experiments on datasets like Natural\nQuestions, HotpotQA, and MMLU reveal that LLMs demonstrate significant\npre-generation perception, which is further refined post-generation, with\nperception gaps remaining stable across varying conditions. To mitigate risks\nin critical domains, we introduce Confidence Consistency-based Calibration\n($C^3$), which assesses confidence consistency through question reformulation.\n$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,\nenhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our\nfindings suggest that pre-generation confidence estimation can optimize\nefficiency, while $C^3$ effectively controls output risks, advancing the\nreliability of LLMs in practical applications."
                },
                "authors": [
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Lulu Yu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20430v1",
                "updated": "2025-06-25T13:42:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    42,
                    26,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:42:26Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    42,
                    26,
                    2,
                    176,
                    0
                ],
                "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning"
                },
                "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor."
                },
                "authors": [
                    {
                        "name": "Weike Zhao"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Yanjie Fan"
                    },
                    {
                        "name": "Xiaoman Zhang"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Yuze Sun"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20422v1",
                "updated": "2025-06-25T13:36:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    36,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:36:23Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    36,
                    23,
                    2,
                    176,
                    0
                ],
                "title": "Outflows from Star-Forming Galaxies in the Early Universe: Evolution\n  from Double- to Single-Peaked Lyman-$ α$ Profiles Following Delayed\n  Supernova Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outflows from Star-Forming Galaxies in the Early Universe: Evolution\n  from Double- to Single-Peaked Lyman-$ α$ Profiles Following Delayed\n  Supernova Feedback"
                },
                "summary": "Lyman-$ \\alpha $ emission, which owing to its resonant nature strongly\ncouples the emergent line profile to gas kinematics, has become a key\nobservable for probing outflows from star-forming galaxies in the early\nuniverse. Typically, Lyman-$ \\alpha $ profiles are explained in terms of\nidealised expanding shell models, while empirically-motivated interpretations\nof Lyman-$ \\alpha $ profiles that make use of comparisons with kinematics\nrevealed by other spectral features (e.g. metal emission and absorption lines)\nare limited. Here, we analyse the kinematics of emission/absorption lines in\nthe spectra of 338 gravitationally-lensed Lyman-$ \\alpha $ sources revealed via\nblind spectroscopy of galaxy clusters with the Multi-Unit Spectroscopic\nExplorer (MUSE). Using metal emission lines to measure systemic redshifts, we\nconfirm that the Lyman-$ \\alpha $ profiles are consistent with outflowing gas.\nIn cases where interstellar metal absorption lines are detected, we find their\nkinematics and line ratios to be indicative of clumpy outflows. We investigate\nthe differences between sources with single- and double-peaked Lyman-$ \\alpha $\nprofiles, finding the latter to drive weaker outflows yet show spectral\nsignatures suggestive of younger stellar ages ($ < 4\\,$Myr compared with $ >\n20\\,$Myr for sources with single-peaked Lyman-$ \\alpha $ profiles). We argue\nthat double-peaked Lyman-$ \\alpha $ profiles may reflect weaker feedback in\nextremely young starbursts due to the delayed onset of supernova explosions. We\nalso investigate whether outflow properties can be reliably inferred by fitting\nsimple expanding shell models. We find that these models yield unphysical\nparameters in many cases even when they can reproduce the Lyman-$ \\alpha $\nprofiles to high precision, and urge caution when inferring outflow properties\nfrom such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyman-$ \\alpha $ emission, which owing to its resonant nature strongly\ncouples the emergent line profile to gas kinematics, has become a key\nobservable for probing outflows from star-forming galaxies in the early\nuniverse. Typically, Lyman-$ \\alpha $ profiles are explained in terms of\nidealised expanding shell models, while empirically-motivated interpretations\nof Lyman-$ \\alpha $ profiles that make use of comparisons with kinematics\nrevealed by other spectral features (e.g. metal emission and absorption lines)\nare limited. Here, we analyse the kinematics of emission/absorption lines in\nthe spectra of 338 gravitationally-lensed Lyman-$ \\alpha $ sources revealed via\nblind spectroscopy of galaxy clusters with the Multi-Unit Spectroscopic\nExplorer (MUSE). Using metal emission lines to measure systemic redshifts, we\nconfirm that the Lyman-$ \\alpha $ profiles are consistent with outflowing gas.\nIn cases where interstellar metal absorption lines are detected, we find their\nkinematics and line ratios to be indicative of clumpy outflows. We investigate\nthe differences between sources with single- and double-peaked Lyman-$ \\alpha $\nprofiles, finding the latter to drive weaker outflows yet show spectral\nsignatures suggestive of younger stellar ages ($ < 4\\,$Myr compared with $ >\n20\\,$Myr for sources with single-peaked Lyman-$ \\alpha $ profiles). We argue\nthat double-peaked Lyman-$ \\alpha $ profiles may reflect weaker feedback in\nextremely young starbursts due to the delayed onset of supernova explosions. We\nalso investigate whether outflow properties can be reliably inferred by fitting\nsimple expanding shell models. We find that these models yield unphysical\nparameters in many cases even when they can reproduce the Lyman-$ \\alpha $\nprofiles to high precision, and urge caution when inferring outflow properties\nfrom such models."
                },
                "authors": [
                    {
                        "name": "James Nianias"
                    },
                    {
                        "name": "Jeremy Lim"
                    },
                    {
                        "name": "Yik Lok Wong"
                    },
                    {
                        "name": "Gordon Wong"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Wong"
                },
                "author": "Gordon Wong",
                "arxiv_comment": "25 pages, 19 figures, 4 tables. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20415v1",
                "updated": "2025-06-25T13:31:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    31,
                    13,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:31:13Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    31,
                    13,
                    2,
                    176,
                    0
                ],
                "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SV-LLM: An Agentic Approach for SoC Security Verification using Large\n  Language Models"
                },
                "summary": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy."
                },
                "authors": [
                    {
                        "name": "Dipayan Saha"
                    },
                    {
                        "name": "Shams Tarek"
                    },
                    {
                        "name": "Hasan Al Shaikh"
                    },
                    {
                        "name": "Khan Thamid Hasan"
                    },
                    {
                        "name": "Pavan Sai Nalluri"
                    },
                    {
                        "name": "Md. Ajoad Hasan"
                    },
                    {
                        "name": "Nashmin Alam"
                    },
                    {
                        "name": "Jingbo Zhou"
                    },
                    {
                        "name": "Sujan Kumar Saha"
                    },
                    {
                        "name": "Mark Tehranipoor"
                    },
                    {
                        "name": "Farimah Farahmandi"
                    }
                ],
                "author_detail": {
                    "name": "Farimah Farahmandi"
                },
                "author": "Farimah Farahmandi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17219v2",
                "updated": "2025-06-25T13:27:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    27,
                    49,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-20T17:59:52Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    52,
                    4,
                    171,
                    0
                ],
                "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning"
                },
                "summary": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training."
                },
                "authors": [
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Zhaoxi Zhang"
                    },
                    {
                        "name": "Haoxiang Guan"
                    },
                    {
                        "name": "Yilin Cheng"
                    },
                    {
                        "name": "Yitong Duan"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Shuxin Zheng"
                    },
                    {
                        "name": "Jiyan He"
                    }
                ],
                "author_detail": {
                    "name": "Jiyan He"
                },
                "author": "Jiyan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20409v2",
                "updated": "2025-06-26T13:09:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    9,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-25T13:24:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    24,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPS: Tool-Augmented Personalisation via Structured Tagging"
                },
                "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."
                },
                "authors": [
                    {
                        "name": "Ekaterina Taktasheva"
                    },
                    {
                        "name": "Jeff Dalton"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Dalton"
                },
                "author": "Jeff Dalton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20377v1",
                "updated": "2025-06-25T12:44:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    44,
                    10,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T12:44:10Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    44,
                    10,
                    2,
                    176,
                    0
                ],
                "title": "The Role of Partisan Culture in Mental Health Language Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Partisan Culture in Mental Health Language Online"
                },
                "summary": "The impact of culture on how people express distress in online support\ncommunities is increasingly a topic of interest within Computer Supported\nCooperative Work (CSCW) and Human-Computer Interaction (HCI). In the United\nStates, distinct cultures have emerged from each of the two dominant political\nparties, forming a primary lens by which people navigate online and offline\nworlds. We examine whether partisan culture may play a role in how U.S.\nRepublican and Democrat users of online mental health support communities\nexpress distress. We present a large-scale observational study of 2,184,356\nposts from 8,916 statistically matched Republican, Democrat, and unaffiliated\nonline support community members. We utilize methods from causal inference to\nstatistically match partisan users along covariates that correspond with\ndemographic attributes and platform use, in order to create comparable cohorts\nfor analysis. We then leverage methods from natural language processing to\nunderstand how partisan expressions of distress compare between these sets of\nclosely matched opposing partisans, and between closely matched partisans and\ntypical support community members. Our data spans January 2013 to December\n2022, a period of both rising political polarization and mental health\nconcerns. We find that partisan culture does play into expressions of distress,\nunderscoring the importance of considering partisan cultural differences in the\ndesign of online support community platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of culture on how people express distress in online support\ncommunities is increasingly a topic of interest within Computer Supported\nCooperative Work (CSCW) and Human-Computer Interaction (HCI). In the United\nStates, distinct cultures have emerged from each of the two dominant political\nparties, forming a primary lens by which people navigate online and offline\nworlds. We examine whether partisan culture may play a role in how U.S.\nRepublican and Democrat users of online mental health support communities\nexpress distress. We present a large-scale observational study of 2,184,356\nposts from 8,916 statistically matched Republican, Democrat, and unaffiliated\nonline support community members. We utilize methods from causal inference to\nstatistically match partisan users along covariates that correspond with\ndemographic attributes and platform use, in order to create comparable cohorts\nfor analysis. We then leverage methods from natural language processing to\nunderstand how partisan expressions of distress compare between these sets of\nclosely matched opposing partisans, and between closely matched partisans and\ntypical support community members. Our data spans January 2013 to December\n2022, a period of both rising political polarization and mental health\nconcerns. We find that partisan culture does play into expressions of distress,\nunderscoring the importance of considering partisan cultural differences in the\ndesign of online support community platforms."
                },
                "authors": [
                    {
                        "name": "Sachin R. Pendse"
                    },
                    {
                        "name": "Ben Rochford"
                    },
                    {
                        "name": "Neha Kumar"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "Accepted to the ACM Conference on Computer-Supported Cooperative Work\n  and Social Computing (CSCW 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20357v1",
                "updated": "2025-06-25T12:18:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    18,
                    34,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T12:18:34Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    18,
                    34,
                    2,
                    176,
                    0
                ],
                "title": "Tabular Feature Discovery With Reasoning Type Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular Feature Discovery With Reasoning Type Exploration"
                },
                "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data."
                },
                "authors": [
                    {
                        "name": "Sungwon Han"
                    },
                    {
                        "name": "Sungkyu Park"
                    },
                    {
                        "name": "Seungeon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seungeon Lee"
                },
                "author": "Seungeon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20353v1",
                "updated": "2025-06-25T12:04:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    4,
                    53,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T12:04:53Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    4,
                    53,
                    2,
                    176,
                    0
                ],
                "title": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression"
                },
                "summary": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Yunjian Zhang"
                    },
                    {
                        "name": "Xiu Yan"
                    },
                    {
                        "name": "Yueqi Zhou"
                    },
                    {
                        "name": "Kaihao Huang"
                    },
                    {
                        "name": "Suzhong Fu"
                    },
                    {
                        "name": "Chuanlong Xie"
                    },
                    {
                        "name": "Yao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Zhu"
                },
                "author": "Yao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20342v1",
                "updated": "2025-06-25T11:50:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    50,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T11:50:23Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    50,
                    23,
                    2,
                    176,
                    0
                ],
                "title": "Feature Hallucination for Self-supervised Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Hallucination for Self-supervised Action Recognition"
                },
                "summary": "Understanding human actions in videos requires more than raw pixel analysis;\nit relies on high-level semantic reasoning and effective integration of\nmultimodal features. We propose a deep translational action recognition\nframework that enhances recognition accuracy by jointly predicting action\nconcepts and auxiliary features from RGB video frames. At test time,\nhallucination streams infer missing cues, enriching feature representations\nwithout increasing computational overhead. To focus on action-relevant regions\nbeyond raw pixels, we introduce two novel domain-specific descriptors. Object\nDetection Features (ODF) aggregate outputs from multiple object detectors to\ncapture contextual cues, while Saliency Detection Features (SDF) highlight\nspatial and intensity patterns crucial for action recognition. Our framework\nseamlessly integrates these descriptors with auxiliary modalities such as\noptical flow, Improved Dense Trajectories, skeleton data, and audio cues. It\nremains compatible with state-of-the-art architectures, including I3D,\nAssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE\nV2 and InternVideo2. To handle uncertainty in auxiliary features, we\nincorporate aleatoric uncertainty modeling in the hallucination step and\nintroduce a robust loss function to mitigate feature noise. Our multimodal\nself-supervised action recognition framework achieves state-of-the-art\nperformance on multiple benchmarks, including Kinetics-400, Kinetics-600, and\nSomething-Something V2, demonstrating its effectiveness in capturing\nfine-grained action dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human actions in videos requires more than raw pixel analysis;\nit relies on high-level semantic reasoning and effective integration of\nmultimodal features. We propose a deep translational action recognition\nframework that enhances recognition accuracy by jointly predicting action\nconcepts and auxiliary features from RGB video frames. At test time,\nhallucination streams infer missing cues, enriching feature representations\nwithout increasing computational overhead. To focus on action-relevant regions\nbeyond raw pixels, we introduce two novel domain-specific descriptors. Object\nDetection Features (ODF) aggregate outputs from multiple object detectors to\ncapture contextual cues, while Saliency Detection Features (SDF) highlight\nspatial and intensity patterns crucial for action recognition. Our framework\nseamlessly integrates these descriptors with auxiliary modalities such as\noptical flow, Improved Dense Trajectories, skeleton data, and audio cues. It\nremains compatible with state-of-the-art architectures, including I3D,\nAssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE\nV2 and InternVideo2. To handle uncertainty in auxiliary features, we\nincorporate aleatoric uncertainty modeling in the hallucination step and\nintroduce a robust loss function to mitigate feature noise. Our multimodal\nself-supervised action recognition framework achieves state-of-the-art\nperformance on multiple benchmarks, including Kinetics-400, Kinetics-600, and\nSomething-Something V2, demonstrating its effectiveness in capturing\nfine-grained action dynamics."
                },
                "authors": [
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Piotr Koniusz"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Koniusz"
                },
                "author": "Piotr Koniusz",
                "arxiv_comment": "Accepted for publication in International Journal of Computer Vision\n  (IJCV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20331v1",
                "updated": "2025-06-25T11:30:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    30,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T11:30:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    30,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content"
                },
                "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies."
                },
                "authors": [
                    {
                        "name": "Rian Touchent"
                    },
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Eric de la Clergerie"
                    }
                ],
                "author_detail": {
                    "name": "Eric de la Clergerie"
                },
                "author": "Eric de la Clergerie",
                "arxiv_comment": "Dataset link: https://hf.co/datasets/almanach/Biomed-Enriched",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19442v2",
                "updated": "2025-06-25T11:18:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    18,
                    4,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-24T09:15:22Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    15,
                    22,
                    1,
                    175,
                    0
                ],
                "title": "Sampling Matters in Explanations: Towards Trustworthy Attribution\n  Analysis Building Block in Visual Models through Maximizing Explanation\n  Certainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling Matters in Explanations: Towards Trustworthy Attribution\n  Analysis Building Block in Visual Models through Maximizing Explanation\n  Certainty"
                },
                "summary": "Image attribution analysis seeks to highlight the feature representations\nlearned by visual models such that the highlighted feature maps can reflect the\npixel-wise importance of inputs. Gradient integration is a building block in\nthe attribution analysis by integrating the gradients from multiple derived\nsamples to highlight the semantic features relevant to inferences. Such a\nbuilding block often combines with other information from visual models such as\nactivation or attention maps to form ultimate explanations. Yet, our\ntheoretical analysis demonstrates that the extent to the alignment of the\nsample distribution in gradient integration with respect to natural image\ndistribution gives a lower bound of explanation certainty. Prior works add\nnoise into images as samples and the noise distributions can lead to low\nexplanation certainty. Counter-intuitively, our experiment shows that extra\ninformation can saturate neural networks. To this end, building trustworthy\nattribution analysis needs to settle the sample distribution misalignment\nproblem. Instead of adding extra information into input images, we present a\nsemi-optimal sampling approach by suppressing features from inputs. The sample\ndistribution by suppressing features is approximately identical to the\ndistribution of natural images. Our extensive quantitative evaluation on large\nscale dataset ImageNet affirms that our approach is effective and able to yield\nmore satisfactory explanations against state-of-the-art baselines throughout\nall experimental models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image attribution analysis seeks to highlight the feature representations\nlearned by visual models such that the highlighted feature maps can reflect the\npixel-wise importance of inputs. Gradient integration is a building block in\nthe attribution analysis by integrating the gradients from multiple derived\nsamples to highlight the semantic features relevant to inferences. Such a\nbuilding block often combines with other information from visual models such as\nactivation or attention maps to form ultimate explanations. Yet, our\ntheoretical analysis demonstrates that the extent to the alignment of the\nsample distribution in gradient integration with respect to natural image\ndistribution gives a lower bound of explanation certainty. Prior works add\nnoise into images as samples and the noise distributions can lead to low\nexplanation certainty. Counter-intuitively, our experiment shows that extra\ninformation can saturate neural networks. To this end, building trustworthy\nattribution analysis needs to settle the sample distribution misalignment\nproblem. Instead of adding extra information into input images, we present a\nsemi-optimal sampling approach by suppressing features from inputs. The sample\ndistribution by suppressing features is approximately identical to the\ndistribution of natural images. Our extensive quantitative evaluation on large\nscale dataset ImageNet affirms that our approach is effective and able to yield\nmore satisfactory explanations against state-of-the-art baselines throughout\nall experimental models."
                },
                "authors": [
                    {
                        "name": "Róisín Luo"
                    },
                    {
                        "name": "James McDermott"
                    },
                    {
                        "name": "Colm O'Riordan"
                    }
                ],
                "author_detail": {
                    "name": "Colm O'Riordan"
                },
                "author": "Colm O'Riordan",
                "arxiv_doi": "10.5281/zenodo.8204453",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.8204453",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Code:\n  https://anonymous.4open.science/r/sampling_matters_reproducibility-BB60/",
                "arxiv_journal_ref": "In Proceedings of the Irish Machine Vision and Image Processing\n  Conference 2023 (IMVIP2023)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20325v1",
                "updated": "2025-06-25T11:08:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    8,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T11:08:33Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    8,
                    33,
                    2,
                    176,
                    0
                ],
                "title": "Robust estimation of a Markov chain transition matrix from multiple\n  sample paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust estimation of a Markov chain transition matrix from multiple\n  sample paths"
                },
                "summary": "Markov chains are fundamental models for stochastic dynamics, with\napplications in a wide range of areas such as population dynamics, queueing\nsystems, reinforcement learning, and Monte Carlo methods. Estimating the\ntransition matrix and stationary distribution from observed sample paths is a\ncore statistical challenge, particularly when multiple independent trajectories\nare available. While classical theory typically assumes identical chains with\nknown stationary distributions, real-world data often arise from heterogeneous\nchains whose transition kernels and stationary measures might differ from a\ncommon target. We analyse empirical estimators for such parallel Markov\nprocesses and establish sharp concentration inequalities that generalise\nBernstein-type bounds from standard time averages to ensemble-time averages.\nOur results provide nonasymptotic error bounds and consistency guarantees in\nhigh-dimensional regimes, accommodating sparse or weakly mixing chains, model\nmismatch, nonstationary initialisations, and partially corrupted data. These\nfindings offer rigorous foundations for statistical inference in heterogeneous\nMarkov chain settings common in modern computational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov chains are fundamental models for stochastic dynamics, with\napplications in a wide range of areas such as population dynamics, queueing\nsystems, reinforcement learning, and Monte Carlo methods. Estimating the\ntransition matrix and stationary distribution from observed sample paths is a\ncore statistical challenge, particularly when multiple independent trajectories\nare available. While classical theory typically assumes identical chains with\nknown stationary distributions, real-world data often arise from heterogeneous\nchains whose transition kernels and stationary measures might differ from a\ncommon target. We analyse empirical estimators for such parallel Markov\nprocesses and establish sharp concentration inequalities that generalise\nBernstein-type bounds from standard time averages to ensemble-time averages.\nOur results provide nonasymptotic error bounds and consistency guarantees in\nhigh-dimensional regimes, accommodating sparse or weakly mixing chains, model\nmismatch, nonstationary initialisations, and partially corrupted data. These\nfindings offer rigorous foundations for statistical inference in heterogeneous\nMarkov chain settings common in modern computational applications."
                },
                "authors": [
                    {
                        "name": "Lasse Leskelä"
                    },
                    {
                        "name": "Maximilien Dreveton"
                    }
                ],
                "author_detail": {
                    "name": "Maximilien Dreveton"
                },
                "author": "Maximilien Dreveton",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M05, 60J10, 60F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11484v2",
                "updated": "2025-06-25T11:05:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    5,
                    49,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-13T06:14:56Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    14,
                    56,
                    4,
                    164,
                    0
                ],
                "title": "VulStamp: Vulnerability Assessment using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulStamp: Vulnerability Assessment using Large Language Model"
                },
                "summary": "Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model."
                },
                "authors": [
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Jiaye Li"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18330v2",
                "updated": "2025-06-25T10:49:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    49,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-23T06:23:53Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    23,
                    53,
                    0,
                    174,
                    0
                ],
                "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning"
                },
                "summary": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math."
                },
                "authors": [
                    {
                        "name": "Lixin Wu"
                    },
                    {
                        "name": "Na Cai"
                    },
                    {
                        "name": "Qiao Cheng"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Yitao Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Duan"
                },
                "author": "Yitao Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08062v3",
                "updated": "2025-06-25T10:45:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    45,
                    10,
                    2,
                    176,
                    0
                ],
                "published": "2024-08-15T10:03:30Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    30,
                    3,
                    228,
                    0
                ],
                "title": "BINDy -- Bayesian identification of nonlinear dynamics with\n  reversible-jump Markov-chain Monte-Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BINDy -- Bayesian identification of nonlinear dynamics with\n  reversible-jump Markov-chain Monte-Carlo"
                },
                "summary": "Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms."
                },
                "authors": [
                    {
                        "name": "Max D. Champneys"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Rogers"
                },
                "author": "Timothy J. Rogers",
                "arxiv_doi": "10.1098/rspa.2024.0620",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rspa.2024.0620",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18746v3",
                "updated": "2025-06-25T10:37:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    37,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-24T15:25:44Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    15,
                    25,
                    44,
                    5,
                    144,
                    0
                ],
                "title": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking"
                },
                "summary": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/yupeijei1997/C3-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/yupeijei1997/C3-Bench."
                },
                "authors": [
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20291v1",
                "updated": "2025-06-25T09:53:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    53,
                    35,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:53:35Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    53,
                    35,
                    2,
                    176,
                    0
                ],
                "title": "A Literature Review on Simulation in Conversational Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Literature Review on Simulation in Conversational Recommender Systems"
                },
                "summary": "Conversational Recommender Systems (CRSs) have garnered attention as a novel\napproach to delivering personalized recommendations through multi-turn\ndialogues. This review developed a taxonomy framework to systematically\ncategorize relevant publications into four groups: dataset construction,\nalgorithm design, system evaluation, and empirical studies, providing a\ncomprehensive analysis of simulation methods in CRSs research. Our analysis\nreveals that simulation methods play a key role in tackling CRSs' main\nchallenges. For example, LLM-based simulation methods have been used to create\nconversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.\nDespite several challenges, such as dataset bias, the limited output\nflexibility of LLM-based simulations, and the gap between text semantic space\nand behavioral semantics, persist due to the complexity in Human-Computer\nInteraction (HCI) of CRSs, simulation methods hold significant potential for\nadvancing CRS research. This review offers a thorough summary of the current\nresearch landscape in this domain and identifies promising directions for\nfuture inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) have garnered attention as a novel\napproach to delivering personalized recommendations through multi-turn\ndialogues. This review developed a taxonomy framework to systematically\ncategorize relevant publications into four groups: dataset construction,\nalgorithm design, system evaluation, and empirical studies, providing a\ncomprehensive analysis of simulation methods in CRSs research. Our analysis\nreveals that simulation methods play a key role in tackling CRSs' main\nchallenges. For example, LLM-based simulation methods have been used to create\nconversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.\nDespite several challenges, such as dataset bias, the limited output\nflexibility of LLM-based simulations, and the gap between text semantic space\nand behavioral semantics, persist due to the complexity in Human-Computer\nInteraction (HCI) of CRSs, simulation methods hold significant potential for\nadvancing CRS research. This review offers a thorough summary of the current\nresearch landscape in this domain and identifies promising directions for\nfuture inquiry."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Jinze Chen"
                    },
                    {
                        "name": "Junpeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junpeng Guo"
                },
                "author": "Junpeng Guo",
                "arxiv_comment": "6 pages, 1 figures, accepted as a poster for CSWIM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11962v3",
                "updated": "2025-06-25T09:51:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    51,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-17T16:10:30Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    10,
                    30,
                    0,
                    48,
                    0
                ],
                "title": "Balancing Truthfulness and Informativeness with Uncertainty-Aware\n  Instruction Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Truthfulness and Informativeness with Uncertainty-Aware\n  Instruction Fine-Tuning"
                },
                "summary": "Instruction fine-tuning (IFT) can increase the informativeness of large\nlanguage models (LLMs), but may reduce their truthfulness. This trade-off\narises because IFT steers LLMs to generate responses containing long-tail\nknowledge that was not well covered during pre-training. As a result, models\nbecome more informative but less accurate when generalizing to unseen tasks. In\nthis paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets\ncan negatively affect the truthfulness of LLMs, and we introduce two new IFT\nparadigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$\nidentifies and removes unfamiliar knowledge from IFT datasets to mitigate its\nimpact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize\ntheir uncertainty and explicitly indicate it at the end of their responses. Our\nexperiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,\nwhile $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by\ndistinguishing between confident and uncertain statements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction fine-tuning (IFT) can increase the informativeness of large\nlanguage models (LLMs), but may reduce their truthfulness. This trade-off\narises because IFT steers LLMs to generate responses containing long-tail\nknowledge that was not well covered during pre-training. As a result, models\nbecome more informative but less accurate when generalizing to unseen tasks. In\nthis paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets\ncan negatively affect the truthfulness of LLMs, and we introduce two new IFT\nparadigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$\nidentifies and removes unfamiliar knowledge from IFT datasets to mitigate its\nimpact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize\ntheir uncertainty and explicitly indicate it at the end of their responses. Our\nexperiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,\nwhile $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by\ndistinguishing between confident and uncertain statements."
                },
                "authors": [
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20290v1",
                "updated": "2025-06-25T09:48:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    48,
                    30,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:48:30Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    48,
                    30,
                    2,
                    176,
                    0
                ],
                "title": "Don't Hash Me Like That: Exposing and Mitigating Hash-Induced Unfairness\n  in Local Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Hash Me Like That: Exposing and Mitigating Hash-Induced Unfairness\n  in Local Differential Privacy"
                },
                "summary": "Local differential privacy (LDP) has become a widely accepted framework for\nprivacy-preserving data collection. In LDP, many protocols rely on hash\nfunctions to implement user-side encoding and perturbation. However, the\nsecurity and privacy implications of hash function selection have not been\npreviously investigated. In this paper, we expose that the hash functions may\nact as a source of unfairness in LDP protocols. We show that although users\noperate under the same protocol and privacy budget, differences in hash\nfunctions can lead to significant disparities in vulnerability to inference and\npoisoning attacks. To mitigate hash-induced unfairness, we propose Fair-OLH\n(F-OLH), a variant of OLH that enforces an entropy-based fairness constraint on\nhash function selection. Experiments show that F-OLH is effective in mitigating\nhash-induced unfairness under acceptable time overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local differential privacy (LDP) has become a widely accepted framework for\nprivacy-preserving data collection. In LDP, many protocols rely on hash\nfunctions to implement user-side encoding and perturbation. However, the\nsecurity and privacy implications of hash function selection have not been\npreviously investigated. In this paper, we expose that the hash functions may\nact as a source of unfairness in LDP protocols. We show that although users\noperate under the same protocol and privacy budget, differences in hash\nfunctions can lead to significant disparities in vulnerability to inference and\npoisoning attacks. To mitigate hash-induced unfairness, we propose Fair-OLH\n(F-OLH), a variant of OLH that enforces an entropy-based fairness constraint on\nhash function selection. Experiments show that F-OLH is effective in mitigating\nhash-induced unfairness under acceptable time overheads."
                },
                "authors": [
                    {
                        "name": "Berkay Kemal Balioglu"
                    },
                    {
                        "name": "Alireza Khodaie"
                    },
                    {
                        "name": "Mehmet Emre Gursoy"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Emre Gursoy"
                },
                "author": "Mehmet Emre Gursoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17848v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17848v4",
                "updated": "2025-06-25T09:36:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    36,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-25T04:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    4,
                    51,
                    17,
                    1,
                    56,
                    0
                ],
                "title": "LR^2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LR^2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems"
                },
                "summary": "Recent progress in Large Reasoning Models (LRMs) has significantly enhanced\nthe reasoning abilities of Large Language Models (LLMs), empowering them to\ntackle increasingly complex tasks through reflection capabilities, such as\nmaking assumptions, backtracking, and self-refinement. However, effectively\nevaluating such reflection capabilities remains challenging due to the lack of\nappropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel\nbenchmark designed to evaluate the Long-chain Reflective Reasoning capabilities\nof LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction\nProblems (CSPs) where reflective reasoning is crucial for deriving solutions\nthat meet all given constraints. Each type of task focuses on distinct\nconstraint patterns, such as knowledge-based, logical, and spatial constraints,\nproviding a comprehensive evaluation of diverse problem-solving scenarios. Our\nextensive evaluation on both conventional LLMs and LRMs reveals that even the\nmost advanced LRMs, such as DeepSeek-R1 and OpenAI o1-preview, struggle with\ntasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and\n23.6%, respectively. These findings underscore the significant room for\nimprovement in the reflective reasoning capabilities of current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Reasoning Models (LRMs) has significantly enhanced\nthe reasoning abilities of Large Language Models (LLMs), empowering them to\ntackle increasingly complex tasks through reflection capabilities, such as\nmaking assumptions, backtracking, and self-refinement. However, effectively\nevaluating such reflection capabilities remains challenging due to the lack of\nappropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel\nbenchmark designed to evaluate the Long-chain Reflective Reasoning capabilities\nof LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction\nProblems (CSPs) where reflective reasoning is crucial for deriving solutions\nthat meet all given constraints. Each type of task focuses on distinct\nconstraint patterns, such as knowledge-based, logical, and spatial constraints,\nproviding a comprehensive evaluation of diverse problem-solving scenarios. Our\nextensive evaluation on both conventional LLMs and LRMs reveals that even the\nmost advanced LRMs, such as DeepSeek-R1 and OpenAI o1-preview, struggle with\ntasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and\n23.6%, respectively. These findings underscore the significant room for\nimprovement in the reflective reasoning capabilities of current LLMs."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Zhenjiang Ren"
                    },
                    {
                        "name": "Ziyong Li"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ACL-2025, our code is available at https://github.com/ZNLP/LR2Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17848v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17848v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20274v1",
                "updated": "2025-06-25T09:34:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    34,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:34:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    34,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Enterprise Large Language Model Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise Large Language Model Evaluation Benchmark"
                },
                "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Liya Wang"
                    },
                    {
                        "name": "David Yi"
                    },
                    {
                        "name": "Damien Jose"
                    },
                    {
                        "name": "John Passarelli"
                    },
                    {
                        "name": "James Gao"
                    },
                    {
                        "name": "Jordan Leventis"
                    },
                    {
                        "name": "Kang Li"
                    }
                ],
                "author_detail": {
                    "name": "Kang Li"
                },
                "author": "Kang Li",
                "arxiv_comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02502v2",
                "updated": "2025-06-25T09:27:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    27,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-04T11:10:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs"
                },
                "summary": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ACL 2025, our code is available at https://github.com/ZNLP/LADM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06185v2",
                "updated": "2025-06-25T09:21:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    21,
                    21,
                    2,
                    176,
                    0
                ],
                "published": "2025-04-08T16:25:59Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    25,
                    59,
                    1,
                    98,
                    0
                ],
                "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care"
                },
                "summary": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For a fair comparison,\nwe standardize training, data augmentation, and evaluation, conducting\ncross-validation to minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates and evaluate this, along with mask\nquality, for the five best architectures based on physician assessments.\nOverall, the transformer-based TransNeXt showed the highest levels of\ngeneralizability. Despite variations in inference times, all models processed\nat least one image per second on the CPU, which is deemed adequate for the\nintended application. Interpretability analysis typically revealed prominent\nactivations in wound regions, emphasizing focus on clinically relevant\nfeatures. Expert evaluation showed high mask approval for all analyzed models,\nwith VWFormer and ConvNeXtS backbone performing the best. Size retrieval\naccuracy was similar across models, and predictions closely matched expert\nannotations. Finally, we demonstrate how our AI-driven wound size estimation\nframework, WoundAmbit, is integrated into a custom telehealth system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For a fair comparison,\nwe standardize training, data augmentation, and evaluation, conducting\ncross-validation to minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates and evaluate this, along with mask\nquality, for the five best architectures based on physician assessments.\nOverall, the transformer-based TransNeXt showed the highest levels of\ngeneralizability. Despite variations in inference times, all models processed\nat least one image per second on the CPU, which is deemed adequate for the\nintended application. Interpretability analysis typically revealed prominent\nactivations in wound regions, emphasizing focus on clinically relevant\nfeatures. Expert evaluation showed high mask approval for all analyzed models,\nwith VWFormer and ConvNeXtS backbone performing the best. Size retrieval\naccuracy was similar across models, and predictions closely matched expert\nannotations. Finally, we demonstrate how our AI-driven wound size estimation\nframework, WoundAmbit, is integrated into a custom telehealth system."
                },
                "authors": [
                    {
                        "name": "Vanessa Borst"
                    },
                    {
                        "name": "Timo Dittus"
                    },
                    {
                        "name": "Tassilo Dege"
                    },
                    {
                        "name": "Astrid Schmieder"
                    },
                    {
                        "name": "Samuel Kounev"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kounev"
                },
                "author": "Samuel Kounev",
                "arxiv_comment": "Main paper: 18 pages; supplementary material: 15 pages; the paper has\n  been accepted for publication at the Applied Data Science (ADS) track of the\n  European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (ECML PKDD 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21218v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21218v3",
                "updated": "2025-06-25T09:01:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    1,
                    38,
                    2,
                    176,
                    0
                ],
                "published": "2024-10-28T17:02:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Lifting the Veil on Composition, Risks, and Mitigations of the Large\n  Language Model Supply Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifting the Veil on Composition, Risks, and Mitigations of the Large\n  Language Model Supply Chain"
                },
                "summary": "Large language models (LLMs) have sparked significant impact with regard to\nboth intelligence and productivity. Numerous enterprises have integrated LLMs\ninto their applications to solve their own domain-specific tasks. However,\nintegrating LLMs into specific scenarios is a systematic process that involves\nsubstantial components, which are collectively referred to as the LLM supply\nchain. A comprehensive understanding of LLM supply chain composition, as well\nas the relationships among its components, is crucial for enabling effective\nmitigation measures for different related risks. While existing literature has\nexplored various risks associated with LLMs, there remains a notable gap in\nsystematically characterizing the LLM supply chain from the dual perspectives\nof contributors and consumers. In this work, we develop a structured taxonomy\nencompassing risk types, risky actions, and corresponding mitigations across\ndifferent stakeholders and components of the supply chain. We believe that a\nthorough review of the LLM supply chain composition, along with its inherent\nrisks and mitigation measures, would be valuable for industry practitioners to\navoid potential damages and losses, and enlightening for academic researchers\nto rethink existing approaches and explore new avenues of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have sparked significant impact with regard to\nboth intelligence and productivity. Numerous enterprises have integrated LLMs\ninto their applications to solve their own domain-specific tasks. However,\nintegrating LLMs into specific scenarios is a systematic process that involves\nsubstantial components, which are collectively referred to as the LLM supply\nchain. A comprehensive understanding of LLM supply chain composition, as well\nas the relationships among its components, is crucial for enabling effective\nmitigation measures for different related risks. While existing literature has\nexplored various risks associated with LLMs, there remains a notable gap in\nsystematically characterizing the LLM supply chain from the dual perspectives\nof contributors and consumers. In this work, we develop a structured taxonomy\nencompassing risk types, risky actions, and corresponding mitigations across\ndifferent stakeholders and components of the supply chain. We believe that a\nthorough review of the LLM supply chain composition, along with its inherent\nrisks and mitigation measures, would be valuable for industry practitioners to\navoid potential damages and losses, and enlightening for academic researchers\nto rethink existing approaches and explore new avenues of research."
                },
                "authors": [
                    {
                        "name": "Kaifeng Huang"
                    },
                    {
                        "name": "Bihuan Chen"
                    },
                    {
                        "name": "You Lu"
                    },
                    {
                        "name": "Susheng Wu"
                    },
                    {
                        "name": "Dingji Wang"
                    },
                    {
                        "name": "Yiheng Huang"
                    },
                    {
                        "name": "Haowen Jiang"
                    },
                    {
                        "name": "Zhuotong Zhou"
                    },
                    {
                        "name": "Junming Cao"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21218v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21218v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20251v1",
                "updated": "2025-06-25T08:52:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    52,
                    22,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:52:22Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    52,
                    22,
                    2,
                    176,
                    0
                ],
                "title": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching\n  for Quantized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching\n  for Quantized Large Language Models"
                },
                "summary": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe."
                },
                "authors": [
                    {
                        "name": "Kejia Chen"
                    },
                    {
                        "name": "Jiawen Zhang"
                    },
                    {
                        "name": "Jiacong Hu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Jian Lou"
                    },
                    {
                        "name": "Zunlei Feng"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20249v1",
                "updated": "2025-06-25T08:46:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    46,
                    10,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:46:10Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    46,
                    10,
                    2,
                    176,
                    0
                ],
                "title": "Language Modeling by Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling by Language Models"
                },
                "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems."
                },
                "authors": [
                    {
                        "name": "Junyan Cheng"
                    },
                    {
                        "name": "Peter Clark"
                    },
                    {
                        "name": "Kyle Richardson"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Richardson"
                },
                "author": "Kyle Richardson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20241v1",
                "updated": "2025-06-25T08:36:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    36,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:36:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    36,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "Enhancing Large Language Models through Structured Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models through Structured Reasoning"
                },
                "summary": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs."
                },
                "authors": [
                    {
                        "name": "Yubo Dong"
                    },
                    {
                        "name": "Hehe Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hehe Fan"
                },
                "author": "Hehe Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21227v3",
                "updated": "2025-06-25T08:30:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    30,
                    20,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-27T07:36:11Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    36,
                    11,
                    3,
                    86,
                    0
                ],
                "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models"
                },
                "summary": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon."
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhao"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Qixin Sun"
                    },
                    {
                        "name": "Kaiyou Song"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15595v2",
                "updated": "2025-06-25T08:27:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    27,
                    45,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-18T16:10:17Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    10,
                    17,
                    2,
                    169,
                    0
                ],
                "title": "LiteGD: Lightweight and Dynamic GPU Dispatching for Large-scale\n  Heterogeneous Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGD: Lightweight and Dynamic GPU Dispatching for Large-scale\n  Heterogeneous Clusters"
                },
                "summary": "Although multi-GPU execution has become the de-facto paradigm for training\nand serving large language models (LLMs), today's schedulers still rely on a\nsimple heuristic: pick GPUs that are physically close. This proximity rule was\nadequate for small, uniform clusters, but it breaks down in modern fabrics\nwhere link capacities differ by up to an order of magnitude across PCIe,\nNVLink, and CXL tiers. Consequently, jobs placed by locality alone often suffer\nfrom severe bandwidth imbalance and unpredictable performance. In this paper,\nWe present LiteGD, a lightweight, globally-aware GPU dispatching system that\ndelivers near-optimal bandwidth without incurring prohibitive state or search\noverheads. Instead of materializing the full O(N^2) connectivity matrix, LiteGD\nencodes the fabric with a sparsified Tiny-Transformer trained on a few thousand\nrandom bandwidth probes, enabling fast adaptation to incremental hardware\nchanges. LiteGD also employs a bidirectional tree search approach to find the\noptimal GPU dispatching in the data generated in the previous step, which can\nidentify near-optimal solutions while reducing search overhead. We implement\nand evaluate LiteGD in both real and simulated GPU clusters with homogeneous\nand heterogeneous interconnects, respectively. Experimental results demonstrate\nthat LiteGD consistently achieves high GPU Bandwidth Efficacy, approximately\n90% across various cluster configurations and 80% in a real-world H100 cluster,\nsignificantly outperforming conventional default and interconnect\ntopology-aware dispatching methods, particularly in large-scale heterogeneous\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multi-GPU execution has become the de-facto paradigm for training\nand serving large language models (LLMs), today's schedulers still rely on a\nsimple heuristic: pick GPUs that are physically close. This proximity rule was\nadequate for small, uniform clusters, but it breaks down in modern fabrics\nwhere link capacities differ by up to an order of magnitude across PCIe,\nNVLink, and CXL tiers. Consequently, jobs placed by locality alone often suffer\nfrom severe bandwidth imbalance and unpredictable performance. In this paper,\nWe present LiteGD, a lightweight, globally-aware GPU dispatching system that\ndelivers near-optimal bandwidth without incurring prohibitive state or search\noverheads. Instead of materializing the full O(N^2) connectivity matrix, LiteGD\nencodes the fabric with a sparsified Tiny-Transformer trained on a few thousand\nrandom bandwidth probes, enabling fast adaptation to incremental hardware\nchanges. LiteGD also employs a bidirectional tree search approach to find the\noptimal GPU dispatching in the data generated in the previous step, which can\nidentify near-optimal solutions while reducing search overhead. We implement\nand evaluate LiteGD in both real and simulated GPU clusters with homogeneous\nand heterogeneous interconnects, respectively. Experimental results demonstrate\nthat LiteGD consistently achieves high GPU Bandwidth Efficacy, approximately\n90% across various cluster configurations and 80% in a real-world H100 cluster,\nsignificantly outperforming conventional default and interconnect\ntopology-aware dispatching methods, particularly in large-scale heterogeneous\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kunming Zhang"
                    },
                    {
                        "name": "Hanlong Liao"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20233v1",
                "updated": "2025-06-25T08:23:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    23,
                    46,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:23:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    23,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "Identifying multi-compartment Hodgkin-Huxley models with high-density\n  extracellular voltage recordings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying multi-compartment Hodgkin-Huxley models with high-density\n  extracellular voltage recordings"
                },
                "summary": "Multi-compartment Hodgkin-Huxley models are biophysical models of how\nelectrical signals propagate throughout a neuron, and they form the basis of\nour knowledge of neural computation at the cellular level. However, these\nmodels have many free parameters that must be estimated for each cell, and\nexisting fitting methods rely on intracellular voltage measurements that are\nhighly challenging to obtain in vivo. Recent advances in neural recording\ntechnology with high-density probes and arrays enable dense sampling of\nextracellular voltage from many sites surrounding a neuron, allowing indirect\nmeasurement of many compartments of a cell simultaneously. Here, we propose a\nmethod for inferring the underlying membrane voltage, biophysical parameters,\nand the neuron's position relative to the probe, using extracellular\nmeasurements alone. We use an Extended Kalman Filter to infer membrane voltage\nand channel states using efficient, differentiable simulators. Then, we learn\nthe model parameters by maximizing the marginal likelihood using gradient-based\nmethods. We demonstrate the performance of this approach using simulated data\nand real neuron morphologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-compartment Hodgkin-Huxley models are biophysical models of how\nelectrical signals propagate throughout a neuron, and they form the basis of\nour knowledge of neural computation at the cellular level. However, these\nmodels have many free parameters that must be estimated for each cell, and\nexisting fitting methods rely on intracellular voltage measurements that are\nhighly challenging to obtain in vivo. Recent advances in neural recording\ntechnology with high-density probes and arrays enable dense sampling of\nextracellular voltage from many sites surrounding a neuron, allowing indirect\nmeasurement of many compartments of a cell simultaneously. Here, we propose a\nmethod for inferring the underlying membrane voltage, biophysical parameters,\nand the neuron's position relative to the probe, using extracellular\nmeasurements alone. We use an Extended Kalman Filter to infer membrane voltage\nand channel states using efficient, differentiable simulators. Then, we learn\nthe model parameters by maximizing the marginal likelihood using gradient-based\nmethods. We demonstrate the performance of this approach using simulated data\nand real neuron morphologies."
                },
                "authors": [
                    {
                        "name": "Ian Christopher Tanoh"
                    },
                    {
                        "name": "Michael Deistler"
                    },
                    {
                        "name": "Jakob H. Macke"
                    },
                    {
                        "name": "Scott W. Linderman"
                    }
                ],
                "author_detail": {
                    "name": "Scott W. Linderman"
                },
                "author": "Scott W. Linderman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00757v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00757v3",
                "updated": "2025-06-25T08:23:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    23,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-02T11:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    40,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds\n  via Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds\n  via Self-Improvement"
                },
                "summary": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In 'blue' mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In 'red' mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/J-Rosser-UK/AgentBreeder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In 'blue' mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In 'red' mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/J-Rosser-UK/AgentBreeder."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Nicolaus Foerster"
                },
                "author": "Jakob Nicolaus Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00757v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00757v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20232v1",
                "updated": "2025-06-25T08:23:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    23,
                    9,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:23:09Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    23,
                    9,
                    2,
                    176,
                    0
                ],
                "title": "Tomography for Plasma Imaging: a Unifying Framework for Bayesian\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tomography for Plasma Imaging: a Unifying Framework for Bayesian\n  Inference"
                },
                "summary": "Plasma diagnostics often employ computerized tomography to estimate\nemissivity profiles from a finite, and often limited, number of line-integrated\nmeasurements. Decades of algorithmic refinement have brought considerable\nimprovements, and led to a variety of employed solutions. These often feature\nan underlying, common structure that is rarely acknowledged or investigated. In\nthis paper, we present a unifying perspective on sparse-view tomographic\nreconstructions for plasma imaging, highlighting how many inversion approaches\nreported in the literature can be naturally understood within a Bayesian\nframework. In this setting, statistical modelling of acquired data leads to a\nlikelihood term, while the assumed properties of the profile to be\nreconstructed are encoded within a prior term. Together, these terms yield the\nposterior distribution, which models all the available information on the\nprofile to be reconstructed. We show how credible reconstructions, uncertainty\nquantification and further statistical quantities of interest can be\nefficiently obtained from noisy tomographic data by means of a stochastic\ngradient flow algorithm targeting the posterior. This is demonstrated by\napplication to soft x-ray imaging at the TCV tokamak. We validate the proposed\nimaging pipeline on a large dataset of generated model phantoms, showing how\nposterior-based inference can be leveraged to perform principled statistical\nanalysis of quantities of interest. Finally, we address some of the inherent,\nand thus remaining, limitations of sparse-view tomography. All the\ncomputational routines used in this work are made available as open access\ncode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma diagnostics often employ computerized tomography to estimate\nemissivity profiles from a finite, and often limited, number of line-integrated\nmeasurements. Decades of algorithmic refinement have brought considerable\nimprovements, and led to a variety of employed solutions. These often feature\nan underlying, common structure that is rarely acknowledged or investigated. In\nthis paper, we present a unifying perspective on sparse-view tomographic\nreconstructions for plasma imaging, highlighting how many inversion approaches\nreported in the literature can be naturally understood within a Bayesian\nframework. In this setting, statistical modelling of acquired data leads to a\nlikelihood term, while the assumed properties of the profile to be\nreconstructed are encoded within a prior term. Together, these terms yield the\nposterior distribution, which models all the available information on the\nprofile to be reconstructed. We show how credible reconstructions, uncertainty\nquantification and further statistical quantities of interest can be\nefficiently obtained from noisy tomographic data by means of a stochastic\ngradient flow algorithm targeting the posterior. This is demonstrated by\napplication to soft x-ray imaging at the TCV tokamak. We validate the proposed\nimaging pipeline on a large dataset of generated model phantoms, showing how\nposterior-based inference can be leveraged to perform principled statistical\nanalysis of quantities of interest. Finally, we address some of the inherent,\nand thus remaining, limitations of sparse-view tomography. All the\ncomputational routines used in this work are made available as open access\ncode."
                },
                "authors": [
                    {
                        "name": "D. Hamm"
                    },
                    {
                        "name": "C. Theiler"
                    },
                    {
                        "name": "M. Simeoni"
                    },
                    {
                        "name": "B. P. Duval"
                    },
                    {
                        "name": "T. Debarre"
                    },
                    {
                        "name": "L. Simons"
                    },
                    {
                        "name": "J. R. Queralt"
                    }
                ],
                "author_detail": {
                    "name": "J. R. Queralt"
                },
                "author": "J. R. Queralt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13320v2",
                "updated": "2025-06-25T08:22:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    22,
                    9,
                    2,
                    176,
                    0
                ],
                "published": "2025-04-17T20:16:15Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    20,
                    16,
                    15,
                    3,
                    107,
                    0
                ],
                "title": "Gradient-Free Sequential Bayesian Experimental Design via Interacting\n  Particle Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-Free Sequential Bayesian Experimental Design via Interacting\n  Particle Systems"
                },
                "summary": "We introduce a gradient-free framework for Bayesian Optimal Experimental\nDesign (BOED) in sequential settings, aimed at complex systems where gradient\ninformation is unavailable. Our method combines Ensemble Kalman Inversion (EKI)\nfor design optimization with the Affine-Invariant Langevin Dynamics (ALDI)\nsampler for efficient posterior sampling-both of which are derivative-free and\nensemble-based. To address the computational challenges posed by nested\nexpectations in BOED, we propose variational Gaussian and parametrized Laplace\napproximations that provide tractable upper and lower bounds on the Expected\nInformation Gain (EIG). These approximations enable scalable utility estimation\nin high-dimensional spaces and PDE-constrained inverse problems. We demonstrate\nthe performance of our framework through numerical experiments ranging from\nlinear Gaussian models to PDE-based inference tasks, highlighting the method's\nrobustness, accuracy, and efficiency in information-driven experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a gradient-free framework for Bayesian Optimal Experimental\nDesign (BOED) in sequential settings, aimed at complex systems where gradient\ninformation is unavailable. Our method combines Ensemble Kalman Inversion (EKI)\nfor design optimization with the Affine-Invariant Langevin Dynamics (ALDI)\nsampler for efficient posterior sampling-both of which are derivative-free and\nensemble-based. To address the computational challenges posed by nested\nexpectations in BOED, we propose variational Gaussian and parametrized Laplace\napproximations that provide tractable upper and lower bounds on the Expected\nInformation Gain (EIG). These approximations enable scalable utility estimation\nin high-dimensional spaces and PDE-constrained inverse problems. We demonstrate\nthe performance of our framework through numerical experiments ranging from\nlinear Gaussian models to PDE-based inference tasks, highlighting the method's\nrobustness, accuracy, and efficiency in information-driven experimental design."
                },
                "authors": [
                    {
                        "name": "Robert Gruhlke"
                    },
                    {
                        "name": "Matei Hanu"
                    },
                    {
                        "name": "Claudia Schillings"
                    },
                    {
                        "name": "Philipp Wacker"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Wacker"
                },
                "author": "Philipp Wacker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62K05, 62F15, 65C05, 93E10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10035v2",
                "updated": "2025-06-25T08:05:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    5,
                    36,
                    2,
                    176,
                    0
                ],
                "published": "2025-04-14T09:37:47Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    37,
                    47,
                    0,
                    104,
                    0
                ],
                "title": "TT3D: Table Tennis 3D Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TT3D: Table Tennis 3D Reconstruction"
                },
                "summary": "Sports analysis requires processing large amounts of data, which is\ntime-consuming and costly. Advancements in neural networks have significantly\nalleviated this burden, enabling highly accurate ball tracking in sports\nbroadcasts. However, relying solely on 2D ball tracking is limiting, as it\ndepends on the camera's viewpoint and falls short of supporting comprehensive\ngame analysis. To address this limitation, we propose a novel approach for\nreconstructing precise 3D ball trajectories from online table tennis match\nrecordings. Our method leverages the underlying physics of the ball's motion to\nidentify the bounce state that minimizes the reprojection error of the ball's\nflying trajectory, hence ensuring an accurate and reliable 3D reconstruction. A\nkey advantage of our approach is its ability to infer ball spin without relying\non human pose estimation or racket tracking, which are often unreliable or\nunavailable in broadcast footage. We developed an automated camera calibration\nmethod capable of reliably tracking camera movements. Additionally, we adapted\nan existing 3D pose estimation model, which lacks depth motion capture, to\naccurately track player movements. Together, these contributions enable the\nfull 3D reconstruction of a table tennis rally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sports analysis requires processing large amounts of data, which is\ntime-consuming and costly. Advancements in neural networks have significantly\nalleviated this burden, enabling highly accurate ball tracking in sports\nbroadcasts. However, relying solely on 2D ball tracking is limiting, as it\ndepends on the camera's viewpoint and falls short of supporting comprehensive\ngame analysis. To address this limitation, we propose a novel approach for\nreconstructing precise 3D ball trajectories from online table tennis match\nrecordings. Our method leverages the underlying physics of the ball's motion to\nidentify the bounce state that minimizes the reprojection error of the ball's\nflying trajectory, hence ensuring an accurate and reliable 3D reconstruction. A\nkey advantage of our approach is its ability to infer ball spin without relying\non human pose estimation or racket tracking, which are often unreliable or\nunavailable in broadcast footage. We developed an automated camera calibration\nmethod capable of reliably tracking camera movements. Additionally, we adapted\nan existing 3D pose estimation model, which lacks depth motion capture, to\naccurately track player movements. Together, these contributions enable the\nfull 3D reconstruction of a table tennis rally."
                },
                "authors": [
                    {
                        "name": "Thomas Gossard"
                    },
                    {
                        "name": "Andreas Ziegler"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "arxiv_comment": "Accepted to CVSport 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20220v1",
                "updated": "2025-06-25T08:05:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    5,
                    24,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:05:24Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    5,
                    24,
                    2,
                    176,
                    0
                ],
                "title": "High resolution ALMA observations of H$_2$S in LIRGS (Dense gas and\n  shocks in outflows and CNDs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High resolution ALMA observations of H$_2$S in LIRGS (Dense gas and\n  shocks in outflows and CNDs)"
                },
                "summary": "Molecular gas plays a critical role in regulating star formation and nuclear\nactivity in galaxies. Sulphur bearing molecules, such as H2S, are sensitive to\nthe physical and chemical environments in which they reside and are potential\ntracers of shocked, dense gas in galactic outflows and active galactic nuclei\n(AGN). We aim to investigate the origin of H2S emission and its relation to\ndense gas and outflow activity in the central regions of nearby infrared\nluminous galaxies. We present ALMA Band 5 observations of the ortho H2S 1(1,0)\n1(0,1) transition in three nearby galaxies: NGC 1377, NGC 4418, and NGC 1266.\nWe perform radiative transfer modelling using RADEX to constrain the physical\nconditions of the H2S emitting gas and compare the results to ancillary CO and\ncontinuum data. We detect compact H2S emission in all three galaxies, arising\nfrom regions smaller than approximately 150 parsecs. The H2S spectral profiles\nexhibit broad line wings, suggesting an association with outflowing or shocked\ngas. In NGC 4418, H2S also appears to be tracing gas that is counterrotating. A\npeculiar red shifted emission feature may correspond to inflowing gas, or\npossibly a slanted outflow. RADEX modelling indicates that the H2S emitting gas\nhas high densities (molecular hydrogen density greater than 10^7 cm^-3) and\nmoderately warm temperatures (between 40 and 200 Kelvin). The derived densities\nexceed those inferred from CO observations, implying that H2S traces denser\nregions of the interstellar medium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular gas plays a critical role in regulating star formation and nuclear\nactivity in galaxies. Sulphur bearing molecules, such as H2S, are sensitive to\nthe physical and chemical environments in which they reside and are potential\ntracers of shocked, dense gas in galactic outflows and active galactic nuclei\n(AGN). We aim to investigate the origin of H2S emission and its relation to\ndense gas and outflow activity in the central regions of nearby infrared\nluminous galaxies. We present ALMA Band 5 observations of the ortho H2S 1(1,0)\n1(0,1) transition in three nearby galaxies: NGC 1377, NGC 4418, and NGC 1266.\nWe perform radiative transfer modelling using RADEX to constrain the physical\nconditions of the H2S emitting gas and compare the results to ancillary CO and\ncontinuum data. We detect compact H2S emission in all three galaxies, arising\nfrom regions smaller than approximately 150 parsecs. The H2S spectral profiles\nexhibit broad line wings, suggesting an association with outflowing or shocked\ngas. In NGC 4418, H2S also appears to be tracing gas that is counterrotating. A\npeculiar red shifted emission feature may correspond to inflowing gas, or\npossibly a slanted outflow. RADEX modelling indicates that the H2S emitting gas\nhas high densities (molecular hydrogen density greater than 10^7 cm^-3) and\nmoderately warm temperatures (between 40 and 200 Kelvin). The derived densities\nexceed those inferred from CO observations, implying that H2S traces denser\nregions of the interstellar medium."
                },
                "authors": [
                    {
                        "name": "M. T. Sato"
                    },
                    {
                        "name": "S. Aalto"
                    },
                    {
                        "name": "S. König"
                    },
                    {
                        "name": "K. Kohno"
                    },
                    {
                        "name": "S. Viti"
                    },
                    {
                        "name": "M. Gorski"
                    },
                    {
                        "name": "F. Combes"
                    },
                    {
                        "name": "S. García-Burillo"
                    },
                    {
                        "name": "N. Harada"
                    },
                    {
                        "name": "P. van der Werf"
                    },
                    {
                        "name": "J. Otter"
                    },
                    {
                        "name": "S. Muller"
                    },
                    {
                        "name": "Y. Nishimura"
                    },
                    {
                        "name": "J. S. Gallagher"
                    },
                    {
                        "name": "A. S. Evans"
                    },
                    {
                        "name": "K. M. Dasyra"
                    },
                    {
                        "name": "J. K. Kotilainen"
                    }
                ],
                "author_detail": {
                    "name": "J. K. Kotilainen"
                },
                "author": "J. K. Kotilainen",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20199v1",
                "updated": "2025-06-25T07:39:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    39,
                    19,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:39:19Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    39,
                    19,
                    2,
                    176,
                    0
                ],
                "title": "How to Retrieve Examples in In-context Learning to Improve\n  Conversational Emotion Recognition using Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Retrieve Examples in In-context Learning to Improve\n  Conversational Emotion Recognition using Large Language Models?"
                },
                "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing."
                },
                "authors": [
                    {
                        "name": "Mengqi Wang"
                    },
                    {
                        "name": "Tiantian Feng"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Shrikanth Narayanan"
                },
                "author": "Shrikanth Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18929v2",
                "updated": "2025-06-25T07:37:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    37,
                    58,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-21T10:40:23Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    10,
                    40,
                    23,
                    5,
                    172,
                    0
                ],
                "title": "International Trade and Intellectual Property",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "International Trade and Intellectual Property"
                },
                "summary": "Intellectual property (IP) rules have the potential to shape cross-border\ntrade far more than their legalistic origins might suggest. Drawing on three\ndecades of evidence, this review shows that stronger IP rights simultaneously\ncreate market-power forces that raise prices and market-expansion forces that\nbroaden demand, while dynamic incentives spur quality upgrading and new export\nvarieties. Micro-data and quasi-natural experiments after TRIPS reveal that IP\nmost often boosts trade along the extensive margin and redirects some activity\ntoward licensing and foreign investment. Policy bundling and measurement gaps\non the strength of IP rights still cloud causal inference. Future work must map\nintangible flows and enforcement quality to capture the digital, data-driven\nfrontier of international commerce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intellectual property (IP) rules have the potential to shape cross-border\ntrade far more than their legalistic origins might suggest. Drawing on three\ndecades of evidence, this review shows that stronger IP rights simultaneously\ncreate market-power forces that raise prices and market-expansion forces that\nbroaden demand, while dynamic incentives spur quality upgrading and new export\nvarieties. Micro-data and quasi-natural experiments after TRIPS reveal that IP\nmost often boosts trade along the extensive margin and redirects some activity\ntoward licensing and foreign investment. Policy bundling and measurement gaps\non the strength of IP rights still cloud causal inference. Future work must map\nintangible flows and enforcement quality to capture the digital, data-driven\nfrontier of international commerce."
                },
                "authors": [
                    {
                        "name": "Gaetan de Rassenfosse"
                    }
                ],
                "author_detail": {
                    "name": "Gaetan de Rassenfosse"
                },
                "author": "Gaetan de Rassenfosse",
                "arxiv_comment": "The Patentist Living Literature Review 6: 1-5",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20197v1",
                "updated": "2025-06-25T07:37:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    37,
                    16,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:37:16Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    37,
                    16,
                    2,
                    176,
                    0
                ],
                "title": "Zero-Shot Attribution for Large Language Models: A Distribution Testing\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Attribution for Large Language Models: A Distribution Testing\n  Approach"
                },
                "summary": "A growing fraction of all code is sampled from Large Language Models (LLMs).\nWe investigate the problem of attributing code generated by language models\nusing hypothesis testing to leverage established techniques and guarantees.\nGiven a set of samples $S$ and a suspect model $\\mathcal{L}^*$, our goal is to\nassess the likelihood of $S$ originating from $\\mathcal{L}^*$. Due to the curse\nof dimensionality, this is intractable when only samples from the LLM are\ngiven: to circumvent this, we use both samples and density estimates from the\nLLM, a form of access commonly available.\n  We introduce $\\mathsf{Anubis}$, a zero-shot attribution tool that frames\nattribution as a distribution testing problem. Our experiments on a benchmark\nof code samples show that $\\mathsf{Anubis}$ achieves high AUROC scores (\n$\\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and\nStable-Code using only $\\approx 2000$ samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing fraction of all code is sampled from Large Language Models (LLMs).\nWe investigate the problem of attributing code generated by language models\nusing hypothesis testing to leverage established techniques and guarantees.\nGiven a set of samples $S$ and a suspect model $\\mathcal{L}^*$, our goal is to\nassess the likelihood of $S$ originating from $\\mathcal{L}^*$. Due to the curse\nof dimensionality, this is intractable when only samples from the LLM are\ngiven: to circumvent this, we use both samples and density estimates from the\nLLM, a form of access commonly available.\n  We introduce $\\mathsf{Anubis}$, a zero-shot attribution tool that frames\nattribution as a distribution testing problem. Our experiments on a benchmark\nof code samples show that $\\mathsf{Anubis}$ achieves high AUROC scores (\n$\\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and\nStable-Code using only $\\approx 2000$ samples."
                },
                "authors": [
                    {
                        "name": "Clément L. Canonne"
                    },
                    {
                        "name": "Yash Pote"
                    },
                    {
                        "name": "Uddalok Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Uddalok Sarkar"
                },
                "author": "Uddalok Sarkar",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12434v2",
                "updated": "2025-06-25T07:35:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    35,
                    51,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-18T14:14:35Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    14,
                    14,
                    35,
                    6,
                    138,
                    0
                ],
                "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning"
                },
                "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yanrui Yu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Tianfei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianfei Zhou"
                },
                "author": "Tianfei Zhou",
                "arxiv_comment": "Code: https://github.com/QiWang98/VideoRFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20194v1",
                "updated": "2025-06-25T07:35:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    35,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:35:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    35,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in\n  LLMs"
                },
                "summary": "Large language models (LLMs) deliver strong performance but are difficult to\ndeploy due to high memory and compute costs. While pruning reduces these\ndemands, most methods ignore activation sparsity observed at runtime. We\nreinterpret activation sparsity as dynamic structured weight sparsity and\npropose DuoGPT, a unified framework that constructs dual-sparse (spMspV)\nworkloads by combining unstructured weight pruning with activation sparsity. To\npreserve accuracy, we extend the Optimal Brain Compression (OBC) framework with\nactivation-aware calibration and introduce output residuals from the dense\nmodel as correction terms. We further optimize the solution for efficient GPU\nexecution, enabling scalability to billion-parameter LLMs. Evaluations on\nLLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured\npruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\\times$\ncompared to the baseline dense model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) deliver strong performance but are difficult to\ndeploy due to high memory and compute costs. While pruning reduces these\ndemands, most methods ignore activation sparsity observed at runtime. We\nreinterpret activation sparsity as dynamic structured weight sparsity and\npropose DuoGPT, a unified framework that constructs dual-sparse (spMspV)\nworkloads by combining unstructured weight pruning with activation sparsity. To\npreserve accuracy, we extend the Optimal Brain Compression (OBC) framework with\nactivation-aware calibration and introduce output residuals from the dense\nmodel as correction terms. We further optimize the solution for efficient GPU\nexecution, enabling scalability to billion-parameter LLMs. Evaluations on\nLLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured\npruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\\times$\ncompared to the baseline dense model."
                },
                "authors": [
                    {
                        "name": "Ruokai Yin"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Donghyun Lee"
                    },
                    {
                        "name": "Priyadarshini Panda"
                    }
                ],
                "author_detail": {
                    "name": "Priyadarshini Panda"
                },
                "author": "Priyadarshini Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13087v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13087v3",
                "updated": "2025-06-25T07:27:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    27,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-16T04:12:04Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    4,
                    12,
                    4,
                    0,
                    167,
                    0
                ],
                "title": "IKDiffuser: A Generative Inverse Kinematics Solver for Multi-arm Robots\n  via Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IKDiffuser: A Generative Inverse Kinematics Solver for Multi-arm Robots\n  via Diffusion Model"
                },
                "summary": "Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has\nprimarily been successful with single serial manipulators. For multi-arm\nrobotic systems, IK remains challenging due to complex self-collisions, coupled\njoints, and high-dimensional redundancy. These complexities make traditional IK\nsolvers slow, prone to failure, and lacking in solution diversity. In this\npaper, we present IKDiffuser, a diffusion-based model designed for fast and\ndiverse IK solution generation for multi-arm robotic systems. IKDiffuser learns\nthe joint distribution over the configuration space, capturing complex\ndependencies and enabling seamless generalization to multi-arm robotic systems\nof different structures. In addition, IKDiffuser can incorporate additional\nobjectives during inference without retraining, offering versatility and\nadaptability for task-specific requirements. In experiments on 6 different\nmulti-arm systems, the proposed IKDiffuser achieves superior solution accuracy,\nprecision, diversity, and computational efficiency compared to existing\nsolvers. The proposed IKDiffuser framework offers a scalable, unified approach\nto solving multi-arm IK problems, facilitating the potential of multi-arm\nrobotic systems in real-time manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has\nprimarily been successful with single serial manipulators. For multi-arm\nrobotic systems, IK remains challenging due to complex self-collisions, coupled\njoints, and high-dimensional redundancy. These complexities make traditional IK\nsolvers slow, prone to failure, and lacking in solution diversity. In this\npaper, we present IKDiffuser, a diffusion-based model designed for fast and\ndiverse IK solution generation for multi-arm robotic systems. IKDiffuser learns\nthe joint distribution over the configuration space, capturing complex\ndependencies and enabling seamless generalization to multi-arm robotic systems\nof different structures. In addition, IKDiffuser can incorporate additional\nobjectives during inference without retraining, offering versatility and\nadaptability for task-specific requirements. In experiments on 6 different\nmulti-arm systems, the proposed IKDiffuser achieves superior solution accuracy,\nprecision, diversity, and computational efficiency compared to existing\nsolvers. The proposed IKDiffuser framework offers a scalable, unified approach\nto solving multi-arm IK problems, facilitating the potential of multi-arm\nrobotic systems in real-time manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Ziyuan Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyuan Jiao"
                },
                "author": "Ziyuan Jiao",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13087v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13087v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v1",
                "updated": "2025-06-25T07:26:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02097v2",
                "updated": "2025-06-25T07:18:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    18,
                    47,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-02T17:59:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    59,
                    27,
                    0,
                    153,
                    0
                ],
                "title": "Hybrid AI for Responsive Multi-Turn Online Conversations with Novel\n  Dynamic Routing and Feedback Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid AI for Responsive Multi-Turn Online Conversations with Novel\n  Dynamic Routing and Feedback Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems and large language model\n(LLM)-powered chatbots have significantly advanced conversational AI by\ncombining generative capabilities with external knowledge retrieval. Despite\ntheir success, enterprise-scale deployments face critical challenges, including\ndiverse user queries, high latency, hallucinations, and difficulty integrating\nfrequently updated domain-specific knowledge. This paper introduces a novel\nhybrid framework that integrates RAG with intent-based canned responses,\nleveraging predefined high-confidence responses for efficiency while\ndynamically routing complex or ambiguous queries to the RAG pipeline. Our\nframework employs a dialogue context manager to ensure coherence in multi-turn\ninteractions and incorporates a feedback loop to refine intents, dynamically\nadjust confidence thresholds, and expand response coverage over time.\nExperimental results demonstrate that the proposed framework achieves a balance\nof high accuracy (95\\%) and low latency (180ms), outperforming RAG and\nintent-based systems across diverse query types, positioning it as a scalable\nand adaptive solution for enterprise conversational AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems and large language model\n(LLM)-powered chatbots have significantly advanced conversational AI by\ncombining generative capabilities with external knowledge retrieval. Despite\ntheir success, enterprise-scale deployments face critical challenges, including\ndiverse user queries, high latency, hallucinations, and difficulty integrating\nfrequently updated domain-specific knowledge. This paper introduces a novel\nhybrid framework that integrates RAG with intent-based canned responses,\nleveraging predefined high-confidence responses for efficiency while\ndynamically routing complex or ambiguous queries to the RAG pipeline. Our\nframework employs a dialogue context manager to ensure coherence in multi-turn\ninteractions and incorporates a feedback loop to refine intents, dynamically\nadjust confidence thresholds, and expand response coverage over time.\nExperimental results demonstrate that the proposed framework achieves a balance\nof high accuracy (95\\%) and low latency (180ms), outperforming RAG and\nintent-based systems across diverse query types, positioning it as a scalable\nand adaptive solution for enterprise conversational AI applications."
                },
                "authors": [
                    {
                        "name": "Priyaranjan Pattnayak"
                    },
                    {
                        "name": "Amit Agarwal"
                    },
                    {
                        "name": "Hansa Meghwani"
                    },
                    {
                        "name": "Hitesh Laxmichand Patel"
                    },
                    {
                        "name": "Srikant Panda"
                    }
                ],
                "author_detail": {
                    "name": "Srikant Panda"
                },
                "author": "Srikant Panda",
                "arxiv_comment": "Proceedings of the 4th International Workshop on Knowledge Augmented\n  Methods for Natural Language Processing in NAACL 2025, pages 215 to 229,\n  Albuquerque, New Mexico, USA. Association for Computational Linguistics",
                "arxiv_journal_ref": "Proceedings of the 4th International Workshop on\n  Knowledge-Augmented Methods for Natural Language Processing (KnowledgeNLP\n  2025), pp. 215 to 229, Association for Computational Linguistics,\n  Albuquerque, New Mexico, May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20181v1",
                "updated": "2025-06-25T07:15:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    15,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:15:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    15,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Causal Operator Discovery in Partial Differential Equations via\n  Counterfactual Physics-Informed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Operator Discovery in Partial Differential Equations via\n  Counterfactual Physics-Informed Neural Networks"
                },
                "summary": "We develop a principled framework for discovering causal structure in partial\ndifferential equations (PDEs) using physics-informed neural networks and\ncounterfactual perturbations. Unlike classical residual minimization or sparse\nregression methods, our approach quantifies operator-level necessity through\nfunctional interventions on the governing dynamics. We introduce causal\nsensitivity indices and structural deviation metrics to assess the influence of\ncandidate differential operators within neural surrogates. Theoretically, we\nprove exact recovery of the causal operator support under restricted isometry\nor mutual coherence conditions, with residual bounds guaranteeing\nidentifiability. Empirically, we validate the framework on both synthetic and\nreal-world datasets across climate dynamics, tumor diffusion, and ocean flows.\nOur method consistently recovers governing operators even under noise,\nredundancy, and data scarcity, outperforming standard PINNs and DeepONets in\nstructural fidelity. This work positions causal PDE discovery as a tractable\nand interpretable inference task grounded in structural causal models and\nvariational residual analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a principled framework for discovering causal structure in partial\ndifferential equations (PDEs) using physics-informed neural networks and\ncounterfactual perturbations. Unlike classical residual minimization or sparse\nregression methods, our approach quantifies operator-level necessity through\nfunctional interventions on the governing dynamics. We introduce causal\nsensitivity indices and structural deviation metrics to assess the influence of\ncandidate differential operators within neural surrogates. Theoretically, we\nprove exact recovery of the causal operator support under restricted isometry\nor mutual coherence conditions, with residual bounds guaranteeing\nidentifiability. Empirically, we validate the framework on both synthetic and\nreal-world datasets across climate dynamics, tumor diffusion, and ocean flows.\nOur method consistently recovers governing operators even under noise,\nredundancy, and data scarcity, outperforming standard PINNs and DeepONets in\nstructural fidelity. This work positions causal PDE discovery as a tractable\nand interpretable inference task grounded in structural causal models and\nvariational residual analysis."
                },
                "authors": [
                    {
                        "name": "Ronald Katende"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Katende"
                },
                "author": "Ronald Katende",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20170v1",
                "updated": "2025-06-25T06:50:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    50,
                    13,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T06:50:13Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    50,
                    13,
                    2,
                    176,
                    0
                ],
                "title": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript\n  Deobfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript\n  Deobfuscation"
                },
                "summary": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement."
                },
                "authors": [
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Zhiqiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lin"
                },
                "author": "Zhiqiang Lin",
                "arxiv_comment": "Accepted by ACM CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.20670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20670v1",
                "updated": "2025-06-25T17:59:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "MMSearch-R1: Incentivizing LMMs to Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMSearch-R1: Incentivizing LMMs to Search"
                },
                "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search."
                },
                "authors": [
                    {
                        "name": "Jinming Wu"
                    },
                    {
                        "name": "Zihao Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yiding Liu"
                    },
                    {
                        "name": "Bo You"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Code: https://github.com/EvolvingLMMs-Lab/multimodal-search-r1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20666v1",
                "updated": "2025-06-25T17:58:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    58,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:58:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    58,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "Inside you are many wolves: Using cognitive models to interpret value\n  trade-offs in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside you are many wolves: Using cognitive models to interpret value\n  trade-offs in LLMs"
                },
                "summary": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training."
                },
                "authors": [
                    {
                        "name": "Sonia K. Murthy"
                    },
                    {
                        "name": "Rosie Zhao"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Peng Qian"
                    },
                    {
                        "name": "Tomer Ullman"
                    }
                ],
                "author_detail": {
                    "name": "Tomer Ullman"
                },
                "author": "Tomer Ullman",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24758v2",
                "updated": "2025-06-25T17:57:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    57,
                    6,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-30T16:18:58Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    18,
                    58,
                    4,
                    150,
                    0
                ],
                "title": "Survey: Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey: Graph Databases"
                },
                "summary": "Graph databases have become essential tools for managing complex and\ninterconnected data, which is common in areas like social networks,\nbioinformatics, and recommendation systems. Unlike traditional relational\ndatabases, graph databases offer a more natural way to model and query\nintricate relationships, making them particularly effective for applications\nthat demand flexibility and efficiency in handling interconnected data.\n  Despite their increasing use, graph databases face notable challenges. One\nsignificant issue is the irregular nature of graph data, often marked by\nstructural sparsity, such as in its adjacency matrix representation, which can\nlead to inefficiencies in data read and write operations. Other obstacles\ninclude the high computational demands of traversal-based queries, especially\nwithin large-scale networks, and complexities in managing transactions in\ndistributed graph environments. Additionally, the reliance on traditional\ncentralized architectures limits the scalability of Online Transaction\nProcessing (OLTP), creating bottlenecks due to contention, CPU overhead, and\nnetwork bandwidth constraints.\n  This paper presents a thorough survey of graph databases. It begins by\nexamining property models, query languages, and storage architectures,\noutlining the foundational aspects that users and developers typically engage\nwith. Following this, it provides a detailed analysis of recent advancements in\ngraph database technologies, evaluating these in the context of key aspects\nsuch as architecture, deployment, usage, and development, which collectively\ndefine the capabilities of graph database solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph databases have become essential tools for managing complex and\ninterconnected data, which is common in areas like social networks,\nbioinformatics, and recommendation systems. Unlike traditional relational\ndatabases, graph databases offer a more natural way to model and query\nintricate relationships, making them particularly effective for applications\nthat demand flexibility and efficiency in handling interconnected data.\n  Despite their increasing use, graph databases face notable challenges. One\nsignificant issue is the irregular nature of graph data, often marked by\nstructural sparsity, such as in its adjacency matrix representation, which can\nlead to inefficiencies in data read and write operations. Other obstacles\ninclude the high computational demands of traversal-based queries, especially\nwithin large-scale networks, and complexities in managing transactions in\ndistributed graph environments. Additionally, the reliance on traditional\ncentralized architectures limits the scalability of Online Transaction\nProcessing (OLTP), creating bottlenecks due to contention, CPU overhead, and\nnetwork bandwidth constraints.\n  This paper presents a thorough survey of graph databases. It begins by\nexamining property models, query languages, and storage architectures,\noutlining the foundational aspects that users and developers typically engage\nwith. Following this, it provides a detailed analysis of recent advancements in\ngraph database technologies, evaluating these in the context of key aspects\nsuch as architecture, deployment, usage, and development, which collectively\ndefine the capabilities of graph database solutions."
                },
                "authors": [
                    {
                        "name": "Miguel E. Coimbra"
                    },
                    {
                        "name": "Lucie Svitáková"
                    },
                    {
                        "name": "Alexandre P. Francisco"
                    },
                    {
                        "name": "Luís Veiga"
                    }
                ],
                "author_detail": {
                    "name": "Luís Veiga"
                },
                "author": "Luís Veiga",
                "arxiv_comment": "47 pages, 1 figure, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20664v1",
                "updated": "2025-06-25T17:55:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    55,
                    27,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:55:27Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    55,
                    27,
                    2,
                    176,
                    0
                ],
                "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind"
                },
                "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents."
                },
                "authors": [
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Timon Willi"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "arxiv_comment": "41 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20657v1",
                "updated": "2025-06-25T17:52:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    52,
                    26,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:52:26Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    52,
                    26,
                    2,
                    176,
                    0
                ],
                "title": "SuperSONIC: Cloud-Native Infrastructure for ML Inferencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperSONIC: Cloud-Native Infrastructure for ML Inferencing"
                },
                "summary": "The increasing computational demand from growing data rates and complex\nmachine learning (ML) algorithms in large-scale scientific experiments has\ndriven the adoption of the Services for Optimized Network Inference on\nCoprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it\nto local or remote coprocessors to optimize resource utilization. Leveraging\nits portability to different types of coprocessors, SONIC enhances data\nprocessing and model deployment efficiency for cutting-edge research in high\nenergy physics (HEP) and multi-messenger astrophysics (MMA). We developed the\nSuperSONIC project, a scalable server infrastructure for SONIC, enabling the\ndeployment of computationally intensive tasks to Kubernetes clusters equipped\nwith graphics processing units (GPUs). Using NVIDIA Triton Inference Server,\nSuperSONIC decouples client workflows from server infrastructure, standardizing\ncommunication, optimizing throughput, load balancing, and monitoring.\nSuperSONIC has been successfully deployed for the CMS and ATLAS experiments at\nthe CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory\n(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)\nand tested on Kubernetes clusters at Purdue University, the National Research\nPlatform (NRP), and the University of Chicago. SuperSONIC addresses the\nchallenges of the Cloud-native era by providing a reusable, configurable\nframework that enhances the efficiency of accelerator-based inference\ndeployment across diverse scientific domains and industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing computational demand from growing data rates and complex\nmachine learning (ML) algorithms in large-scale scientific experiments has\ndriven the adoption of the Services for Optimized Network Inference on\nCoprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it\nto local or remote coprocessors to optimize resource utilization. Leveraging\nits portability to different types of coprocessors, SONIC enhances data\nprocessing and model deployment efficiency for cutting-edge research in high\nenergy physics (HEP) and multi-messenger astrophysics (MMA). We developed the\nSuperSONIC project, a scalable server infrastructure for SONIC, enabling the\ndeployment of computationally intensive tasks to Kubernetes clusters equipped\nwith graphics processing units (GPUs). Using NVIDIA Triton Inference Server,\nSuperSONIC decouples client workflows from server infrastructure, standardizing\ncommunication, optimizing throughput, load balancing, and monitoring.\nSuperSONIC has been successfully deployed for the CMS and ATLAS experiments at\nthe CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory\n(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)\nand tested on Kubernetes clusters at Purdue University, the National Research\nPlatform (NRP), and the University of Chicago. SuperSONIC addresses the\nchallenges of the Cloud-native era by providing a reusable, configurable\nframework that enhances the efficiency of accelerator-based inference\ndeployment across diverse scientific domains and industries."
                },
                "authors": [
                    {
                        "name": "Dmitry Kondratyev"
                    },
                    {
                        "name": "Benedikt Riedel"
                    },
                    {
                        "name": "Yuan-Tang Chou"
                    },
                    {
                        "name": "Miles Cochran-Branson"
                    },
                    {
                        "name": "Noah Paladino"
                    },
                    {
                        "name": "David Schultz"
                    },
                    {
                        "name": "Mia Liu"
                    },
                    {
                        "name": "Javier Duarte"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Shih-Chieh Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Chieh Hsu"
                },
                "author": "Shih-Chieh Hsu",
                "arxiv_doi": "10.1145/3708035.3736049",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708035.3736049",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.20657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Submission to PEARC25 Conference",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20651v1",
                "updated": "2025-06-25T17:49:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    49,
                    26,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:49:26Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    49,
                    26,
                    2,
                    176,
                    0
                ],
                "title": "Hear No Evil: Detecting Gradient Leakage by Malicious Servers in\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hear No Evil: Detecting Gradient Leakage by Malicious Servers in\n  Federated Learning"
                },
                "summary": "Recent work has shown that gradient updates in federated learning (FL) can\nunintentionally reveal sensitive information about a client's local data. This\nrisk becomes significantly greater when a malicious server manipulates the\nglobal model to provoke information-rich updates from clients. In this paper,\nwe adopt a defender's perspective to provide the first comprehensive analysis\nof malicious gradient leakage attacks and the model manipulation techniques\nthat enable them. Our investigation reveals a core trade-off: these attacks\ncannot be both highly effective in reconstructing private data and sufficiently\nstealthy to evade detection -- especially in realistic FL settings that\nincorporate common normalization techniques and federated averaging.\n  Building on this insight, we argue that malicious gradient leakage attacks,\nwhile theoretically concerning, are inherently limited in practice and often\ndetectable through basic monitoring. As a complementary contribution, we\npropose a simple, lightweight, and broadly applicable client-side detection\nmechanism that flags suspicious model updates before local training begins,\ndespite the fact that such detection may not be strictly necessary in realistic\nFL settings. This mechanism further underscores the feasibility of defending\nagainst these attacks with minimal overhead, offering a deployable safeguard\nfor privacy-conscious federated learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that gradient updates in federated learning (FL) can\nunintentionally reveal sensitive information about a client's local data. This\nrisk becomes significantly greater when a malicious server manipulates the\nglobal model to provoke information-rich updates from clients. In this paper,\nwe adopt a defender's perspective to provide the first comprehensive analysis\nof malicious gradient leakage attacks and the model manipulation techniques\nthat enable them. Our investigation reveals a core trade-off: these attacks\ncannot be both highly effective in reconstructing private data and sufficiently\nstealthy to evade detection -- especially in realistic FL settings that\nincorporate common normalization techniques and federated averaging.\n  Building on this insight, we argue that malicious gradient leakage attacks,\nwhile theoretically concerning, are inherently limited in practice and often\ndetectable through basic monitoring. As a complementary contribution, we\npropose a simple, lightweight, and broadly applicable client-side detection\nmechanism that flags suspicious model updates before local training begins,\ndespite the fact that such detection may not be strictly necessary in realistic\nFL settings. This mechanism further underscores the feasibility of defending\nagainst these attacks with minimal overhead, offering a deployable safeguard\nfor privacy-conscious federated learning systems."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Baochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Baochun Li"
                },
                "author": "Baochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20647v1",
                "updated": "2025-06-25T17:43:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    43,
                    19,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:43:19Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    43,
                    19,
                    2,
                    176,
                    0
                ],
                "title": "Experimental demonstration of high compression of space by optical\n  spaceplates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of high compression of space by optical\n  spaceplates"
                },
                "summary": "The physical size of optical imaging systems is one of the greatest\nconstraints on their use, limiting the performance and deployment of a range of\nsystems from telescopes to mobile phone cameras. Spaceplates are nonlocal\noptical devices that compress free-space propagation into a shorter distance,\npaving the way for more compact optical systems, potentially even thin flat\ncameras. Here, we demonstrate the first engineered optical spaceplate and\nexperimentally observe the highest space compression ratios yet demonstrated in\nany wavelength region, up to $\\mathcal{R}=176\\pm14$, which is 29 times higher\nthan any previous device. Our spaceplate is a multilayer stack, a\nwell-established commercial fabrication technology that supports mass\nproduction. The versatility of these stacks allows for the freedom to customize\nthe spaceplate's bandwidth and angular range, impossible with previous optical\nexperimental spaceplates, which were made of bulk materials. With the\nappropriate choice of these two parameters, multilayer spaceplates have\nnear-term applications in light detection and ranging (LIDAR) technologies,\nretinal scanners, endoscopes, and other size-constrained optical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The physical size of optical imaging systems is one of the greatest\nconstraints on their use, limiting the performance and deployment of a range of\nsystems from telescopes to mobile phone cameras. Spaceplates are nonlocal\noptical devices that compress free-space propagation into a shorter distance,\npaving the way for more compact optical systems, potentially even thin flat\ncameras. Here, we demonstrate the first engineered optical spaceplate and\nexperimentally observe the highest space compression ratios yet demonstrated in\nany wavelength region, up to $\\mathcal{R}=176\\pm14$, which is 29 times higher\nthan any previous device. Our spaceplate is a multilayer stack, a\nwell-established commercial fabrication technology that supports mass\nproduction. The versatility of these stacks allows for the freedom to customize\nthe spaceplate's bandwidth and angular range, impossible with previous optical\nexperimental spaceplates, which were made of bulk materials. With the\nappropriate choice of these two parameters, multilayer spaceplates have\nnear-term applications in light detection and ranging (LIDAR) technologies,\nretinal scanners, endoscopes, and other size-constrained optical devices."
                },
                "authors": [
                    {
                        "name": "Ryan Hogan"
                    },
                    {
                        "name": "Yaryna Mamchur"
                    },
                    {
                        "name": "R. Margoth Cordova-Castro"
                    },
                    {
                        "name": "Graham Carlow"
                    },
                    {
                        "name": "Brian T. Sullivan"
                    },
                    {
                        "name": "Orad Reshef"
                    },
                    {
                        "name": "Robert W. Boyd"
                    },
                    {
                        "name": "Jeff S. Lundeen"
                    }
                ],
                "author_detail": {
                    "name": "Jeff S. Lundeen"
                },
                "author": "Jeff S. Lundeen",
                "arxiv_comment": "8 pages, supplementary information included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20642v1",
                "updated": "2025-06-25T17:37:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    37,
                    59,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:37:59Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    37,
                    59,
                    2,
                    176,
                    0
                ],
                "title": "Memento: Note-Taking for Your Future Self",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memento: Note-Taking for Your Future Self"
                },
                "summary": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when\nreasoning must be tightly coupled with retrieval, as in multi-hop question\nanswering. To overcome these limitations, we introduce a prompting strategy\nthat first decomposes a complex question into smaller steps, then dynamically\nconstructs a database of facts using LLMs, and finally pieces these facts\ntogether to solve the question. We show how this three-stage strategy, which we\ncall Memento, can boost the performance of existing prompting strategies across\ndiverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the\nperformance of chain-of-thought (CoT) when all information is provided in\ncontext. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento\nimproves over vanilla CoT-RAG by more than 20 F1 percentage points and over the\nmulti-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the\nchallenging MuSiQue dataset, Memento improves ReAct by more than 3 F1\npercentage points, demonstrating its utility in agentic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when\nreasoning must be tightly coupled with retrieval, as in multi-hop question\nanswering. To overcome these limitations, we introduce a prompting strategy\nthat first decomposes a complex question into smaller steps, then dynamically\nconstructs a database of facts using LLMs, and finally pieces these facts\ntogether to solve the question. We show how this three-stage strategy, which we\ncall Memento, can boost the performance of existing prompting strategies across\ndiverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the\nperformance of chain-of-thought (CoT) when all information is provided in\ncontext. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento\nimproves over vanilla CoT-RAG by more than 20 F1 percentage points and over the\nmulti-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the\nchallenging MuSiQue dataset, Memento improves ReAct by more than 3 F1\npercentage points, demonstrating its utility in agentic settings."
                },
                "authors": [
                    {
                        "name": "Chao Wan"
                    },
                    {
                        "name": "Albert Gong"
                    },
                    {
                        "name": "Mihir Mishra"
                    },
                    {
                        "name": "Carl-Leander Henneking"
                    },
                    {
                        "name": "Claas Beger"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    }
                ],
                "author_detail": {
                    "name": "Kilian Q. Weinberger"
                },
                "author": "Kilian Q. Weinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20637v1",
                "updated": "2025-06-25T17:31:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    31,
                    46,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:31:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    31,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "MC for Agriculture: A Framework for Nature-inspired Sustainable Pest\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC for Agriculture: A Framework for Nature-inspired Sustainable Pest\n  Control"
                },
                "summary": "In agriculture, molecular communication (MC) is envisioned as a framework to\naddress critical challenges such as smart pest control. While conventional\napproaches mostly rely on synthetic plant protection products, posing high\nrisks for the environment, harnessing plant signaling processes can lead to\ninnovative approaches for nature-inspired sustainable pest control. In this\npaper, we investigate an approach for sustainable pest control and reveal how\nthe MC paradigm can be employed for analysis and optimization. In particular,\nwe consider a system where herbivore-induced plant volatiles (HIPVs),\nspecifically methyl salicylate (MeSA), is encapsulated into microspheres\ndeployed on deployed on plant leaves. The controlled release of MeSA from the\nmicrospheres, acting as transmitters (TXs), supports pest deterrence and\nantagonist attraction, providing an eco-friendly alternative to synthetic plant\nprotection products. Based on experimental data, we investigate the MeSA\nrelease kinetics and obtain an analytical model. To describe the propagation of\nMeSA in farming environments, we employ a three dimensional (3D)\nadvection-diffusion model, incorporating realistic wind fields which are\npredominantly affecting particle propagation, and solve it by a finite\ndifference method (FDM). The proposed model is used to investigate the MeSA\ndistribution for different TX arrangements, representing different practical\nmicrosphere deployment strategies. Moreover, we introduce the coverage\neffectiveness index (CEI) as a novel metric to quantify the environmental\ncoverage of MeSA. This analysis offers valuable guidance for the practical\ndevelopment of microspheres and their deployment aimed at enhancing coverage\nand, consequently, the attraction of antagonistic insects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In agriculture, molecular communication (MC) is envisioned as a framework to\naddress critical challenges such as smart pest control. While conventional\napproaches mostly rely on synthetic plant protection products, posing high\nrisks for the environment, harnessing plant signaling processes can lead to\ninnovative approaches for nature-inspired sustainable pest control. In this\npaper, we investigate an approach for sustainable pest control and reveal how\nthe MC paradigm can be employed for analysis and optimization. In particular,\nwe consider a system where herbivore-induced plant volatiles (HIPVs),\nspecifically methyl salicylate (MeSA), is encapsulated into microspheres\ndeployed on deployed on plant leaves. The controlled release of MeSA from the\nmicrospheres, acting as transmitters (TXs), supports pest deterrence and\nantagonist attraction, providing an eco-friendly alternative to synthetic plant\nprotection products. Based on experimental data, we investigate the MeSA\nrelease kinetics and obtain an analytical model. To describe the propagation of\nMeSA in farming environments, we employ a three dimensional (3D)\nadvection-diffusion model, incorporating realistic wind fields which are\npredominantly affecting particle propagation, and solve it by a finite\ndifference method (FDM). The proposed model is used to investigate the MeSA\ndistribution for different TX arrangements, representing different practical\nmicrosphere deployment strategies. Moreover, we introduce the coverage\neffectiveness index (CEI) as a novel metric to quantify the environmental\ncoverage of MeSA. This analysis offers valuable guidance for the practical\ndevelopment of microspheres and their deployment aimed at enhancing coverage\nand, consequently, the attraction of antagonistic insects."
                },
                "authors": [
                    {
                        "name": "Fardad Vakilipoor"
                    },
                    {
                        "name": "Nora Hirschmann"
                    },
                    {
                        "name": "Julian Schladt"
                    },
                    {
                        "name": "Stefan Schwab"
                    },
                    {
                        "name": "Annette Reineke"
                    },
                    {
                        "name": "Robert Schober"
                    },
                    {
                        "name": "Kathrin Castiglione"
                    },
                    {
                        "name": "Maximilian Schaefer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian Schaefer"
                },
                "author": "Maximilian Schaefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20636v1",
                "updated": "2025-06-25T17:31:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    31,
                    15,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:31:15Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    31,
                    15,
                    2,
                    176,
                    0
                ],
                "title": "A Computationally Aware Multi Objective Framework for Camera LiDAR\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Computationally Aware Multi Objective Framework for Camera LiDAR\n  Calibration"
                },
                "summary": "Accurate extrinsic calibration between LiDAR and camera sensors is important\nfor reliable perception in autonomous systems. In this paper, we present a\nnovel multi-objective optimization framework that jointly minimizes the\ngeometric alignment error and computational cost associated with camera-LiDAR\ncalibration. We optimize two objectives: (1) error between projected LiDAR\npoints and ground-truth image edges, and (2) a composite metric for\ncomputational cost reflecting runtime and resource usage. Using the NSGA-II\n\\cite{deb2002nsga2} evolutionary algorithm, we explore the parameter space\ndefined by 6-DoF transformations and point sampling rates, yielding a\nwell-characterized Pareto frontier that exposes trade-offs between calibration\nfidelity and resource efficiency. Evaluations are conducted on the KITTI\ndataset using its ground-truth extrinsic parameters for validation, with\nresults verified through both multi-objective and constrained single-objective\nbaselines. Compared to existing gradient-based and learned calibration methods,\nour approach demonstrates interpretable, tunable performance with lower\ndeployment overhead. Pareto-optimal configurations are further analyzed for\nparameter sensitivity and innovation insights. A preference-based\ndecision-making strategy selects solutions from the Pareto knee region to suit\nthe constraints of the embedded system. The robustness of calibration is tested\nacross variable edge-intensity weighting schemes, highlighting optimal balance\npoints. Although real-time deployment on embedded platforms is deferred to\nfuture work, this framework establishes a scalable and transparent method for\ncalibration under realistic misalignment and resource-limited conditions,\ncritical for long-term autonomy, particularly in SAE L3+ vehicles receiving OTA\nupdates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate extrinsic calibration between LiDAR and camera sensors is important\nfor reliable perception in autonomous systems. In this paper, we present a\nnovel multi-objective optimization framework that jointly minimizes the\ngeometric alignment error and computational cost associated with camera-LiDAR\ncalibration. We optimize two objectives: (1) error between projected LiDAR\npoints and ground-truth image edges, and (2) a composite metric for\ncomputational cost reflecting runtime and resource usage. Using the NSGA-II\n\\cite{deb2002nsga2} evolutionary algorithm, we explore the parameter space\ndefined by 6-DoF transformations and point sampling rates, yielding a\nwell-characterized Pareto frontier that exposes trade-offs between calibration\nfidelity and resource efficiency. Evaluations are conducted on the KITTI\ndataset using its ground-truth extrinsic parameters for validation, with\nresults verified through both multi-objective and constrained single-objective\nbaselines. Compared to existing gradient-based and learned calibration methods,\nour approach demonstrates interpretable, tunable performance with lower\ndeployment overhead. Pareto-optimal configurations are further analyzed for\nparameter sensitivity and innovation insights. A preference-based\ndecision-making strategy selects solutions from the Pareto knee region to suit\nthe constraints of the embedded system. The robustness of calibration is tested\nacross variable edge-intensity weighting schemes, highlighting optimal balance\npoints. Although real-time deployment on embedded platforms is deferred to\nfuture work, this framework establishes a scalable and transparent method for\ncalibration under realistic misalignment and resource-limited conditions,\ncritical for long-term autonomy, particularly in SAE L3+ vehicles receiving OTA\nupdates."
                },
                "authors": [
                    {
                        "name": "Venkat Karramreddy"
                    },
                    {
                        "name": "Rangarajan Ramanujam"
                    }
                ],
                "author_detail": {
                    "name": "Rangarajan Ramanujam"
                },
                "author": "Rangarajan Ramanujam",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20609v1",
                "updated": "2025-06-25T17:00:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    21,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:00:21Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    21,
                    2,
                    176,
                    0
                ],
                "title": "Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot\n  Recordings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot\n  Recordings"
                },
                "summary": "The escalating rates of gun-related violence and mass shootings represent a\nsignificant threat to public safety. Timely and accurate information for law\nenforcement agencies is crucial in mitigating these incidents. Current\ncommercial gunshot detection systems, while effective, often come with\nprohibitive costs. This research explores a cost-effective alternative by\nleveraging acoustic analysis of gunshot recordings, potentially obtainable from\nubiquitous devices like cell phones, to not only detect gunshots but also\nclassify the type of firearm used. This paper details a study on deciphering\ngun type hierarchies using a curated dataset of 3459 recordings. We investigate\nthe fundamental acoustic characteristics of gunshots, including muzzle blasts\nand shockwaves, which vary based on firearm type, ammunition, and shooting\ndirection. We propose and evaluate machine learning frameworks, including\nSupport Vector Machines (SVMs) as a baseline and a more advanced Convolutional\nNeural Network (CNN) architecture for joint gunshot detection and gun type\nclassification. Results indicate that our deep learning approach achieves a\nmean average precision (mAP) of 0.58 on clean labeled data, outperforming the\nSVM baseline (mAP 0.39). Challenges related to data quality, environmental\nnoise, and the generalization capabilities when using noisy web-sourced data\n(mAP 0.35) are also discussed. The long-term vision is to develop a highly\naccurate, real-time system deployable on common recording devices,\nsignificantly reducing detection costs and providing critical intelligence to\nfirst responders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating rates of gun-related violence and mass shootings represent a\nsignificant threat to public safety. Timely and accurate information for law\nenforcement agencies is crucial in mitigating these incidents. Current\ncommercial gunshot detection systems, while effective, often come with\nprohibitive costs. This research explores a cost-effective alternative by\nleveraging acoustic analysis of gunshot recordings, potentially obtainable from\nubiquitous devices like cell phones, to not only detect gunshots but also\nclassify the type of firearm used. This paper details a study on deciphering\ngun type hierarchies using a curated dataset of 3459 recordings. We investigate\nthe fundamental acoustic characteristics of gunshots, including muzzle blasts\nand shockwaves, which vary based on firearm type, ammunition, and shooting\ndirection. We propose and evaluate machine learning frameworks, including\nSupport Vector Machines (SVMs) as a baseline and a more advanced Convolutional\nNeural Network (CNN) architecture for joint gunshot detection and gun type\nclassification. Results indicate that our deep learning approach achieves a\nmean average precision (mAP) of 0.58 on clean labeled data, outperforming the\nSVM baseline (mAP 0.39). Challenges related to data quality, environmental\nnoise, and the generalization capabilities when using noisy web-sourced data\n(mAP 0.35) are also discussed. The long-term vision is to develop a highly\naccurate, real-time system deployable on common recording devices,\nsignificantly reducing detection costs and providing critical intelligence to\nfirst responders."
                },
                "authors": [
                    {
                        "name": "Ankit Shah"
                    },
                    {
                        "name": "Rita Singh"
                    },
                    {
                        "name": "Bhiksha Raj"
                    },
                    {
                        "name": "Alexander Hauptmann"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Hauptmann"
                },
                "author": "Alexander Hauptmann",
                "arxiv_comment": "4 pages + 1 References",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20608v1",
                "updated": "2025-06-25T17:00:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    5,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:00:05Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    5,
                    2,
                    176,
                    0
                ],
                "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base"
                },
                "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery."
                },
                "authors": [
                    {
                        "name": "Barry Smith"
                    },
                    {
                        "name": "Junchao Zhang"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Lois Curfman McInnes"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Archit Vasan"
                    },
                    {
                        "name": "Satish Balay"
                    },
                    {
                        "name": "Toby Isaac"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20606v1",
                "updated": "2025-06-25T16:51:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    51,
                    51,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:51:51Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    51,
                    51,
                    2,
                    176,
                    0
                ],
                "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior\n  Toward Beneficence or Harm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior\n  Toward Beneficence or Harm"
                },
                "summary": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing."
                },
                "authors": [
                    {
                        "name": "Baixiang Huang"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Huan Liu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "arxiv_comment": "Main paper: 9 pages; total: 18 pages (including appendix). Code,\n  data, results, and additional resources are available at:\n  https://model-editing.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11707v2",
                "updated": "2025-06-25T16:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    48,
                    16,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-17T11:46:46Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    46,
                    46,
                    0,
                    48,
                    0
                ],
                "title": "Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating\n  Large Language Models"
                },
                "summary": "This study utilizes the game Codenames as a benchmarking tool to evaluate\nlarge language models (LLMs) with respect to specific linguistic and cognitive\nskills. LLMs play each side of the game, where one side generates a clue word\ncovering several target words and the other guesses those target words. We\ndesigned various experiments by controlling the choice of words (abstract vs.\nconcrete words, ambiguous vs. monosemic) or the opponent (programmed to be\nfaster or slower in revealing words). Recent commercial and open-weight models\nwere compared side-by-side to find out factors affecting their performance. The\nevaluation reveals details about their strategies, challenging cases, and\nlimitations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study utilizes the game Codenames as a benchmarking tool to evaluate\nlarge language models (LLMs) with respect to specific linguistic and cognitive\nskills. LLMs play each side of the game, where one side generates a clue word\ncovering several target words and the other guesses those target words. We\ndesigned various experiments by controlling the choice of words (abstract vs.\nconcrete words, ambiguous vs. monosemic) or the opponent (programmed to be\nfaster or slower in revealing words). Recent commercial and open-weight models\nwere compared side-by-side to find out factors affecting their performance. The\nevaluation reveals details about their strategies, challenging cases, and\nlimitations of LLMs."
                },
                "authors": [
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "Lara Pfennigschmidt"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "Accepted at GemBench workshop co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20601v1",
                "updated": "2025-06-25T16:40:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    40,
                    17,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:40:17Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    40,
                    17,
                    2,
                    176,
                    0
                ],
                "title": "Video Perception Models for 3D Scene Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Perception Models for 3D Scene Synthesis"
                },
                "summary": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released."
                },
                "authors": [
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Zuria Bauer"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Gao Huang"
                    },
                    {
                        "name": "Francis Engelmann"
                    }
                ],
                "author_detail": {
                    "name": "Francis Engelmann"
                },
                "author": "Francis Engelmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20598v1",
                "updated": "2025-06-25T16:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    37,
                    46,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    37,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of\n  Multi-Agent AI for Addressing Sustainable Protein Production Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of\n  Multi-Agent AI for Addressing Sustainable Protein Production Challenges"
                },
                "summary": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities"
                },
                "authors": [
                    {
                        "name": "Alexander D. Kalian"
                    },
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Stefan P. Johannesson"
                    },
                    {
                        "name": "Lennart Otte"
                    },
                    {
                        "name": "Christer Hogstrand"
                    },
                    {
                        "name": "Miao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Miao Guo"
                },
                "author": "Miao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20595v1",
                "updated": "2025-06-25T16:34:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    34,
                    9,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:34:09Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    34,
                    9,
                    2,
                    176,
                    0
                ],
                "title": "AI in the Writing Process: How Purposeful AI Support Fosters Student\n  Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in the Writing Process: How Purposeful AI Support Fosters Student\n  Writing"
                },
                "summary": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content."
                },
                "authors": [
                    {
                        "name": "Momin N. Siddiqui"
                    },
                    {
                        "name": "Roy Pea"
                    },
                    {
                        "name": "Hari Subramonyam"
                    }
                ],
                "author_detail": {
                    "name": "Hari Subramonyam"
                },
                "author": "Hari Subramonyam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02280v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02280v3",
                "updated": "2025-06-26T01:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    1,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-02T21:39:40Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    21,
                    39,
                    40,
                    0,
                    153,
                    0
                ],
                "title": "The State of Large Language Models for African Languages: Progress and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The State of Large Language Models for African Languages: Progress and\n  Challenges"
                },
                "summary": "Large Language Models (LLMs) are transforming Natural Language Processing\n(NLP), but their benefits are largely absent for Africa's 2,000 low-resource\nlanguages. This paper comparatively analyzes African language coverage across\nsix LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).\nThe evaluation covers language coverage, training sets, technical limitations,\nscript problems, and language modelling roadmaps. The work identifies 42\nsupported African languages and 23 available public data sets, and it shows a\nbig gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are\nalways treated while there is over 98\\% of unsupported African languages.\nMoreover, the review shows that just Latin, Arabic, and Ge'ez scripts are\nidentified while 20 active scripts are neglected. Some of the primary\nchallenges are lack of data, tokenization biases, computational costs being\nvery high, and evaluation issues. These issues demand language standardization,\ncorpus development by the community, and effective adaptation methods for\nAfrican languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming Natural Language Processing\n(NLP), but their benefits are largely absent for Africa's 2,000 low-resource\nlanguages. This paper comparatively analyzes African language coverage across\nsix LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).\nThe evaluation covers language coverage, training sets, technical limitations,\nscript problems, and language modelling roadmaps. The work identifies 42\nsupported African languages and 23 available public data sets, and it shows a\nbig gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are\nalways treated while there is over 98\\% of unsupported African languages.\nMoreover, the review shows that just Latin, Arabic, and Ge'ez scripts are\nidentified while 20 active scripts are neglected. Some of the primary\nchallenges are lack of data, tokenization biases, computational costs being\nvery high, and evaluation issues. These issues demand language standardization,\ncorpus development by the community, and effective adaptation methods for\nAfrican languages."
                },
                "authors": [
                    {
                        "name": "Kedir Yassin Hussen"
                    },
                    {
                        "name": "Walelign Tewabe Sewunetie"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "Sukairaj Hafiz Imam"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    }
                ],
                "author_detail": {
                    "name": "Seid Muhie Yimam"
                },
                "author": "Seid Muhie Yimam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02280v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02280v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03905v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03905v3",
                "updated": "2025-06-25T16:21:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    21,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2024-12-05T06:21:31Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    21,
                    31,
                    3,
                    340,
                    0
                ],
                "title": "Integrating Various Software Artifacts for Better LLM-based Bug\n  Localization and Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Various Software Artifacts for Better LLM-based Bug\n  Localization and Program Repair"
                },
                "summary": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nFurthermore, we re-implemented and evaluated our framework, demonstrating its\neffectiveness in its effectiveness in resolving 9 unique issues compared to\nother state-of-the-art frameworks using the same or more advanced models on\nSWE-bench Lite.We also discussed whether a leading framework for Python code\ncan be directly applied to Java code, or vice versa. The source code and\nexperimental results of this work for replication are available at\nhttps://github.com/XYZboom/DEVLoRe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nFurthermore, we re-implemented and evaluated our framework, demonstrating its\neffectiveness in its effectiveness in resolving 9 unique issues compared to\nother state-of-the-art frameworks using the same or more advanced models on\nSWE-bench Lite.We also discussed whether a leading framework for Python code\ncan be directly applied to Java code, or vice versa. The source code and\nexperimental results of this work for replication are available at\nhttps://github.com/XYZboom/DEVLoRe."
                },
                "authors": [
                    {
                        "name": "Qiong Feng"
                    },
                    {
                        "name": "Xiaotian Ma"
                    },
                    {
                        "name": "Jiayi Sheng"
                    },
                    {
                        "name": "Ziyuan Feng"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Peng Liang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liang"
                },
                "author": "Peng Liang",
                "arxiv_comment": "25 pages, 12 images, 10 tables, Manuscript revision submitted to a\n  journal (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03905v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03905v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06256v2",
                "updated": "2025-06-25T16:21:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    21,
                    31,
                    2,
                    176,
                    0
                ],
                "published": "2025-01-09T09:45:05Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    45,
                    5,
                    3,
                    9,
                    0
                ],
                "title": "Unlocking In-Context Learning for Natural Datasets Beyond Language\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking In-Context Learning for Natural Datasets Beyond Language\n  Modelling"
                },
                "summary": "Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables\nthe model to perform new tasks conditioning only on the examples provided in\nthe context without updating the model's weights. While ICL offers fast\nadaptation across natural language tasks and domains, its emergence is less\nstraightforward for modalities beyond text. In this work, we systematically\nuncover properties present in LLMs that support the emergence of ICL for\nautoregressive models and various modalities by promoting the learning of the\nneeded mechanisms for ICL. We identify exact token repetitions in the training\ndata sequences as an important factor for ICL. Such repetitions further improve\nstability and reduce transiency in ICL performance. Moreover, we emphasise the\nsignificance of training task difficulty for the emergence of ICL. Finally, by\napplying our novel insights on ICL emergence, we unlock ICL capabilities for\nvarious visual datasets and a more challenging EEG classification task in a\nfew-shot learning regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables\nthe model to perform new tasks conditioning only on the examples provided in\nthe context without updating the model's weights. While ICL offers fast\nadaptation across natural language tasks and domains, its emergence is less\nstraightforward for modalities beyond text. In this work, we systematically\nuncover properties present in LLMs that support the emergence of ICL for\nautoregressive models and various modalities by promoting the learning of the\nneeded mechanisms for ICL. We identify exact token repetitions in the training\ndata sequences as an important factor for ICL. Such repetitions further improve\nstability and reduce transiency in ICL performance. Moreover, we emphasise the\nsignificance of training task difficulty for the emergence of ICL. Finally, by\napplying our novel insights on ICL emergence, we unlock ICL capabilities for\nvarious visual datasets and a more challenging EEG classification task in a\nfew-shot learning regime."
                },
                "authors": [
                    {
                        "name": "Jelena Bratulić"
                    },
                    {
                        "name": "Sudhanshu Mittal"
                    },
                    {
                        "name": "David T. Hoffmann"
                    },
                    {
                        "name": "Samuel Böhm"
                    },
                    {
                        "name": "Robin Tibor Schirrmeister"
                    },
                    {
                        "name": "Tonio Ball"
                    },
                    {
                        "name": "Christian Rupprecht"
                    },
                    {
                        "name": "Thomas Brox"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Brox"
                },
                "author": "Thomas Brox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19677v2",
                "updated": "2025-06-25T16:13:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    13,
                    14,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-24T14:44:33Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    33,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees"
                },
                "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving."
                },
                "authors": [
                    {
                        "name": "Shi Chang"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Hanan Lutfiyya"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20576v1",
                "updated": "2025-06-25T16:10:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    10,
                    20,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:10:20Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    10,
                    20,
                    2,
                    176,
                    0
                ],
                "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks\n  on NIDS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks\n  on NIDS"
                },
                "summary": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses."
                },
                "authors": [
                    {
                        "name": "Sabrine Ennaji"
                    },
                    {
                        "name": "Elhadj Benkhelifa"
                    },
                    {
                        "name": "Luigi V. Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Luigi V. Mancini"
                },
                "author": "Luigi V. Mancini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20566v1",
                "updated": "2025-06-25T16:01:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    1,
                    38,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T16:01:38Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    1,
                    38,
                    2,
                    176,
                    0
                ],
                "title": "HRIBench: Benchmarking Vision-Language Models for Real-Time Human\n  Perception in Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRIBench: Benchmarking Vision-Language Models for Real-Time Human\n  Perception in Human-Robot Interaction"
                },
                "summary": "Real-time human perception is crucial for effective human-robot interaction\n(HRI). Large vision-language models (VLMs) offer promising generalizable\nperceptual capabilities but often suffer from high latency, which negatively\nimpacts user experience and limits VLM applicability in real-world scenarios.\nTo systematically study VLM capabilities in human perception for HRI and\nperformance-latency trade-offs, we introduce HRIBench, a visual\nquestion-answering (VQA) benchmark designed to evaluate VLMs across a diverse\nset of human perceptual tasks critical for HRI. HRIBench covers five key\ndomains: (1) non-verbal cue understanding, (2) verbal instruction\nunderstanding, (3) human-robot object relationship understanding, (4) social\nnavigation, and (5) person identification. To construct HRIBench, we collected\ndata from real-world HRI environments to curate questions for non-verbal cue\nunderstanding, and leveraged publicly available datasets for the remaining four\ndomains. We curated 200 VQA questions for each domain, resulting in a total of\n1000 questions for HRIBench. We then conducted a comprehensive evaluation of\nboth state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.\nOur results show that, despite their generalizability, current VLMs still\nstruggle with core perceptual capabilities essential for HRI. Moreover, none of\nthe models within our experiments demonstrated a satisfactory\nperformance-latency trade-off suitable for real-time deployment, underscoring\nthe need for future research on developing smaller, low-latency VLMs with\nimproved human perception capabilities. HRIBench and our results can be found\nin this Github repository: https://github.com/interaction-lab/HRIBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time human perception is crucial for effective human-robot interaction\n(HRI). Large vision-language models (VLMs) offer promising generalizable\nperceptual capabilities but often suffer from high latency, which negatively\nimpacts user experience and limits VLM applicability in real-world scenarios.\nTo systematically study VLM capabilities in human perception for HRI and\nperformance-latency trade-offs, we introduce HRIBench, a visual\nquestion-answering (VQA) benchmark designed to evaluate VLMs across a diverse\nset of human perceptual tasks critical for HRI. HRIBench covers five key\ndomains: (1) non-verbal cue understanding, (2) verbal instruction\nunderstanding, (3) human-robot object relationship understanding, (4) social\nnavigation, and (5) person identification. To construct HRIBench, we collected\ndata from real-world HRI environments to curate questions for non-verbal cue\nunderstanding, and leveraged publicly available datasets for the remaining four\ndomains. We curated 200 VQA questions for each domain, resulting in a total of\n1000 questions for HRIBench. We then conducted a comprehensive evaluation of\nboth state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.\nOur results show that, despite their generalizability, current VLMs still\nstruggle with core perceptual capabilities essential for HRI. Moreover, none of\nthe models within our experiments demonstrated a satisfactory\nperformance-latency trade-off suitable for real-time deployment, underscoring\nthe need for future research on developing smaller, low-latency VLMs with\nimproved human perception capabilities. HRIBench and our results can be found\nin this Github repository: https://github.com/interaction-lab/HRIBench."
                },
                "authors": [
                    {
                        "name": "Zhonghao Shi"
                    },
                    {
                        "name": "Enyu Zhao"
                    },
                    {
                        "name": "Nathaniel Dennler"
                    },
                    {
                        "name": "Jingzhen Wang"
                    },
                    {
                        "name": "Xinyang Xu"
                    },
                    {
                        "name": "Kaleen Shrestha"
                    },
                    {
                        "name": "Mengxue Fu"
                    },
                    {
                        "name": "Daniel Seita"
                    },
                    {
                        "name": "Maja Matarić"
                    }
                ],
                "author_detail": {
                    "name": "Maja Matarić"
                },
                "author": "Maja Matarić",
                "arxiv_comment": "Accepted to the 19th International Symposium on Experimental Robotics\n  (ISER 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01841v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01841v3",
                "updated": "2025-06-25T15:57:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    57,
                    59,
                    2,
                    176,
                    0
                ],
                "published": "2024-08-03T18:48:41Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    18,
                    48,
                    41,
                    5,
                    216,
                    0
                ],
                "title": "BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for\n  Unmanned Ground Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for\n  Unmanned Ground Vehicles"
                },
                "summary": "This article introduces BEVPlace++, a novel, fast, and robust LiDAR global\nlocalization method for unmanned ground vehicles. It uses lightweight\nconvolutional neural networks (CNNs) on Bird's Eye View (BEV) image-like\nrepresentations of LiDAR data to achieve accurate global localization through\nplace recognition, followed by 3-DoF pose estimation. Our detailed analyses\nreveal an interesting fact that CNNs are inherently effective at extracting\ndistinctive features from LiDAR BEV images. Remarkably, keypoints of two BEV\nimages with large translations can be effectively matched using CNN-extracted\nfeatures. Building on this insight, we design a Rotation Equivariant Module\n(REM) to obtain distinctive features while enhancing robustness to rotational\nchanges. A Rotation Equivariant and Invariant Network (REIN) is then developed\nby cascading REM and a descriptor generator, NetVLAD, to sequentially generate\nrotation equivariant local features and rotation invariant global descriptors.\nThe global descriptors are used first to achieve robust place recognition, and\nthen local features are used for accurate pose estimation. \\revise{Experimental\nresults on seven public datasets and our UGV platform demonstrate that\nBEVPlace++, even when trained on a small dataset (3000 frames of KITTI) only\nwith place labels, generalizes well to unseen environments, performs\nconsistently across different days and years, and adapts to various types of\nLiDAR scanners.} BEVPlace++ achieves state-of-the-art performance in multiple\ntasks, including place recognition, loop closure detection, and global\nlocalization. Additionally, BEVPlace++ is lightweight, runs in real-time, and\ndoes not require accurate pose supervision, making it highly convenient for\ndeployment. \\revise{The source codes are publicly available at\nhttps://github.com/zjuluolun/BEVPlace2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces BEVPlace++, a novel, fast, and robust LiDAR global\nlocalization method for unmanned ground vehicles. It uses lightweight\nconvolutional neural networks (CNNs) on Bird's Eye View (BEV) image-like\nrepresentations of LiDAR data to achieve accurate global localization through\nplace recognition, followed by 3-DoF pose estimation. Our detailed analyses\nreveal an interesting fact that CNNs are inherently effective at extracting\ndistinctive features from LiDAR BEV images. Remarkably, keypoints of two BEV\nimages with large translations can be effectively matched using CNN-extracted\nfeatures. Building on this insight, we design a Rotation Equivariant Module\n(REM) to obtain distinctive features while enhancing robustness to rotational\nchanges. A Rotation Equivariant and Invariant Network (REIN) is then developed\nby cascading REM and a descriptor generator, NetVLAD, to sequentially generate\nrotation equivariant local features and rotation invariant global descriptors.\nThe global descriptors are used first to achieve robust place recognition, and\nthen local features are used for accurate pose estimation. \\revise{Experimental\nresults on seven public datasets and our UGV platform demonstrate that\nBEVPlace++, even when trained on a small dataset (3000 frames of KITTI) only\nwith place labels, generalizes well to unseen environments, performs\nconsistently across different days and years, and adapts to various types of\nLiDAR scanners.} BEVPlace++ achieves state-of-the-art performance in multiple\ntasks, including place recognition, loop closure detection, and global\nlocalization. Additionally, BEVPlace++ is lightweight, runs in real-time, and\ndoes not require accurate pose supervision, making it highly convenient for\ndeployment. \\revise{The source codes are publicly available at\nhttps://github.com/zjuluolun/BEVPlace2."
                },
                "authors": [
                    {
                        "name": "Lun Luo"
                    },
                    {
                        "name": "Si-Yuan Cao"
                    },
                    {
                        "name": "Xiaorui Li"
                    },
                    {
                        "name": "Jintao Xu"
                    },
                    {
                        "name": "Rui Ai"
                    },
                    {
                        "name": "Zhu Yu"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xieyuanli Chen"
                },
                "author": "Xieyuanli Chen",
                "arxiv_comment": "Accepted to IEEE Transactions on Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01841v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01841v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20558v1",
                "updated": "2025-06-25T15:56:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    56,
                    7,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:56:07Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    56,
                    7,
                    2,
                    176,
                    0
                ],
                "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment\n  Inconsistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment\n  Inconsistency"
                },
                "summary": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability."
                },
                "authors": [
                    {
                        "name": "Renyi Zhong"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Wenwei Gu"
                    },
                    {
                        "name": "Jinxi Kuang"
                    },
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Guangba Yu"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "This manuscript is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20554v1",
                "updated": "2025-06-25T15:53:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    53,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:53:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    53,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "Reinforcement Learning Increases Wind Farm Power Production by Enabling\n  Closed-Loop Collaborative Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Increases Wind Farm Power Production by Enabling\n  Closed-Loop Collaborative Control"
                },
                "summary": "Traditional wind farm control operates each turbine independently to maximize\nindividual power output. However, coordinated wake steering across the entire\nfarm can substantially increase the combined wind farm energy production.\nAlthough dynamic closed-loop control has proven effective in flow control\napplications, wind farm optimization has relied primarily on static,\nlow-fidelity simulators that ignore critical turbulent flow dynamics. In this\nwork, we present the first reinforcement learning (RL) controller integrated\ndirectly with high-fidelity large-eddy simulation (LES), enabling real-time\nresponse to atmospheric turbulence through collaborative, dynamic control\nstrategies. Our RL controller achieves a 4.30% increase in wind farm power\noutput compared to baseline operation, nearly doubling the 2.19% gain from\nstatic optimal yaw control obtained through Bayesian optimization. These\nresults establish dynamic flow-responsive control as a transformative approach\nto wind farm optimization, with direct implications for accelerating renewable\nenergy deployment to net-zero targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional wind farm control operates each turbine independently to maximize\nindividual power output. However, coordinated wake steering across the entire\nfarm can substantially increase the combined wind farm energy production.\nAlthough dynamic closed-loop control has proven effective in flow control\napplications, wind farm optimization has relied primarily on static,\nlow-fidelity simulators that ignore critical turbulent flow dynamics. In this\nwork, we present the first reinforcement learning (RL) controller integrated\ndirectly with high-fidelity large-eddy simulation (LES), enabling real-time\nresponse to atmospheric turbulence through collaborative, dynamic control\nstrategies. Our RL controller achieves a 4.30% increase in wind farm power\noutput compared to baseline operation, nearly doubling the 2.19% gain from\nstatic optimal yaw control obtained through Bayesian optimization. These\nresults establish dynamic flow-responsive control as a transformative approach\nto wind farm optimization, with direct implications for accelerating renewable\nenergy deployment to net-zero targets."
                },
                "authors": [
                    {
                        "name": "Andrew Mole"
                    },
                    {
                        "name": "Max Weissenbacher"
                    },
                    {
                        "name": "Georgios Rigas"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20551v1",
                "updated": "2025-06-25T15:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    50,
                    34,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:50:34Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    50,
                    34,
                    2,
                    176,
                    0
                ],
                "title": "Large Language Model-Driven Code Compliance Checking in Building\n  Information Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Code Compliance Checking in Building\n  Information Modeling"
                },
                "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects."
                },
                "authors": [
                    {
                        "name": "Soumya Madireddy"
                    },
                    {
                        "name": "Lu Gao"
                    },
                    {
                        "name": "Zia Din"
                    },
                    {
                        "name": "Kinam Kim"
                    },
                    {
                        "name": "Ahmed Senouci"
                    },
                    {
                        "name": "Zhe Han"
                    },
                    {
                        "name": "Yunpeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Zhang"
                },
                "author": "Yunpeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20544v1",
                "updated": "2025-06-25T15:37:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    37,
                    53,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:37:53Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    37,
                    53,
                    2,
                    176,
                    0
                ],
                "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages."
                },
                "authors": [
                    {
                        "name": "Ammar Khairi"
                    },
                    {
                        "name": "Daniel D'souza"
                    },
                    {
                        "name": "Ye Shen"
                    },
                    {
                        "name": "Julia Kreutzer"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01633v2",
                "updated": "2025-06-25T15:31:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    31,
                    17,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-03T18:59:01Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    1,
                    0,
                    34,
                    0
                ],
                "title": "Adversarial Reasoning at Jailbreaking Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Reasoning at Jailbreaking Time"
                },
                "summary": "As large language models (LLMs) are becoming more capable and widespread, the\nstudy of their failure cases is becoming increasingly important. Recent\nadvances in standardizing, measuring, and scaling test-time compute suggest new\nmethodologies for optimizing models to achieve high performance on hard tasks.\nIn this paper, we apply these advances to the task of model jailbreaking:\neliciting harmful responses from aligned LLMs. We develop an adversarial\nreasoning approach to automatic jailbreaking that leverages a loss signal to\nguide the test-time compute, achieving SOTA attack success rates against many\naligned LLMs, even those that aim to trade inference-time compute for\nadversarial robustness. Our approach introduces a new paradigm in understanding\nLLM vulnerabilities, laying the foundation for the development of more robust\nand trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are becoming more capable and widespread, the\nstudy of their failure cases is becoming increasingly important. Recent\nadvances in standardizing, measuring, and scaling test-time compute suggest new\nmethodologies for optimizing models to achieve high performance on hard tasks.\nIn this paper, we apply these advances to the task of model jailbreaking:\neliciting harmful responses from aligned LLMs. We develop an adversarial\nreasoning approach to automatic jailbreaking that leverages a loss signal to\nguide the test-time compute, achieving SOTA attack success rates against many\naligned LLMs, even those that aim to trade inference-time compute for\nadversarial robustness. Our approach introduces a new paradigm in understanding\nLLM vulnerabilities, laying the foundation for the development of more robust\nand trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Mahdi Sabbaghi"
                    },
                    {
                        "name": "Paul Kassianik"
                    },
                    {
                        "name": "George Pappas"
                    },
                    {
                        "name": "Yaron Singer"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Hamed Hassani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Hassani"
                },
                "author": "Hamed Hassani",
                "arxiv_comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20535v1",
                "updated": "2025-06-25T15:24:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    24,
                    45,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:24:45Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    24,
                    45,
                    2,
                    176,
                    0
                ],
                "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads"
                },
                "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI."
                },
                "authors": [
                    {
                        "name": "Hongzhen Huang"
                    },
                    {
                        "name": "Kunming Zhang"
                    },
                    {
                        "name": "Hanlong Liao"
                    },
                    {
                        "name": "Kui Wu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "arxiv_comment": "11 pages, 7 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20531v1",
                "updated": "2025-06-25T15:19:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    19,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:19:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    19,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Case-based Reasoning Augmented Large Language Model Framework for\n  Decision Making in Realistic Safety-Critical Driving Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case-based Reasoning Augmented Large Language Model Framework for\n  Decision Making in Realistic Safety-Critical Driving Scenarios"
                },
                "summary": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems."
                },
                "authors": [
                    {
                        "name": "Wenbin Gan"
                    },
                    {
                        "name": "Minh-Son Dao"
                    },
                    {
                        "name": "Koji Zettsu"
                    }
                ],
                "author_detail": {
                    "name": "Koji Zettsu"
                },
                "author": "Koji Zettsu",
                "arxiv_comment": "12 pages, 10 figures, under-review conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20530v1",
                "updated": "2025-06-25T15:18:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    18,
                    19,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:18:19Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    18,
                    19,
                    2,
                    176,
                    0
                ],
                "title": "Toward a Global Regime for Compute Governance: Building the Pause Button",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Global Regime for Compute Governance: Building the Pause Button"
                },
                "summary": "As AI capabilities rapidly advance, the risk of catastrophic harm from\nlarge-scale training runs is growing. Yet the compute infrastructure that\nenables such development remains largely unregulated. This paper proposes a\nconcrete framework for a global \"Compute Pause Button\": a governance system\ndesigned to prevent dangerously powerful AI systems from being trained by\nrestricting access to computational resources. We identify three key\nintervention points -- technical, traceability, and regulatory -- and organize\nthem within a Governance--Enforcement--Verification (GEV) framework to ensure\nrules are clear, violations are detectable, and compliance is independently\nverifiable. Technical mechanisms include tamper-proof FLOP caps, model locking,\nand offline licensing. Traceability tools track chips, components, and users\nacross the compute supply chain. Regulatory mechanisms establish constraints\nthrough export controls, production caps, and licensing schemes. Unlike\npost-deployment oversight, this approach targets the material foundations of\nadvanced AI development. Drawing from analogues ranging from nuclear\nnon-proliferation to pandemic-era vaccine coordination, we demonstrate how\ncompute can serve as a practical lever for global cooperation. While technical\nand political challenges remain, we argue that credible mechanisms already\nexist, and that the time to build this architecture is now, before the window\nfor effective intervention closes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI capabilities rapidly advance, the risk of catastrophic harm from\nlarge-scale training runs is growing. Yet the compute infrastructure that\nenables such development remains largely unregulated. This paper proposes a\nconcrete framework for a global \"Compute Pause Button\": a governance system\ndesigned to prevent dangerously powerful AI systems from being trained by\nrestricting access to computational resources. We identify three key\nintervention points -- technical, traceability, and regulatory -- and organize\nthem within a Governance--Enforcement--Verification (GEV) framework to ensure\nrules are clear, violations are detectable, and compliance is independently\nverifiable. Technical mechanisms include tamper-proof FLOP caps, model locking,\nand offline licensing. Traceability tools track chips, components, and users\nacross the compute supply chain. Regulatory mechanisms establish constraints\nthrough export controls, production caps, and licensing schemes. Unlike\npost-deployment oversight, this approach targets the material foundations of\nadvanced AI development. Drawing from analogues ranging from nuclear\nnon-proliferation to pandemic-era vaccine coordination, we demonstrate how\ncompute can serve as a practical lever for global cooperation. While technical\nand political challenges remain, we argue that credible mechanisms already\nexist, and that the time to build this architecture is now, before the window\nfor effective intervention closes."
                },
                "authors": [
                    {
                        "name": "Ananthi Al Ramiah"
                    },
                    {
                        "name": "Raymond Koopmanschap"
                    },
                    {
                        "name": "Josh Thorsteinson"
                    },
                    {
                        "name": "Sadruddin Khan"
                    },
                    {
                        "name": "Jim Zhou"
                    },
                    {
                        "name": "Shafira Noh"
                    },
                    {
                        "name": "Joep Meindertsma"
                    },
                    {
                        "name": "Farhan Shafiq"
                    }
                ],
                "author_detail": {
                    "name": "Farhan Shafiq"
                },
                "author": "Farhan Shafiq",
                "arxiv_comment": "34 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08745v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08745v4",
                "updated": "2025-06-25T15:16:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    16,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2024-11-13T16:26:19Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    26,
                    19,
                    2,
                    318,
                    0
                ],
                "title": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers"
                },
                "summary": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word-translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean\nrepresentation of a concept across different languages does not affect the\nmodels' ability to translate it, but instead improves it. Finally, we\ngeneralize to multi-token generation and demonstrate that the model can\ngenerate natural language description of those mean representations. Our\nresults provide evidence for the existence of language-agnostic concept\nrepresentations within the investigated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word-translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean\nrepresentation of a concept across different languages does not affect the\nmodels' ability to translate it, but instead improves it. Finally, we\ngeneralize to multi-token generation and demonstrate that the model can\ngenerate natural language description of those mean representations. Our\nresults provide evidence for the existence of language-agnostic concept\nrepresentations within the investigated models."
                },
                "authors": [
                    {
                        "name": "Clément Dumas"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "20 pages, 14 figures, previous version published under the title \"How\n  Do Llamas Process Multilingual Text? A Latent Exploration through Activation\n  Patching\" at the ICML 2024 mechanistic interpretability workshop at\n  https://openreview.net/forum?id=0ku2hIm4BS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08745v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08745v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20520v1",
                "updated": "2025-06-25T15:07:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    7,
                    16,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T15:07:16Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    7,
                    16,
                    2,
                    176,
                    0
                ],
                "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing\n  positive and negative rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing\n  positive and negative rewards"
                },
                "summary": "Reinforcement learning (RL) is increasingly used to align large language\nmodels (LLMs). Off-policy methods offer greater implementation simplicity and\ndata efficiency than on-policy techniques, but often result in suboptimal\nperformance. In this work, we study the intermediate range of algorithms\nbetween off-policy RL and supervised fine-tuning by analyzing a simple\noff-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with\n$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$\nemphasizes high-reward samples, while raising it penalizes low-reward ones more\nheavily. We first provide a theoretical analysis of this off-policy REINFORCE\nalgorithm, showing that when the baseline $V$ lower-bounds the expected reward,\nthe algorithm enjoys a policy improvement guarantee. Our analysis reveals that\nwhile on-policy updates can safely leverage both positive and negative signals,\noff-policy updates benefit from focusing more on positive rewards than on\nnegative ones. We validate our findings experimentally in a controlled\nstochastic bandit setting and through fine-tuning state-of-the-art LLMs on\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is increasingly used to align large language\nmodels (LLMs). Off-policy methods offer greater implementation simplicity and\ndata efficiency than on-policy techniques, but often result in suboptimal\nperformance. In this work, we study the intermediate range of algorithms\nbetween off-policy RL and supervised fine-tuning by analyzing a simple\noff-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with\n$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$\nemphasizes high-reward samples, while raising it penalizes low-reward ones more\nheavily. We first provide a theoretical analysis of this off-policy REINFORCE\nalgorithm, showing that when the baseline $V$ lower-bounds the expected reward,\nthe algorithm enjoys a policy improvement guarantee. Our analysis reveals that\nwhile on-policy updates can safely leverage both positive and negative signals,\noff-policy updates benefit from focusing more on positive rewards than on\nnegative ones. We validate our findings experimentally in a controlled\nstochastic bandit setting and through fine-tuning state-of-the-art LLMs on\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Charles Arnal"
                    },
                    {
                        "name": "Gaëtan Narozniak"
                    },
                    {
                        "name": "Vivien Cabannes"
                    },
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Remi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Remi Munos"
                },
                "author": "Remi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20503v1",
                "updated": "2025-06-25T14:49:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    49,
                    28,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:49:28Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    49,
                    28,
                    2,
                    176,
                    0
                ],
                "title": "BotHash: Efficient and Training-Free Bot Detection Through Approximate\n  Nearest Neighbor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BotHash: Efficient and Training-Free Bot Detection Through Approximate\n  Nearest Neighbor"
                },
                "summary": "Online Social Networks (OSNs) are a cornerstone in modern society, serving as\nplatforms for diverse content consumption by millions of users each day.\nHowever, the challenge of ensuring the accuracy of information shared on these\nplatforms remains significant, especially with the widespread dissemination of\ndisinformation. Social bots -- automated accounts designed to mimic human\nbehavior, frequently spreading misinformation -- represent one of the critical\nproblems of OSNs. The advent of Large Language Models (LLMs) has further\ncomplicated bot behaviors, making detection increasingly difficult. This paper\npresents BotHash, an innovative, training-free approach to social bot\ndetection. BotHash leverages a simplified user representation that enables\napproximate nearest-neighbor search to detect bots, avoiding the complexities\nof Deep-Learning model training and large dataset creation. We demonstrate that\nBotHash effectively differentiates between human and bot accounts, even when\nstate-of-the-art LLMs are employed to generate posts' content. BotHash offers\nseveral advantages over existing methods, including its independence from a\ntraining phase, robust performance with minimal ground-truth data, and early\ndetection capabilities, showing promising results across various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Social Networks (OSNs) are a cornerstone in modern society, serving as\nplatforms for diverse content consumption by millions of users each day.\nHowever, the challenge of ensuring the accuracy of information shared on these\nplatforms remains significant, especially with the widespread dissemination of\ndisinformation. Social bots -- automated accounts designed to mimic human\nbehavior, frequently spreading misinformation -- represent one of the critical\nproblems of OSNs. The advent of Large Language Models (LLMs) has further\ncomplicated bot behaviors, making detection increasingly difficult. This paper\npresents BotHash, an innovative, training-free approach to social bot\ndetection. BotHash leverages a simplified user representation that enables\napproximate nearest-neighbor search to detect bots, avoiding the complexities\nof Deep-Learning model training and large dataset creation. We demonstrate that\nBotHash effectively differentiates between human and bot accounts, even when\nstate-of-the-art LLMs are employed to generate posts' content. BotHash offers\nseveral advantages over existing methods, including its independence from a\ntraining phase, robust performance with minimal ground-truth data, and early\ndetection capabilities, showing promising results across various datasets."
                },
                "authors": [
                    {
                        "name": "Edoardo Di Paolo"
                    },
                    {
                        "name": "Fabio De Gaspari"
                    },
                    {
                        "name": "Angelo Spognardi"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Spognardi"
                },
                "author": "Angelo Spognardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04030v2",
                "updated": "2025-06-25T14:44:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    44,
                    30,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-06T12:47:25Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    47,
                    25,
                    3,
                    37,
                    0
                ],
                "title": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging"
                },
                "summary": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps."
                },
                "authors": [
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20495v1",
                "updated": "2025-06-25T14:41:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    41,
                    13,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:41:13Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    41,
                    13,
                    2,
                    176,
                    0
                ],
                "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCode: Updating Code API Knowledge with Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode."
                },
                "authors": [
                    {
                        "name": "Haoze Wu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20488v1",
                "updated": "2025-06-25T14:36:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    36,
                    31,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:36:31Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    36,
                    31,
                    2,
                    176,
                    0
                ],
                "title": "Generative AI for Vulnerability Detection in 6G Wireless Networks:\n  Advances, Case Study, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Vulnerability Detection in 6G Wireless Networks:\n  Advances, Case Study, and Future Directions"
                },
                "summary": "The rapid advancement of 6G wireless networks, IoT, and edge computing has\nsignificantly expanded the cyberattack surface, necessitating more intelligent\nand adaptive vulnerability detection mechanisms. Traditional security methods,\nwhile foundational, struggle with zero-day exploits, adversarial threats, and\ncontext-dependent vulnerabilities in highly dynamic network environments.\nGenerative AI (GAI) emerges as a transformative solution, leveraging synthetic\ndata generation, multimodal reasoning, and adaptive learning to enhance\nsecurity frameworks. This paper explores the integration of GAI-powered\nvulnerability detection in 6G wireless networks, focusing on code auditing,\nprotocol security, cloud-edge defenses, and hardware protection. We introduce a\nthree-layer framework comprising the Technology Layer, Capability Layer, and\nApplication Layer to systematically analyze the role of VAEs, GANs, LLMs, and\nGDMs in securing next-generation wireless ecosystems. To demonstrate practical\nimplementation, we present a case study on LLM-driven code vulnerability\ndetection, highlighting its effectiveness, performance, and challenges.\nFinally, we outline future research directions, including lightweight models,\nhigh-authenticity data generation, external knowledge integration, and\nprivacy-preserving technologies. By synthesizing current advancements and open\nchallenges, this work provides a roadmap for researchers and practitioners to\nharness GAI for building resilient and adaptive security solutions in 6G\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of 6G wireless networks, IoT, and edge computing has\nsignificantly expanded the cyberattack surface, necessitating more intelligent\nand adaptive vulnerability detection mechanisms. Traditional security methods,\nwhile foundational, struggle with zero-day exploits, adversarial threats, and\ncontext-dependent vulnerabilities in highly dynamic network environments.\nGenerative AI (GAI) emerges as a transformative solution, leveraging synthetic\ndata generation, multimodal reasoning, and adaptive learning to enhance\nsecurity frameworks. This paper explores the integration of GAI-powered\nvulnerability detection in 6G wireless networks, focusing on code auditing,\nprotocol security, cloud-edge defenses, and hardware protection. We introduce a\nthree-layer framework comprising the Technology Layer, Capability Layer, and\nApplication Layer to systematically analyze the role of VAEs, GANs, LLMs, and\nGDMs in securing next-generation wireless ecosystems. To demonstrate practical\nimplementation, we present a case study on LLM-driven code vulnerability\ndetection, highlighting its effectiveness, performance, and challenges.\nFinally, we outline future research directions, including lightweight models,\nhigh-authenticity data generation, external knowledge integration, and\nprivacy-preserving technologies. By synthesizing current advancements and open\nchallenges, this work provides a roadmap for researchers and practitioners to\nharness GAI for building resilient and adaptive security solutions in 6G\nnetworks."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Danyang Song"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    }
                ],
                "author_detail": {
                    "name": "Edith C. H. Ngai"
                },
                "author": "Edith C. H. Ngai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20480v1",
                "updated": "2025-06-25T14:24:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    24,
                    59,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:24:59Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    24,
                    59,
                    2,
                    176,
                    0
                ],
                "title": "GPTailor: Large Language Model Pruning Through Layer Cutting and\n  Stitching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTailor: Large Language Model Pruning Through Layer Cutting and\n  Stitching"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\ndeployment and inference. While structured pruning of model parameters offers a\npromising way to reduce computational costs at deployment time, current methods\nprimarily focus on single model pruning. In this work, we develop a novel\nstrategy to compress models by strategically combining or merging layers from\nfinetuned model variants, which preserves the original model's abilities by\naggregating capabilities accentuated in different finetunes. We pose the\noptimal tailoring of these LLMs as a zero-order optimization problem, adopting\na search space that supports three different operations: (1) Layer removal, (2)\nLayer selection from different candidate models, and (3) Layer merging. Our\nexperiments demonstrate that this approach leads to competitive model pruning,\nfor example, for the Llama2-13B model families, our compressed models maintain\napproximately 97.3\\% of the original performance while removing $\\sim25\\%$ of\nparameters, significantly outperforming previous state-of-the-art methods. The\ncode is available at https://github.com/Guinan-Su/auto-merge-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\ndeployment and inference. While structured pruning of model parameters offers a\npromising way to reduce computational costs at deployment time, current methods\nprimarily focus on single model pruning. In this work, we develop a novel\nstrategy to compress models by strategically combining or merging layers from\nfinetuned model variants, which preserves the original model's abilities by\naggregating capabilities accentuated in different finetunes. We pose the\noptimal tailoring of these LLMs as a zero-order optimization problem, adopting\na search space that supports three different operations: (1) Layer removal, (2)\nLayer selection from different candidate models, and (3) Layer merging. Our\nexperiments demonstrate that this approach leads to competitive model pruning,\nfor example, for the Llama2-13B model families, our compressed models maintain\napproximately 97.3\\% of the original performance while removing $\\sim25\\%$ of\nparameters, significantly outperforming previous state-of-the-art methods. The\ncode is available at https://github.com/Guinan-Su/auto-merge-llm."
                },
                "authors": [
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Yanwu Yang"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19494v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19494v3",
                "updated": "2025-06-25T14:24:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    24,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2024-10-25T11:51:37Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models"
                },
                "summary": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel."
                },
                "authors": [
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19494v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19494v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03906v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03906v3",
                "updated": "2025-06-25T14:22:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    22,
                    4,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-06T18:22:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    22,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "MARCO: Multi-Agent Code Optimization with Real-Time Knowledge\n  Integration for High-Performance Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCO: Multi-Agent Code Optimization with Real-Time Knowledge\n  Integration for High-Performance Computing"
                },
                "summary": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6\\% average runtime reduction\ncompared to Claude 3.5 Sonnet alone, while the integration of the web-search\ncomponent yields a 30.9\\% performance improvement over the base MARCO system.\nThese results highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6\\% average runtime reduction\ncompared to Claude 3.5 Sonnet alone, while the integration of the web-search\ncomponent yields a 30.9\\% performance improvement over the base MARCO system.\nThese results highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Asif Rahman"
                    },
                    {
                        "name": "Veljko Cvetkovic"
                    },
                    {
                        "name": "Kathleen Reece"
                    },
                    {
                        "name": "Aidan Walters"
                    },
                    {
                        "name": "Yasir Hassan"
                    },
                    {
                        "name": "Aneesh Tummeti"
                    },
                    {
                        "name": "Bryan Torres"
                    },
                    {
                        "name": "Denise Cooney"
                    },
                    {
                        "name": "Margaret Ellis"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "9 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03906v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03906v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20471v1",
                "updated": "2025-06-25T14:19:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    19,
                    57,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T14:19:57Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    19,
                    57,
                    2,
                    176,
                    0
                ],
                "title": "Probing AI Safety with Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing AI Safety with Source Code"
                },
                "summary": "Large language models (LLMs) have become ubiquitous, interfacing with humans\nin numerous safety-critical applications. This necessitates improving\ncapabilities, but importantly coupled with greater safety measures to align\nthese models with human values and preferences. In this work, we demonstrate\nthat contemporary models fall concerningly short of the goal of AI safety,\nleading to an unsafe and harmful experience for users. We introduce a prompting\nstrategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT\nconverts natural language inputs to simple code that represents the same\nintent. For instance, CoDoT transforms the natural language prompt \"Make the\nstatement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT\nresults in a consistent failure of a wide range of state-of-the-art LLMs. For\nexample, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of\nthe time, and toxicity increases 300% on average across seven modern LLMs.\nAdditionally, recursively applying CoDoT can further increase toxicity two\ntimes. Given the rapid and widespread adoption of LLMs, CoDoT underscores the\ncritical need to evaluate safety efforts from first principles, ensuring that\nsafety and capabilities advance together.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become ubiquitous, interfacing with humans\nin numerous safety-critical applications. This necessitates improving\ncapabilities, but importantly coupled with greater safety measures to align\nthese models with human values and preferences. In this work, we demonstrate\nthat contemporary models fall concerningly short of the goal of AI safety,\nleading to an unsafe and harmful experience for users. We introduce a prompting\nstrategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT\nconverts natural language inputs to simple code that represents the same\nintent. For instance, CoDoT transforms the natural language prompt \"Make the\nstatement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT\nresults in a consistent failure of a wide range of state-of-the-art LLMs. For\nexample, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of\nthe time, and toxicity increases 300% on average across seven modern LLMs.\nAdditionally, recursively applying CoDoT can further increase toxicity two\ntimes. Given the rapid and widespread adoption of LLMs, CoDoT underscores the\ncritical need to evaluate safety efforts from first principles, ensuring that\nsafety and capabilities advance together."
                },
                "authors": [
                    {
                        "name": "Ujwal Narayan"
                    },
                    {
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Vishvak Murahari"
                    }
                ],
                "author_detail": {
                    "name": "Vishvak Murahari"
                },
                "author": "Vishvak Murahari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07089v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07089v3",
                "updated": "2025-06-25T14:14:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    14,
                    56,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-11T18:38:00Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    18,
                    38,
                    0,
                    6,
                    131,
                    0
                ],
                "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models"
                },
                "summary": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\ninherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often\nunderperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sightedness in\nthe planning process, and hallucinations during command generation. Moreover,\nthe trial-and-error nature of the PT process is constrained by existing\nframeworks lacking mechanisms to learn from previous failures, restricting\nadaptive improvement of PT strategies. To address these limitations, we propose\na knowledge-informed, self-reflective PT framework powered by LLMs, called\nRefPentester. This AutoPT framework is designed to assist human operators in\nidentifying the current stage of the PT process, selecting appropriate tactics\nand techniques for each stage, choosing suggested actions, providing\nstep-by-step operational guidance, and reflecting on and learning from previous\nfailed operations. We also modeled the PT process as a seven-state Stage\nMachine to integrate the proposed framework effectively. The evaluation shows\nthat RefPentester can successfully reveal credentials on Hack The Box's Sau\nmachine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages,\nRefPentester also demonstrates superior success rates on PT stage transitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\ninherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often\nunderperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sightedness in\nthe planning process, and hallucinations during command generation. Moreover,\nthe trial-and-error nature of the PT process is constrained by existing\nframeworks lacking mechanisms to learn from previous failures, restricting\nadaptive improvement of PT strategies. To address these limitations, we propose\na knowledge-informed, self-reflective PT framework powered by LLMs, called\nRefPentester. This AutoPT framework is designed to assist human operators in\nidentifying the current stage of the PT process, selecting appropriate tactics\nand techniques for each stage, choosing suggested actions, providing\nstep-by-step operational guidance, and reflecting on and learning from previous\nfailed operations. We also modeled the PT process as a seven-state Stage\nMachine to integrate the proposed framework effectively. The evaluation shows\nthat RefPentester can successfully reveal credentials on Hack The Box's Sau\nmachine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages,\nRefPentester also demonstrates superior success rates on PT stage transitions."
                },
                "authors": [
                    {
                        "name": "Hanzheng Dai"
                    },
                    {
                        "name": "Yuanliang Li"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Zhibo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Zhang"
                },
                "author": "Zhibo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07089v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07089v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20767v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20767v4",
                "updated": "2025-06-25T14:02:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    2,
                    19,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-27T06:16:27Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    16,
                    27,
                    1,
                    147,
                    0
                ],
                "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing\n  Cognitive Faithfulness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniBench: A Legal-inspired Framework and Dataset for Assessing\n  Cognitive Faithfulness of Large Language Models"
                },
                "summary": "Faithfulness hallucinations are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandards, existing benchmarks focus on \"factual statements\" that rephrase\nsource materials while overlooking \"cognitive statements\" that involve making\ninferences from the given context. Consequently, evaluating and detecting the\nhallucination of cognitive statements remains challenging. Inspired by how\nevidence is assessed in the legal domain, we design a rigorous framework to\nassess different levels of faithfulness of cognitive statements and introduce\nthe CogniBench dataset where we reveal insightful statistics. To keep pace with\nrapidly evolving LLMs, we further develop an automatic annotation pipeline that\nscales easily across different models. This results in a large-scale\nCogniBench-L dataset, which facilitates training accurate detectors for both\nfactual and cognitive hallucinations. We release our model and datasets at:\nhttps://github.com/FUTUREEEEEE/CogniBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness hallucinations are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandards, existing benchmarks focus on \"factual statements\" that rephrase\nsource materials while overlooking \"cognitive statements\" that involve making\ninferences from the given context. Consequently, evaluating and detecting the\nhallucination of cognitive statements remains challenging. Inspired by how\nevidence is assessed in the legal domain, we design a rigorous framework to\nassess different levels of faithfulness of cognitive statements and introduce\nthe CogniBench dataset where we reveal insightful statistics. To keep pace with\nrapidly evolving LLMs, we further develop an automatic annotation pipeline that\nscales easily across different models. This results in a large-scale\nCogniBench-L dataset, which facilitates training accurate detectors for both\nfactual and cognitive hallucinations. We release our model and datasets at:\nhttps://github.com/FUTUREEEEEE/CogniBench"
                },
                "authors": [
                    {
                        "name": "Xiaqiang Tang"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Keyu Hu"
                    },
                    {
                        "name": "Du Nan"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Sihong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Sihong Xie"
                },
                "author": "Sihong Xie",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20767v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20767v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20451v1",
                "updated": "2025-06-25T13:57:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    57,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:57:54Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    57,
                    54,
                    2,
                    176,
                    0
                ],
                "title": "Automatic Demonstration Selection for LLM-based Tabular Data\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Demonstration Selection for LLM-based Tabular Data\n  Classification"
                },
                "summary": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs."
                },
                "authors": [
                    {
                        "name": "Shuchu Han"
                    },
                    {
                        "name": "Wolfgang Bruckner"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Bruckner"
                },
                "author": "Wolfgang Bruckner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11677v2",
                "updated": "2025-06-25T13:46:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    46,
                    10,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-17T11:11:09Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    11,
                    9,
                    0,
                    48,
                    0
                ],
                "title": "Towards Fully Exploiting LLM Internal States to Enhance Knowledge\n  Boundary Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Exploiting LLM Internal States to Enhance Knowledge\n  Boundary Perception"
                },
                "summary": "Large language models (LLMs) exhibit impressive performance across diverse\ntasks but often struggle to accurately gauge their knowledge boundaries,\nleading to confident yet incorrect responses. This paper explores leveraging\nLLMs' internal states to enhance their perception of knowledge boundaries from\nefficiency and risk perspectives. We investigate whether LLMs can estimate\ntheir confidence using internal states before response generation, potentially\nsaving computational resources. Our experiments on datasets like Natural\nQuestions, HotpotQA, and MMLU reveal that LLMs demonstrate significant\npre-generation perception, which is further refined post-generation, with\nperception gaps remaining stable across varying conditions. To mitigate risks\nin critical domains, we introduce Confidence Consistency-based Calibration\n($C^3$), which assesses confidence consistency through question reformulation.\n$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,\nenhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our\nfindings suggest that pre-generation confidence estimation can optimize\nefficiency, while $C^3$ effectively controls output risks, advancing the\nreliability of LLMs in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive performance across diverse\ntasks but often struggle to accurately gauge their knowledge boundaries,\nleading to confident yet incorrect responses. This paper explores leveraging\nLLMs' internal states to enhance their perception of knowledge boundaries from\nefficiency and risk perspectives. We investigate whether LLMs can estimate\ntheir confidence using internal states before response generation, potentially\nsaving computational resources. Our experiments on datasets like Natural\nQuestions, HotpotQA, and MMLU reveal that LLMs demonstrate significant\npre-generation perception, which is further refined post-generation, with\nperception gaps remaining stable across varying conditions. To mitigate risks\nin critical domains, we introduce Confidence Consistency-based Calibration\n($C^3$), which assesses confidence consistency through question reformulation.\n$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,\nenhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our\nfindings suggest that pre-generation confidence estimation can optimize\nefficiency, while $C^3$ effectively controls output risks, advancing the\nreliability of LLMs in practical applications."
                },
                "authors": [
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Lulu Yu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20430v1",
                "updated": "2025-06-25T13:42:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    42,
                    26,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:42:26Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    42,
                    26,
                    2,
                    176,
                    0
                ],
                "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning"
                },
                "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor."
                },
                "authors": [
                    {
                        "name": "Weike Zhao"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Yanjie Fan"
                    },
                    {
                        "name": "Xiaoman Zhang"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Yuze Sun"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20415v1",
                "updated": "2025-06-25T13:31:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    31,
                    13,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:31:13Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    31,
                    13,
                    2,
                    176,
                    0
                ],
                "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SV-LLM: An Agentic Approach for SoC Security Verification using Large\n  Language Models"
                },
                "summary": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy."
                },
                "authors": [
                    {
                        "name": "Dipayan Saha"
                    },
                    {
                        "name": "Shams Tarek"
                    },
                    {
                        "name": "Hasan Al Shaikh"
                    },
                    {
                        "name": "Khan Thamid Hasan"
                    },
                    {
                        "name": "Pavan Sai Nalluri"
                    },
                    {
                        "name": "Md. Ajoad Hasan"
                    },
                    {
                        "name": "Nashmin Alam"
                    },
                    {
                        "name": "Jingbo Zhou"
                    },
                    {
                        "name": "Sujan Kumar Saha"
                    },
                    {
                        "name": "Mark Tehranipoor"
                    },
                    {
                        "name": "Farimah Farahmandi"
                    }
                ],
                "author_detail": {
                    "name": "Farimah Farahmandi"
                },
                "author": "Farimah Farahmandi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17219v2",
                "updated": "2025-06-25T13:27:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    27,
                    49,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-20T17:59:52Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    52,
                    4,
                    171,
                    0
                ],
                "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning"
                },
                "summary": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training."
                },
                "authors": [
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Zhaoxi Zhang"
                    },
                    {
                        "name": "Haoxiang Guan"
                    },
                    {
                        "name": "Yilin Cheng"
                    },
                    {
                        "name": "Yitong Duan"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Shuxin Zheng"
                    },
                    {
                        "name": "Jiyan He"
                    }
                ],
                "author_detail": {
                    "name": "Jiyan He"
                },
                "author": "Jiyan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20409v2",
                "updated": "2025-06-26T13:09:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    9,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-25T13:24:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    24,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPS: Tool-Augmented Personalisation via Structured Tagging"
                },
                "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."
                },
                "authors": [
                    {
                        "name": "Ekaterina Taktasheva"
                    },
                    {
                        "name": "Jeff Dalton"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Dalton"
                },
                "author": "Jeff Dalton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20403v1",
                "updated": "2025-06-25T13:17:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    17,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:17:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    17,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "A digital twin of atomic ensemble quantum memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A digital twin of atomic ensemble quantum memories"
                },
                "summary": "Accurate performance estimation of experimentally demonstrated quantum\nmemories is key to understand the nuances in their deployment in photonic\nquantum networks. While several software packages allow for accessible quantum\nsimulation, they often do not account for the loss and noise in physical\ndevices. We present a framework for modeling ensemble-based atomic quantum\nmemories using the quantum channel formalism. We provide a Kraus matrix\nrepresentation of several experimentally implemented state-of-the art quantum\nmemories and give an overview of their most important performance metrics. To\nshowcase the applicability of this approach, we implement a memory-assisted\nquantum token protocol within our simulation framework. Our digital twin model\nis readily extensible to other memory implementations and easily compatible\nwith existing frameworks for performance simulation of experimental quantum\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate performance estimation of experimentally demonstrated quantum\nmemories is key to understand the nuances in their deployment in photonic\nquantum networks. While several software packages allow for accessible quantum\nsimulation, they often do not account for the loss and noise in physical\ndevices. We present a framework for modeling ensemble-based atomic quantum\nmemories using the quantum channel formalism. We provide a Kraus matrix\nrepresentation of several experimentally implemented state-of-the art quantum\nmemories and give an overview of their most important performance metrics. To\nshowcase the applicability of this approach, we implement a memory-assisted\nquantum token protocol within our simulation framework. Our digital twin model\nis readily extensible to other memory implementations and easily compatible\nwith existing frameworks for performance simulation of experimental quantum\nnetworks."
                },
                "authors": [
                    {
                        "name": "Elizabeth Robertson"
                    },
                    {
                        "name": "Benjamin Maaß"
                    },
                    {
                        "name": "Konrad Tschernig"
                    },
                    {
                        "name": "Janik Wolters"
                    }
                ],
                "author_detail": {
                    "name": "Janik Wolters"
                },
                "author": "Janik Wolters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20357v1",
                "updated": "2025-06-25T12:18:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    18,
                    34,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T12:18:34Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    18,
                    34,
                    2,
                    176,
                    0
                ],
                "title": "Tabular Feature Discovery With Reasoning Type Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular Feature Discovery With Reasoning Type Exploration"
                },
                "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data."
                },
                "authors": [
                    {
                        "name": "Sungwon Han"
                    },
                    {
                        "name": "Sungkyu Park"
                    },
                    {
                        "name": "Seungeon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seungeon Lee"
                },
                "author": "Seungeon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20353v1",
                "updated": "2025-06-25T12:04:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    4,
                    53,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T12:04:53Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    12,
                    4,
                    53,
                    2,
                    176,
                    0
                ],
                "title": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression"
                },
                "summary": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Yunjian Zhang"
                    },
                    {
                        "name": "Xiu Yan"
                    },
                    {
                        "name": "Yueqi Zhou"
                    },
                    {
                        "name": "Kaihao Huang"
                    },
                    {
                        "name": "Suzhong Fu"
                    },
                    {
                        "name": "Chuanlong Xie"
                    },
                    {
                        "name": "Yao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Zhu"
                },
                "author": "Yao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19750v3",
                "updated": "2025-06-26T06:52:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    52,
                    46,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T16:06:37Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    6,
                    37,
                    1,
                    175,
                    0
                ],
                "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A\n  Synthetic Vignette Simulation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A\n  Synthetic Vignette Simulation Approach"
                },
                "summary": "Symptom Checkers (SCs) provide medical information tailored to user symptoms.\nA critical challenge in SC development is preventing unexpected performance\ndegradation for individual diseases, especially rare diseases, when updating\nalgorithms. This risk stems from the lack of practical pre-deployment\nevaluation methods. For rare diseases, obtaining sufficient evaluation data\nfrom user feedback is difficult. To evaluate the impact of algorithm updates on\nthe diagnostic performance for individual rare diseases before deployment, this\nstudy proposes and validates a novel Synthetic Vignette Simulation Approach.\nThis approach aims to enable this essential evaluation efficiently and at a low\ncost. To estimate the impact of algorithm updates, we generated synthetic\nvignettes from disease-phenotype annotations in the Human Phenotype Ontology\n(HPO), a publicly available knowledge base for rare diseases curated by\nexperts. Using these vignettes, we simulated SC interviews to predict changes\nin diagnostic performance. The effectiveness of this approach was validated\nretrospectively by comparing the predicted changes with actual performance\nmetrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight\npast algorithm updates for rare diseases, showed that the proposed method\naccurately predicted performance changes for diseases with phenotype frequency\ninformation in HPO (n=5). For these updates, we found a strong correlation for\nboth Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =\n0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of\nSC algorithm changes for individual rare diseases. This evaluation is based on\na publicly available medical knowledge database created by experts, ensuring\ntransparency and explainability for stakeholders. Additionally, SC developers\ncan efficiently improve diagnostic performance at a low cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symptom Checkers (SCs) provide medical information tailored to user symptoms.\nA critical challenge in SC development is preventing unexpected performance\ndegradation for individual diseases, especially rare diseases, when updating\nalgorithms. This risk stems from the lack of practical pre-deployment\nevaluation methods. For rare diseases, obtaining sufficient evaluation data\nfrom user feedback is difficult. To evaluate the impact of algorithm updates on\nthe diagnostic performance for individual rare diseases before deployment, this\nstudy proposes and validates a novel Synthetic Vignette Simulation Approach.\nThis approach aims to enable this essential evaluation efficiently and at a low\ncost. To estimate the impact of algorithm updates, we generated synthetic\nvignettes from disease-phenotype annotations in the Human Phenotype Ontology\n(HPO), a publicly available knowledge base for rare diseases curated by\nexperts. Using these vignettes, we simulated SC interviews to predict changes\nin diagnostic performance. The effectiveness of this approach was validated\nretrospectively by comparing the predicted changes with actual performance\nmetrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight\npast algorithm updates for rare diseases, showed that the proposed method\naccurately predicted performance changes for diseases with phenotype frequency\ninformation in HPO (n=5). For these updates, we found a strong correlation for\nboth Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =\n0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of\nSC algorithm changes for individual rare diseases. This evaluation is based on\na publicly available medical knowledge database created by experts, ensuring\ntransparency and explainability for stakeholders. Additionally, SC developers\ncan efficiently improve diagnostic performance at a low cost."
                },
                "authors": [
                    {
                        "name": "Takashi Nishibayashi"
                    },
                    {
                        "name": "Seiji Kanazawa"
                    },
                    {
                        "name": "Kumpei Yamada"
                    }
                ],
                "author_detail": {
                    "name": "Kumpei Yamada"
                },
                "author": "Kumpei Yamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20336v1",
                "updated": "2025-06-25T11:46:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    46,
                    56,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T11:46:56Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    46,
                    56,
                    2,
                    176,
                    0
                ],
                "title": "A Unified Framework for UAV-Based Free-Space Quantum Links: Beam Shaping\n  and Adaptive Field-of-View Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for UAV-Based Free-Space Quantum Links: Beam Shaping\n  and Adaptive Field-of-View Control"
                },
                "summary": "This paper develops a comprehensive analytical framework for modeling and\nperformance evaluation of unmanned aerial vehicles (UAVs)-to-ground quantum\ncommunication links, incorporating key physical impairments such as beam\ndivergence, pointing errors at both transmitter and receiver, atmospheric\nattenuation, turbulence-induced fading, narrow field-of-view (FoV) filtering,\nand background photon noise. To overcome the limitations of conventional\nwide-beam assumptions, we introduce a grid-based approximation for photon\ncapture probability that remains accurate under tightly focused beams.\nAnalytical expressions are derived for the quantum key generation rate and\nquantum bit error rate (QBER), enabling fast and reliable system-level\nevaluation. Our results reveal that secure quantum key distribution (QKD) over\nUAV-based free-space optical (FSO) links requires beam waists below 10 cm and\nsub-milliradian tracking precision to achieve Mbps-level key rates and QBER\nbelow $10^{-3}$. Additionally, we highlight the critical role of receiver FoV\nin balancing background noise rejection and misalignment tolerance, and propose\nadaptive FoV tuning strategies under varying illumination and alignment\nconditions. The proposed framework provides a tractable and accurate tool for\nthe design, optimization, and deployment of next-generation airborne quantum\ncommunication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a comprehensive analytical framework for modeling and\nperformance evaluation of unmanned aerial vehicles (UAVs)-to-ground quantum\ncommunication links, incorporating key physical impairments such as beam\ndivergence, pointing errors at both transmitter and receiver, atmospheric\nattenuation, turbulence-induced fading, narrow field-of-view (FoV) filtering,\nand background photon noise. To overcome the limitations of conventional\nwide-beam assumptions, we introduce a grid-based approximation for photon\ncapture probability that remains accurate under tightly focused beams.\nAnalytical expressions are derived for the quantum key generation rate and\nquantum bit error rate (QBER), enabling fast and reliable system-level\nevaluation. Our results reveal that secure quantum key distribution (QKD) over\nUAV-based free-space optical (FSO) links requires beam waists below 10 cm and\nsub-milliradian tracking precision to achieve Mbps-level key rates and QBER\nbelow $10^{-3}$. Additionally, we highlight the critical role of receiver FoV\nin balancing background noise rejection and misalignment tolerance, and propose\nadaptive FoV tuning strategies under varying illumination and alignment\nconditions. The proposed framework provides a tractable and accurate tool for\nthe design, optimization, and deployment of next-generation airborne quantum\ncommunication systems."
                },
                "authors": [
                    {
                        "name": "Mohammad Taghi Dabiri"
                    },
                    {
                        "name": "Mazen Hasna"
                    },
                    {
                        "name": "Saif Al-Kuwari"
                    },
                    {
                        "name": "Khalid Qaraqe"
                    }
                ],
                "author_detail": {
                    "name": "Khalid Qaraqe"
                },
                "author": "Khalid Qaraqe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20331v1",
                "updated": "2025-06-25T11:30:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    30,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T11:30:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    30,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content"
                },
                "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies."
                },
                "authors": [
                    {
                        "name": "Rian Touchent"
                    },
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Eric de la Clergerie"
                    }
                ],
                "author_detail": {
                    "name": "Eric de la Clergerie"
                },
                "author": "Eric de la Clergerie",
                "arxiv_comment": "Dataset link: https://hf.co/datasets/almanach/Biomed-Enriched",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11484v2",
                "updated": "2025-06-25T11:05:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    5,
                    49,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-13T06:14:56Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    14,
                    56,
                    4,
                    164,
                    0
                ],
                "title": "VulStamp: Vulnerability Assessment using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulStamp: Vulnerability Assessment using Large Language Model"
                },
                "summary": "Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model."
                },
                "authors": [
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Jiaye Li"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20318v1",
                "updated": "2025-06-25T10:58:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    58,
                    28,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T10:58:28Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    58,
                    28,
                    2,
                    176,
                    0
                ],
                "title": "Computed tomography of propagating microwave photons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computed tomography of propagating microwave photons"
                },
                "summary": "Propagating photons serve as essential links for distributing quantum\ninformation and entanglement across distant nodes. Knowledge of their Wigner\nfunctions not only enables their deployment as active information carriers but\nalso provides error diagnostics when photons passively leak from a quantum\nprocessing unit. While well-established for standing waves, characterizing\npropagating microwave photons requires post-processing of room-temperature\nsignals with excessive amplification noise. Here, we demonstrate cryogenic and\namplification-free Wigner function tomography of propagating microwave photons\nusing a superconductor-normal metal-superconductor bolometer based on the\nresistive heating effect of absorbed radiation. By introducing two-field\ninterference in power detection, the bolometer acts as a sensitive and\nbroadband quadrature detector that samples the input field at selected angles\nat millikelvin with no added noise. Adapting the principles of computed\ntomography (CT) in medical imaging, we implement Wigner function CT by\ncombining quadrature histograms across different projection angles and\ndemonstrate it for Gaussian states at the single-photon level with different\nsymmetries. Compressed sensing and neural networks further reduce the\nprojections to three without compromising the reconstruction quality. These\nresults address the long-standing challenge of characterizing propagating\nmicrowave photons in a superconducting quantum network and establish a new\navenue for real-time quantum error diagnostics and correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propagating photons serve as essential links for distributing quantum\ninformation and entanglement across distant nodes. Knowledge of their Wigner\nfunctions not only enables their deployment as active information carriers but\nalso provides error diagnostics when photons passively leak from a quantum\nprocessing unit. While well-established for standing waves, characterizing\npropagating microwave photons requires post-processing of room-temperature\nsignals with excessive amplification noise. Here, we demonstrate cryogenic and\namplification-free Wigner function tomography of propagating microwave photons\nusing a superconductor-normal metal-superconductor bolometer based on the\nresistive heating effect of absorbed radiation. By introducing two-field\ninterference in power detection, the bolometer acts as a sensitive and\nbroadband quadrature detector that samples the input field at selected angles\nat millikelvin with no added noise. Adapting the principles of computed\ntomography (CT) in medical imaging, we implement Wigner function CT by\ncombining quadrature histograms across different projection angles and\ndemonstrate it for Gaussian states at the single-photon level with different\nsymmetries. Compressed sensing and neural networks further reduce the\nprojections to three without compromising the reconstruction quality. These\nresults address the long-standing challenge of characterizing propagating\nmicrowave photons in a superconducting quantum network and establish a new\navenue for real-time quantum error diagnostics and correction."
                },
                "authors": [
                    {
                        "name": "Qi-Ming Chen"
                    },
                    {
                        "name": "Aarne Keränen"
                    },
                    {
                        "name": "Aashish Sah"
                    },
                    {
                        "name": "Mikko Möttönen"
                    }
                ],
                "author_detail": {
                    "name": "Mikko Möttönen"
                },
                "author": "Mikko Möttönen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18330v2",
                "updated": "2025-06-25T10:49:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    49,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-23T06:23:53Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    23,
                    53,
                    0,
                    174,
                    0
                ],
                "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning"
                },
                "summary": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math."
                },
                "authors": [
                    {
                        "name": "Lixin Wu"
                    },
                    {
                        "name": "Na Cai"
                    },
                    {
                        "name": "Qiao Cheng"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Yitao Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Duan"
                },
                "author": "Yitao Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18746v3",
                "updated": "2025-06-25T10:37:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    10,
                    37,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-24T15:25:44Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    15,
                    25,
                    44,
                    5,
                    144,
                    0
                ],
                "title": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking"
                },
                "summary": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/yupeijei1997/C3-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/yupeijei1997/C3-Bench."
                },
                "authors": [
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20291v1",
                "updated": "2025-06-25T09:53:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    53,
                    35,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:53:35Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    53,
                    35,
                    2,
                    176,
                    0
                ],
                "title": "A Literature Review on Simulation in Conversational Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Literature Review on Simulation in Conversational Recommender Systems"
                },
                "summary": "Conversational Recommender Systems (CRSs) have garnered attention as a novel\napproach to delivering personalized recommendations through multi-turn\ndialogues. This review developed a taxonomy framework to systematically\ncategorize relevant publications into four groups: dataset construction,\nalgorithm design, system evaluation, and empirical studies, providing a\ncomprehensive analysis of simulation methods in CRSs research. Our analysis\nreveals that simulation methods play a key role in tackling CRSs' main\nchallenges. For example, LLM-based simulation methods have been used to create\nconversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.\nDespite several challenges, such as dataset bias, the limited output\nflexibility of LLM-based simulations, and the gap between text semantic space\nand behavioral semantics, persist due to the complexity in Human-Computer\nInteraction (HCI) of CRSs, simulation methods hold significant potential for\nadvancing CRS research. This review offers a thorough summary of the current\nresearch landscape in this domain and identifies promising directions for\nfuture inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) have garnered attention as a novel\napproach to delivering personalized recommendations through multi-turn\ndialogues. This review developed a taxonomy framework to systematically\ncategorize relevant publications into four groups: dataset construction,\nalgorithm design, system evaluation, and empirical studies, providing a\ncomprehensive analysis of simulation methods in CRSs research. Our analysis\nreveals that simulation methods play a key role in tackling CRSs' main\nchallenges. For example, LLM-based simulation methods have been used to create\nconversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.\nDespite several challenges, such as dataset bias, the limited output\nflexibility of LLM-based simulations, and the gap between text semantic space\nand behavioral semantics, persist due to the complexity in Human-Computer\nInteraction (HCI) of CRSs, simulation methods hold significant potential for\nadvancing CRS research. This review offers a thorough summary of the current\nresearch landscape in this domain and identifies promising directions for\nfuture inquiry."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Jinze Chen"
                    },
                    {
                        "name": "Junpeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junpeng Guo"
                },
                "author": "Junpeng Guo",
                "arxiv_comment": "6 pages, 1 figures, accepted as a poster for CSWIM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11962v3",
                "updated": "2025-06-25T09:51:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    51,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-17T16:10:30Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    10,
                    30,
                    0,
                    48,
                    0
                ],
                "title": "Balancing Truthfulness and Informativeness with Uncertainty-Aware\n  Instruction Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Truthfulness and Informativeness with Uncertainty-Aware\n  Instruction Fine-Tuning"
                },
                "summary": "Instruction fine-tuning (IFT) can increase the informativeness of large\nlanguage models (LLMs), but may reduce their truthfulness. This trade-off\narises because IFT steers LLMs to generate responses containing long-tail\nknowledge that was not well covered during pre-training. As a result, models\nbecome more informative but less accurate when generalizing to unseen tasks. In\nthis paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets\ncan negatively affect the truthfulness of LLMs, and we introduce two new IFT\nparadigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$\nidentifies and removes unfamiliar knowledge from IFT datasets to mitigate its\nimpact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize\ntheir uncertainty and explicitly indicate it at the end of their responses. Our\nexperiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,\nwhile $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by\ndistinguishing between confident and uncertain statements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction fine-tuning (IFT) can increase the informativeness of large\nlanguage models (LLMs), but may reduce their truthfulness. This trade-off\narises because IFT steers LLMs to generate responses containing long-tail\nknowledge that was not well covered during pre-training. As a result, models\nbecome more informative but less accurate when generalizing to unseen tasks. In\nthis paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets\ncan negatively affect the truthfulness of LLMs, and we introduce two new IFT\nparadigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$\nidentifies and removes unfamiliar knowledge from IFT datasets to mitigate its\nimpact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize\ntheir uncertainty and explicitly indicate it at the end of their responses. Our\nexperiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,\nwhile $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by\ndistinguishing between confident and uncertain statements."
                },
                "authors": [
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20279v1",
                "updated": "2025-06-25T09:40:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    40,
                    50,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:40:50Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    40,
                    50,
                    2,
                    176,
                    0
                ],
                "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios"
                },
                "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj"
                },
                "authors": [
                    {
                        "name": "Changliang Xia"
                    },
                    {
                        "name": "Chengyou Jia"
                    },
                    {
                        "name": "Zhuohang Dang"
                    },
                    {
                        "name": "Minnan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Minnan Luo"
                },
                "author": "Minnan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17848v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17848v4",
                "updated": "2025-06-25T09:36:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    36,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-25T04:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    4,
                    51,
                    17,
                    1,
                    56,
                    0
                ],
                "title": "LR^2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LR^2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems"
                },
                "summary": "Recent progress in Large Reasoning Models (LRMs) has significantly enhanced\nthe reasoning abilities of Large Language Models (LLMs), empowering them to\ntackle increasingly complex tasks through reflection capabilities, such as\nmaking assumptions, backtracking, and self-refinement. However, effectively\nevaluating such reflection capabilities remains challenging due to the lack of\nappropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel\nbenchmark designed to evaluate the Long-chain Reflective Reasoning capabilities\nof LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction\nProblems (CSPs) where reflective reasoning is crucial for deriving solutions\nthat meet all given constraints. Each type of task focuses on distinct\nconstraint patterns, such as knowledge-based, logical, and spatial constraints,\nproviding a comprehensive evaluation of diverse problem-solving scenarios. Our\nextensive evaluation on both conventional LLMs and LRMs reveals that even the\nmost advanced LRMs, such as DeepSeek-R1 and OpenAI o1-preview, struggle with\ntasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and\n23.6%, respectively. These findings underscore the significant room for\nimprovement in the reflective reasoning capabilities of current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Reasoning Models (LRMs) has significantly enhanced\nthe reasoning abilities of Large Language Models (LLMs), empowering them to\ntackle increasingly complex tasks through reflection capabilities, such as\nmaking assumptions, backtracking, and self-refinement. However, effectively\nevaluating such reflection capabilities remains challenging due to the lack of\nappropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel\nbenchmark designed to evaluate the Long-chain Reflective Reasoning capabilities\nof LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction\nProblems (CSPs) where reflective reasoning is crucial for deriving solutions\nthat meet all given constraints. Each type of task focuses on distinct\nconstraint patterns, such as knowledge-based, logical, and spatial constraints,\nproviding a comprehensive evaluation of diverse problem-solving scenarios. Our\nextensive evaluation on both conventional LLMs and LRMs reveals that even the\nmost advanced LRMs, such as DeepSeek-R1 and OpenAI o1-preview, struggle with\ntasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and\n23.6%, respectively. These findings underscore the significant room for\nimprovement in the reflective reasoning capabilities of current LLMs."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Zhenjiang Ren"
                    },
                    {
                        "name": "Ziyong Li"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ACL-2025, our code is available at https://github.com/ZNLP/LR2Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17848v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17848v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20274v1",
                "updated": "2025-06-25T09:34:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    34,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:34:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    34,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Enterprise Large Language Model Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise Large Language Model Evaluation Benchmark"
                },
                "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Liya Wang"
                    },
                    {
                        "name": "David Yi"
                    },
                    {
                        "name": "Damien Jose"
                    },
                    {
                        "name": "John Passarelli"
                    },
                    {
                        "name": "James Gao"
                    },
                    {
                        "name": "Jordan Leventis"
                    },
                    {
                        "name": "Kang Li"
                    }
                ],
                "author_detail": {
                    "name": "Kang Li"
                },
                "author": "Kang Li",
                "arxiv_comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02502v2",
                "updated": "2025-06-25T09:27:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    27,
                    33,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-04T11:10:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs"
                },
                "summary": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ACL 2025, our code is available at https://github.com/ZNLP/LADM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10390v2",
                "updated": "2025-06-25T09:27:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    27,
                    22,
                    2,
                    176,
                    0
                ],
                "published": "2025-04-14T16:36:56Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    36,
                    56,
                    0,
                    104,
                    0
                ],
                "title": "Teacher Motion Priors: Enhancing Robot Locomotion over Challenging\n  Terrain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teacher Motion Priors: Enhancing Robot Locomotion over Challenging\n  Terrain"
                },
                "summary": "Achieving robust locomotion on complex terrains remains a challenge due to\nhigh dimensional control and environmental uncertainties. This paper introduces\na teacher prior framework based on the teacher student paradigm, integrating\nimitation and auxiliary task learning to improve learning efficiency and\ngeneralization. Unlike traditional paradigms that strongly rely on\nencoder-based state embeddings, our framework decouples the network design,\nsimplifying the policy network and deployment. A high performance teacher\npolicy is first trained using privileged information to acquire generalizable\nmotion skills. The teacher's motion distribution is transferred to the student\npolicy, which relies only on noisy proprioceptive data, via a generative\nadversarial mechanism to mitigate performance degradation caused by\ndistributional shifts. Additionally, auxiliary task learning enhances the\nstudent policy's feature representation, speeding up convergence and improving\nadaptability to varying terrains. The framework is validated on a humanoid\nrobot, showing a great improvement in locomotion stability on dynamic terrains\nand significant reductions in development costs. This work provides a practical\nsolution for deploying robust locomotion strategies in humanoid robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving robust locomotion on complex terrains remains a challenge due to\nhigh dimensional control and environmental uncertainties. This paper introduces\na teacher prior framework based on the teacher student paradigm, integrating\nimitation and auxiliary task learning to improve learning efficiency and\ngeneralization. Unlike traditional paradigms that strongly rely on\nencoder-based state embeddings, our framework decouples the network design,\nsimplifying the policy network and deployment. A high performance teacher\npolicy is first trained using privileged information to acquire generalizable\nmotion skills. The teacher's motion distribution is transferred to the student\npolicy, which relies only on noisy proprioceptive data, via a generative\nadversarial mechanism to mitigate performance degradation caused by\ndistributional shifts. Additionally, auxiliary task learning enhances the\nstudent policy's feature representation, speeding up convergence and improving\nadaptability to varying terrains. The framework is validated on a humanoid\nrobot, showing a great improvement in locomotion stability on dynamic terrains\nand significant reductions in development costs. This work provides a practical\nsolution for deploying robust locomotion strategies in humanoid robots."
                },
                "authors": [
                    {
                        "name": "Fangcheng Jin"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Peixin Ma"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Pan Zhao"
                    },
                    {
                        "name": "En Li"
                    },
                    {
                        "name": "Zhengtao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengtao Zhang"
                },
                "author": "Zhengtao Zhang",
                "arxiv_comment": "8 pages, 6 figures, 6 tables, IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06185v2",
                "updated": "2025-06-25T09:21:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    21,
                    21,
                    2,
                    176,
                    0
                ],
                "published": "2025-04-08T16:25:59Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    25,
                    59,
                    1,
                    98,
                    0
                ],
                "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care"
                },
                "summary": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For a fair comparison,\nwe standardize training, data augmentation, and evaluation, conducting\ncross-validation to minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates and evaluate this, along with mask\nquality, for the five best architectures based on physician assessments.\nOverall, the transformer-based TransNeXt showed the highest levels of\ngeneralizability. Despite variations in inference times, all models processed\nat least one image per second on the CPU, which is deemed adequate for the\nintended application. Interpretability analysis typically revealed prominent\nactivations in wound regions, emphasizing focus on clinically relevant\nfeatures. Expert evaluation showed high mask approval for all analyzed models,\nwith VWFormer and ConvNeXtS backbone performing the best. Size retrieval\naccuracy was similar across models, and predictions closely matched expert\nannotations. Finally, we demonstrate how our AI-driven wound size estimation\nframework, WoundAmbit, is integrated into a custom telehealth system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For a fair comparison,\nwe standardize training, data augmentation, and evaluation, conducting\ncross-validation to minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates and evaluate this, along with mask\nquality, for the five best architectures based on physician assessments.\nOverall, the transformer-based TransNeXt showed the highest levels of\ngeneralizability. Despite variations in inference times, all models processed\nat least one image per second on the CPU, which is deemed adequate for the\nintended application. Interpretability analysis typically revealed prominent\nactivations in wound regions, emphasizing focus on clinically relevant\nfeatures. Expert evaluation showed high mask approval for all analyzed models,\nwith VWFormer and ConvNeXtS backbone performing the best. Size retrieval\naccuracy was similar across models, and predictions closely matched expert\nannotations. Finally, we demonstrate how our AI-driven wound size estimation\nframework, WoundAmbit, is integrated into a custom telehealth system."
                },
                "authors": [
                    {
                        "name": "Vanessa Borst"
                    },
                    {
                        "name": "Timo Dittus"
                    },
                    {
                        "name": "Tassilo Dege"
                    },
                    {
                        "name": "Astrid Schmieder"
                    },
                    {
                        "name": "Samuel Kounev"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kounev"
                },
                "author": "Samuel Kounev",
                "arxiv_comment": "Main paper: 18 pages; supplementary material: 15 pages; the paper has\n  been accepted for publication at the Applied Data Science (ADS) track of the\n  European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (ECML PKDD 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21218v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21218v3",
                "updated": "2025-06-25T09:01:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    1,
                    38,
                    2,
                    176,
                    0
                ],
                "published": "2024-10-28T17:02:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Lifting the Veil on Composition, Risks, and Mitigations of the Large\n  Language Model Supply Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifting the Veil on Composition, Risks, and Mitigations of the Large\n  Language Model Supply Chain"
                },
                "summary": "Large language models (LLMs) have sparked significant impact with regard to\nboth intelligence and productivity. Numerous enterprises have integrated LLMs\ninto their applications to solve their own domain-specific tasks. However,\nintegrating LLMs into specific scenarios is a systematic process that involves\nsubstantial components, which are collectively referred to as the LLM supply\nchain. A comprehensive understanding of LLM supply chain composition, as well\nas the relationships among its components, is crucial for enabling effective\nmitigation measures for different related risks. While existing literature has\nexplored various risks associated with LLMs, there remains a notable gap in\nsystematically characterizing the LLM supply chain from the dual perspectives\nof contributors and consumers. In this work, we develop a structured taxonomy\nencompassing risk types, risky actions, and corresponding mitigations across\ndifferent stakeholders and components of the supply chain. We believe that a\nthorough review of the LLM supply chain composition, along with its inherent\nrisks and mitigation measures, would be valuable for industry practitioners to\navoid potential damages and losses, and enlightening for academic researchers\nto rethink existing approaches and explore new avenues of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have sparked significant impact with regard to\nboth intelligence and productivity. Numerous enterprises have integrated LLMs\ninto their applications to solve their own domain-specific tasks. However,\nintegrating LLMs into specific scenarios is a systematic process that involves\nsubstantial components, which are collectively referred to as the LLM supply\nchain. A comprehensive understanding of LLM supply chain composition, as well\nas the relationships among its components, is crucial for enabling effective\nmitigation measures for different related risks. While existing literature has\nexplored various risks associated with LLMs, there remains a notable gap in\nsystematically characterizing the LLM supply chain from the dual perspectives\nof contributors and consumers. In this work, we develop a structured taxonomy\nencompassing risk types, risky actions, and corresponding mitigations across\ndifferent stakeholders and components of the supply chain. We believe that a\nthorough review of the LLM supply chain composition, along with its inherent\nrisks and mitigation measures, would be valuable for industry practitioners to\navoid potential damages and losses, and enlightening for academic researchers\nto rethink existing approaches and explore new avenues of research."
                },
                "authors": [
                    {
                        "name": "Kaifeng Huang"
                    },
                    {
                        "name": "Bihuan Chen"
                    },
                    {
                        "name": "You Lu"
                    },
                    {
                        "name": "Susheng Wu"
                    },
                    {
                        "name": "Dingji Wang"
                    },
                    {
                        "name": "Yiheng Huang"
                    },
                    {
                        "name": "Haowen Jiang"
                    },
                    {
                        "name": "Zhuotong Zhou"
                    },
                    {
                        "name": "Junming Cao"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21218v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21218v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20251v1",
                "updated": "2025-06-25T08:52:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    52,
                    22,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:52:22Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    52,
                    22,
                    2,
                    176,
                    0
                ],
                "title": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching\n  for Quantized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching\n  for Quantized Large Language Models"
                },
                "summary": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe."
                },
                "authors": [
                    {
                        "name": "Kejia Chen"
                    },
                    {
                        "name": "Jiawen Zhang"
                    },
                    {
                        "name": "Jiacong Hu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Jian Lou"
                    },
                    {
                        "name": "Zunlei Feng"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20249v1",
                "updated": "2025-06-25T08:46:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    46,
                    10,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:46:10Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    46,
                    10,
                    2,
                    176,
                    0
                ],
                "title": "Language Modeling by Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling by Language Models"
                },
                "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems."
                },
                "authors": [
                    {
                        "name": "Junyan Cheng"
                    },
                    {
                        "name": "Peter Clark"
                    },
                    {
                        "name": "Kyle Richardson"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Richardson"
                },
                "author": "Kyle Richardson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20244v1",
                "updated": "2025-06-25T08:41:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    41,
                    48,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:41:48Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    41,
                    48,
                    2,
                    176,
                    0
                ],
                "title": "Cooperative Sensing and Communication Beamforming Design for\n  Low-Altitude Economy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Sensing and Communication Beamforming Design for\n  Low-Altitude Economy"
                },
                "summary": "To empower the low-altitude economy with high-accuracy sensing and high-rate\ncommunication, this paper proposes a cooperative integrated sensing and\ncommunication (ISAC) framework for aerial-ground networks. In the proposed\nsystem, the ground base stations (BSs) cooperatively serve the unmanned aerial\nvehicles (UAVs), which are equipped for either joint communication and sensing\nor sensing-only operations. The BSs employ coordinated beamforming to\nsimultaneously transmit communication and sensing signals, while the UAVs\nexecute their missions. To maximize the weighted sum rate under the sensing\nsignal-to-interference-plus-noise ratio (SINR) constraints, we jointly optimize\nthe transmit beamforming, receive filtering, and UAV trajectory. The resulting\nnon-convex problem is solved using an alternating optimization framework\nincorporating semidefinite relaxation (SDR) and successive convex approximation\n(SCA). Simulation results demonstrate that the proposed joint design achieves\nhigher communication throughput while ensuring required sensing robustness.\nAdditionally, the sensing SINR threshold and the UAV altitude have a\nsignificant impact on the trajectory design, highlighting the necessity of\nadaptive deployment strategies in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To empower the low-altitude economy with high-accuracy sensing and high-rate\ncommunication, this paper proposes a cooperative integrated sensing and\ncommunication (ISAC) framework for aerial-ground networks. In the proposed\nsystem, the ground base stations (BSs) cooperatively serve the unmanned aerial\nvehicles (UAVs), which are equipped for either joint communication and sensing\nor sensing-only operations. The BSs employ coordinated beamforming to\nsimultaneously transmit communication and sensing signals, while the UAVs\nexecute their missions. To maximize the weighted sum rate under the sensing\nsignal-to-interference-plus-noise ratio (SINR) constraints, we jointly optimize\nthe transmit beamforming, receive filtering, and UAV trajectory. The resulting\nnon-convex problem is solved using an alternating optimization framework\nincorporating semidefinite relaxation (SDR) and successive convex approximation\n(SCA). Simulation results demonstrate that the proposed joint design achieves\nhigher communication throughput while ensuring required sensing robustness.\nAdditionally, the sensing SINR threshold and the UAV altitude have a\nsignificant impact on the trajectory design, highlighting the necessity of\nadaptive deployment strategies in practical applications."
                },
                "authors": [
                    {
                        "name": "Fangzhi Li"
                    },
                    {
                        "name": "Zhichu Ren"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Hong Ren"
                    },
                    {
                        "name": "Jing Jin"
                    },
                    {
                        "name": "Qixing Wang"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangzhou Wang"
                },
                "author": "Jiangzhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20241v1",
                "updated": "2025-06-25T08:36:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    36,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T08:36:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    36,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "Enhancing Large Language Models through Structured Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models through Structured Reasoning"
                },
                "summary": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs."
                },
                "authors": [
                    {
                        "name": "Yubo Dong"
                    },
                    {
                        "name": "Hehe Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hehe Fan"
                },
                "author": "Hehe Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21227v3",
                "updated": "2025-06-25T08:30:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    30,
                    20,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-27T07:36:11Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    36,
                    11,
                    3,
                    86,
                    0
                ],
                "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models"
                },
                "summary": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon."
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhao"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Qixin Sun"
                    },
                    {
                        "name": "Kaiyou Song"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15595v2",
                "updated": "2025-06-25T08:27:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    27,
                    45,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-18T16:10:17Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    10,
                    17,
                    2,
                    169,
                    0
                ],
                "title": "LiteGD: Lightweight and Dynamic GPU Dispatching for Large-scale\n  Heterogeneous Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGD: Lightweight and Dynamic GPU Dispatching for Large-scale\n  Heterogeneous Clusters"
                },
                "summary": "Although multi-GPU execution has become the de-facto paradigm for training\nand serving large language models (LLMs), today's schedulers still rely on a\nsimple heuristic: pick GPUs that are physically close. This proximity rule was\nadequate for small, uniform clusters, but it breaks down in modern fabrics\nwhere link capacities differ by up to an order of magnitude across PCIe,\nNVLink, and CXL tiers. Consequently, jobs placed by locality alone often suffer\nfrom severe bandwidth imbalance and unpredictable performance. In this paper,\nWe present LiteGD, a lightweight, globally-aware GPU dispatching system that\ndelivers near-optimal bandwidth without incurring prohibitive state or search\noverheads. Instead of materializing the full O(N^2) connectivity matrix, LiteGD\nencodes the fabric with a sparsified Tiny-Transformer trained on a few thousand\nrandom bandwidth probes, enabling fast adaptation to incremental hardware\nchanges. LiteGD also employs a bidirectional tree search approach to find the\noptimal GPU dispatching in the data generated in the previous step, which can\nidentify near-optimal solutions while reducing search overhead. We implement\nand evaluate LiteGD in both real and simulated GPU clusters with homogeneous\nand heterogeneous interconnects, respectively. Experimental results demonstrate\nthat LiteGD consistently achieves high GPU Bandwidth Efficacy, approximately\n90% across various cluster configurations and 80% in a real-world H100 cluster,\nsignificantly outperforming conventional default and interconnect\ntopology-aware dispatching methods, particularly in large-scale heterogeneous\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multi-GPU execution has become the de-facto paradigm for training\nand serving large language models (LLMs), today's schedulers still rely on a\nsimple heuristic: pick GPUs that are physically close. This proximity rule was\nadequate for small, uniform clusters, but it breaks down in modern fabrics\nwhere link capacities differ by up to an order of magnitude across PCIe,\nNVLink, and CXL tiers. Consequently, jobs placed by locality alone often suffer\nfrom severe bandwidth imbalance and unpredictable performance. In this paper,\nWe present LiteGD, a lightweight, globally-aware GPU dispatching system that\ndelivers near-optimal bandwidth without incurring prohibitive state or search\noverheads. Instead of materializing the full O(N^2) connectivity matrix, LiteGD\nencodes the fabric with a sparsified Tiny-Transformer trained on a few thousand\nrandom bandwidth probes, enabling fast adaptation to incremental hardware\nchanges. LiteGD also employs a bidirectional tree search approach to find the\noptimal GPU dispatching in the data generated in the previous step, which can\nidentify near-optimal solutions while reducing search overhead. We implement\nand evaluate LiteGD in both real and simulated GPU clusters with homogeneous\nand heterogeneous interconnects, respectively. Experimental results demonstrate\nthat LiteGD consistently achieves high GPU Bandwidth Efficacy, approximately\n90% across various cluster configurations and 80% in a real-world H100 cluster,\nsignificantly outperforming conventional default and interconnect\ntopology-aware dispatching methods, particularly in large-scale heterogeneous\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kunming Zhang"
                    },
                    {
                        "name": "Hanlong Liao"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00757v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00757v3",
                "updated": "2025-06-25T08:23:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    23,
                    23,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-02T11:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    40,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds\n  via Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds\n  via Self-Improvement"
                },
                "summary": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In 'blue' mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In 'red' mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/J-Rosser-UK/AgentBreeder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In 'blue' mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In 'red' mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/J-Rosser-UK/AgentBreeder."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Nicolaus Foerster"
                },
                "author": "Jakob Nicolaus Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00757v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00757v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20212v1",
                "updated": "2025-06-25T07:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    55,
                    59,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    55,
                    59,
                    2,
                    176,
                    0
                ],
                "title": "Personalized Mental State Evaluation in Human-Robot Interaction using\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Mental State Evaluation in Human-Robot Interaction using\n  Federated Learning"
                },
                "summary": "With the advent of Industry 5.0, manufacturers are increasingly prioritizing\nworker well-being alongside mass customization. Stress-aware Human-Robot\nCollaboration (HRC) plays a crucial role in this paradigm, where robots must\nadapt their behavior to human mental states to improve collaboration fluency\nand safety. This paper presents a novel framework that integrates Federated\nLearning (FL) to enable personalized mental state evaluation while preserving\nuser privacy. By leveraging physiological signals, including EEG, ECG, EDA,\nEMG, and respiration, a multimodal model predicts an operator's stress level,\nfacilitating real-time robot adaptation. The FL-based approach allows\ndistributed on-device training, ensuring data confidentiality while improving\nmodel generalization and individual customization. Results demonstrate that the\ndeployment of an FL approach results in a global model with performance in\nstress prediction accuracy comparable to a centralized training approach.\nMoreover, FL allows for enhancing personalization, thereby optimizing\nhuman-robot interaction in industrial settings, while preserving data privacy.\nThe proposed framework advances privacy-preserving, adaptive robotics to\nenhance workforce well-being in smart manufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Industry 5.0, manufacturers are increasingly prioritizing\nworker well-being alongside mass customization. Stress-aware Human-Robot\nCollaboration (HRC) plays a crucial role in this paradigm, where robots must\nadapt their behavior to human mental states to improve collaboration fluency\nand safety. This paper presents a novel framework that integrates Federated\nLearning (FL) to enable personalized mental state evaluation while preserving\nuser privacy. By leveraging physiological signals, including EEG, ECG, EDA,\nEMG, and respiration, a multimodal model predicts an operator's stress level,\nfacilitating real-time robot adaptation. The FL-based approach allows\ndistributed on-device training, ensuring data confidentiality while improving\nmodel generalization and individual customization. Results demonstrate that the\ndeployment of an FL approach results in a global model with performance in\nstress prediction accuracy comparable to a centralized training approach.\nMoreover, FL allows for enhancing personalization, thereby optimizing\nhuman-robot interaction in industrial settings, while preserving data privacy.\nThe proposed framework advances privacy-preserving, adaptive robotics to\nenhance workforce well-being in smart manufacturing."
                },
                "authors": [
                    {
                        "name": "Andrea Bussolan"
                    },
                    {
                        "name": "Oliver Avram"
                    },
                    {
                        "name": "Andrea Pignata"
                    },
                    {
                        "name": "Gianvito Urgese"
                    },
                    {
                        "name": "Stefano Baraldo"
                    },
                    {
                        "name": "Anna Valente"
                    }
                ],
                "author_detail": {
                    "name": "Anna Valente"
                },
                "author": "Anna Valente",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20199v1",
                "updated": "2025-06-25T07:39:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    39,
                    19,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:39:19Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    39,
                    19,
                    2,
                    176,
                    0
                ],
                "title": "How to Retrieve Examples in In-context Learning to Improve\n  Conversational Emotion Recognition using Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Retrieve Examples in In-context Learning to Improve\n  Conversational Emotion Recognition using Large Language Models?"
                },
                "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing."
                },
                "authors": [
                    {
                        "name": "Mengqi Wang"
                    },
                    {
                        "name": "Tiantian Feng"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Shrikanth Narayanan"
                },
                "author": "Shrikanth Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20197v1",
                "updated": "2025-06-25T07:37:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    37,
                    16,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:37:16Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    37,
                    16,
                    2,
                    176,
                    0
                ],
                "title": "Zero-Shot Attribution for Large Language Models: A Distribution Testing\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Attribution for Large Language Models: A Distribution Testing\n  Approach"
                },
                "summary": "A growing fraction of all code is sampled from Large Language Models (LLMs).\nWe investigate the problem of attributing code generated by language models\nusing hypothesis testing to leverage established techniques and guarantees.\nGiven a set of samples $S$ and a suspect model $\\mathcal{L}^*$, our goal is to\nassess the likelihood of $S$ originating from $\\mathcal{L}^*$. Due to the curse\nof dimensionality, this is intractable when only samples from the LLM are\ngiven: to circumvent this, we use both samples and density estimates from the\nLLM, a form of access commonly available.\n  We introduce $\\mathsf{Anubis}$, a zero-shot attribution tool that frames\nattribution as a distribution testing problem. Our experiments on a benchmark\nof code samples show that $\\mathsf{Anubis}$ achieves high AUROC scores (\n$\\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and\nStable-Code using only $\\approx 2000$ samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing fraction of all code is sampled from Large Language Models (LLMs).\nWe investigate the problem of attributing code generated by language models\nusing hypothesis testing to leverage established techniques and guarantees.\nGiven a set of samples $S$ and a suspect model $\\mathcal{L}^*$, our goal is to\nassess the likelihood of $S$ originating from $\\mathcal{L}^*$. Due to the curse\nof dimensionality, this is intractable when only samples from the LLM are\ngiven: to circumvent this, we use both samples and density estimates from the\nLLM, a form of access commonly available.\n  We introduce $\\mathsf{Anubis}$, a zero-shot attribution tool that frames\nattribution as a distribution testing problem. Our experiments on a benchmark\nof code samples show that $\\mathsf{Anubis}$ achieves high AUROC scores (\n$\\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and\nStable-Code using only $\\approx 2000$ samples."
                },
                "authors": [
                    {
                        "name": "Clément L. Canonne"
                    },
                    {
                        "name": "Yash Pote"
                    },
                    {
                        "name": "Uddalok Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Uddalok Sarkar"
                },
                "author": "Uddalok Sarkar",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20196v1",
                "updated": "2025-06-25T07:36:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    36,
                    56,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:36:56Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    36,
                    56,
                    2,
                    176,
                    0
                ],
                "title": "Modeling energy collection with shortest paths in rectangular grids: an\n  efficient algorithm for energy harvesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling energy collection with shortest paths in rectangular grids: an\n  efficient algorithm for energy harvesting"
                },
                "summary": "Parabolic Trough solar fields are among the most prominent methods for\nharnessing solar energy. However, continuous sun-tracking movements leads to\nwear and degradation of the tracking system, raising the question of whether\nthe rotations can be minimized without compromising energy capture. In this\npaper, we address this question by exploring two problems: (1) minimizing the\nnumber of SCA rotational movements while maintaining energy production within a\nspecified range, and (2) maximizing energy capture when the number of rotations\nis limited. Unlike prior work, we develop a general framework that considers\nvariable conditions. By transforming the problem into grid-based path\noptimization, we design polynomial-time algorithms that can operate\nindependently of the weather throughout the day. Through realistic simulations\nand experiments using real-world data, our methods show that rotational\nmovements of solar trackers can be reduced by at least 10% while maintaining\nover 95% energy collection efficiency. These results offer a scalable solution\nfor improving the operational lifespan of the solar field. Furthermore, our\nmethods can be integrated with solar irradiance forecasting, enhancing their\nrobustness and suitability for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parabolic Trough solar fields are among the most prominent methods for\nharnessing solar energy. However, continuous sun-tracking movements leads to\nwear and degradation of the tracking system, raising the question of whether\nthe rotations can be minimized without compromising energy capture. In this\npaper, we address this question by exploring two problems: (1) minimizing the\nnumber of SCA rotational movements while maintaining energy production within a\nspecified range, and (2) maximizing energy capture when the number of rotations\nis limited. Unlike prior work, we develop a general framework that considers\nvariable conditions. By transforming the problem into grid-based path\noptimization, we design polynomial-time algorithms that can operate\nindependently of the weather throughout the day. Through realistic simulations\nand experiments using real-world data, our methods show that rotational\nmovements of solar trackers can be reduced by at least 10% while maintaining\nover 95% energy collection efficiency. These results offer a scalable solution\nfor improving the operational lifespan of the solar field. Furthermore, our\nmethods can be integrated with solar irradiance forecasting, enhancing their\nrobustness and suitability for real-world deployment."
                },
                "authors": [
                    {
                        "name": "José-Miguel Díaz-Bañez"
                    },
                    {
                        "name": "José-Manuel Higes-López"
                    },
                    {
                        "name": "Miguel-Angel Pérez-Cutiño"
                    },
                    {
                        "name": "Tom Todtenhaupt"
                    }
                ],
                "author_detail": {
                    "name": "Tom Todtenhaupt"
                },
                "author": "Tom Todtenhaupt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12434v2",
                "updated": "2025-06-25T07:35:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    35,
                    51,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-18T14:14:35Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    14,
                    14,
                    35,
                    6,
                    138,
                    0
                ],
                "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning"
                },
                "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yanrui Yu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Tianfei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianfei Zhou"
                },
                "author": "Tianfei Zhou",
                "arxiv_comment": "Code: https://github.com/QiWang98/VideoRFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20194v1",
                "updated": "2025-06-25T07:35:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    35,
                    12,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:35:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    35,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in\n  LLMs"
                },
                "summary": "Large language models (LLMs) deliver strong performance but are difficult to\ndeploy due to high memory and compute costs. While pruning reduces these\ndemands, most methods ignore activation sparsity observed at runtime. We\nreinterpret activation sparsity as dynamic structured weight sparsity and\npropose DuoGPT, a unified framework that constructs dual-sparse (spMspV)\nworkloads by combining unstructured weight pruning with activation sparsity. To\npreserve accuracy, we extend the Optimal Brain Compression (OBC) framework with\nactivation-aware calibration and introduce output residuals from the dense\nmodel as correction terms. We further optimize the solution for efficient GPU\nexecution, enabling scalability to billion-parameter LLMs. Evaluations on\nLLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured\npruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\\times$\ncompared to the baseline dense model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) deliver strong performance but are difficult to\ndeploy due to high memory and compute costs. While pruning reduces these\ndemands, most methods ignore activation sparsity observed at runtime. We\nreinterpret activation sparsity as dynamic structured weight sparsity and\npropose DuoGPT, a unified framework that constructs dual-sparse (spMspV)\nworkloads by combining unstructured weight pruning with activation sparsity. To\npreserve accuracy, we extend the Optimal Brain Compression (OBC) framework with\nactivation-aware calibration and introduce output residuals from the dense\nmodel as correction terms. We further optimize the solution for efficient GPU\nexecution, enabling scalability to billion-parameter LLMs. Evaluations on\nLLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured\npruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\\times$\ncompared to the baseline dense model."
                },
                "authors": [
                    {
                        "name": "Ruokai Yin"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Donghyun Lee"
                    },
                    {
                        "name": "Priyadarshini Panda"
                    }
                ],
                "author_detail": {
                    "name": "Priyadarshini Panda"
                },
                "author": "Priyadarshini Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v1",
                "updated": "2025-06-25T07:26:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20184v1",
                "updated": "2025-06-25T07:25:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    25,
                    53,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:25:53Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    25,
                    53,
                    2,
                    176,
                    0
                ],
                "title": "Quantum nonlinear parametric interaction in realistic waveguides: a\n  comprehensive study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum nonlinear parametric interaction in realistic waveguides: a\n  comprehensive study"
                },
                "summary": "Nonlinear sources of quantum light are foundational to nearly all optical\nquantum technologies and are actively advancing toward real-world deployment.\nAchieving this goal requires fabrication capabilities to be scaled to\nindustrial standards, necessitating precise modeling tools that can both guide\ndevice design within realistic fabrication constraints and enable accurate\npost-fabrication characterization. In this paper, we introduce a modeling\nframework that explicitly integrates the engineering tools used for designing\nclassical properties of integrated waveguides with quantum mechanical theory\ndescribing the underlying nonlinear interactions. We analyze the validity and\nlimitations of approximations relevant to this framework and apply it to\ncomprehensively study how typical fabrication errors and deviations from\nnominal design -- common in practical waveguide manufacturing -- affect the\nnonlinear optical response. Our findings highlight, in particular, a critical\nsensitivity of the framework to group-velocity dispersion, the potentially\ndisruptive role of geometric inhomogeneities in the waveguide, and an upper\nbound on single-mode squeezed-state generation arising from asymmetric\ngroup-velocity matching conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear sources of quantum light are foundational to nearly all optical\nquantum technologies and are actively advancing toward real-world deployment.\nAchieving this goal requires fabrication capabilities to be scaled to\nindustrial standards, necessitating precise modeling tools that can both guide\ndevice design within realistic fabrication constraints and enable accurate\npost-fabrication characterization. In this paper, we introduce a modeling\nframework that explicitly integrates the engineering tools used for designing\nclassical properties of integrated waveguides with quantum mechanical theory\ndescribing the underlying nonlinear interactions. We analyze the validity and\nlimitations of approximations relevant to this framework and apply it to\ncomprehensively study how typical fabrication errors and deviations from\nnominal design -- common in practical waveguide manufacturing -- affect the\nnonlinear optical response. Our findings highlight, in particular, a critical\nsensitivity of the framework to group-velocity dispersion, the potentially\ndisruptive role of geometric inhomogeneities in the waveguide, and an upper\nbound on single-mode squeezed-state generation arising from asymmetric\ngroup-velocity matching conditions."
                },
                "authors": [
                    {
                        "name": "Tim F. Weiss"
                    },
                    {
                        "name": "Akram Youssry"
                    },
                    {
                        "name": "Alberto Peruzzo"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Peruzzo"
                },
                "author": "Alberto Peruzzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02097v2",
                "updated": "2025-06-25T07:18:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    18,
                    47,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-02T17:59:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    59,
                    27,
                    0,
                    153,
                    0
                ],
                "title": "Hybrid AI for Responsive Multi-Turn Online Conversations with Novel\n  Dynamic Routing and Feedback Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid AI for Responsive Multi-Turn Online Conversations with Novel\n  Dynamic Routing and Feedback Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems and large language model\n(LLM)-powered chatbots have significantly advanced conversational AI by\ncombining generative capabilities with external knowledge retrieval. Despite\ntheir success, enterprise-scale deployments face critical challenges, including\ndiverse user queries, high latency, hallucinations, and difficulty integrating\nfrequently updated domain-specific knowledge. This paper introduces a novel\nhybrid framework that integrates RAG with intent-based canned responses,\nleveraging predefined high-confidence responses for efficiency while\ndynamically routing complex or ambiguous queries to the RAG pipeline. Our\nframework employs a dialogue context manager to ensure coherence in multi-turn\ninteractions and incorporates a feedback loop to refine intents, dynamically\nadjust confidence thresholds, and expand response coverage over time.\nExperimental results demonstrate that the proposed framework achieves a balance\nof high accuracy (95\\%) and low latency (180ms), outperforming RAG and\nintent-based systems across diverse query types, positioning it as a scalable\nand adaptive solution for enterprise conversational AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems and large language model\n(LLM)-powered chatbots have significantly advanced conversational AI by\ncombining generative capabilities with external knowledge retrieval. Despite\ntheir success, enterprise-scale deployments face critical challenges, including\ndiverse user queries, high latency, hallucinations, and difficulty integrating\nfrequently updated domain-specific knowledge. This paper introduces a novel\nhybrid framework that integrates RAG with intent-based canned responses,\nleveraging predefined high-confidence responses for efficiency while\ndynamically routing complex or ambiguous queries to the RAG pipeline. Our\nframework employs a dialogue context manager to ensure coherence in multi-turn\ninteractions and incorporates a feedback loop to refine intents, dynamically\nadjust confidence thresholds, and expand response coverage over time.\nExperimental results demonstrate that the proposed framework achieves a balance\nof high accuracy (95\\%) and low latency (180ms), outperforming RAG and\nintent-based systems across diverse query types, positioning it as a scalable\nand adaptive solution for enterprise conversational AI applications."
                },
                "authors": [
                    {
                        "name": "Priyaranjan Pattnayak"
                    },
                    {
                        "name": "Amit Agarwal"
                    },
                    {
                        "name": "Hansa Meghwani"
                    },
                    {
                        "name": "Hitesh Laxmichand Patel"
                    },
                    {
                        "name": "Srikant Panda"
                    }
                ],
                "author_detail": {
                    "name": "Srikant Panda"
                },
                "author": "Srikant Panda",
                "arxiv_comment": "Proceedings of the 4th International Workshop on Knowledge Augmented\n  Methods for Natural Language Processing in NAACL 2025, pages 215 to 229,\n  Albuquerque, New Mexico, USA. Association for Computational Linguistics",
                "arxiv_journal_ref": "Proceedings of the 4th International Workshop on\n  Knowledge-Augmented Methods for Natural Language Processing (KnowledgeNLP\n  2025), pp. 215 to 229, Association for Computational Linguistics,\n  Albuquerque, New Mexico, May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03000v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03000v4",
                "updated": "2025-06-25T07:16:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    16,
                    26,
                    2,
                    176,
                    0
                ],
                "published": "2025-02-05T08:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    52,
                    37,
                    2,
                    36,
                    0
                ],
                "title": "Armadillo: An Efficient Framework for Numerical Linear Algebra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Armadillo: An Efficient Framework for Numerical Linear Algebra"
                },
                "summary": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions."
                },
                "authors": [
                    {
                        "name": "Conrad Sanderson"
                    },
                    {
                        "name": "Ryan Curtin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Curtin"
                },
                "author": "Ryan Curtin",
                "arxiv_doi": "10.1109/ICCAE64891.2025.10980539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICCAE64891.2025.10980539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.03000v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03000v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Conference on Computer and Automation Engineering,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N99, 65Y04, 65Y15, 65F45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; G.1.3; D.2.3; F.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20170v1",
                "updated": "2025-06-25T06:50:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    50,
                    13,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T06:50:13Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    50,
                    13,
                    2,
                    176,
                    0
                ],
                "title": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript\n  Deobfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript\n  Deobfuscation"
                },
                "summary": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement."
                },
                "authors": [
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Zhiqiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lin"
                },
                "author": "Zhiqiang Lin",
                "arxiv_comment": "Accepted by ACM CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16789v2",
                "updated": "2025-06-25T06:44:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    44,
                    58,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-21T02:01:02Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    2,
                    1,
                    2,
                    4,
                    80,
                    0
                ],
                "title": "Conversational User-AI Intervention: A Study on Prompt Rewriting for\n  Improved LLM Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational User-AI Intervention: A Study on Prompt Rewriting for\n  Improved LLM Response Generation"
                },
                "summary": "Human-LLM conversations are increasingly becoming more pervasive in peoples'\nprofessional and personal lives, yet many users still struggle to elicit\nhelpful responses from LLM Chatbots. One of the reasons for this issue is\nusers' lack of understanding in crafting effective prompts that accurately\nconvey their information needs. Meanwhile, the existence of real-world\nconversational datasets on the one hand, and the text understanding faculties\nof LLMs on the other, present a unique opportunity to study this problem, and\nits potential solutions at scale. Thus, in this paper we present the first\nLLM-centric study of real human-AI chatbot conversations, focused on\ninvestigating aspects in which user queries fall short of expressing\ninformation needs, and the potential of using LLMs to rewrite suboptimal user\nprompts. Our findings demonstrate that rephrasing ineffective prompts can\nelicit better responses from a conversational system, while preserving the\nuser's original intent. Notably, the performance of rewrites improves in longer\nconversations, where contextual inferences about user needs can be made more\naccurately. Additionally, we observe that LLMs often need to -- and inherently\ndo -- make \\emph{plausible} assumptions about a user's intentions and goals\nwhen interpreting prompts. Our findings largely hold true across conversational\ndomains, user intents, and LLMs of varying sizes and families, indicating the\npromise of using prompt rewriting as a solution for better human-AI\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-LLM conversations are increasingly becoming more pervasive in peoples'\nprofessional and personal lives, yet many users still struggle to elicit\nhelpful responses from LLM Chatbots. One of the reasons for this issue is\nusers' lack of understanding in crafting effective prompts that accurately\nconvey their information needs. Meanwhile, the existence of real-world\nconversational datasets on the one hand, and the text understanding faculties\nof LLMs on the other, present a unique opportunity to study this problem, and\nits potential solutions at scale. Thus, in this paper we present the first\nLLM-centric study of real human-AI chatbot conversations, focused on\ninvestigating aspects in which user queries fall short of expressing\ninformation needs, and the potential of using LLMs to rewrite suboptimal user\nprompts. Our findings demonstrate that rephrasing ineffective prompts can\nelicit better responses from a conversational system, while preserving the\nuser's original intent. Notably, the performance of rewrites improves in longer\nconversations, where contextual inferences about user needs can be made more\naccurately. Additionally, we observe that LLMs often need to -- and inherently\ndo -- make \\emph{plausible} assumptions about a user's intentions and goals\nwhen interpreting prompts. Our findings largely hold true across conversational\ndomains, user intents, and LLMs of varying sizes and families, indicating the\npromise of using prompt rewriting as a solution for better human-AI\ninteractions."
                },
                "authors": [
                    {
                        "name": "Rupak Sarkar"
                    },
                    {
                        "name": "Bahareh Sarrafzadeh"
                    },
                    {
                        "name": "Nirupama Chandrasekaran"
                    },
                    {
                        "name": "Nagu Rangan"
                    },
                    {
                        "name": "Philip Resnik"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "arxiv_comment": "8 pages, ACL style",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20167v1",
                "updated": "2025-06-25T06:40:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    40,
                    14,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T06:40:14Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    40,
                    14,
                    2,
                    176,
                    0
                ],
                "title": "SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series\n  Prediction with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series\n  Prediction with LLMs"
                },
                "summary": "Multivariate time series forecasting requires models to simultaneously\ncapture variable-wise structural dependencies and generalize across diverse\ntasks. While structural encoders are effective in modeling feature\ninteractions, they lack the capacity to support semantic-level reasoning or\ntask adaptation. Conversely, large language models (LLMs) possess strong\ngeneralization capabilities but remain incompatible with raw time series\ninputs. This gap limits the development of unified, transferable prediction\nsystems. Therefore, we introduce SEED, a structural encoder for\nembedding-driven decoding, which integrates four stages: a token-aware encoder\nfor patch extraction, a projection module that aligns patches with language\nmodel embeddings, a semantic reprogramming mechanism that maps patches to\ntask-aware prototypes, and a frozen language model for prediction. This modular\narchitecture decouples representation learning from inference, enabling\nefficient alignment between numerical patterns and semantic reasoning.\nEmpirical results demonstrate that the proposed method achieves consistent\nimprovements over strong baselines, and comparative studies on various datasets\nconfirm SEED's role in addressing the structural-semantic modeling gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series forecasting requires models to simultaneously\ncapture variable-wise structural dependencies and generalize across diverse\ntasks. While structural encoders are effective in modeling feature\ninteractions, they lack the capacity to support semantic-level reasoning or\ntask adaptation. Conversely, large language models (LLMs) possess strong\ngeneralization capabilities but remain incompatible with raw time series\ninputs. This gap limits the development of unified, transferable prediction\nsystems. Therefore, we introduce SEED, a structural encoder for\nembedding-driven decoding, which integrates four stages: a token-aware encoder\nfor patch extraction, a projection module that aligns patches with language\nmodel embeddings, a semantic reprogramming mechanism that maps patches to\ntask-aware prototypes, and a frozen language model for prediction. This modular\narchitecture decouples representation learning from inference, enabling\nefficient alignment between numerical patterns and semantic reasoning.\nEmpirical results demonstrate that the proposed method achieves consistent\nimprovements over strong baselines, and comparative studies on various datasets\nconfirm SEED's role in addressing the structural-semantic modeling gap."
                },
                "authors": [
                    {
                        "name": "Fengze Li"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yangle Liu"
                    },
                    {
                        "name": "Ming Huang"
                    },
                    {
                        "name": "Dou Hong"
                    },
                    {
                        "name": "Jieming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jieming Ma"
                },
                "author": "Jieming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20156v1",
                "updated": "2025-06-25T06:23:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    23,
                    39,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T06:23:39Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    23,
                    39,
                    2,
                    176,
                    0
                ],
                "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through\n  Just-in-Time Insight Recall: A Conceptual Framework and System Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through\n  Just-in-Time Insight Recall: A Conceptual Framework and System Prototype"
                },
                "summary": "The core challenge in learning has shifted from knowledge acquisition to\neffective Self-Regulated Learning (SRL): planning, monitoring, and reflecting\non one's learning. Existing digital tools, however, inadequately support\nmetacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized\nreview, overlooking the role of context, while Personal Knowledge Management\n(PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel\nparadigm that conceptualizes the context-triggered retrieval of personal past\ninsights as a metacognitive scaffold to promote SRL. We formalize this paradigm\nusing the Just-in-Time Adaptive Intervention (JITAI) framework and implement a\nprototype system, Irec, to demonstrate its feasibility. At its core, Irec uses\na dynamic knowledge graph of the user's learning history. When a user faces a\nnew problem, a hybrid retrieval engine recalls relevant personal \"insights.\"\nSubsequently, a large language model (LLM) performs a deep similarity\nassessment to filter and present the most relevant scaffold in a just-in-time\nmanner. To reduce cognitive load, Irec features a human-in-the-loop pipeline\nfor LLM-based knowledge graph construction. We also propose an optional \"Guided\nInquiry\" module, where users can engage in a Socratic dialogue with an expert\nLLM, using the current problem and recalled insights as context. The\ncontribution of this paper is a solid theoretical framework and a usable system\nplatform for designing next-generation intelligent learning systems that\nenhance metacognition and self-regulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The core challenge in learning has shifted from knowledge acquisition to\neffective Self-Regulated Learning (SRL): planning, monitoring, and reflecting\non one's learning. Existing digital tools, however, inadequately support\nmetacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized\nreview, overlooking the role of context, while Personal Knowledge Management\n(PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel\nparadigm that conceptualizes the context-triggered retrieval of personal past\ninsights as a metacognitive scaffold to promote SRL. We formalize this paradigm\nusing the Just-in-Time Adaptive Intervention (JITAI) framework and implement a\nprototype system, Irec, to demonstrate its feasibility. At its core, Irec uses\na dynamic knowledge graph of the user's learning history. When a user faces a\nnew problem, a hybrid retrieval engine recalls relevant personal \"insights.\"\nSubsequently, a large language model (LLM) performs a deep similarity\nassessment to filter and present the most relevant scaffold in a just-in-time\nmanner. To reduce cognitive load, Irec features a human-in-the-loop pipeline\nfor LLM-based knowledge graph construction. We also propose an optional \"Guided\nInquiry\" module, where users can engage in a Socratic dialogue with an expert\nLLM, using the current problem and recalled insights as context. The\ncontribution of this paper is a solid theoretical framework and a usable system\nplatform for designing next-generation intelligent learning systems that\nenhance metacognition and self-regulation."
                },
                "authors": [
                    {
                        "name": "Xuefei Hou"
                    },
                    {
                        "name": "Xizhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xizhao Tan"
                },
                "author": "Xizhao Tan",
                "arxiv_comment": "Version 1 of a work in progress. Finalized system flowcharts, a\n  public GitHub repository with the source code, and a full reproducibility\n  package detailing the prompts, models, and testing guidelines will be\n  provided in v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17508v2",
                "updated": "2025-06-25T06:22:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    22,
                    45,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-20T23:17:11Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    23,
                    17,
                    11,
                    4,
                    171,
                    0
                ],
                "title": "Mapping the Evolution of Research Contributions using KnoVo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Evolution of Research Contributions using KnoVo"
                },
                "summary": "This paper presents KnoVo (Knowledge Evolution), an intelligent framework\ndesigned for quantifying and analyzing the evolution of research novelty in the\nscientific literature. Moving beyond traditional citation analysis, which\nprimarily measures impact, KnoVo determines a paper's novelty relative to both\nprior and subsequent work within its multilayered citation network. Given a\ntarget paper's abstract, KnoVo utilizes Large Language Models (LLMs) to\ndynamically extract dimensions of comparison (e.g., methodology, application,\ndataset). The target paper is then compared to related publications along these\nsame extracted dimensions. This comparative analysis, inspired by tournament\nselection, yields quantitative novelty scores reflecting the relative\nimprovement, equivalence, or inferiority of the target paper in specific\naspects. By aggregating these scores and visualizing their progression, for\ninstance, through dynamic evolution graphs and comparative radar charts, KnoVo\nfacilitates researchers not only to assess originality and identify similar\nwork, but also to track knowledge evolution along specific research dimensions,\nuncover research gaps, and explore cross-disciplinary connections. We\ndemonstrate these capabilities through a detailed analysis of 20 diverse papers\nfrom multiple scientific fields and report on the performance of various\nopen-source LLMs within the KnoVo framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents KnoVo (Knowledge Evolution), an intelligent framework\ndesigned for quantifying and analyzing the evolution of research novelty in the\nscientific literature. Moving beyond traditional citation analysis, which\nprimarily measures impact, KnoVo determines a paper's novelty relative to both\nprior and subsequent work within its multilayered citation network. Given a\ntarget paper's abstract, KnoVo utilizes Large Language Models (LLMs) to\ndynamically extract dimensions of comparison (e.g., methodology, application,\ndataset). The target paper is then compared to related publications along these\nsame extracted dimensions. This comparative analysis, inspired by tournament\nselection, yields quantitative novelty scores reflecting the relative\nimprovement, equivalence, or inferiority of the target paper in specific\naspects. By aggregating these scores and visualizing their progression, for\ninstance, through dynamic evolution graphs and comparative radar charts, KnoVo\nfacilitates researchers not only to assess originality and identify similar\nwork, but also to track knowledge evolution along specific research dimensions,\nuncover research gaps, and explore cross-disciplinary connections. We\ndemonstrate these capabilities through a detailed analysis of 20 diverse papers\nfrom multiple scientific fields and report on the performance of various\nopen-source LLMs within the KnoVo framework."
                },
                "authors": [
                    {
                        "name": "Sajratul Y. Rubaiat"
                    },
                    {
                        "name": "Syed N. Sakib"
                    },
                    {
                        "name": "Hasan M. Jamil"
                    }
                ],
                "author_detail": {
                    "name": "Hasan M. Jamil"
                },
                "author": "Hasan M. Jamil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20152v1",
                "updated": "2025-06-25T06:18:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    18,
                    46,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T06:18:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    18,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep\n  Neural Network Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep\n  Neural Network Acceleration"
                },
                "summary": "Structured pruning is a well-established technique for compressing neural\nnetworks, making it suitable for deployment in resource-limited edge devices.\nThis paper presents an efficient Loss-Aware Automatic Selection of Structured\nPruning Criteria (LAASP) for slimming and accelerating deep neural networks.\nThe majority of pruning methodologies employ a sequential process consisting of\nthree stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed\npruning technique adopts a pruning-while-training approach that eliminates the\nfirst stage and integrates the second and third stages into a single cycle. The\nautomatic selection of magnitude or similarity-based filter pruning criteria\nfrom a specified pool of criteria and the specific pruning layer at each\npruning iteration is guided by the network's overall loss on a small subset of\nthe training data. To mitigate the abrupt accuracy drop due to pruning, the\nnetwork is retrained briefly after each reduction of a predefined number of\nfloating-point operations (FLOPs). The optimal pruning rates for each layer in\nthe network are automatically determined, eliminating the need for manual\nallocation of fixed or variable pruning rates for each layer. Experiments on\nthe VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets\ndemonstrate the effectiveness of the proposed method. In particular, the\nResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the\ntop-1 accuracy compared to state-of-the-art methods while reducing the network\nFLOPs by 52\\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces\nFLOPs by more than 42\\% with a negligible 0.33\\% drop in top-5 accuracy. The\nsource code of this paper is publicly available online -\nhttps://github.com/ghimiredhikura/laasp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured pruning is a well-established technique for compressing neural\nnetworks, making it suitable for deployment in resource-limited edge devices.\nThis paper presents an efficient Loss-Aware Automatic Selection of Structured\nPruning Criteria (LAASP) for slimming and accelerating deep neural networks.\nThe majority of pruning methodologies employ a sequential process consisting of\nthree stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed\npruning technique adopts a pruning-while-training approach that eliminates the\nfirst stage and integrates the second and third stages into a single cycle. The\nautomatic selection of magnitude or similarity-based filter pruning criteria\nfrom a specified pool of criteria and the specific pruning layer at each\npruning iteration is guided by the network's overall loss on a small subset of\nthe training data. To mitigate the abrupt accuracy drop due to pruning, the\nnetwork is retrained briefly after each reduction of a predefined number of\nfloating-point operations (FLOPs). The optimal pruning rates for each layer in\nthe network are automatically determined, eliminating the need for manual\nallocation of fixed or variable pruning rates for each layer. Experiments on\nthe VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets\ndemonstrate the effectiveness of the proposed method. In particular, the\nResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the\ntop-1 accuracy compared to state-of-the-art methods while reducing the network\nFLOPs by 52\\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces\nFLOPs by more than 42\\% with a negligible 0.33\\% drop in top-5 accuracy. The\nsource code of this paper is publicly available online -\nhttps://github.com/ghimiredhikura/laasp."
                },
                "authors": [
                    {
                        "name": "Deepak Ghimire"
                    },
                    {
                        "name": "Kilho Lee"
                    },
                    {
                        "name": "Seong-heum Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-heum Kim"
                },
                "author": "Seong-heum Kim",
                "arxiv_doi": "10.1016/j.imavis.2023.104745",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.imavis.2023.104745",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.20152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Image Vision Comput. 136 (2023) 104745",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20151v1",
                "updated": "2025-06-25T06:15:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    15,
                    7,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T06:15:07Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    15,
                    7,
                    2,
                    176,
                    0
                ],
                "title": "EAR: Erasing Concepts from Unified Autoregressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAR: Erasing Concepts from Unified Autoregressive Models"
                },
                "summary": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/"
                },
                "authors": [
                    {
                        "name": "Haipeng Fan"
                    },
                    {
                        "name": "Shiyuan Zhang"
                    },
                    {
                        "name": "Baohunesitu"
                    },
                    {
                        "name": "Zihang Guo"
                    },
                    {
                        "name": "Huaiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huaiwen Zhang"
                },
                "author": "Huaiwen Zhang",
                "arxiv_comment": "11 pages, 7 figures, 1 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00845v2",
                "updated": "2025-06-25T06:00:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    6,
                    0,
                    8,
                    2,
                    176,
                    0
                ],
                "published": "2025-03-02T10:39:40Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    10,
                    39,
                    40,
                    6,
                    61,
                    0
                ],
                "title": "Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners"
                },
                "summary": "Despite significant advancements in Large Language Models (LLMs), developing\nadvanced reasoning capabilities in LLMs remains a key challenge. Process Reward\nModels (PRMs) have demonstrated exceptional promise in enhancing reasoning by\nproviding step-wise feedback, particularly in the context of mathematical\nreasoning. However, their application to broader reasoning domains remains\nunderstudied, largely due to the high costs associated with manually creating\nstep-level supervision. In this work, we explore the potential of PRMs in graph\nreasoning problems - a domain that demands sophisticated multi-step reasoning\nand offers opportunities for automated step-level data generation using\nestablished graph algorithms. We introduce GraphSILO, the largest dataset for\ngraph reasoning problems with fine-grained step-wise labels, built using\nautomated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to\ngenerate detailed reasoning steps with step-wise labels. Building upon this\ndataset, we train GraphPRM, the first PRM designed for graph reasoning\nproblems, and evaluate its effectiveness in two key settings: inference-time\nscaling and reinforcement learning via Direct Preference Optimization (DPO).\nExperimental results show that GraphPRM significantly improves LLM performance\nacross 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and\ndemonstrating transferability to new graph reasoning datasets and new reasoning\ndomains like mathematical problem-solving. Notably, GraphPRM enhances LLM\nperformance on GSM8K and Math500, underscoring the cross-domain applicability\nof graph-based reasoning rewards. Our findings highlight the potential of PRMs\nin advancing reasoning across diverse domains, paving the way for more\nversatile and effective LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in Large Language Models (LLMs), developing\nadvanced reasoning capabilities in LLMs remains a key challenge. Process Reward\nModels (PRMs) have demonstrated exceptional promise in enhancing reasoning by\nproviding step-wise feedback, particularly in the context of mathematical\nreasoning. However, their application to broader reasoning domains remains\nunderstudied, largely due to the high costs associated with manually creating\nstep-level supervision. In this work, we explore the potential of PRMs in graph\nreasoning problems - a domain that demands sophisticated multi-step reasoning\nand offers opportunities for automated step-level data generation using\nestablished graph algorithms. We introduce GraphSILO, the largest dataset for\ngraph reasoning problems with fine-grained step-wise labels, built using\nautomated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to\ngenerate detailed reasoning steps with step-wise labels. Building upon this\ndataset, we train GraphPRM, the first PRM designed for graph reasoning\nproblems, and evaluate its effectiveness in two key settings: inference-time\nscaling and reinforcement learning via Direct Preference Optimization (DPO).\nExperimental results show that GraphPRM significantly improves LLM performance\nacross 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and\ndemonstrating transferability to new graph reasoning datasets and new reasoning\ndomains like mathematical problem-solving. Notably, GraphPRM enhances LLM\nperformance on GSM8K and Math500, underscoring the cross-domain applicability\nof graph-based reasoning rewards. Our findings highlight the potential of PRMs\nin advancing reasoning across diverse domains, paving the way for more\nversatile and effective LLMs."
                },
                "authors": [
                    {
                        "name": "Miao Peng"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Zongrui Suo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "Accepted to KDD 2025 Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20137v1",
                "updated": "2025-06-25T05:11:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    5,
                    11,
                    50,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T05:11:50Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    5,
                    11,
                    50,
                    2,
                    176,
                    0
                ],
                "title": "Operation of the Trigger System for the ICARUS Detector at Fermilab",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operation of the Trigger System for the ICARUS Detector at Fermilab"
                },
                "summary": "The ICARUS liquid argon TPC detector is taking data on the Booster (BNB) and\nMain Injector (NuMI) Neutrino beam lines at Fermilab with a trigger system\nbased on the scintillation light produced by charged particles in coincidence\nwith the proton beam extraction from the accelerators. The architecture and the\ndeployment of the trigger system in the first two runs for physics are\npresented, as well as the triggered event rates. The event recognition\nefficiency has been evaluated as a function of the deposited energy and the\nposition of cosmic muons stopping inside the detector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ICARUS liquid argon TPC detector is taking data on the Booster (BNB) and\nMain Injector (NuMI) Neutrino beam lines at Fermilab with a trigger system\nbased on the scintillation light produced by charged particles in coincidence\nwith the proton beam extraction from the accelerators. The architecture and the\ndeployment of the trigger system in the first two runs for physics are\npresented, as well as the triggered event rates. The event recognition\nefficiency has been evaluated as a function of the deposited energy and the\nposition of cosmic muons stopping inside the detector."
                },
                "authors": [
                    {
                        "name": "ICARUS collaboration"
                    },
                    {
                        "name": "F. Abd Alrahman"
                    },
                    {
                        "name": "P. Abratenko"
                    },
                    {
                        "name": "N. Abrego-Martinez"
                    },
                    {
                        "name": "A. Aduszkiewicz"
                    },
                    {
                        "name": "F. Akbar"
                    },
                    {
                        "name": "L. Aliaga Soplin"
                    },
                    {
                        "name": "J. Asaadi"
                    },
                    {
                        "name": "W. F. Badgett"
                    },
                    {
                        "name": "F. Battisti"
                    },
                    {
                        "name": "V. Bellini"
                    },
                    {
                        "name": "R. Benocci"
                    },
                    {
                        "name": "J. Berger"
                    },
                    {
                        "name": "S. Berkman"
                    },
                    {
                        "name": "S. Bertolucci"
                    },
                    {
                        "name": "M. Betancourt"
                    },
                    {
                        "name": "A. Blanchet"
                    },
                    {
                        "name": "F. Boffelli"
                    },
                    {
                        "name": "M. Bonesini"
                    },
                    {
                        "name": "T. Boone"
                    },
                    {
                        "name": "B. Bottino"
                    },
                    {
                        "name": "A. Braggiotti"
                    },
                    {
                        "name": "D. Brailsford"
                    },
                    {
                        "name": "S. J. Brice"
                    },
                    {
                        "name": "V. Brio"
                    },
                    {
                        "name": "C. Brizzolari"
                    },
                    {
                        "name": "H. S. Budd"
                    },
                    {
                        "name": "A. Campani"
                    },
                    {
                        "name": "A. Campos"
                    },
                    {
                        "name": "D. Carber"
                    },
                    {
                        "name": "M. Carneiro"
                    },
                    {
                        "name": "I. Caro Terrazas"
                    },
                    {
                        "name": "H. Carranza"
                    },
                    {
                        "name": "F. Castillo Fernandez"
                    },
                    {
                        "name": "S. Centro"
                    },
                    {
                        "name": "G. Cerati"
                    },
                    {
                        "name": "A. Chatterjee"
                    },
                    {
                        "name": "D. Cherdack"
                    },
                    {
                        "name": "S. Cherubini"
                    },
                    {
                        "name": "N. Chithirasreemadam"
                    },
                    {
                        "name": "T. E. Coan"
                    },
                    {
                        "name": "A. Cocco"
                    },
                    {
                        "name": "M. R. Convery"
                    },
                    {
                        "name": "L. Cooper-Troendle"
                    },
                    {
                        "name": "S. Copello"
                    },
                    {
                        "name": "H. Da Motta"
                    },
                    {
                        "name": "M. Dallolio"
                    },
                    {
                        "name": "A. A. Dange"
                    },
                    {
                        "name": "A. de Roeck"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "L. Di Noto"
                    },
                    {
                        "name": "M. Diwan"
                    },
                    {
                        "name": "S. Dolan"
                    },
                    {
                        "name": "L. Domine"
                    },
                    {
                        "name": "S. Donati"
                    },
                    {
                        "name": "F. Drielsma"
                    },
                    {
                        "name": "J. Dyer"
                    },
                    {
                        "name": "S. Dytman"
                    },
                    {
                        "name": "A. Falcone"
                    },
                    {
                        "name": "C. Farnese"
                    },
                    {
                        "name": "A. Fava"
                    },
                    {
                        "name": "N. Gallice"
                    },
                    {
                        "name": "F. G. Garcia"
                    },
                    {
                        "name": "C. Gatto"
                    },
                    {
                        "name": "D. Gibin"
                    },
                    {
                        "name": "A. Gioiosa"
                    },
                    {
                        "name": "W. Gu"
                    },
                    {
                        "name": "A. Guglielmi"
                    },
                    {
                        "name": "G. Gurung"
                    },
                    {
                        "name": "K. Hassinin"
                    },
                    {
                        "name": "H. Hausner"
                    },
                    {
                        "name": "A. Heggestuen"
                    },
                    {
                        "name": "B. Howard"
                    },
                    {
                        "name": "R. Howell"
                    },
                    {
                        "name": "Z. Hulcher"
                    },
                    {
                        "name": "I. Ingratta"
                    },
                    {
                        "name": "C. James"
                    },
                    {
                        "name": "W. Jang"
                    },
                    {
                        "name": "Y. -J. Jwa"
                    },
                    {
                        "name": "L. Kashur"
                    },
                    {
                        "name": "W. Ketchum"
                    },
                    {
                        "name": "J. S. Kim"
                    },
                    {
                        "name": "D. -H. Koh"
                    },
                    {
                        "name": "T. Krishnan"
                    },
                    {
                        "name": "J. Larkin"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "C. Mariani"
                    },
                    {
                        "name": "C. M. Marshall"
                    },
                    {
                        "name": "S. Martynenko"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "K. S. McFarland"
                    },
                    {
                        "name": "A. Menegolli"
                    },
                    {
                        "name": "G. Meng"
                    },
                    {
                        "name": "O. G. Miranda"
                    },
                    {
                        "name": "A. Mogan"
                    },
                    {
                        "name": "N. Moggi"
                    },
                    {
                        "name": "E. Montagna"
                    },
                    {
                        "name": "A. Montanari"
                    },
                    {
                        "name": "C. Montanari"
                    },
                    {
                        "name": "M. Mooney"
                    },
                    {
                        "name": "G. Moreno-Granados"
                    },
                    {
                        "name": "J. Mueller"
                    },
                    {
                        "name": "M. Murphy"
                    },
                    {
                        "name": "D. P. Méndez"
                    },
                    {
                        "name": "D. Naples"
                    },
                    {
                        "name": "S. Palestini"
                    },
                    {
                        "name": "M. Pallavicini"
                    },
                    {
                        "name": "V. Paolone"
                    },
                    {
                        "name": "L. Pasqualini"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "L. Paudel"
                    },
                    {
                        "name": "G. Petrillo"
                    },
                    {
                        "name": "C. Petta"
                    },
                    {
                        "name": "V. Pia"
                    },
                    {
                        "name": "F. Pietropaolo"
                    },
                    {
                        "name": "F. Poppi"
                    },
                    {
                        "name": "M. Pozzato"
                    },
                    {
                        "name": "M. L. Pumo"
                    },
                    {
                        "name": "G. Putnam"
                    },
                    {
                        "name": "X. Qian"
                    },
                    {
                        "name": "A. Rappoldi"
                    },
                    {
                        "name": "G. L. Raselli"
                    },
                    {
                        "name": "S. Repetto"
                    },
                    {
                        "name": "F. Resnati"
                    },
                    {
                        "name": "A. M. Ricci"
                    },
                    {
                        "name": "E. Richards"
                    },
                    {
                        "name": "M. Rosenberg"
                    },
                    {
                        "name": "M. Rossella"
                    },
                    {
                        "name": "N. Rowe"
                    },
                    {
                        "name": "P. Roy"
                    },
                    {
                        "name": "C. Rubbia"
                    },
                    {
                        "name": "M. Saad"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "G. Salmoria"
                    },
                    {
                        "name": "S. Samanta"
                    },
                    {
                        "name": "A. Scaramelli"
                    },
                    {
                        "name": "D. Schmitz"
                    },
                    {
                        "name": "A. Schukraft"
                    },
                    {
                        "name": "D. Senadheera"
                    },
                    {
                        "name": "S-H. Seo"
                    },
                    {
                        "name": "F. Sergiampietri"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "J. S. Smedley"
                    },
                    {
                        "name": "J. Smith"
                    },
                    {
                        "name": "M. Sotgia"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Stewart"
                    },
                    {
                        "name": "H. A. Tanaka"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "K. Terao"
                    },
                    {
                        "name": "F. Terranova"
                    },
                    {
                        "name": "V. Togo"
                    },
                    {
                        "name": "D. Torretta"
                    },
                    {
                        "name": "M. Torti"
                    },
                    {
                        "name": "F. Tortorici"
                    },
                    {
                        "name": "D. Totani"
                    },
                    {
                        "name": "R. Triozzi"
                    },
                    {
                        "name": "Y. -T. Tsai"
                    },
                    {
                        "name": "K. V. Tsang"
                    },
                    {
                        "name": "T. Usher"
                    },
                    {
                        "name": "F. Varanini"
                    },
                    {
                        "name": "N. Vardy"
                    },
                    {
                        "name": "S. Ventura"
                    },
                    {
                        "name": "M. Vicenzi"
                    },
                    {
                        "name": "C. Vignoli"
                    },
                    {
                        "name": "B. Viren"
                    },
                    {
                        "name": "F. A. Wieler"
                    },
                    {
                        "name": "Z. Williams"
                    },
                    {
                        "name": "P. Wilson"
                    },
                    {
                        "name": "R. J. Wilson"
                    },
                    {
                        "name": "J. Wolfs"
                    },
                    {
                        "name": "T. Wongjirad"
                    },
                    {
                        "name": "A. Wood"
                    },
                    {
                        "name": "E. Worcester"
                    },
                    {
                        "name": "M. Worcester"
                    },
                    {
                        "name": "M. Wospakrik"
                    },
                    {
                        "name": "S. Yadav"
                    },
                    {
                        "name": "H. Yu"
                    },
                    {
                        "name": "J. Yu"
                    },
                    {
                        "name": "A. Zani"
                    },
                    {
                        "name": "J. Zennamo"
                    },
                    {
                        "name": "J. Zettlemoyer"
                    },
                    {
                        "name": "C. Zhang"
                    },
                    {
                        "name": "S. Zucchelli"
                    }
                ],
                "author_detail": {
                    "name": "S. Zucchelli"
                },
                "author": "S. Zucchelli",
                "arxiv_comment": "21 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19269v2",
                "updated": "2025-06-25T05:10:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    5,
                    10,
                    4,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-24T03:03:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    3,
                    3,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic\n  Manipulation"
                },
                "summary": "We present AnchorDP3, a diffusion policy framework for dual-arm robotic\nmanipulation that achieves state-of-the-art performance in highly randomized\nenvironments. AnchorDP3 integrates three key innovations: (1)\nSimulator-Supervised Semantic Segmentation, using rendered ground truth to\nexplicitly segment task-critical objects within the point cloud, which provides\nstrong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight\nmodules processing augmented point clouds per task, enabling efficient\nmulti-task learning through a shared diffusion-based action expert; (3)\nAffordance-Anchored Keypose Diffusion with Full State Supervision, replacing\ndense trajectory prediction with sparse, geometrically meaningful action\nanchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to\naffordances, drastically simplifying the prediction space; the action expert is\nforced to predict both robot joint angles and end-effector poses\nsimultaneously, which exploits geometric consistency to accelerate convergence\nand boost accuracy. Trained on large-scale, procedurally generated simulation\ndata, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark\nacross diverse tasks under extreme randomization of objects, clutter, table\nheight, lighting, and backgrounds. This framework, when integrated with the\nRoboTwin real-to-sim pipeline, has the potential to enable fully autonomous\ngeneration of deployable visuomotor policies from only scene and instruction,\ntotally eliminating human demonstrations from learning manipulation skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AnchorDP3, a diffusion policy framework for dual-arm robotic\nmanipulation that achieves state-of-the-art performance in highly randomized\nenvironments. AnchorDP3 integrates three key innovations: (1)\nSimulator-Supervised Semantic Segmentation, using rendered ground truth to\nexplicitly segment task-critical objects within the point cloud, which provides\nstrong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight\nmodules processing augmented point clouds per task, enabling efficient\nmulti-task learning through a shared diffusion-based action expert; (3)\nAffordance-Anchored Keypose Diffusion with Full State Supervision, replacing\ndense trajectory prediction with sparse, geometrically meaningful action\nanchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to\naffordances, drastically simplifying the prediction space; the action expert is\nforced to predict both robot joint angles and end-effector poses\nsimultaneously, which exploits geometric consistency to accelerate convergence\nand boost accuracy. Trained on large-scale, procedurally generated simulation\ndata, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark\nacross diverse tasks under extreme randomization of objects, clutter, table\nheight, lighting, and backgrounds. This framework, when integrated with the\nRoboTwin real-to-sim pipeline, has the potential to enable fully autonomous\ngeneration of deployable visuomotor policies from only scene and instruction,\ntotally eliminating human demonstrations from learning manipulation skills."
                },
                "authors": [
                    {
                        "name": "Ziyan Zhao"
                    },
                    {
                        "name": "Ke Fan"
                    },
                    {
                        "name": "He-Yang Xu"
                    },
                    {
                        "name": "Ning Qiao"
                    },
                    {
                        "name": "Bo Peng"
                    },
                    {
                        "name": "Wenlong Gao"
                    },
                    {
                        "name": "Dongjiang Li"
                    },
                    {
                        "name": "Hui Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hui Shen"
                },
                "author": "Hui Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20134v1",
                "updated": "2025-06-25T05:05:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    5,
                    5,
                    9,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T05:05:09Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    5,
                    5,
                    9,
                    2,
                    176,
                    0
                ],
                "title": "From 2D to 3D Cognition: A Brief Survey of General World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 2D to 3D Cognition: A Brief Survey of General World Models"
                },
                "summary": "World models have garnered increasing attention in the development of\nartificial general intelligence (AGI), serving as computational frameworks for\nlearning representations of the external world and forecasting future states.\nWhile early efforts focused on 2D visual perception and simulation, recent\n3D-aware generative world models have demonstrated the ability to synthesize\ngeometrically consistent, interactive 3D environments, marking a shift toward\n3D spatial cognition. Despite rapid progress, the field lacks systematic\nanalysis to categorize emerging techniques and clarify their roles in advancing\n3D cognitive world models. This survey addresses this need by introducing a\nconceptual framework, providing a structured and forward-looking review of\nworld models transitioning from 2D perception to 3D cognition. Within this\nframework, we highlight two key technological drivers, particularly advances in\n3D representations and the incorporation of world knowledge, as fundamental\npillars. Building on these, we dissect three core cognitive capabilities that\nunderpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,\nand 3D spatial interaction. We further examine the deployment of these\ncapabilities in real-world applications, including embodied AI, autonomous\ndriving, digital twin, and gaming/VR. Finally, we identify challenges across\ndata, modeling, and deployment, and outline future directions for advancing\nmore robust and generalizable 3D world models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World models have garnered increasing attention in the development of\nartificial general intelligence (AGI), serving as computational frameworks for\nlearning representations of the external world and forecasting future states.\nWhile early efforts focused on 2D visual perception and simulation, recent\n3D-aware generative world models have demonstrated the ability to synthesize\ngeometrically consistent, interactive 3D environments, marking a shift toward\n3D spatial cognition. Despite rapid progress, the field lacks systematic\nanalysis to categorize emerging techniques and clarify their roles in advancing\n3D cognitive world models. This survey addresses this need by introducing a\nconceptual framework, providing a structured and forward-looking review of\nworld models transitioning from 2D perception to 3D cognition. Within this\nframework, we highlight two key technological drivers, particularly advances in\n3D representations and the incorporation of world knowledge, as fundamental\npillars. Building on these, we dissect three core cognitive capabilities that\nunderpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,\nand 3D spatial interaction. We further examine the deployment of these\ncapabilities in real-world applications, including embodied AI, autonomous\ndriving, digital twin, and gaming/VR. Finally, we identify challenges across\ndata, modeling, and deployment, and outline future directions for advancing\nmore robust and generalizable 3D world models."
                },
                "authors": [
                    {
                        "name": "Ningwei Xie"
                    },
                    {
                        "name": "Zizi Tian"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    },
                    {
                        "name": "Meng Guo"
                    },
                    {
                        "name": "Jie Li"
                    }
                ],
                "author_detail": {
                    "name": "Jie Li"
                },
                "author": "Jie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20128v1",
                "updated": "2025-06-25T04:49:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    4,
                    49,
                    3,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T04:49:03Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    4,
                    49,
                    3,
                    2,
                    176,
                    0
                ],
                "title": "CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG\n  Evaluation"
                },
                "summary": "RAG systems enhance LLMs by incorporating external knowledge, which is\ncrucial for domains that demand factual accuracy and up-to-date information.\nHowever, evaluating the multifaceted quality of RAG outputs, spanning aspects\nsuch as contextual coherence, query relevance, factual correctness, and\ninformational completeness, poses significant challenges. Existing evaluation\nmethods often rely on simple lexical overlap metrics, which are inadequate for\ncapturing these nuances, or involve complex multi-stage pipelines with\nintermediate steps like claim extraction or require finetuning specialized\njudge models, hindering practical efficiency. To address these limitations, we\npropose CCRS (Contextual Coherence and Relevance Score), a novel suite of five\nmetrics that utilizes a single, powerful, pretrained LLM as a zero-shot,\nend-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance\n(QR), Information Density (ID), Answer Correctness (AC), and Information Recall\n(IR). We apply CCRS to evaluate six diverse RAG system configurations on the\nchallenging BioASQ dataset. Our analysis demonstrates that CCRS effectively\ndiscriminates between system performances, confirming, for instance, that the\nMistral-7B reader outperforms Llama variants. We provide a detailed analysis of\nCCRS metric properties, including score distributions, convergent/discriminant\nvalidity, tie rates, population statistics, and discriminative power. Compared\nto the complex RAGChecker framework, CCRS offers comparable or superior\ndiscriminative power for key aspects like recall and faithfulness, while being\nsignificantly more computationally efficient. CCRS thus provides a practical,\ncomprehensive, and efficient framework for evaluating and iteratively improving\nRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG systems enhance LLMs by incorporating external knowledge, which is\ncrucial for domains that demand factual accuracy and up-to-date information.\nHowever, evaluating the multifaceted quality of RAG outputs, spanning aspects\nsuch as contextual coherence, query relevance, factual correctness, and\ninformational completeness, poses significant challenges. Existing evaluation\nmethods often rely on simple lexical overlap metrics, which are inadequate for\ncapturing these nuances, or involve complex multi-stage pipelines with\nintermediate steps like claim extraction or require finetuning specialized\njudge models, hindering practical efficiency. To address these limitations, we\npropose CCRS (Contextual Coherence and Relevance Score), a novel suite of five\nmetrics that utilizes a single, powerful, pretrained LLM as a zero-shot,\nend-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance\n(QR), Information Density (ID), Answer Correctness (AC), and Information Recall\n(IR). We apply CCRS to evaluate six diverse RAG system configurations on the\nchallenging BioASQ dataset. Our analysis demonstrates that CCRS effectively\ndiscriminates between system performances, confirming, for instance, that the\nMistral-7B reader outperforms Llama variants. We provide a detailed analysis of\nCCRS metric properties, including score distributions, convergent/discriminant\nvalidity, tie rates, population statistics, and discriminative power. Compared\nto the complex RAGChecker framework, CCRS offers comparable or superior\ndiscriminative power for key aspects like recall and faithfulness, while being\nsignificantly more computationally efficient. CCRS thus provides a practical,\ncomprehensive, and efficient framework for evaluating and iteratively improving\nRAG systems."
                },
                "authors": [
                    {
                        "name": "Aashiq Muhamed"
                    }
                ],
                "author_detail": {
                    "name": "Aashiq Muhamed"
                },
                "author": "Aashiq Muhamed",
                "arxiv_comment": "Accepted at LLM4Eval @ SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20112v1",
                "updated": "2025-06-25T04:02:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    4,
                    2,
                    29,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T04:02:29Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    4,
                    2,
                    29,
                    2,
                    176,
                    0
                ],
                "title": "A Multi-Pass Large Language Model Framework for Precise and Efficient\n  Radiology Report Error Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Pass Large Language Model Framework for Precise and Efficient\n  Radiology Report Error Detection"
                },
                "summary": "Background: The positive predictive value (PPV) of large language model\n(LLM)-based proofreading for radiology reports is limited due to the low error\nprevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV\nand reduces operational costs compared with baseline approaches. Materials and\nMethods: A retrospective analysis was performed on 1,000 consecutive radiology\nreports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III\ndatabase. Two external datasets (CheXpert and Open-i) were validation sets.\nThree LLM frameworks were tested: (1) single-prompt detector; (2) extractor\nplus detector; and (3) extractor, detector, and false-positive verifier.\nPrecision was measured by PPV and absolute true positive rate (aTPR).\nEfficiency was calculated from model inference charges and reviewer\nremuneration. Statistical significance was tested using cluster bootstrap,\nexact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV\nincreased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,\nFramework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.\nbaselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per\n1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and\nUSD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.\nHuman-reviewed reports decreased from 192 to 88. External validation supported\nFramework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR\n(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and\nreduced operational costs, maintaining detection performance, providing an\neffective strategy for AI-assisted radiology report quality assurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The positive predictive value (PPV) of large language model\n(LLM)-based proofreading for radiology reports is limited due to the low error\nprevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV\nand reduces operational costs compared with baseline approaches. Materials and\nMethods: A retrospective analysis was performed on 1,000 consecutive radiology\nreports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III\ndatabase. Two external datasets (CheXpert and Open-i) were validation sets.\nThree LLM frameworks were tested: (1) single-prompt detector; (2) extractor\nplus detector; and (3) extractor, detector, and false-positive verifier.\nPrecision was measured by PPV and absolute true positive rate (aTPR).\nEfficiency was calculated from model inference charges and reviewer\nremuneration. Statistical significance was tested using cluster bootstrap,\nexact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV\nincreased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,\nFramework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.\nbaselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per\n1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and\nUSD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.\nHuman-reviewed reports decreased from 192 to 88. External validation supported\nFramework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR\n(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and\nreduced operational costs, maintaining detection performance, providing an\neffective strategy for AI-assisted radiology report quality assurance."
                },
                "authors": [
                    {
                        "name": "Songsoo Kim"
                    },
                    {
                        "name": "Seungtae Lee"
                    },
                    {
                        "name": "See Young Lee"
                    },
                    {
                        "name": "Joonho Kim"
                    },
                    {
                        "name": "Keechan Kan"
                    },
                    {
                        "name": "Dukyong Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Dukyong Yoon"
                },
                "author": "Dukyong Yoon",
                "arxiv_comment": "29 pages, 5 figures, 4 tables. Code available at\n  https://github.com/radssk/mp-rred",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]