[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v2",
                "updated": "2024-08-21T17:55:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    55,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v1",
                "updated": "2024-08-16T15:11:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Koucký"
                    },
                    {
                        "name": "Josef Matějka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matějka"
                },
                "author": "Josef Matějka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes Müller"
                    },
                    {
                        "name": "Anna Lena Duque Antón"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagić"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11053v1",
                "updated": "2024-08-20T17:58:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    58,
                    56,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:58:56Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    58,
                    56,
                    1,
                    233,
                    0
                ],
                "title": "Revisiting VerilogEval: Newer LLMs, In-Context Learning, and\n  Specification-to-RTL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting VerilogEval: Newer LLMs, In-Context Learning, and\n  Specification-to-RTL Tasks"
                },
                "summary": "The application of large-language models (LLMs) to digital hardware code\ngeneration is an emerging field. Most LLMs are primarily trained on natural\nlanguage and software code. Hardware code, such as Verilog, represents only a\nsmall portion of the training data and few hardware benchmarks exist. To\naddress this gap, the open-source VerilogEval benchmark was released in 2023,\nproviding a consistent evaluation framework for LLMs on code completion tasks.\nIt was tested on state-of-the-art models at the time including GPT-4. However,\nVerilogEval and other Verilog generation benchmarks lack failure analysis and,\nin present form, are not conducive to exploring prompting techniques. Also,\nsince VerilogEval's release, both commercial and open-source models have seen\ncontinued development.\n  In this work, we evaluate new commercial and open-source models of varying\nsizes against an improved VerilogEval benchmark suite. We enhance VerilogEval's\ninfrastructure and dataset by automatically classifying failures, introduce new\nprompts for supporting in-context learning (ICL) examples, and extend the\nsupported tasks to specification-to-RTL translation. We find a measurable\nimprovement in commercial state-of-the-art models, with GPT-4 Turbo achieving a\n59% pass rate on spec-to-RTL tasks. We also study the performance of\nopen-source and domain-specific models that have emerged, and demonstrate that\nmodels can benefit substantially from ICL. We find that recently-released Llama\n3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo,\nand that the much smaller domain-specific RTL-Coder 6.7B models achieve an\nimpressive 37% pass rate. However, prompt engineering is key to achieving good\npass rates, and varies widely with model and task. A benchmark infrastructure\nthat allows for prompt engineering and failure analysis is key to continued\nmodel development and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large-language models (LLMs) to digital hardware code\ngeneration is an emerging field. Most LLMs are primarily trained on natural\nlanguage and software code. Hardware code, such as Verilog, represents only a\nsmall portion of the training data and few hardware benchmarks exist. To\naddress this gap, the open-source VerilogEval benchmark was released in 2023,\nproviding a consistent evaluation framework for LLMs on code completion tasks.\nIt was tested on state-of-the-art models at the time including GPT-4. However,\nVerilogEval and other Verilog generation benchmarks lack failure analysis and,\nin present form, are not conducive to exploring prompting techniques. Also,\nsince VerilogEval's release, both commercial and open-source models have seen\ncontinued development.\n  In this work, we evaluate new commercial and open-source models of varying\nsizes against an improved VerilogEval benchmark suite. We enhance VerilogEval's\ninfrastructure and dataset by automatically classifying failures, introduce new\nprompts for supporting in-context learning (ICL) examples, and extend the\nsupported tasks to specification-to-RTL translation. We find a measurable\nimprovement in commercial state-of-the-art models, with GPT-4 Turbo achieving a\n59% pass rate on spec-to-RTL tasks. We also study the performance of\nopen-source and domain-specific models that have emerged, and demonstrate that\nmodels can benefit substantially from ICL. We find that recently-released Llama\n3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo,\nand that the much smaller domain-specific RTL-Coder 6.7B models achieve an\nimpressive 37% pass rate. However, prompt engineering is key to achieving good\npass rates, and varies widely with model and task. A benchmark infrastructure\nthat allows for prompt engineering and failure analysis is key to continued\nmodel development and deployment."
                },
                "authors": [
                    {
                        "name": "Nathaniel Pinckney"
                    },
                    {
                        "name": "Christopher Batten"
                    },
                    {
                        "name": "Mingjie Liu"
                    },
                    {
                        "name": "Haoxing Ren"
                    },
                    {
                        "name": "Brucek Khailany"
                    }
                ],
                "author_detail": {
                    "name": "Brucek Khailany"
                },
                "author": "Brucek Khailany",
                "arxiv_comment": "This paper revisits and improves the benchmark first presented in\n  arXiv:2309.07544. Seven pages, three figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11051v1",
                "updated": "2024-08-20T17:57:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    46,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:57:46Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    46,
                    1,
                    233,
                    0
                ],
                "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io"
                },
                "authors": [
                    {
                        "name": "Yunzhe Xu"
                    },
                    {
                        "name": "Yiyuan Pan"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v2",
                "updated": "2024-08-21T17:55:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    55,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10188v3",
                "updated": "2024-08-21T17:47:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    47,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-19T17:48:08Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    48,
                    8,
                    0,
                    232,
                    0
                ],
                "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos"
                },
                "summary": "Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long supervised fine-tuning. However, training on long video is\ncomputationally and memory intensive. We introduce the long-context Multi-Modal\nSequence Parallelism (MM-SP) system that efficiently parallelizes long video\ntraining and inference, enabling 2M context length training on 256 GPUs without\nany gradient checkpointing. LongVILA efficiently extends the number of video\nframes of VILA from 8 to 1024, improving the long video captioning score from\n2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k context\nlength) video needle-in-a-haystack. LongVILA-8B demonstrates consistent\naccuracy improvements on long videos in the VideoMME benchmark as the number of\nframes increases. Besides, MM-SP is 2.1x - 5.7x faster than ring sequence\nparallelism and 1.1x - 1.4x faster than Megatron with context parallelism +\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long supervised fine-tuning. However, training on long video is\ncomputationally and memory intensive. We introduce the long-context Multi-Modal\nSequence Parallelism (MM-SP) system that efficiently parallelizes long video\ntraining and inference, enabling 2M context length training on 256 GPUs without\nany gradient checkpointing. LongVILA efficiently extends the number of video\nframes of VILA from 8 to 1024, improving the long video captioning score from\n2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k context\nlength) video needle-in-a-haystack. LongVILA-8B demonstrates consistent\naccuracy improvements on long videos in the VideoMME benchmark as the number of\nframes increases. Besides, MM-SP is 2.1x - 5.7x faster than ring sequence\nparallelism and 1.1x - 1.4x faster than Megatron with context parallelism +\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers."
                },
                "authors": [
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Ethan He"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Linxi Fan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01099v2",
                "updated": "2024-08-20T17:54:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    54,
                    8,
                    1,
                    233,
                    0
                ],
                "published": "2024-04-01T13:12:30Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    13,
                    12,
                    30,
                    0,
                    92,
                    0
                ],
                "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety"
                },
                "summary": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking."
                },
                "authors": [
                    {
                        "name": "Luxi He"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Peter Henderson"
                    }
                ],
                "author_detail": {
                    "name": "Peter Henderson"
                },
                "author": "Peter Henderson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2204.09457v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2204.09457v6",
                "updated": "2024-08-20T17:51:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    51,
                    8,
                    1,
                    233,
                    0
                ],
                "published": "2022-04-20T13:43:01Z",
                "published_parsed": [
                    2022,
                    4,
                    20,
                    13,
                    43,
                    1,
                    2,
                    110,
                    0
                ],
                "title": "On the relative asymptotic expressivity of inference frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the relative asymptotic expressivity of inference frameworks"
                },
                "summary": "We consider logics with truth values in the unit interval $[0,1]$. Such\nlogics are used to define queries and to define probability distributions. In\nthis context the notion of almost sure equivalence of formulas is generalized\nto the notion of asymptotic equivalence. We prove two new results about the\nasymptotic equivalence of formulas where each result has a convergence law as a\ncorollary. These results as well as several older results can be formulated as\nresults about the relative asymptotic expressivity of inference frameworks. An\ninference framework $\\mathbf{F}$ is a class of pairs $(\\mathbb{P}, L)$, where\n$\\mathbb{P} = (\\mathbb{P}_n : n = 1, 2, 3, \\ldots)$, $\\mathbb{P}_n$ are\nprobability distributions on the set $\\mathbf{W}_n$ of all $\\sigma$-structures\nwith domain $\\{1, \\ldots, n\\}$ (where $\\sigma$ is a first-order signature) and\n$L$ is a logic with truth values in the unit interval $[0, 1]$. An inference\nframework $\\mathbf{F}'$ is asymptotically at least as expressive as an\ninference framework $\\mathbf{F}$ if for every $(\\mathbb{P}, L) \\in \\mathbf{F}$\nthere is $(\\mathbb{P}', L') \\in \\mathbf{F}'$ such that $\\mathbb{P}$ is\nasymptotically total variation equivalent to $\\mathbb{P}'$ and for every\n$\\varphi(\\bar{x}) \\in L$ there is $\\varphi'(\\bar{x}) \\in L'$ such that\n$\\varphi'(\\bar{x})$ is asymptotically equivalent to $\\varphi(\\bar{x})$ with\nrespect to $\\mathbb{P}$. This relation is a preorder. If, in addition,\n$\\mathbf{F}$ is at least as expressive as $\\mathbf{F}'$ then we say that\n$\\mathbf{F}$ and $\\mathbf{F}'$ are asymptotically equally expressive. Our third\ncontribution is to systematize the new results of this paper and several\nprevious results in order to get a preorder on a number of inference systems\nthat are of relevance in the context of machine learning and artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider logics with truth values in the unit interval $[0,1]$. Such\nlogics are used to define queries and to define probability distributions. In\nthis context the notion of almost sure equivalence of formulas is generalized\nto the notion of asymptotic equivalence. We prove two new results about the\nasymptotic equivalence of formulas where each result has a convergence law as a\ncorollary. These results as well as several older results can be formulated as\nresults about the relative asymptotic expressivity of inference frameworks. An\ninference framework $\\mathbf{F}$ is a class of pairs $(\\mathbb{P}, L)$, where\n$\\mathbb{P} = (\\mathbb{P}_n : n = 1, 2, 3, \\ldots)$, $\\mathbb{P}_n$ are\nprobability distributions on the set $\\mathbf{W}_n$ of all $\\sigma$-structures\nwith domain $\\{1, \\ldots, n\\}$ (where $\\sigma$ is a first-order signature) and\n$L$ is a logic with truth values in the unit interval $[0, 1]$. An inference\nframework $\\mathbf{F}'$ is asymptotically at least as expressive as an\ninference framework $\\mathbf{F}$ if for every $(\\mathbb{P}, L) \\in \\mathbf{F}$\nthere is $(\\mathbb{P}', L') \\in \\mathbf{F}'$ such that $\\mathbb{P}$ is\nasymptotically total variation equivalent to $\\mathbb{P}'$ and for every\n$\\varphi(\\bar{x}) \\in L$ there is $\\varphi'(\\bar{x}) \\in L'$ such that\n$\\varphi'(\\bar{x})$ is asymptotically equivalent to $\\varphi(\\bar{x})$ with\nrespect to $\\mathbb{P}$. This relation is a preorder. If, in addition,\n$\\mathbf{F}$ is at least as expressive as $\\mathbf{F}'$ then we say that\n$\\mathbf{F}$ and $\\mathbf{F}'$ are asymptotically equally expressive. Our third\ncontribution is to systematize the new results of this paper and several\nprevious results in order to get a preorder on a number of inference systems\nthat are of relevance in the context of machine learning and artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Vera Koponen"
                    },
                    {
                        "name": "Felix Weitkämper"
                    }
                ],
                "author_detail": {
                    "name": "Felix Weitkämper"
                },
                "author": "Felix Weitkämper",
                "arxiv_comment": "52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2204.09457v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2204.09457v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03C13, 68T27, 68Q87",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11043v1",
                "updated": "2024-08-20T17:49:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    49,
                    51,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:49:51Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    49,
                    51,
                    1,
                    233,
                    0
                ],
                "title": "Reconciling Methodological Paradigms: Employing Large Language Models as\n  Novice Qualitative Research Assistants in Talent Management Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconciling Methodological Paradigms: Employing Large Language Models as\n  Novice Qualitative Research Assistants in Talent Management Research"
                },
                "summary": "Qualitative data collection and analysis approaches, such as those employing\ninterviews and focus groups, provide rich insights into customer attitudes,\nsentiment, and behavior. However, manually analyzing qualitative data requires\nextensive time and effort to identify relevant topics and thematic insights.\nThis study proposes a novel approach to address this challenge by leveraging\nRetrieval Augmented Generation (RAG) based Large Language Models (LLMs) for\nanalyzing interview transcripts. The novelty of this work lies in strategizing\nthe research inquiry as one that is augmented by an LLM that serves as a novice\nresearch assistant. This research explores the mental model of LLMs to serve as\nnovice qualitative research assistants for researchers in the talent management\nspace. A RAG-based LLM approach is extended to enable topic modeling of\nsemi-structured interview data, showcasing the versatility of these models\nbeyond their traditional use in information retrieval and search. Our findings\ndemonstrate that the LLM-augmented RAG approach can successfully extract topics\nof interest, with significant coverage compared to manually generated topics\nfrom the same dataset. This establishes the viability of employing LLMs as\nnovice qualitative research assistants. Additionally, the study recommends that\nresearchers leveraging such models lean heavily on quality criteria used in\ntraditional qualitative research to ensure rigor and trustworthiness of their\napproach. Finally, the paper presents key recommendations for industry\npractitioners seeking to reconcile the use of LLMs with established qualitative\nresearch paradigms, providing a roadmap for the effective integration of these\npowerful, albeit novice, AI tools in the analysis of qualitative datasets\nwithin talent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative data collection and analysis approaches, such as those employing\ninterviews and focus groups, provide rich insights into customer attitudes,\nsentiment, and behavior. However, manually analyzing qualitative data requires\nextensive time and effort to identify relevant topics and thematic insights.\nThis study proposes a novel approach to address this challenge by leveraging\nRetrieval Augmented Generation (RAG) based Large Language Models (LLMs) for\nanalyzing interview transcripts. The novelty of this work lies in strategizing\nthe research inquiry as one that is augmented by an LLM that serves as a novice\nresearch assistant. This research explores the mental model of LLMs to serve as\nnovice qualitative research assistants for researchers in the talent management\nspace. A RAG-based LLM approach is extended to enable topic modeling of\nsemi-structured interview data, showcasing the versatility of these models\nbeyond their traditional use in information retrieval and search. Our findings\ndemonstrate that the LLM-augmented RAG approach can successfully extract topics\nof interest, with significant coverage compared to manually generated topics\nfrom the same dataset. This establishes the viability of employing LLMs as\nnovice qualitative research assistants. Additionally, the study recommends that\nresearchers leveraging such models lean heavily on quality criteria used in\ntraditional qualitative research to ensure rigor and trustworthiness of their\napproach. Finally, the paper presents key recommendations for industry\npractitioners seeking to reconcile the use of LLMs with established qualitative\nresearch paradigms, providing a roadmap for the effective integration of these\npowerful, albeit novice, AI tools in the analysis of qualitative datasets\nwithin talent"
                },
                "authors": [
                    {
                        "name": "Sreyoshi Bhaduri"
                    },
                    {
                        "name": "Satya Kapoor"
                    },
                    {
                        "name": "Alex Gil"
                    },
                    {
                        "name": "Anshul Mittal"
                    },
                    {
                        "name": "Rutu Mulkar"
                    }
                ],
                "author_detail": {
                    "name": "Rutu Mulkar"
                },
                "author": "Rutu Mulkar",
                "arxiv_comment": "Accepted to KDD '24 workshop on Talent Management and Computing (TMC\n  2024). 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10830v2",
                "updated": "2024-08-20T17:28:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    28,
                    14,
                    1,
                    233,
                    0
                ],
                "published": "2023-10-16T21:05:12Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    21,
                    5,
                    12,
                    0,
                    289,
                    0
                ],
                "title": "Fake News in Sheep's Clothing: Robust Fake News Detection Against\n  LLM-Empowered Style Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake News in Sheep's Clothing: Robust Fake News Detection Against\n  LLM-Empowered Style Attacks"
                },
                "summary": "It is commonly perceived that fake news and real news exhibit distinct\nwriting styles, such as the use of sensationalist versus objective language.\nHowever, we emphasize that style-related features can also be exploited for\nstyle-based attacks. Notably, the advent of powerful Large Language Models\n(LLMs) has empowered malicious actors to mimic the style of trustworthy news\nsources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals\nthat LLM-camouflaged fake news content significantly undermines the\neffectiveness of state-of-the-art text-based detectors (up to 38% decrease in\nF1 Score), implying a severe vulnerability to stylistic variations. To address\nthis, we introduce SheepDog, a style-robust fake news detector that prioritizes\ncontent over style in determining news veracity. SheepDog achieves this\nresilience through (1) LLM-empowered news reframings that inject style\ndiversity into the training process by customizing articles to match different\nstyles; (2) a style-agnostic training scheme that ensures consistent veracity\npredictions across style-diverse reframings; and (3) content-focused veracity\nattributions that distill content-centric guidelines from LLMs for debunking\nfake news, offering supplementary cues and potential intepretability that\nassist veracity prediction. Extensive experiments on three real-world\nbenchmarks demonstrate SheepDog's style robustness and adaptability to various\nbackbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is commonly perceived that fake news and real news exhibit distinct\nwriting styles, such as the use of sensationalist versus objective language.\nHowever, we emphasize that style-related features can also be exploited for\nstyle-based attacks. Notably, the advent of powerful Large Language Models\n(LLMs) has empowered malicious actors to mimic the style of trustworthy news\nsources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals\nthat LLM-camouflaged fake news content significantly undermines the\neffectiveness of state-of-the-art text-based detectors (up to 38% decrease in\nF1 Score), implying a severe vulnerability to stylistic variations. To address\nthis, we introduce SheepDog, a style-robust fake news detector that prioritizes\ncontent over style in determining news veracity. SheepDog achieves this\nresilience through (1) LLM-empowered news reframings that inject style\ndiversity into the training process by customizing articles to match different\nstyles; (2) a style-agnostic training scheme that ensures consistent veracity\npredictions across style-diverse reframings; and (3) content-focused veracity\nattributions that distill content-centric guidelines from LLMs for debunking\nfake news, offering supplementary cues and potential intepretability that\nassist veracity prediction. Extensive experiments on three real-world\nbenchmarks demonstrate SheepDog's style robustness and adaptability to various\nbackbones."
                },
                "authors": [
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3637528.3671977",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671977",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.10830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to KDD 2024 (Research Track)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11021v1",
                "updated": "2024-08-20T17:21:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    21,
                    10,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:21:10Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    21,
                    10,
                    1,
                    233,
                    0
                ],
                "title": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning"
                },
                "summary": "Due to emergent capabilities, large language models (LLMs) have been utilized\nas language-based agents to perform a variety of tasks and make decisions with\nan increasing degree of autonomy. These autonomous agents can understand\nhigh-level instructions, interact with their environments, and execute complex\ntasks using a selection of tools available to them. As the capabilities of the\nagents expand, ensuring their safety and trustworthiness becomes more\nimperative. In this study, we introduce the Athena framework which leverages\nthe concept of verbal contrastive learning where past safe and unsafe\ntrajectories are used as in-context (contrastive) examples to guide the agent\ntowards safety while fulfilling a given task. The framework also incorporates a\ncritiquing mechanism to guide the agent to prevent risky actions at every step.\nFurthermore, due to the lack of existing benchmarks on the safety reasoning\nability of LLM-based agents, we curate a set of 80 toolkits across 8 categories\nwith 180 scenarios to provide a safety evaluation benchmark. Our experimental\nevaluation, with both closed- and open-source LLMs, indicates verbal\ncontrastive learning and interaction-level critiquing improve the safety rate\nsignificantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to emergent capabilities, large language models (LLMs) have been utilized\nas language-based agents to perform a variety of tasks and make decisions with\nan increasing degree of autonomy. These autonomous agents can understand\nhigh-level instructions, interact with their environments, and execute complex\ntasks using a selection of tools available to them. As the capabilities of the\nagents expand, ensuring their safety and trustworthiness becomes more\nimperative. In this study, we introduce the Athena framework which leverages\nthe concept of verbal contrastive learning where past safe and unsafe\ntrajectories are used as in-context (contrastive) examples to guide the agent\ntowards safety while fulfilling a given task. The framework also incorporates a\ncritiquing mechanism to guide the agent to prevent risky actions at every step.\nFurthermore, due to the lack of existing benchmarks on the safety reasoning\nability of LLM-based agents, we curate a set of 80 toolkits across 8 categories\nwith 180 scenarios to provide a safety evaluation benchmark. Our experimental\nevaluation, with both closed- and open-source LLMs, indicates verbal\ncontrastive learning and interaction-level critiquing improve the safety rate\nsignificantly."
                },
                "authors": [
                    {
                        "name": "Tanmana Sadhu"
                    },
                    {
                        "name": "Ali Pesaranghader"
                    },
                    {
                        "name": "Yanan Chen"
                    },
                    {
                        "name": "Dong Hoon Yi"
                    }
                ],
                "author_detail": {
                    "name": "Dong Hoon Yi"
                },
                "author": "Dong Hoon Yi",
                "arxiv_comment": "9 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.00050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.00050v3",
                "updated": "2024-08-20T17:16:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    16,
                    20,
                    1,
                    233,
                    0
                ],
                "published": "2023-04-28T19:00:43Z",
                "published_parsed": [
                    2023,
                    4,
                    28,
                    19,
                    0,
                    43,
                    4,
                    118,
                    0
                ],
                "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for\n  Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning and Large Language Models: Opening a New Frontier for\n  Causality"
                },
                "summary": "The causal capabilities of large language models (LLMs) are a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nconduct a \"behavorial\" study of LLMs to benchmark their capability in\ngenerating causal arguments. Across a wide range of tasks, we find that LLMs\ncan generate text corresponding to correct causal arguments with high\nprobability, surpassing the best-performing existing methods. Algorithms based\non GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery\ntask (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain)\nand event causality (86% accuracy in determining necessary and sufficient\ncauses in vignettes). We perform robustness checks across tasks and show that\nthe capabilities cannot be explained by dataset memorization alone, especially\nsince LLMs generalize to novel datasets that were created after the training\ncutoff date.\n  That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds\nof errors that may be improved and what are the fundamental limits of LLM-based\nanswers. Overall, by operating on the text metadata, LLMs bring capabilities so\nfar understood to be restricted to humans, such as using collected knowledge to\ngenerate causal graphs or identifying background causal context from natural\nlanguage. As a result, LLMs may be used by human domain experts to save effort\nin setting up a causal analysis, one of the biggest impediments to the\nwidespread adoption of causal methods. Given that LLMs ignore the actual data,\nour results also point to a fruitful research direction of developing\nalgorithms that combine LLMs with existing causal techniques. Code and datasets\nare available at https://github.com/py-why/pywhy-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The causal capabilities of large language models (LLMs) are a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nconduct a \"behavorial\" study of LLMs to benchmark their capability in\ngenerating causal arguments. Across a wide range of tasks, we find that LLMs\ncan generate text corresponding to correct causal arguments with high\nprobability, surpassing the best-performing existing methods. Algorithms based\non GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery\ntask (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain)\nand event causality (86% accuracy in determining necessary and sufficient\ncauses in vignettes). We perform robustness checks across tasks and show that\nthe capabilities cannot be explained by dataset memorization alone, especially\nsince LLMs generalize to novel datasets that were created after the training\ncutoff date.\n  That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds\nof errors that may be improved and what are the fundamental limits of LLM-based\nanswers. Overall, by operating on the text metadata, LLMs bring capabilities so\nfar understood to be restricted to humans, such as using collected knowledge to\ngenerate causal graphs or identifying background causal context from natural\nlanguage. As a result, LLMs may be used by human domain experts to save effort\nin setting up a causal analysis, one of the biggest impediments to the\nwidespread adoption of causal methods. Given that LLMs ignore the actual data,\nour results also point to a fruitful research direction of developing\nalgorithms that combine LLMs with existing causal techniques. Code and datasets\nare available at https://github.com/py-why/pywhy-llm."
                },
                "authors": [
                    {
                        "name": "Emre Kıcıman"
                    },
                    {
                        "name": "Robert Ness"
                    },
                    {
                        "name": "Amit Sharma"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "arxiv_comment": "Added three novel datasets. To be published in TMLR. Authors listed\n  alphabetically",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.00050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.00050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03629v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03629v3",
                "updated": "2024-08-20T17:08:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    8,
                    53,
                    1,
                    233,
                    0
                ],
                "published": "2024-02-06T01:56:29Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    1,
                    56,
                    29,
                    1,
                    37,
                    0
                ],
                "title": "Disparate Impact on Group Accuracy of Linearization for Private\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disparate Impact on Group Accuracy of Linearization for Private\n  Inference"
                },
                "summary": "Ensuring privacy-preserving inference on cryptographically secure data is a\nwell-known computational challenge. To alleviate the bottleneck of costly\ncryptographic computations in non-linear activations, recent methods have\nsuggested linearizing a targeted portion of these activations in neural\nnetworks. This technique results in significantly reduced runtimes with often\nnegligible impacts on accuracy. In this paper, we demonstrate that such\ncomputational benefits may lead to increased fairness costs. Specifically, we\nfind that reducing the number of ReLU activations disproportionately decreases\nthe accuracy for minority groups compared to majority groups. To explain these\nobservations, we provide a mathematical interpretation under restricted\nassumptions about the nature of the decision boundary, while also showing the\nprevalence of this problem across widely used datasets and architectures.\nFinally, we show how a simple procedure altering the fine-tuning step for\nlinearized models can serve as an effective mitigation strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring privacy-preserving inference on cryptographically secure data is a\nwell-known computational challenge. To alleviate the bottleneck of costly\ncryptographic computations in non-linear activations, recent methods have\nsuggested linearizing a targeted portion of these activations in neural\nnetworks. This technique results in significantly reduced runtimes with often\nnegligible impacts on accuracy. In this paper, we demonstrate that such\ncomputational benefits may lead to increased fairness costs. Specifically, we\nfind that reducing the number of ReLU activations disproportionately decreases\nthe accuracy for minority groups compared to majority groups. To explain these\nobservations, we provide a mathematical interpretation under restricted\nassumptions about the nature of the decision boundary, while also showing the\nprevalence of this problem across widely used datasets and architectures.\nFinally, we show how a simple procedure altering the fine-tuning step for\nlinearized models can serve as an effective mitigation strategy."
                },
                "authors": [
                    {
                        "name": "Saswat Das"
                    },
                    {
                        "name": "Marco Romanelli"
                    },
                    {
                        "name": "Ferdinando Fioretto"
                    }
                ],
                "author_detail": {
                    "name": "Ferdinando Fioretto"
                },
                "author": "Ferdinando Fioretto",
                "arxiv_comment": "Extended version of the paper accepted to appear at the Forty-first\n  International Conference on Machine Learning (ICML) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03629v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03629v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14722v2",
                "updated": "2024-08-20T17:08:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    8,
                    13,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-20T20:37:55Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    20,
                    37,
                    55,
                    3,
                    172,
                    0
                ],
                "title": "Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's\n  Understanding of Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's\n  Understanding of Algorithms"
                },
                "summary": "As Large Language Models (LLMs) perform (and sometimes excel at) more and\nmore complex cognitive tasks, a natural question is whether AI really\nunderstands. The study of understanding in LLMs is in its infancy, and the\ncommunity has yet to incorporate well-trodden research in philosophy,\npsychology, and education. We initiate this, specifically focusing on\nunderstanding algorithms, and propose a hierarchy of levels of understanding.\nWe use the hierarchy to design and conduct a study with human subjects\n(undergraduate and graduate students) as well as large language models\n(generations of GPT), revealing interesting similarities and differences. We\nexpect that our rigorous criteria will be useful to keep track of AI's progress\nin such cognitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) perform (and sometimes excel at) more and\nmore complex cognitive tasks, a natural question is whether AI really\nunderstands. The study of understanding in LLMs is in its infancy, and the\ncommunity has yet to incorporate well-trodden research in philosophy,\npsychology, and education. We initiate this, specifically focusing on\nunderstanding algorithms, and propose a hierarchy of levels of understanding.\nWe use the hierarchy to design and conduct a study with human subjects\n(undergraduate and graduate students) as well as large language models\n(generations of GPT), revealing interesting similarities and differences. We\nexpect that our rigorous criteria will be useful to keep track of AI's progress\nin such cognitive domains."
                },
                "authors": [
                    {
                        "name": "Mirabel Reid"
                    },
                    {
                        "name": "Santosh S. Vempala"
                    }
                ],
                "author_detail": {
                    "name": "Santosh S. Vempala"
                },
                "author": "Santosh S. Vempala",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m; F.1.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v1",
                "updated": "2024-08-20T17:00:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "While GitHub Copilot Excels at Coding, Does It Ensure Responsible\n  Output?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While GitHub Copilot Excels at Coding, Does It Ensure Responsible\n  Output?"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11000v1",
                "updated": "2024-08-20T16:53:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    53,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:53:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    53,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite\n  Self-Supervised Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite\n  Self-Supervised Pretraining"
                },
                "summary": "This paper introduces SenPa-MAE, a transformer architecture that encodes the\nsensor parameters of an observed multispectral signal into the image\nembeddings. SenPa-MAE can be pre-trained on imagery of different satellites\nwith non-matching spectral or geometrical sensor characteristics. To\nincorporate sensor parameters, we propose a versatile sensor parameter encoding\nmodule as well as a data augmentation strategy for the diversification of the\npre-training dataset. This enables the model to effectively differentiate\nbetween various sensors and gain an understanding of sensor parameters and the\ncorrelation to the observed signal. Given the rising number of Earth\nobservation satellite missions and the diversity in their sensor\nspecifications, our approach paves the way towards a sensor-independent Earth\nobservation foundation model. This opens up possibilities such as cross-sensor\ntraining and sensor-independent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SenPa-MAE, a transformer architecture that encodes the\nsensor parameters of an observed multispectral signal into the image\nembeddings. SenPa-MAE can be pre-trained on imagery of different satellites\nwith non-matching spectral or geometrical sensor characteristics. To\nincorporate sensor parameters, we propose a versatile sensor parameter encoding\nmodule as well as a data augmentation strategy for the diversification of the\npre-training dataset. This enables the model to effectively differentiate\nbetween various sensors and gain an understanding of sensor parameters and the\ncorrelation to the observed signal. Given the rising number of Earth\nobservation satellite missions and the diversity in their sensor\nspecifications, our approach paves the way towards a sensor-independent Earth\nobservation foundation model. This opens up possibilities such as cross-sensor\ntraining and sensor-independent inference."
                },
                "authors": [
                    {
                        "name": "Jonathan Prexl"
                    },
                    {
                        "name": "Michael Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Michael Schmitt"
                },
                "author": "Michael Schmitt",
                "arxiv_comment": "GCPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10995v1",
                "updated": "2024-08-20T16:43:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    43,
                    5,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:43:05Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    43,
                    5,
                    1,
                    233,
                    0
                ],
                "title": "CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language\n  Models"
                },
                "summary": "New medical treatment development requires multiple phases of clinical\ntrials. Despite the significant human and financial costs of bringing a drug to\nmarket, less than 20% of drugs in testing will make it from the first phase to\nfinal approval. Recent literature indicates that the design of the trial\nprotocols significantly contributes to trial performance. We investigated\nClinical Trial Outcome Prediction (CTOP) using trial design documents to\npredict phase transitions automatically. We propose CTP-LLM, the first Large\nLanguage Model (LLM) based model for CTOP. We also introduce the\nPhaseTransition (PT) Dataset; which labels trials based on their progression\nthrough the regulatory process and serves as a benchmark for CTOP evaluation.\nOur fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase\ntransition by analyzing the trial's original protocol texts without requiring\nhuman-selected features. CTP-LLM achieves a 67% accuracy rate in predicting\ntrial phase transitions across all phases and a 75% accuracy rate specifically\nin predicting the transition from Phase~III to final approval. Our experimental\nperformance highlights the potential of LLM-powered applications in forecasting\nclinical trial outcomes and assessing trial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New medical treatment development requires multiple phases of clinical\ntrials. Despite the significant human and financial costs of bringing a drug to\nmarket, less than 20% of drugs in testing will make it from the first phase to\nfinal approval. Recent literature indicates that the design of the trial\nprotocols significantly contributes to trial performance. We investigated\nClinical Trial Outcome Prediction (CTOP) using trial design documents to\npredict phase transitions automatically. We propose CTP-LLM, the first Large\nLanguage Model (LLM) based model for CTOP. We also introduce the\nPhaseTransition (PT) Dataset; which labels trials based on their progression\nthrough the regulatory process and serves as a benchmark for CTOP evaluation.\nOur fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase\ntransition by analyzing the trial's original protocol texts without requiring\nhuman-selected features. CTP-LLM achieves a 67% accuracy rate in predicting\ntrial phase transitions across all phases and a 75% accuracy rate specifically\nin predicting the transition from Phase~III to final approval. Our experimental\nperformance highlights the potential of LLM-powered applications in forecasting\nclinical trial outcomes and assessing trial design."
                },
                "authors": [
                    {
                        "name": "Michael Reinisch"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Chenxi Liao"
                    },
                    {
                        "name": "Sauleh Ahmad Siddiqui"
                    },
                    {
                        "name": "Bei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bei Xiao"
                },
                "author": "Bei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07248v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07248v4",
                "updated": "2024-08-20T16:31:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    31,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2023-08-14T16:32:38Z",
                "published_parsed": [
                    2023,
                    8,
                    14,
                    16,
                    32,
                    38,
                    0,
                    226,
                    0
                ],
                "title": "Maintaining the validity of inference from linear mixed models in\n  stepped-wedge cluster randomized trials under misspecified random-effects\n  structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining the validity of inference from linear mixed models in\n  stepped-wedge cluster randomized trials under misspecified random-effects\n  structures"
                },
                "summary": "Linear mixed models are commonly used in analyzing stepped-wedge cluster\nrandomized trials (SW-CRTs). A key consideration for analyzing a SW-CRT is\naccounting for the potentially complex correlation structure, which can be\nachieved by specifying a random effects structure. Common random effects\nstructures for a SW-CRT include random intercept, random cluster-by-period, and\ndiscrete-time decay. Recently, more complex structures, such as the random\nintervention structure, have been proposed. In practice, specifying appropriate\nrandom effects can be challenging. Robust variance estimators (RVE) may be\napplied to linear mixed models to provide consistent estimators of standard\nerrors of fixed effect parameters in the presence of random-effects\nmisspecification. However, there has been no empirical investigation of RVE for\nSW-CRT. In this paper, we first review five RVEs (both standard and\nsmall-sample bias-corrected RVEs) that are available for linear mixed models.\nWe then describe a comprehensive simulation study to examine the performance of\nthese RVEs for SW-CRTs with a continuous outcome under different data\ngenerators. For each data generator, we investigate whether the use of a RVE\nwith either the random intercept model or the random cluster-by-period model is\nsufficient to provide valid statistical inference for fixed effect parameters,\nwhen these working models are subject to misspecification. Our results indicate\nthat the random intercept and random cluster-by-period models with RVEs\nperformed similarly. The CR3 RVE estimator, coupled with the number of clusters\nminus two degrees of freedom correction, consistently gave the best coverage\nresults, but could be slightly conservative when the number of clusters was\nbelow 16. We summarize the implications of our results for linear mixed model\nanalysis of SW-CRTs in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear mixed models are commonly used in analyzing stepped-wedge cluster\nrandomized trials (SW-CRTs). A key consideration for analyzing a SW-CRT is\naccounting for the potentially complex correlation structure, which can be\nachieved by specifying a random effects structure. Common random effects\nstructures for a SW-CRT include random intercept, random cluster-by-period, and\ndiscrete-time decay. Recently, more complex structures, such as the random\nintervention structure, have been proposed. In practice, specifying appropriate\nrandom effects can be challenging. Robust variance estimators (RVE) may be\napplied to linear mixed models to provide consistent estimators of standard\nerrors of fixed effect parameters in the presence of random-effects\nmisspecification. However, there has been no empirical investigation of RVE for\nSW-CRT. In this paper, we first review five RVEs (both standard and\nsmall-sample bias-corrected RVEs) that are available for linear mixed models.\nWe then describe a comprehensive simulation study to examine the performance of\nthese RVEs for SW-CRTs with a continuous outcome under different data\ngenerators. For each data generator, we investigate whether the use of a RVE\nwith either the random intercept model or the random cluster-by-period model is\nsufficient to provide valid statistical inference for fixed effect parameters,\nwhen these working models are subject to misspecification. Our results indicate\nthat the random intercept and random cluster-by-period models with RVEs\nperformed similarly. The CR3 RVE estimator, coupled with the number of clusters\nminus two degrees of freedom correction, consistently gave the best coverage\nresults, but could be slightly conservative when the number of clusters was\nbelow 16. We summarize the implications of our results for linear mixed model\nanalysis of SW-CRTs in practice."
                },
                "authors": [
                    {
                        "name": "Yongdong Ouyang"
                    },
                    {
                        "name": "Monica Taljaard"
                    },
                    {
                        "name": "Andrew B Forbes"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "arxiv_comment": "update a typo in abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07248v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07248v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v2",
                "updated": "2024-08-20T16:09:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    9,
                    33,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09172v2",
                "updated": "2024-08-20T15:51:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    51,
                    59,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-17T11:33:23Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    11,
                    33,
                    23,
                    5,
                    230,
                    0
                ],
                "title": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context\n  Example Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context\n  Example Selection"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have demonstrated exceptional\nperformance across various downstream tasks. However, it is challenging for\nusers to discern whether the responses are generated with certainty or are\nfabricated to meet user expectations. Estimating the uncertainty of LLMs is\nparticularly challenging due to their vast scale and the lack of white-box\naccess. In this work, we propose a novel Uncertainty Tripartite Testing\nParadigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency\nof LLM outputs when incorporating label interference into the sampling-based\napproach. Based on Unc-TTP outputs, we aggregate instances into certain and\nuncertain categories. Further, we conduct a detailed analysis of the\nuncertainty properties of LLMs and show Unc-TTP's superiority over the existing\nsampling-based methods. In addition, we leverage the obtained uncertainty\ninformation to guide in-context example selection, demonstrating that Unc-TTP\nobviously outperforms retrieval-based and sampling-based approaches in\nselecting more informative examples. Our work paves a new way to classify the\nuncertainty of both open- and closed-source LLMs, and introduces a practical\napproach to exploit this uncertainty to improve LLMs performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have demonstrated exceptional\nperformance across various downstream tasks. However, it is challenging for\nusers to discern whether the responses are generated with certainty or are\nfabricated to meet user expectations. Estimating the uncertainty of LLMs is\nparticularly challenging due to their vast scale and the lack of white-box\naccess. In this work, we propose a novel Uncertainty Tripartite Testing\nParadigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency\nof LLM outputs when incorporating label interference into the sampling-based\napproach. Based on Unc-TTP outputs, we aggregate instances into certain and\nuncertain categories. Further, we conduct a detailed analysis of the\nuncertainty properties of LLMs and show Unc-TTP's superiority over the existing\nsampling-based methods. In addition, we leverage the obtained uncertainty\ninformation to guide in-context example selection, demonstrating that Unc-TTP\nobviously outperforms retrieval-based and sampling-based approaches in\nselecting more informative examples. Our work paves a new way to classify the\nuncertainty of both open- and closed-source LLMs, and introduces a practical\napproach to exploit this uncertainty to improve LLMs performance."
                },
                "authors": [
                    {
                        "name": "Hsiu-Yuan Huang"
                    },
                    {
                        "name": "Zichen Wu"
                    },
                    {
                        "name": "Yutong Yang"
                    },
                    {
                        "name": "Junzhao Zhang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "9 pages, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03151v3",
                "updated": "2024-08-20T15:41:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    41,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-05T11:15:45Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    11,
                    15,
                    45,
                    2,
                    157,
                    0
                ],
                "title": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument\n  Summarisation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument\n  Summarisation and Evaluation"
                },
                "summary": "With the recent advances of large language models (LLMs), it is no longer\ninfeasible to build an automated debate system that helps people to synthesise\npersuasive arguments. Previous work attempted this task by integrating multiple\ncomponents. In our work, we introduce an argument mining dataset that captures\nthe end-to-end process of preparing an argumentative essay for a debate, which\ncovers the tasks of claim and evidence identification (Task 1 ED), evidence\nconvincingness ranking (Task 2 ECR), argumentative essay summarisation and\nhuman preference ranking (Task 3 ASR) and metric learning for automated\nevaluation of resulting essays, based on human feedback along argument quality\ndimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are\nfully annotated with the various properties supporting the aforementioned\ntasks. We evaluate multiple generative baselines for each of these tasks,\nincluding representative LLMs. We find, that while they show promising results\non individual tasks in our benchmark, their end-to-end performance on all four\ntasks in succession deteriorates significantly, both in automated measures as\nwell as in human-centred evaluation. This challenge presented by our proposed\ndataset motivates future research on end-to-end argument mining and\nsummarisation. The repository of this project is available at\nhttps://github.com/HaoBytes/ArgSum-Datatset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent advances of large language models (LLMs), it is no longer\ninfeasible to build an automated debate system that helps people to synthesise\npersuasive arguments. Previous work attempted this task by integrating multiple\ncomponents. In our work, we introduce an argument mining dataset that captures\nthe end-to-end process of preparing an argumentative essay for a debate, which\ncovers the tasks of claim and evidence identification (Task 1 ED), evidence\nconvincingness ranking (Task 2 ECR), argumentative essay summarisation and\nhuman preference ranking (Task 3 ASR) and metric learning for automated\nevaluation of resulting essays, based on human feedback along argument quality\ndimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are\nfully annotated with the various properties supporting the aforementioned\ntasks. We evaluate multiple generative baselines for each of these tasks,\nincluding representative LLMs. We find, that while they show promising results\non individual tasks in our benchmark, their end-to-end performance on all four\ntasks in succession deteriorates significantly, both in automated measures as\nwell as in human-centred evaluation. This challenge presented by our proposed\ndataset motivates future research on end-to-end argument mining and\nsummarisation. The repository of this project is available at\nhttps://github.com/HaoBytes/ArgSum-Datatset"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yuping Wu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Tharindu Madusanka"
                    },
                    {
                        "name": "Iqra Zahid"
                    },
                    {
                        "name": "Jiayan Zeng"
                    },
                    {
                        "name": "Xiaochi Wang"
                    },
                    {
                        "name": "Xinran He"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "Published on ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10947v1",
                "updated": "2024-08-20T15:36:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:36:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in\n  Education for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in\n  Education for Large Language Models"
                },
                "summary": "Teachers are important to imparting knowledge and guiding learners, and the\nrole of large language models (LLMs) as potential educators is emerging as an\nimportant area of study. Recognizing LLMs' capability to generate educational\ncontent can lead to advances in automated and personalized learning. While LLMs\nhave been tested for their comprehension and problem-solving skills, their\ncapability in teaching remains largely unexplored. In teaching, questioning is\na key skill that guides students to analyze, evaluate, and synthesize core\nconcepts and principles. Therefore, our research introduces a benchmark to\nevaluate the questioning capability in education as a teacher of LLMs through\nevaluating their generated educational questions, utilizing Anderson and\nKrathwohl's taxonomy across general, monodisciplinary, and interdisciplinary\ndomains. We shift the focus from LLMs as learners to LLMs as educators,\nassessing their teaching capability through guiding them to generate questions.\nWe apply four metrics, including relevance, coverage, representativeness, and\nconsistency, to evaluate the educational quality of LLMs' outputs. Our results\nindicate that GPT-4 demonstrates significant potential in teaching general,\nhumanities, and science courses; Claude2 appears more apt as an\ninterdisciplinary teacher. Furthermore, the automatic scores align with human\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teachers are important to imparting knowledge and guiding learners, and the\nrole of large language models (LLMs) as potential educators is emerging as an\nimportant area of study. Recognizing LLMs' capability to generate educational\ncontent can lead to advances in automated and personalized learning. While LLMs\nhave been tested for their comprehension and problem-solving skills, their\ncapability in teaching remains largely unexplored. In teaching, questioning is\na key skill that guides students to analyze, evaluate, and synthesize core\nconcepts and principles. Therefore, our research introduces a benchmark to\nevaluate the questioning capability in education as a teacher of LLMs through\nevaluating their generated educational questions, utilizing Anderson and\nKrathwohl's taxonomy across general, monodisciplinary, and interdisciplinary\ndomains. We shift the focus from LLMs as learners to LLMs as educators,\nassessing their teaching capability through guiding them to generate questions.\nWe apply four metrics, including relevance, coverage, representativeness, and\nconsistency, to evaluate the educational quality of LLMs' outputs. Our results\nindicate that GPT-4 demonstrates significant potential in teaching general,\nhumanities, and science courses; Claude2 appears more apt as an\ninterdisciplinary teacher. Furthermore, the automatic scores align with human\nperspectives."
                },
                "authors": [
                    {
                        "name": "Yuyan Chen"
                    },
                    {
                        "name": "Chenwei Wu"
                    },
                    {
                        "name": "Songzhou Yan"
                    },
                    {
                        "name": "Panjun Liu"
                    },
                    {
                        "name": "Haoyu Zhou"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Accepted to ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10946v1",
                "updated": "2024-08-20T15:36:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    24,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:36:24Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    24,
                    1,
                    233,
                    0
                ],
                "title": "Large Language Model Driven Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Driven Recommendation"
                },
                "summary": "While previous chapters focused on recommendation systems (RSs) based on\nstandardized, non-verbal user feedback such as purchases, views, and clicks --\nthe advent of LLMs has unlocked the use of natural language (NL) interactions\nfor recommendation. This chapter discusses how LLMs' abilities for general NL\nreasoning present novel opportunities to build highly personalized RSs -- which\ncan effectively connect nuanced and diverse user preferences to items,\npotentially via interactive dialogues. To begin this discussion, we first\npresent a taxonomy of the key data sources for language-driven recommendation,\ncovering item descriptions, user-system interactions, and user profiles. We\nthen proceed to fundamental techniques for LLM recommendation, reviewing the\nuse of encoder-only and autoregressive LLM recommendation in both tuned and\nuntuned settings. Afterwards, we move to multi-module recommendation\narchitectures in which LLMs interact with components such as retrievers and RSs\nin multi-stage pipelines. This brings us to architectures for conversational\nrecommender systems (CRSs), in which LLMs facilitate multi-turn dialogues where\neach turn presents an opportunity not only to make recommendations, but also to\nengage with the user in interactive preference elicitation, critiquing, and\nquestion-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While previous chapters focused on recommendation systems (RSs) based on\nstandardized, non-verbal user feedback such as purchases, views, and clicks --\nthe advent of LLMs has unlocked the use of natural language (NL) interactions\nfor recommendation. This chapter discusses how LLMs' abilities for general NL\nreasoning present novel opportunities to build highly personalized RSs -- which\ncan effectively connect nuanced and diverse user preferences to items,\npotentially via interactive dialogues. To begin this discussion, we first\npresent a taxonomy of the key data sources for language-driven recommendation,\ncovering item descriptions, user-system interactions, and user profiles. We\nthen proceed to fundamental techniques for LLM recommendation, reviewing the\nuse of encoder-only and autoregressive LLM recommendation in both tuned and\nuntuned settings. Afterwards, we move to multi-module recommendation\narchitectures in which LLMs interact with components such as retrievers and RSs\nin multi-stage pipelines. This brings us to architectures for conversational\nrecommender systems (CRSs), in which LLMs facilitate multi-turn dialogues where\neach turn presents an opportunity not only to make recommendations, but also to\nengage with the user in interactive preference elicitation, critiquing, and\nquestion-answering."
                },
                "authors": [
                    {
                        "name": "Anton Korikov"
                    },
                    {
                        "name": "Scott Sanner"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Zhankui He"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Arnau Ramisa"
                    },
                    {
                        "name": "Rene Vidal"
                    },
                    {
                        "name": "Mahesh Sathiamoorthy"
                    },
                    {
                        "name": "Atoosa Kasrizadeh"
                    },
                    {
                        "name": "Silvia Milano"
                    },
                    {
                        "name": "Francesco Ricci"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Ricci"
                },
                "author": "Francesco Ricci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10945v1",
                "updated": "2024-08-20T15:34:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    34,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:34:27Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    34,
                    27,
                    1,
                    233,
                    0
                ],
                "title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments"
                },
                "summary": "High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference."
                },
                "authors": [
                    {
                        "name": "Kazi Hasan Ibn Arif"
                    },
                    {
                        "name": "JinYi Yoon"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    },
                    {
                        "name": "Deepu John"
                    },
                    {
                        "name": "Bo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ji"
                },
                "author": "Bo Ji",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10943v1",
                "updated": "2024-08-20T15:33:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    33,
                    16,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:33:16Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    33,
                    16,
                    1,
                    233,
                    0
                ],
                "title": "SysBench: Can Large Language Models Follow System Messages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SysBench: Can Large Language Models Follow System Messages?"
                },
                "summary": "Large Language Models (LLMs) have become instrumental across various\napplications, with the customization of these models to specific scenarios\nbecoming increasingly critical. System message, a fundamental component of\nLLMs, is consist of carefully crafted instructions that guide the behavior of\nmodel to meet intended goals. Despite the recognized potential of system\nmessages to optimize AI-driven solutions, there is a notable absence of a\ncomprehensive benchmark for evaluating how well different LLMs follow these\nsystem messages. To fill this gap, we introduce SysBench, a benchmark that\nsystematically analyzes system message following ability in terms of three\nchallenging aspects: constraint complexity, instruction misalignment and\nmulti-turn stability. In order to enable effective evaluation, SysBench\nconstructs multi-turn user conversations covering various interaction\nrelationships, based on six common types of constraints from system messages in\nreal-world scenarios. Our dataset contains 500 system messages from various\ndomains, each paired with 5 turns of user conversations, which have been\nmanually formulated and checked to guarantee high quality. SysBench provides\nextensive evaluation across various LLMs, measuring their ability to follow\nspecified constraints given in system messages. The results highlight both the\nstrengths and weaknesses of existing models, offering key insights and\ndirections for future research. The open source library SysBench is available\nat https://github.com/PKU-Baichuan-MLSystemLab/SysBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become instrumental across various\napplications, with the customization of these models to specific scenarios\nbecoming increasingly critical. System message, a fundamental component of\nLLMs, is consist of carefully crafted instructions that guide the behavior of\nmodel to meet intended goals. Despite the recognized potential of system\nmessages to optimize AI-driven solutions, there is a notable absence of a\ncomprehensive benchmark for evaluating how well different LLMs follow these\nsystem messages. To fill this gap, we introduce SysBench, a benchmark that\nsystematically analyzes system message following ability in terms of three\nchallenging aspects: constraint complexity, instruction misalignment and\nmulti-turn stability. In order to enable effective evaluation, SysBench\nconstructs multi-turn user conversations covering various interaction\nrelationships, based on six common types of constraints from system messages in\nreal-world scenarios. Our dataset contains 500 system messages from various\ndomains, each paired with 5 turns of user conversations, which have been\nmanually formulated and checked to guarantee high quality. SysBench provides\nextensive evaluation across various LLMs, measuring their ability to follow\nspecified constraints given in system messages. The results highlight both the\nstrengths and weaknesses of existing models, offering key insights and\ndirections for future research. The open source library SysBench is available\nat https://github.com/PKU-Baichuan-MLSystemLab/SysBench."
                },
                "authors": [
                    {
                        "name": "Yanzhao Qin"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yanjun Shen"
                    },
                    {
                        "name": "Wenjing Luo"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yujing Qiao"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10942v1",
                "updated": "2024-08-20T15:32:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    32,
                    47,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:32:47Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    32,
                    47,
                    1,
                    233,
                    0
                ],
                "title": "Robust Regression with Ensembles Communicating over Noisy Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Regression with Ensembles Communicating over Noisy Channels"
                },
                "summary": "As machine-learning models grow in size, their implementation requirements\ncannot be met by a single computer system. This observation motivates\ndistributed settings, in which intermediate computations are performed across a\nnetwork of processing units, while the central node only aggregates their\noutputs. However, distributing inference tasks across low-precision or faulty\nedge devices, operating over a network of noisy communication channels, gives\nrise to serious reliability challenges. We study the problem of an ensemble of\ndevices, implementing regression algorithms, that communicate through additive\nnoisy channels in order to collaboratively perform a joint regression task. We\ndefine the problem formally, and develop methods for optimizing the aggregation\ncoefficients for the parameters of the noise in the channels, which can\npotentially be correlated. Our results apply to the leading state-of-the-art\nensemble regression methods: bagging and gradient boosting. We demonstrate the\neffectiveness of our algorithms on both synthetic and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine-learning models grow in size, their implementation requirements\ncannot be met by a single computer system. This observation motivates\ndistributed settings, in which intermediate computations are performed across a\nnetwork of processing units, while the central node only aggregates their\noutputs. However, distributing inference tasks across low-precision or faulty\nedge devices, operating over a network of noisy communication channels, gives\nrise to serious reliability challenges. We study the problem of an ensemble of\ndevices, implementing regression algorithms, that communicate through additive\nnoisy channels in order to collaboratively perform a joint regression task. We\ndefine the problem formally, and develop methods for optimizing the aggregation\ncoefficients for the parameters of the noise in the channels, which can\npotentially be correlated. Our results apply to the leading state-of-the-art\nensemble regression methods: bagging and gradient boosting. We demonstrate the\neffectiveness of our algorithms on both synthetic and real-world datasets."
                },
                "authors": [
                    {
                        "name": "Yuval Ben-Hur"
                    },
                    {
                        "name": "Yuval Cassuto"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Cassuto"
                },
                "author": "Yuval Cassuto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10937v1",
                "updated": "2024-08-20T15:20:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    20,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:20:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    20,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Proxona: Leveraging LLM-Driven Personas to Enhance Creators'\n  Understanding of Their Audience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proxona: Leveraging LLM-Driven Personas to Enhance Creators'\n  Understanding of Their Audience"
                },
                "summary": "Creators are nothing without their audience, and thereby understanding their\naudience is the cornerstone of their professional achievement. Yet many\ncreators feel lost while comprehending audiences with existing tools, which\noffer insufficient insights for tailoring content to audience needs. To address\nthe challenges creators face in understanding their audience, we present\nProxona, a system for defining and extracting representative audience personas\nfrom the comments. Creators converse with personas to gain insights into their\npreferences and engagement, solicit feedback, and implement evidence-based\nimprovements to their content. Powered by large language models, Proxona\nanalyzes audience comments, distilling the latent characteristics of audiences\ninto tangible dimensions (classification categories) and values (category\nattributes). Proxona then clusters these into synthetic personas. Our technical\nevaluations demonstrated that our pipelines effectively generated relevant and\ndistinct dimensions and values, enabling the deduction of audience-reflecting\npersonas, while minimizing the likelihood of hallucinations in persona\nresponses. Our user evaluation with 11 creators showed that Proxona supported\ncreators to gain new insights about their audience, make informed decisions,\nand successfully complete content creation with high confidence. Proxona's\ndata-driven audience personas empower creators to seamlessly integrate audience\nperspectives into their creative processes, fostering a collaborative approach\nto content creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creators are nothing without their audience, and thereby understanding their\naudience is the cornerstone of their professional achievement. Yet many\ncreators feel lost while comprehending audiences with existing tools, which\noffer insufficient insights for tailoring content to audience needs. To address\nthe challenges creators face in understanding their audience, we present\nProxona, a system for defining and extracting representative audience personas\nfrom the comments. Creators converse with personas to gain insights into their\npreferences and engagement, solicit feedback, and implement evidence-based\nimprovements to their content. Powered by large language models, Proxona\nanalyzes audience comments, distilling the latent characteristics of audiences\ninto tangible dimensions (classification categories) and values (category\nattributes). Proxona then clusters these into synthetic personas. Our technical\nevaluations demonstrated that our pipelines effectively generated relevant and\ndistinct dimensions and values, enabling the deduction of audience-reflecting\npersonas, while minimizing the likelihood of hallucinations in persona\nresponses. Our user evaluation with 11 creators showed that Proxona supported\ncreators to gain new insights about their audience, make informed decisions,\nand successfully complete content creation with high confidence. Proxona's\ndata-driven audience personas empower creators to seamlessly integrate audience\nperspectives into their creative processes, fostering a collaborative approach\nto content creation."
                },
                "authors": [
                    {
                        "name": "Yoonseo Choi"
                    },
                    {
                        "name": "Eun Jeong Kang"
                    },
                    {
                        "name": "Seulgi Choi"
                    },
                    {
                        "name": "Min Kyung Lee"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "arxiv_comment": "32 pages (including 14 pages of Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09235v2",
                "updated": "2024-08-20T15:12:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    12,
                    8,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-17T16:01:45Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    16,
                    1,
                    45,
                    5,
                    230,
                    0
                ],
                "title": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of\n  Free-Form Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of\n  Free-Form Text"
                },
                "summary": "The emergence of Large Language Models (LLMs) as chat assistants capable of\ngenerating human-like conversations has amplified the need for robust\nevaluation methods, particularly for open-ended tasks. Conventional metrics\nlike BLEU and ROUGE, while useful, are increasingly inadequate for capturing\nthe subtle semantics and contextual richness of such generative outputs. We\npropose a reference-guided verdict method that automates the evaluation process\nby leveraging multiple LLMs-as-judges. Through experiments on three open-ended\nquestion-answering tasks, we demonstrate that combining multiple LLMs-as-judges\nsignificantly improves the reliability and accuracy of evaluations,\nparticularly in complex tasks where a single model might struggle. Our findings\nreveal a strong correlation with human evaluations, establishing our method as\na viable and effective alternative to traditional metrics and human judgments,\nparticularly in the context of LLM-based chat assistants where the complexity\nand diversity of responses challenge existing benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) as chat assistants capable of\ngenerating human-like conversations has amplified the need for robust\nevaluation methods, particularly for open-ended tasks. Conventional metrics\nlike BLEU and ROUGE, while useful, are increasingly inadequate for capturing\nthe subtle semantics and contextual richness of such generative outputs. We\npropose a reference-guided verdict method that automates the evaluation process\nby leveraging multiple LLMs-as-judges. Through experiments on three open-ended\nquestion-answering tasks, we demonstrate that combining multiple LLMs-as-judges\nsignificantly improves the reliability and accuracy of evaluations,\nparticularly in complex tasks where a single model might struggle. Our findings\nreveal a strong correlation with human evaluations, establishing our method as\na viable and effective alternative to traditional metrics and human judgments,\nparticularly in the context of LLM-based chat assistants where the complexity\nand diversity of responses challenge existing benchmarks."
                },
                "authors": [
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Hassan Sajjad"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sajjad"
                },
                "author": "Hassan Sajjad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10923v2",
                "updated": "2024-08-21T15:51:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    51,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T15:05:02Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    5,
                    2,
                    1,
                    233,
                    0
                ],
                "title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization"
                },
                "summary": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at\nhttps://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at\nhttps://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks."
                },
                "authors": [
                    {
                        "name": "Kangjun Noh"
                    },
                    {
                        "name": "Baekryun Seong"
                    },
                    {
                        "name": "Hoyoon Byun"
                    },
                    {
                        "name": "Youngjun Choi"
                    },
                    {
                        "name": "Sungjin Song"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "16 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10921v1",
                "updated": "2024-08-20T15:04:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    4,
                    38,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:04:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    4,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "MTFinEval:A Multi-domain Chinese Financial Benchmark with Eurypalynous\n  questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTFinEval:A Multi-domain Chinese Financial Benchmark with Eurypalynous\n  questions"
                },
                "summary": "With the emergence of more and more economy-specific LLMS, how to measure\nwhether they can be safely invested in production becomes a problem. Previous\nresearch has primarily focused on evaluating the performance of LLMs within\nspecific application scenarios. However, these benchmarks cannot reflect the\ntheoretical level and generalization ability, and the backward datasets are\nincreasingly unsuitable for problems in real scenarios. In this paper, we have\ncompiled a new benchmark, MTFinEval, focusing on the LLMs' basic knowledge of\neconomics, which can always be used as a basis for judgment. To examine only\ntheoretical knowledge as much as possible, MTFinEval is build with foundational\nquestions from university textbooks,and exam papers in economics and management\nmajor. Aware of the overall performance of LLMs do not depend solely on one\nsubdiscipline of economics, MTFinEval comprise 360 questions refined from six\nmajor disciplines of economics, and reflect capabilities more comprehensively.\nExperiment result shows all LLMs perform poorly on MTFinEval, which proves that\nour benchmark built on basic knowledge is very successful. Our research not\nonly offers guidance for selecting the appropriate LLM for specific use cases,\nbut also put forward increase the rigor reliability of LLMs from the basics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of more and more economy-specific LLMS, how to measure\nwhether they can be safely invested in production becomes a problem. Previous\nresearch has primarily focused on evaluating the performance of LLMs within\nspecific application scenarios. However, these benchmarks cannot reflect the\ntheoretical level and generalization ability, and the backward datasets are\nincreasingly unsuitable for problems in real scenarios. In this paper, we have\ncompiled a new benchmark, MTFinEval, focusing on the LLMs' basic knowledge of\neconomics, which can always be used as a basis for judgment. To examine only\ntheoretical knowledge as much as possible, MTFinEval is build with foundational\nquestions from university textbooks,and exam papers in economics and management\nmajor. Aware of the overall performance of LLMs do not depend solely on one\nsubdiscipline of economics, MTFinEval comprise 360 questions refined from six\nmajor disciplines of economics, and reflect capabilities more comprehensively.\nExperiment result shows all LLMs perform poorly on MTFinEval, which proves that\nour benchmark built on basic knowledge is very successful. Our research not\nonly offers guidance for selecting the appropriate LLM for specific use cases,\nbut also put forward increase the rigor reliability of LLMs from the basics."
                },
                "authors": [
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Ke Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ke Jin"
                },
                "author": "Ke Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10918v1",
                "updated": "2024-08-20T15:03:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    3,
                    35,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:03:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    3,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CHECKWHY: Causal Fact Verification via Argument Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHECKWHY: Causal Fact Verification via Argument Structure"
                },
                "summary": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Jiasheng Si"
                    },
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Haiyang Zhu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Accepted by ACL2024; Awarded as Outstanding Paper Award and Area\n  Chair Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10914v1",
                "updated": "2024-08-20T14:58:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    58,
                    13,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:58:13Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    58,
                    13,
                    1,
                    233,
                    0
                ],
                "title": "To Code, or Not To Code? Exploring Impact of Code in Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Code, or Not To Code? Exploring Impact of Code in Pre-training"
                },
                "summary": "Including code in the pre-training data mixture, even for models not\nspecifically designed for code, has become a common practice in LLMs\npre-training. While there has been anecdotal consensus among practitioners that\ncode data plays a vital role in general LLMs' performance, there is only\nlimited work analyzing the precise impact of code on non-code tasks. In this\nwork, we systematically investigate the impact of code data on general\nperformance. We ask \"what is the impact of code data used in pre-training on a\nlarge variety of downstream tasks beyond code generation\". We conduct extensive\nablations and evaluate across a broad range of natural language reasoning\ntasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for\nmodels with sizes ranging from 470M to 2.8B parameters. Across settings, we\nfind a consistent results that code is a critical building block for\ngeneralization far beyond coding tasks and improvements to code quality have an\noutsized impact across all tasks. In particular, compared to text-only\npre-training, the addition of code results in up to relative increase of 8.2%\nin natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement\nin generative win-rates, and a 12x boost in code performance respectively. Our\nwork suggests investments in code quality and preserving code during\npre-training have positive impacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Including code in the pre-training data mixture, even for models not\nspecifically designed for code, has become a common practice in LLMs\npre-training. While there has been anecdotal consensus among practitioners that\ncode data plays a vital role in general LLMs' performance, there is only\nlimited work analyzing the precise impact of code on non-code tasks. In this\nwork, we systematically investigate the impact of code data on general\nperformance. We ask \"what is the impact of code data used in pre-training on a\nlarge variety of downstream tasks beyond code generation\". We conduct extensive\nablations and evaluate across a broad range of natural language reasoning\ntasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for\nmodels with sizes ranging from 470M to 2.8B parameters. Across settings, we\nfind a consistent results that code is a critical building block for\ngeneralization far beyond coding tasks and improvements to code quality have an\noutsized impact across all tasks. In particular, compared to text-only\npre-training, the addition of code results in up to relative increase of 8.2%\nin natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement\nin generative win-rates, and a 12x boost in code performance respectively. Our\nwork suggests investments in code quality and preserving code during\npre-training have positive impacts."
                },
                "authors": [
                    {
                        "name": "Viraat Aryabumi"
                    },
                    {
                        "name": "Yixuan Su"
                    },
                    {
                        "name": "Raymond Ma"
                    },
                    {
                        "name": "Adrien Morisot"
                    },
                    {
                        "name": "Ivan Zhang"
                    },
                    {
                        "name": "Acyr Locatelli"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Ahmet Üstün"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09236v2",
                "updated": "2024-08-20T14:57:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    57,
                    15,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-17T16:04:31Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    16,
                    4,
                    31,
                    5,
                    230,
                    0
                ],
                "title": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords"
                },
                "summary": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes."
                },
                "authors": [
                    {
                        "name": "Aman Ahluwalia"
                    },
                    {
                        "name": "Bishwajit Sutradhar"
                    },
                    {
                        "name": "Karishma Ghosh"
                    },
                    {
                        "name": "Indrapal Yadav"
                    },
                    {
                        "name": "Arpan Sheetal"
                    },
                    {
                        "name": "Prashant Patil"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Patil"
                },
                "author": "Prashant Patil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05890v2",
                "updated": "2024-08-20T14:51:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    51,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-08T12:52:46Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    12,
                    52,
                    46,
                    0,
                    190,
                    0
                ],
                "title": "Affordances-Oriented Planning using Foundation Models for Continuous\n  Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordances-Oriented Planning using Foundation Models for Continuous\n  Vision-Language Navigation"
                },
                "summary": "LLM-based agents have demonstrated impressive zero-shot performance in\nvision-language navigation (VLN) task. However, existing LLM-based methods\noften focus only on solving high-level task planning by selecting nodes in\npredefined navigation graphs for movements, overlooking low-level control in\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\nAffordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates\nvarious foundation models to achieve affordances-oriented low-level motion\nplanning and high-level decision-making, both performed in a zero-shot setting.\nSpecifically, we employ a Visual Affordances Prompting (VAP) approach, where\nthe visible ground is segmented by SAM to provide navigational affordances,\nbased on which the LLM selects potential candidate waypoints and plans\nlow-level paths towards selected waypoints. We further propose a high-level\nPathAgent which marks planned paths into the image input and reasons the most\nprobable path by comprehending all environmental information. Finally, we\nconvert the selected path into 3D coordinates using camera intrinsic parameters\nand depth information, avoiding challenging 3D predictions for LLMs.\nExperiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner\nachieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our\nmethod can also serve as a data annotator to obtain pseudo-labels, distilling\nits waypoint prediction ability into a learning-based predictor. This new\npredictor does not require any waypoint data from the simulator and achieves\n47% SR competing with supervised methods. We establish an effective connection\nbetween LLM and 3D world, presenting novel prospects for employing foundation\nmodels in low-level motion control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have demonstrated impressive zero-shot performance in\nvision-language navigation (VLN) task. However, existing LLM-based methods\noften focus only on solving high-level task planning by selecting nodes in\npredefined navigation graphs for movements, overlooking low-level control in\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\nAffordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates\nvarious foundation models to achieve affordances-oriented low-level motion\nplanning and high-level decision-making, both performed in a zero-shot setting.\nSpecifically, we employ a Visual Affordances Prompting (VAP) approach, where\nthe visible ground is segmented by SAM to provide navigational affordances,\nbased on which the LLM selects potential candidate waypoints and plans\nlow-level paths towards selected waypoints. We further propose a high-level\nPathAgent which marks planned paths into the image input and reasons the most\nprobable path by comprehending all environmental information. Finally, we\nconvert the selected path into 3D coordinates using camera intrinsic parameters\nand depth information, avoiding challenging 3D predictions for LLMs.\nExperiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner\nachieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our\nmethod can also serve as a data annotator to obtain pseudo-labels, distilling\nits waypoint prediction ability into a learning-based predictor. This new\npredictor does not require any waypoint data from the simulator and achieves\n47% SR competing with supervised methods. We establish an effective connection\nbetween LLM and 3D world, presenting novel prospects for employing foundation\nmodels in low-level motion control."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Bingqian Lin"
                    },
                    {
                        "name": "Xinmin Liu"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v2",
                "updated": "2024-08-21T03:31:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    31,
                    25,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Rusheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10902v1",
                "updated": "2024-08-20T14:45:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    45,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:45:23Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    45,
                    23,
                    1,
                    233,
                    0
                ],
                "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs"
                },
                "summary": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation."
                },
                "authors": [
                    {
                        "name": "John Mendonça"
                    },
                    {
                        "name": "Isabel Trancoso"
                    },
                    {
                        "name": "Alon Lavie"
                    }
                ],
                "author_detail": {
                    "name": "Alon Lavie"
                },
                "author": "Alon Lavie",
                "arxiv_comment": "22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10900v1",
                "updated": "2024-08-20T14:43:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    43,
                    33,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:43:33Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    43,
                    33,
                    1,
                    233,
                    0
                ],
                "title": "Towards Efficient Formal Verification of Spiking Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Formal Verification of Spiking Neural Network"
                },
                "summary": "Recently, AI research has primarily focused on large language models (LLMs),\nand increasing accuracy often involves scaling up and consuming more power. The\npower consumption of AI has become a significant societal issue; in this\ncontext, spiking neural networks (SNNs) offer a promising solution. SNNs\noperate event-driven, like the human brain, and compress information\ntemporally. These characteristics allow SNNs to significantly reduce power\nconsumption compared to perceptron-based artificial neural networks (ANNs),\nhighlighting them as a next-generation neural network technology. However,\nsocietal concerns regarding AI go beyond power consumption, with the\nreliability of AI models being a global issue. For instance, adversarial\nattacks on AI models are a well-studied problem in the context of traditional\nneural networks. Despite their importance, the stability and property\nverification of SNNs remains in the early stages of research. Most SNN\nverification methods are time-consuming and barely scalable, making practical\napplications challenging. In this paper, we introduce temporal encoding to\nachieve practical performance in verifying the adversarial robustness of SNNs.\nWe conduct a theoretical analysis of this approach and demonstrate its success\nin verifying SNNs at previously unmanageable scales. Our contribution advances\nSNN verification to a practical level, facilitating the safer application of\nSNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, AI research has primarily focused on large language models (LLMs),\nand increasing accuracy often involves scaling up and consuming more power. The\npower consumption of AI has become a significant societal issue; in this\ncontext, spiking neural networks (SNNs) offer a promising solution. SNNs\noperate event-driven, like the human brain, and compress information\ntemporally. These characteristics allow SNNs to significantly reduce power\nconsumption compared to perceptron-based artificial neural networks (ANNs),\nhighlighting them as a next-generation neural network technology. However,\nsocietal concerns regarding AI go beyond power consumption, with the\nreliability of AI models being a global issue. For instance, adversarial\nattacks on AI models are a well-studied problem in the context of traditional\nneural networks. Despite their importance, the stability and property\nverification of SNNs remains in the early stages of research. Most SNN\nverification methods are time-consuming and barely scalable, making practical\napplications challenging. In this paper, we introduce temporal encoding to\nachieve practical performance in verifying the adversarial robustness of SNNs.\nWe conduct a theoretical analysis of this approach and demonstrate its success\nin verifying SNNs at previously unmanageable scales. Our contribution advances\nSNN verification to a practical level, facilitating the safer application of\nSNNs."
                },
                "authors": [
                    {
                        "name": "Baekryun Seong"
                    },
                    {
                        "name": "Jieung Kim"
                    },
                    {
                        "name": "Sang-Ki Ko"
                    }
                ],
                "author_detail": {
                    "name": "Sang-Ki Ko"
                },
                "author": "Sang-Ki Ko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09248v2",
                "updated": "2024-08-20T14:39:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    39,
                    46,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-17T16:34:03Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    16,
                    34,
                    3,
                    5,
                    230,
                    0
                ],
                "title": "MagicID: Flexible ID Fidelity Generation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicID: Flexible ID Fidelity Generation System"
                },
                "summary": "Portrait Fidelity Generation is a prominent research area in generative\nmodels, with a primary focus on enhancing both controllability and fidelity.\nCurrent methods face challenges in generating high-fidelity portrait results\nwhen faces occupy a small portion of the image with a low resolution,\nespecially in multi-person group photo settings. To tackle these issues, we\npropose a systematic solution called MagicID, based on a self-constructed\nmillion-level multi-modal dataset named IDZoom. MagicID consists of Multi-Mode\nFusion training strategy (MMF) and DDIM Inversion based ID Restoration\ninference framework (DIIR). During training, MMF iteratively uses the skeleton\nand landmark modalities from IDZoom as conditional guidance. By introducing the\nClone Face Tuning in training stage and Mask Guided Multi-ID Cross Attention\n(MGMICA) in inference stage, explicit constraints on face positional features\nare achieved for multi-ID group photo generation. The DIIR aims to address the\nissue of artifacts. The DDIM Inversion is used in conjunction with face\nlandmarks, global and local face features to achieve face restoration while\nkeeping the background unchanged. Additionally, DIIR is plug-and-play and can\nbe applied to any diffusion-based portrait generation method. To validate the\neffectiveness of MagicID, we conducted extensive comparative and ablation\nexperiments. The experimental results demonstrate that MagicID has significant\nadvantages in both subjective and objective metrics, and achieves controllable\ngeneration in multi-person scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portrait Fidelity Generation is a prominent research area in generative\nmodels, with a primary focus on enhancing both controllability and fidelity.\nCurrent methods face challenges in generating high-fidelity portrait results\nwhen faces occupy a small portion of the image with a low resolution,\nespecially in multi-person group photo settings. To tackle these issues, we\npropose a systematic solution called MagicID, based on a self-constructed\nmillion-level multi-modal dataset named IDZoom. MagicID consists of Multi-Mode\nFusion training strategy (MMF) and DDIM Inversion based ID Restoration\ninference framework (DIIR). During training, MMF iteratively uses the skeleton\nand landmark modalities from IDZoom as conditional guidance. By introducing the\nClone Face Tuning in training stage and Mask Guided Multi-ID Cross Attention\n(MGMICA) in inference stage, explicit constraints on face positional features\nare achieved for multi-ID group photo generation. The DIIR aims to address the\nissue of artifacts. The DDIM Inversion is used in conjunction with face\nlandmarks, global and local face features to achieve face restoration while\nkeeping the background unchanged. Additionally, DIIR is plug-and-play and can\nbe applied to any diffusion-based portrait generation method. To validate the\neffectiveness of MagicID, we conducted extensive comparative and ablation\nexperiments. The experimental results demonstrate that MagicID has significant\nadvantages in both subjective and objective metrics, and achieves controllable\ngeneration in multi-person scenarios."
                },
                "authors": [
                    {
                        "name": "Zhaoli Deng"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Fanyi Wang"
                    },
                    {
                        "name": "Junkang Zhang"
                    },
                    {
                        "name": "Fan Chen"
                    },
                    {
                        "name": "Meng Zhang"
                    },
                    {
                        "name": "Wendong Zhang"
                    },
                    {
                        "name": "Zhenpeng Mi"
                    }
                ],
                "author_detail": {
                    "name": "Zhenpeng Mi"
                },
                "author": "Zhenpeng Mi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06209v2",
                "updated": "2024-08-20T14:35:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    35,
                    3,
                    1,
                    233,
                    0
                ],
                "published": "2024-04-09T10:58:21Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    10,
                    58,
                    21,
                    1,
                    100,
                    0
                ],
                "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models"
                },
                "summary": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets."
                },
                "authors": [
                    {
                        "name": "Sebastian Bordt"
                    },
                    {
                        "name": "Harsha Nori"
                    },
                    {
                        "name": "Vanessa Rodrigues"
                    },
                    {
                        "name": "Besmira Nushi"
                    },
                    {
                        "name": "Rich Caruana"
                    }
                ],
                "author_detail": {
                    "name": "Rich Caruana"
                },
                "author": "Rich Caruana",
                "arxiv_comment": "COLM camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00338v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00338v3",
                "updated": "2024-08-20T14:33:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    33,
                    32,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-01T06:23:54Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    6,
                    23,
                    54,
                    2,
                    122,
                    0
                ],
                "title": "Distillation Matters: Empowering Sequential Recommenders to Match the\n  Performance of Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation Matters: Empowering Sequential Recommenders to Match the\n  Performance of Large Language Model"
                },
                "summary": "Owing to their powerful semantic reasoning capabilities, Large Language\nModels (LLMs) have been effectively utilized as recommenders, achieving\nimpressive performance. However, the high inference latency of LLMs\nsignificantly restricts their practical deployment. To address this issue, this\nwork investigates knowledge distillation from cumbersome LLM-based\nrecommendation models to lightweight conventional sequential models. It\nencounters three challenges: 1) the teacher's knowledge may not always be\nreliable; 2) the capacity gap between the teacher and student makes it\ndifficult for the student to assimilate the teacher's knowledge; 3) divergence\nin semantic space poses a challenge to distill the knowledge from embeddings.\nTo tackle these challenges, this work proposes a novel distillation strategy,\nDLLM2Rec, specifically tailored for knowledge distillation from LLM-based\nrecommendation models to conventional sequential models. DLLM2Rec comprises: 1)\nImportance-aware ranking distillation, which filters reliable and\nstudent-friendly knowledge by weighting instances according to teacher\nconfidence and student-teacher consistency; 2) Collaborative embedding\ndistillation integrates knowledge from teacher embeddings with collaborative\nsignals mined from the data. Extensive experiments demonstrate the\neffectiveness of the proposed DLLM2Rec, boosting three typical sequential\nmodels with an average improvement of 47.97%, even enabling them to surpass\nLLM-based recommenders in some cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to their powerful semantic reasoning capabilities, Large Language\nModels (LLMs) have been effectively utilized as recommenders, achieving\nimpressive performance. However, the high inference latency of LLMs\nsignificantly restricts their practical deployment. To address this issue, this\nwork investigates knowledge distillation from cumbersome LLM-based\nrecommendation models to lightweight conventional sequential models. It\nencounters three challenges: 1) the teacher's knowledge may not always be\nreliable; 2) the capacity gap between the teacher and student makes it\ndifficult for the student to assimilate the teacher's knowledge; 3) divergence\nin semantic space poses a challenge to distill the knowledge from embeddings.\nTo tackle these challenges, this work proposes a novel distillation strategy,\nDLLM2Rec, specifically tailored for knowledge distillation from LLM-based\nrecommendation models to conventional sequential models. DLLM2Rec comprises: 1)\nImportance-aware ranking distillation, which filters reliable and\nstudent-friendly knowledge by weighting instances according to teacher\nconfidence and student-teacher consistency; 2) Collaborative embedding\ndistillation integrates knowledge from teacher embeddings with collaborative\nsignals mined from the data. Extensive experiments demonstrate the\neffectiveness of the proposed DLLM2Rec, boosting three typical sequential\nmodels with an average improvement of 47.97%, even enabling them to surpass\nLLM-based recommenders in some cases."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Pengbo Wang"
                    },
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Heng Tang"
                    },
                    {
                        "name": "Yi Wan"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jiawei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Chen"
                },
                "author": "Jiawei Chen",
                "arxiv_doi": "10.1145/3640457.3688118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640457.3688118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00338v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00338v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10895v1",
                "updated": "2024-08-20T14:29:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    29,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:29:23Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    29,
                    23,
                    1,
                    233,
                    0
                ],
                "title": "Analytical and Empirical Study of Herding Effects in Recommendation\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical and Empirical Study of Herding Effects in Recommendation\n  Systems"
                },
                "summary": "Online rating systems are often used in numerous web or mobile applications,\ne.g., Amazon and TripAdvisor, to assess the ground-truth quality of products.\nDue to herding effects, the aggregation of historical ratings (or historical\ncollective opinion) can significantly influence subsequent ratings, leading to\nmisleading and erroneous assessments. We study how to manage product ratings\nvia rating aggregation rules and shortlisted representative reviews, for the\npurpose of correcting the assessment error. We first develop a mathematical\nmodel to characterize important factors of herding effects in product ratings.\nWe then identify sufficient conditions (via the stochastic approximation\ntheory), under which the historical collective opinion converges to the\nground-truth collective opinion of the whole user population. These conditions\nidentify a class of rating aggregation rules and review selection mechanisms\nthat can reveal the ground-truth product quality. We also quantify the speed of\nconvergence (via the martingale theory), which reflects the efficiency of\nrating aggregation rules and review selection mechanisms. We prove that the\nherding effects slow down the speed of convergence while an accurate review\nselection mechanism can speed it up. We also study the speed of convergence\nnumerically and reveal trade-offs in selecting rating aggregation rules and\nreview selection mechanisms. To show the utility of our framework, we design a\nmaximum likelihood algorithm to infer model parameters from ratings, and\nconduct experiments on rating datasets from Amazon and TripAdvisor. We show\nthat proper recency aware rating aggregation rules can improve the speed of\nconvergence in Amazon and TripAdvisor by 41% and 62% respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online rating systems are often used in numerous web or mobile applications,\ne.g., Amazon and TripAdvisor, to assess the ground-truth quality of products.\nDue to herding effects, the aggregation of historical ratings (or historical\ncollective opinion) can significantly influence subsequent ratings, leading to\nmisleading and erroneous assessments. We study how to manage product ratings\nvia rating aggregation rules and shortlisted representative reviews, for the\npurpose of correcting the assessment error. We first develop a mathematical\nmodel to characterize important factors of herding effects in product ratings.\nWe then identify sufficient conditions (via the stochastic approximation\ntheory), under which the historical collective opinion converges to the\nground-truth collective opinion of the whole user population. These conditions\nidentify a class of rating aggregation rules and review selection mechanisms\nthat can reveal the ground-truth product quality. We also quantify the speed of\nconvergence (via the martingale theory), which reflects the efficiency of\nrating aggregation rules and review selection mechanisms. We prove that the\nherding effects slow down the speed of convergence while an accurate review\nselection mechanism can speed it up. We also study the speed of convergence\nnumerically and reveal trade-offs in selecting rating aggregation rules and\nreview selection mechanisms. To show the utility of our framework, we design a\nmaximum likelihood algorithm to infer model parameters from ratings, and\nconduct experiments on rating datasets from Amazon and TripAdvisor. We show\nthat proper recency aware rating aggregation rules can improve the speed of\nconvergence in Amazon and TripAdvisor by 41% and 62% respectively."
                },
                "authors": [
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Mingze Zhong"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.03146v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.03146v4",
                "updated": "2024-08-20T14:28:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    28,
                    6,
                    1,
                    233,
                    0
                ],
                "published": "2022-01-24T17:01:59Z",
                "published_parsed": [
                    2022,
                    1,
                    24,
                    17,
                    1,
                    59,
                    0,
                    24,
                    0
                ],
                "title": "Time-Series K-means in Causal Inference and Mechanism Clustering for\n  Financial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Series K-means in Causal Inference and Mechanism Clustering for\n  Financial Data"
                },
                "summary": "This paper investigates the application of Time Series K-means (TS-K-means)\nwithin the context of causal inference and mechanism clustering of financial\ntime series data. Traditional clustering approaches like K-means often rely on\nstatic distance metrics, such as Euclidean distance, which inadequately capture\nthe temporal dependencies intrinsic to financial returns. By incorporating\nDynamic Time Warping (DTW) as a distance metric, TS-K-means addresses this\nlimitation, improving the robustness of clustering in time-dependent financial\ndata. This study extends the Additive Noise Model Mixture Model (ANM-MM)\nframework by integrating TS-K-means, facilitating more accurate causal\ninference and mechanism clustering. The approach is validated through\nsimulations and applied to real-world financial data, demonstrating its\neffectiveness in enhancing the analysis of complex financial time series,\nparticularly in identifying causal relationships and clustering data based on\nunderlying generative mechanisms. The results show that TS-K-means outperforms\ntraditional K-means, especially with smaller datasets, while maintaining robust\ncausal direction detection as the dataset size changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the application of Time Series K-means (TS-K-means)\nwithin the context of causal inference and mechanism clustering of financial\ntime series data. Traditional clustering approaches like K-means often rely on\nstatic distance metrics, such as Euclidean distance, which inadequately capture\nthe temporal dependencies intrinsic to financial returns. By incorporating\nDynamic Time Warping (DTW) as a distance metric, TS-K-means addresses this\nlimitation, improving the robustness of clustering in time-dependent financial\ndata. This study extends the Additive Noise Model Mixture Model (ANM-MM)\nframework by integrating TS-K-means, facilitating more accurate causal\ninference and mechanism clustering. The approach is validated through\nsimulations and applied to real-world financial data, demonstrating its\neffectiveness in enhancing the analysis of complex financial time series,\nparticularly in identifying causal relationships and clustering data based on\nunderlying generative mechanisms. The results show that TS-K-means outperforms\ntraditional K-means, especially with smaller datasets, while maintaining robust\ncausal direction detection as the dataset size changes."
                },
                "authors": [
                    {
                        "name": "Shi Bo"
                    },
                    {
                        "name": "Minheng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Minheng Xiao"
                },
                "author": "Minheng Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2202.03146v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.03146v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08890v2",
                "updated": "2024-08-20T14:19:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    19,
                    38,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-14T18:07:04Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    18,
                    7,
                    4,
                    1,
                    135,
                    0
                ],
                "title": "Language-Guided Self-Supervised Video Summarization Using Text Semantic\n  Matching Considering the Diversity of the Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Guided Self-Supervised Video Summarization Using Text Semantic\n  Matching Considering the Diversity of the Video"
                },
                "summary": "Current video summarization methods rely heavily on supervised computer\nvision techniques, which demands time-consuming and subjective manual\nannotations. To overcome these limitations, we investigated self-supervised\nvideo summarization. Inspired by the success of Large Language Models (LLMs),\nwe explored the feasibility in transforming the video summarization task into a\nNatural Language Processing (NLP) task. By leveraging the advantages of LLMs in\ncontext understanding, we aim to enhance the effectiveness of self-supervised\nvideo summarization. Our method begins by generating captions for individual\nvideo frames, which are then synthesized into text summaries by LLMs.\nSubsequently, we measure semantic distance between the captions and the text\nsummary. Notably, we propose a novel loss function to optimize our model\naccording to the diversity of the video. Finally, the summarized video can be\ngenerated by selecting the frames with captions similar to the text summary.\nOur method achieves state-of-the-art performance on the SumMe dataset in rank\ncorrelation coefficients. In addition, our method has a novel feature of being\nable to achieve personalized summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video summarization methods rely heavily on supervised computer\nvision techniques, which demands time-consuming and subjective manual\nannotations. To overcome these limitations, we investigated self-supervised\nvideo summarization. Inspired by the success of Large Language Models (LLMs),\nwe explored the feasibility in transforming the video summarization task into a\nNatural Language Processing (NLP) task. By leveraging the advantages of LLMs in\ncontext understanding, we aim to enhance the effectiveness of self-supervised\nvideo summarization. Our method begins by generating captions for individual\nvideo frames, which are then synthesized into text summaries by LLMs.\nSubsequently, we measure semantic distance between the captions and the text\nsummary. Notably, we propose a novel loss function to optimize our model\naccording to the diversity of the video. Finally, the summarized video can be\ngenerated by selecting the frames with captions similar to the text summary.\nOur method achieves state-of-the-art performance on the SumMe dataset in rank\ncorrelation coefficients. In addition, our method has a novel feature of being\nable to achieve personalized summarization."
                },
                "authors": [
                    {
                        "name": "Tomoya Sugihara"
                    },
                    {
                        "name": "Shuntaro Masuda"
                    },
                    {
                        "name": "Ling Xiao"
                    },
                    {
                        "name": "Toshihiko Yamasaki"
                    }
                ],
                "author_detail": {
                    "name": "Toshihiko Yamasaki"
                },
                "author": "Toshihiko Yamasaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10886v1",
                "updated": "2024-08-20T14:17:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    17,
                    50,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:17:50Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    17,
                    50,
                    1,
                    233,
                    0
                ],
                "title": "Leveraging LLMs for the Quality Assurance of Software Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for the Quality Assurance of Software Requirements"
                },
                "summary": "Successful software projects depend on the quality of software requirements.\nCreating high-quality requirements is a crucial step toward successful software\ndevelopment. Effective support in this area can significantly reduce\ndevelopment costs and enhance the software quality. In this paper, we introduce\nand assess the capabilities of a Large Language Model (LLM) to evaluate the\nquality characteristics of software requirements according to the ISO 29148\nstandard. We aim to further improve the support of stakeholders engaged in\nrequirements engineering (RE). We show how an LLM can assess requirements,\nexplain its decision-making process, and examine its capacity to propose\nimproved versions of requirements. We conduct a study with software engineers\nto validate our approach. Our findings emphasize the potential of LLMs for\nimproving the quality of software requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful software projects depend on the quality of software requirements.\nCreating high-quality requirements is a crucial step toward successful software\ndevelopment. Effective support in this area can significantly reduce\ndevelopment costs and enhance the software quality. In this paper, we introduce\nand assess the capabilities of a Large Language Model (LLM) to evaluate the\nquality characteristics of software requirements according to the ISO 29148\nstandard. We aim to further improve the support of stakeholders engaged in\nrequirements engineering (RE). We show how an LLM can assess requirements,\nexplain its decision-making process, and examine its capacity to propose\nimproved versions of requirements. We conduct a study with software engineers\nto validate our approach. Our findings emphasize the potential of LLMs for\nimproving the quality of software requirements."
                },
                "authors": [
                    {
                        "name": "Sebastian Lubos"
                    },
                    {
                        "name": "Alexander Felfernig"
                    },
                    {
                        "name": "Thi Ngoc Trang Tran"
                    },
                    {
                        "name": "Damian Garber"
                    },
                    {
                        "name": "Merfat El Mansi"
                    },
                    {
                        "name": "Seda Polat Erdeniz"
                    },
                    {
                        "name": "Viet-Man Le"
                    }
                ],
                "author_detail": {
                    "name": "Viet-Man Le"
                },
                "author": "Viet-Man Le",
                "arxiv_comment": "Accepted for publication at the RE@Next! track of RE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10883v1",
                "updated": "2024-08-20T14:13:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    13,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:13:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    13,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News\n  Detection"
                },
                "summary": "In current web environment, fake news spreads rapidly across online social\nnetworks, posing serious threats to society. Existing multimodal fake news\ndetection (MFND) methods can be classified into knowledge-based and\nsemantic-based approaches. However, these methods are overly dependent on human\nexpertise and feedback, lacking flexibility. To address this challenge, we\npropose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake\nnews detection. For knowledge-based methods, we introduce the Monte Carlo Tree\nSearch (MCTS) algorithm to leverage the self-reflective capabilities of large\nlanguage models (LLMs) for prompt optimization, providing richer,\ndomain-specific details and guidance to the LLMs, while enabling more flexible\nintegration of LLM comment on news content. For semantic-based methods, we\ndefine four typical deceit patterns: emotional exaggeration, logical\ninconsistency, image manipulation, and semantic inconsistency, to reveal the\nmechanisms behind fake news creation. To detect these patterns, we carefully\ndesign four discriminators and expand them in depth and breadth, using the\nsoft-routing mechanism to explore optimal detection models. Experimental\nresults on three real-world datasets demonstrate the superiority of our\napproach. The code will be available at: https://github.com/SuXinqi/DAAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In current web environment, fake news spreads rapidly across online social\nnetworks, posing serious threats to society. Existing multimodal fake news\ndetection (MFND) methods can be classified into knowledge-based and\nsemantic-based approaches. However, these methods are overly dependent on human\nexpertise and feedback, lacking flexibility. To address this challenge, we\npropose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake\nnews detection. For knowledge-based methods, we introduce the Monte Carlo Tree\nSearch (MCTS) algorithm to leverage the self-reflective capabilities of large\nlanguage models (LLMs) for prompt optimization, providing richer,\ndomain-specific details and guidance to the LLMs, while enabling more flexible\nintegration of LLM comment on news content. For semantic-based methods, we\ndefine four typical deceit patterns: emotional exaggeration, logical\ninconsistency, image manipulation, and semantic inconsistency, to reveal the\nmechanisms behind fake news creation. To detect these patterns, we carefully\ndesign four discriminators and expand them in depth and breadth, using the\nsoft-routing mechanism to explore optimal detection models. Experimental\nresults on three real-world datasets demonstrate the superiority of our\napproach. The code will be available at: https://github.com/SuXinqi/DAAD."
                },
                "authors": [
                    {
                        "name": "Xinqi Su"
                    },
                    {
                        "name": "Yawen Cui"
                    },
                    {
                        "name": "Ajian Liu"
                    },
                    {
                        "name": "Xun Lin"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Haochen Liang"
                    },
                    {
                        "name": "Wenhui Li"
                    },
                    {
                        "name": "Zitong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zitong Yu"
                },
                "author": "Zitong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2004.12571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2004.12571v3",
                "updated": "2024-08-20T14:11:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    11,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2020-04-27T03:45:48Z",
                "published_parsed": [
                    2020,
                    4,
                    27,
                    3,
                    45,
                    48,
                    0,
                    118,
                    0
                ],
                "title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Defenses against GAN-Based Feature Inference Attacks in\n  Federated Learning"
                },
                "summary": "Federated learning (FL) is a decentralized model training framework that aims\nto merge isolated data islands while maintaining data privacy. However, recent\nstudies have revealed that Generative Adversarial Network (GAN) based attacks\ncan be employed in FL to learn the distribution of private datasets and\nreconstruct recognizable images. In this paper, we exploit defenses against\nGAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to manipulate the visual features of private training images to\nmake them indistinguishable to human eyes even restored by attackers.\nSpecifically, Anti-GAN projects the private dataset onto a GAN's generator and\ncombines the generated fake images with the actual images to create the\ntraining dataset, which is then used for federated model training. The\nexperimental results demonstrate that Anti-GAN is effective in preventing\nattackers from learning the distribution of private images while causing\nminimal harm to the accuracy of the federated model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a decentralized model training framework that aims\nto merge isolated data islands while maintaining data privacy. However, recent\nstudies have revealed that Generative Adversarial Network (GAN) based attacks\ncan be employed in FL to learn the distribution of private datasets and\nreconstruct recognizable images. In this paper, we exploit defenses against\nGAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to manipulate the visual features of private training images to\nmake them indistinguishable to human eyes even restored by attackers.\nSpecifically, Anti-GAN projects the private dataset onto a GAN's generator and\ncombines the generated fake images with the actual images to create the\ntraining dataset, which is then used for federated model training. The\nexperimental results demonstrate that Anti-GAN is effective in preventing\nattackers from learning the distribution of private images while causing\nminimal harm to the accuracy of the federated model."
                },
                "authors": [
                    {
                        "name": "Xinjian Luo"
                    },
                    {
                        "name": "Xianglong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Zhang"
                },
                "author": "Xianglong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2004.12571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2004.12571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02709v2",
                "updated": "2024-08-20T13:58:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    58,
                    58,
                    1,
                    233,
                    0
                ],
                "published": "2024-02-05T04:03:44Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    4,
                    3,
                    44,
                    0,
                    36,
                    0
                ],
                "title": "Passive decoy-state quantum secure direct communication with heralded\n  single-photon source",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive decoy-state quantum secure direct communication with heralded\n  single-photon source"
                },
                "summary": "Quantum secure direct communications (QSDC) can directly transmit secret\nmessages through a quantum channel without keys. The imperfect photon source is\na major obstacle for QSDC's practical implementation. The unwanted vacuum state\nand multiphoton components emitted from imperfect photon source largely reduce\nQSDC's secrecy message capacity and even threaten its security. In the paper,\nwe propose a high-efficient passive decoy-state QSDC protocol with the heralded\nsingle-photon source (HSPS). We adopt a spontaneous parametric down-conversion\nsource to emit entangled photon pairs in two spatial modes. By detecting the\nphotons in one of the two correlated spatial modes, we can infer the\nphoton-number distribution of the other spatial mode. Meanwhile, our protocol\nallows a simple passive preparation of the signal states and decoy state. The\nHSPS can effectively reduce the probability of vacuum state and increase QSDC's\nsecrecy message capacity. Meanwhile, the passive decoy-state method can\nsimplify the experimental operations and enhance QSDC's robustness against the\nthird-party side-channel attacks. Under the communication distance of 10 km,\nthe secrecy message capacity of our QSDC protocol can achieve 81.85 times with\naverage photon number of 0.1 and 12.79 times with average photon number of 0.01\nof that in the original single-photon-based QSDC protocol without the HSPS. Our\nQSDC protocol has longer maximal communication distance about 17.975 km with\naverage photon number of 0.01. Our work serves as a major step toward the\nfurther development of practical passive decoy-state QSDC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum secure direct communications (QSDC) can directly transmit secret\nmessages through a quantum channel without keys. The imperfect photon source is\na major obstacle for QSDC's practical implementation. The unwanted vacuum state\nand multiphoton components emitted from imperfect photon source largely reduce\nQSDC's secrecy message capacity and even threaten its security. In the paper,\nwe propose a high-efficient passive decoy-state QSDC protocol with the heralded\nsingle-photon source (HSPS). We adopt a spontaneous parametric down-conversion\nsource to emit entangled photon pairs in two spatial modes. By detecting the\nphotons in one of the two correlated spatial modes, we can infer the\nphoton-number distribution of the other spatial mode. Meanwhile, our protocol\nallows a simple passive preparation of the signal states and decoy state. The\nHSPS can effectively reduce the probability of vacuum state and increase QSDC's\nsecrecy message capacity. Meanwhile, the passive decoy-state method can\nsimplify the experimental operations and enhance QSDC's robustness against the\nthird-party side-channel attacks. Under the communication distance of 10 km,\nthe secrecy message capacity of our QSDC protocol can achieve 81.85 times with\naverage photon number of 0.1 and 12.79 times with average photon number of 0.01\nof that in the original single-photon-based QSDC protocol without the HSPS. Our\nQSDC protocol has longer maximal communication distance about 17.975 km with\naverage photon number of 0.01. Our work serves as a major step toward the\nfurther development of practical passive decoy-state QSDC systems."
                },
                "authors": [
                    {
                        "name": "Jia-Wei Ying"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Wei Zhong"
                    },
                    {
                        "name": "Ming-Ming Du"
                    },
                    {
                        "name": "Xi-Yun Li"
                    },
                    {
                        "name": "Shu-Ting Shen"
                    },
                    {
                        "name": "An-Lei Zhang"
                    },
                    {
                        "name": "Lan Zhou"
                    },
                    {
                        "name": "Yu-Bo Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Bo Sheng"
                },
                "author": "Yu-Bo Sheng",
                "arxiv_comment": "13 pages, 5 figures",
                "arxiv_journal_ref": "Phys. Rev. Appl. 22,024040 (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01631v2",
                "updated": "2024-08-20T13:56:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    56,
                    21,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-01T11:56:08Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    11,
                    56,
                    8,
                    5,
                    153,
                    0
                ],
                "title": "SUBER: An RL Environment with Simulated Human Behavior for Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBER: An RL Environment with Simulated Human Behavior for Recommender\n  Systems"
                },
                "summary": "Reinforcement learning (RL) has gained popularity in the realm of recommender\nsystems due to its ability to optimize long-term rewards and guide users in\ndiscovering relevant content. However, the successful implementation of RL in\nrecommender systems is challenging because of several factors, including the\nlimited availability of online data for training on-policy methods. This\nscarcity requires expensive human interaction for online model training.\nFurthermore, the development of effective evaluation frameworks that accurately\nreflect the quality of models remains a fundamental challenge in recommender\nsystems. To address these challenges, we propose a comprehensive framework for\nsynthetic environments that simulate human behavior by harnessing the\ncapabilities of large language models (LLMs). We complement our framework with\nin-depth ablation studies and demonstrate its effectiveness with experiments on\nmovie and book recommendations. Using LLMs as synthetic users, this work\nintroduces a modular and novel framework to train RL-based recommender systems.\nThe software, including the RL environment, is publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has gained popularity in the realm of recommender\nsystems due to its ability to optimize long-term rewards and guide users in\ndiscovering relevant content. However, the successful implementation of RL in\nrecommender systems is challenging because of several factors, including the\nlimited availability of online data for training on-policy methods. This\nscarcity requires expensive human interaction for online model training.\nFurthermore, the development of effective evaluation frameworks that accurately\nreflect the quality of models remains a fundamental challenge in recommender\nsystems. To address these challenges, we propose a comprehensive framework for\nsynthetic environments that simulate human behavior by harnessing the\ncapabilities of large language models (LLMs). We complement our framework with\nin-depth ablation studies and demonstrate its effectiveness with experiments on\nmovie and book recommendations. Using LLMs as synthetic users, this work\nintroduces a modular and novel framework to train RL-based recommender systems.\nThe software, including the RL environment, is publicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Nathan Corecco"
                    },
                    {
                        "name": "Giorgio Piatti"
                    },
                    {
                        "name": "Luca A. Lanzendörfer"
                    },
                    {
                        "name": "Flint Xiaofeng Fan"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10856v1",
                "updated": "2024-08-20T13:47:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    47,
                    29,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:47:29Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    47,
                    29,
                    1,
                    233,
                    0
                ],
                "title": "Conditional Delta-Method for Resampling Empirical Processes in Multiple\n  Sample Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Delta-Method for Resampling Empirical Processes in Multiple\n  Sample Problems"
                },
                "summary": "The functional delta-method has a wide range of applications in statistics.\nApplications on functionals of empirical processes yield various limit results\nfor classical statistics. To improve the finite sample properties of\nstatistical inference procedures that are based on the limit results,\nresampling procedures such as random permutation and bootstrap methods are a\npopular solution. In order to analyze the behaviour of the functionals of the\nresampling empirical processes, corresponding conditional functional\ndelta-methods are desirable. While conditional functional delta-methods for\nsome special cases already exist, there is a lack of more general conditional\nfunctional delta-methods for resampling procedures for empirical processes,\nsuch as the permutation and pooled bootstrap method. This gap is addressed in\nthe present paper. Thereby, a general multiple sample problem is considered.\nThe flexible application of the developed conditional delta-method is shown in\nvarious relevant examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The functional delta-method has a wide range of applications in statistics.\nApplications on functionals of empirical processes yield various limit results\nfor classical statistics. To improve the finite sample properties of\nstatistical inference procedures that are based on the limit results,\nresampling procedures such as random permutation and bootstrap methods are a\npopular solution. In order to analyze the behaviour of the functionals of the\nresampling empirical processes, corresponding conditional functional\ndelta-methods are desirable. While conditional functional delta-methods for\nsome special cases already exist, there is a lack of more general conditional\nfunctional delta-methods for resampling procedures for empirical processes,\nsuch as the permutation and pooled bootstrap method. This gap is addressed in\nthe present paper. Thereby, a general multiple sample problem is considered.\nThe flexible application of the developed conditional delta-method is shown in\nvarious relevant examples."
                },
                "authors": [
                    {
                        "name": "Merle Munko"
                    },
                    {
                        "name": "Dennis Dobler"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Dobler"
                },
                "author": "Dennis Dobler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20557v2",
                "updated": "2024-08-20T13:42:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    42,
                    25,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-30T05:24:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    5,
                    24,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "CELLM: An Efficient Communication in Large Language Models Training for\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CELLM: An Efficient Communication in Large Language Models Training for\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training."
                },
                "authors": [
                    {
                        "name": "Raja Vavekanand"
                    },
                    {
                        "name": "Kira Sam"
                    }
                ],
                "author_detail": {
                    "name": "Kira Sam"
                },
                "author": "Kira Sam",
                "arxiv_comment": "arXiv admin note: This submission has been withdrawn by arXiv\n  administrators due to inappropriate text overlap with external sources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v1",
                "updated": "2024-08-20T13:40:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17657v2",
                "updated": "2024-08-20T13:39:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    39,
                    51,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-24T21:50:36Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    21,
                    50,
                    36,
                    2,
                    206,
                    0
                ],
                "title": "My Ontologist: Evaluating BFO-Based AI for Definition Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "My Ontologist: Evaluating BFO-Based AI for Definition Support"
                },
                "summary": "Generative artificial intelligence (AI), exemplified by the release of\nGPT-3.5 in 2022, has significantly advanced the potential applications of large\nlanguage models (LLMs), including in the realms of ontology development and\nknowledge graph creation. Ontologies, which are structured frameworks for\norganizing information, and knowledge graphs, which combine ontologies with\nactual data, are essential for enabling interoperability and automated\nreasoning. However, current research has largely overlooked the generation of\nontologies extending from established upper-level frameworks like the Basic\nFormal Ontology (BFO), risking the creation of non-integrable ontology silos.\nThis study explores the extent to which LLMs, particularly GPT-4, can support\nontologists trained in BFO. Through iterative development of a specialized GPT\nmodel named \"My Ontologist,\" we aimed to generate BFO-conformant ontologies.\nInitial versions faced challenges in maintaining definition conventions and\nleveraging foundational texts effectively. My Ontologist 3.0 showed promise by\nadhering to structured rules and modular ontology suites, yet the release of\nGPT-4o disrupted this progress by altering the model's behavior. Our findings\nunderscore the importance of aligning LLM-generated ontologies with top-level\nstandards and highlight the complexities of integrating evolving AI\ncapabilities in ontology engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (AI), exemplified by the release of\nGPT-3.5 in 2022, has significantly advanced the potential applications of large\nlanguage models (LLMs), including in the realms of ontology development and\nknowledge graph creation. Ontologies, which are structured frameworks for\norganizing information, and knowledge graphs, which combine ontologies with\nactual data, are essential for enabling interoperability and automated\nreasoning. However, current research has largely overlooked the generation of\nontologies extending from established upper-level frameworks like the Basic\nFormal Ontology (BFO), risking the creation of non-integrable ontology silos.\nThis study explores the extent to which LLMs, particularly GPT-4, can support\nontologists trained in BFO. Through iterative development of a specialized GPT\nmodel named \"My Ontologist,\" we aimed to generate BFO-conformant ontologies.\nInitial versions faced challenges in maintaining definition conventions and\nleveraging foundational texts effectively. My Ontologist 3.0 showed promise by\nadhering to structured rules and modular ontology suites, yet the release of\nGPT-4o disrupted this progress by altering the model's behavior. Our findings\nunderscore the importance of aligning LLM-generated ontologies with top-level\nstandards and highlight the complexities of integrating evolving AI\ncapabilities in ontology engineering."
                },
                "authors": [
                    {
                        "name": "Carter Benson"
                    },
                    {
                        "name": "Alec Sculley"
                    },
                    {
                        "name": "Austin Liebers"
                    },
                    {
                        "name": "John Beverley"
                    }
                ],
                "author_detail": {
                    "name": "John Beverley"
                },
                "author": "John Beverley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10839v1",
                "updated": "2024-08-20T13:34:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    34,
                    17,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:34:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    34,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Benchmarking Large Language Models for Math Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Math Reasoning Tasks"
                },
                "summary": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research."
                },
                "authors": [
                    {
                        "name": "Kathrin Seßler"
                    },
                    {
                        "name": "Yao Rong"
                    },
                    {
                        "name": "Emek Gözlüklü"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05724v2",
                "updated": "2024-08-20T13:29:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    29,
                    41,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-08T08:28:23Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    8,
                    28,
                    23,
                    0,
                    190,
                    0
                ],
                "title": "Learning Stochastic Reduced Models from Data: A Nonintrusive Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Stochastic Reduced Models from Data: A Nonintrusive Approach"
                },
                "summary": "A nonintrusive model order reduction method for bilinear stochastic\ndifferential equations with additive noise is proposed. A reduced order model\n(ROM) is designed in order to approximate the statistical properties of\nhigh-dimensional systems. The drift and diffusion coefficients of the ROM are\ninferred from state observations by solving appropriate least-squares problems.\nThe closeness of the ROM obtained by the presented approach to the intrusive\nROM obtained by the proper orthogonal decomposition (POD) method is\ninvestigated. Two generalisations of the snapshot-based dominant subspace\nconstruction to the stochastic case are presented. Numerical experiments are\nprovided to compare the developed approach to POD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A nonintrusive model order reduction method for bilinear stochastic\ndifferential equations with additive noise is proposed. A reduced order model\n(ROM) is designed in order to approximate the statistical properties of\nhigh-dimensional systems. The drift and diffusion coefficients of the ROM are\ninferred from state observations by solving appropriate least-squares problems.\nThe closeness of the ROM obtained by the presented approach to the intrusive\nROM obtained by the proper orthogonal decomposition (POD) method is\ninvestigated. Two generalisations of the snapshot-based dominant subspace\nconstruction to the stochastic case are presented. Numerical experiments are\nprovided to compare the developed approach to POD."
                },
                "authors": [
                    {
                        "name": "M. A. Freitag"
                    },
                    {
                        "name": "J. M. Nicolaus"
                    },
                    {
                        "name": "M. Redmann"
                    }
                ],
                "author_detail": {
                    "name": "M. Redmann"
                },
                "author": "M. Redmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60H10, 60H35, 65C30, 60G51",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09380v2",
                "updated": "2024-08-20T13:24:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    24,
                    50,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-18T06:41:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    6,
                    41,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression"
                },
                "summary": "State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available."
                },
                "authors": [
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Song Lu"
                    },
                    {
                        "name": "Yinfeng Li"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Yuanjun Liu"
                    },
                    {
                        "name": "Peixing Xu"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "We hereby withdraw this paper from arXiv due to incomplete\n  experiments. Upon further review, we have determined that additional\n  experimental work is necessary to fully validate our findings and conclusions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10819v1",
                "updated": "2024-08-20T13:13:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    13,
                    41,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:13:41Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    13,
                    41,
                    1,
                    233,
                    0
                ],
                "title": "Exploiting Large Language Models Capabilities for Question Answer-Driven\n  Knowledge Graph Completion Across Static and Temporal Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Large Language Models Capabilities for Question Answer-Driven\n  Knowledge Graph Completion Across Static and Temporal Domains"
                },
                "summary": "Knowledge graph completion (KGC) aims to identify missing triples in a\nknowledge graph (KG). This is typically achieved through tasks such as link\nprediction and instance completion. However, these methods often focus on\neither static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),\naddressing only within-scope triples. This paper introduces a new generative\ncompletion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC\nemploys a question-answering format to directly generate target entities,\naddressing the challenge of questions having multiple possible answers. We\npropose a strategy that extracts subgraphs centered on entities and\nrelationships within the KG, from which negative samples and neighborhood\ninformation are separately obtained to address the one-to-many problem. Our\nmethod generates negative samples using known facts to facilitate the discovery\nof new information. Furthermore, we collect and refine neighborhood path data\nof known entities, providing contextual information to enhance reasoning in\nlarge language models (LLMs). Our experiments evaluated the proposed method on\nfour SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five\ndatasets. Analysis of the results shows that GS-KGC can discover new triples\nwithin existing KGs and generate new facts beyond the closed KG, effectively\nbridging the gap between closed-world and open-world KGC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph completion (KGC) aims to identify missing triples in a\nknowledge graph (KG). This is typically achieved through tasks such as link\nprediction and instance completion. However, these methods often focus on\neither static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),\naddressing only within-scope triples. This paper introduces a new generative\ncompletion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC\nemploys a question-answering format to directly generate target entities,\naddressing the challenge of questions having multiple possible answers. We\npropose a strategy that extracts subgraphs centered on entities and\nrelationships within the KG, from which negative samples and neighborhood\ninformation are separately obtained to address the one-to-many problem. Our\nmethod generates negative samples using known facts to facilitate the discovery\nof new information. Furthermore, we collect and refine neighborhood path data\nof known entities, providing contextual information to enhance reasoning in\nlarge language models (LLMs). Our experiments evaluated the proposed method on\nfour SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five\ndatasets. Analysis of the results shows that GS-KGC can discover new triples\nwithin existing KGs and generate new facts beyond the closed KG, effectively\nbridging the gap between closed-world and open-world KGC."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiahao Zhu"
                    },
                    {
                        "name": "Jianping Man"
                    },
                    {
                        "name": "Li Fang"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10811v1",
                "updated": "2024-08-20T13:05:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    5,
                    41,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:05:41Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    5,
                    41,
                    1,
                    233,
                    0
                ],
                "title": "Beyond English-Centric LLMs: What Language Do Multilingual Language\n  Models Think in?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond English-Centric LLMs: What Language Do Multilingual Language\n  Models Think in?"
                },
                "summary": "In this study, we investigate whether non-English-centric LLMs, despite their\nstrong performance, `think' in their respective dominant language: more\nprecisely, `think' refers to how the representations of intermediate layers,\nwhen un-embedded into the vocabulary space, exhibit higher probabilities for\ncertain dominant languages during generation. We term such languages as\ninternal $\\textbf{latent languages}$.\n  We examine the latent language of three typical categories of models for\nJapanese processing: Llama2, an English-centric model; Swallow, an\nEnglish-centric model with continued pre-training in Japanese; and LLM-jp, a\nmodel pre-trained on balanced English and Japanese corpora. Our empirical\nfindings reveal that, unlike Llama2 which relies exclusively on English as the\ninternal latent language, Japanese-specific Swallow and LLM-jp employ both\nJapanese and English, exhibiting dual internal latent languages. For any given\ntarget language, the model preferentially activates the latent language most\nclosely related to it. In addition, we explore how intermediate layers respond\nto questions involving cultural conflicts between latent internal and target\noutput languages. We further explore how the language identity shifts across\nlayers while keeping consistent semantic meaning reflected in the intermediate\nlayer representations.\n  This study deepens the understanding of non-English-centric large language\nmodels, highlighting the intricate dynamics of language representation within\ntheir intermediate layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether non-English-centric LLMs, despite their\nstrong performance, `think' in their respective dominant language: more\nprecisely, `think' refers to how the representations of intermediate layers,\nwhen un-embedded into the vocabulary space, exhibit higher probabilities for\ncertain dominant languages during generation. We term such languages as\ninternal $\\textbf{latent languages}$.\n  We examine the latent language of three typical categories of models for\nJapanese processing: Llama2, an English-centric model; Swallow, an\nEnglish-centric model with continued pre-training in Japanese; and LLM-jp, a\nmodel pre-trained on balanced English and Japanese corpora. Our empirical\nfindings reveal that, unlike Llama2 which relies exclusively on English as the\ninternal latent language, Japanese-specific Swallow and LLM-jp employ both\nJapanese and English, exhibiting dual internal latent languages. For any given\ntarget language, the model preferentially activates the latent language most\nclosely related to it. In addition, we explore how intermediate layers respond\nto questions involving cultural conflicts between latent internal and target\noutput languages. We further explore how the language identity shifts across\nlayers while keeping consistent semantic meaning reflected in the intermediate\nlayer representations.\n  This study deepens the understanding of non-English-centric large language\nmodels, highlighting the intricate dynamics of language representation within\ntheir intermediate layers."
                },
                "authors": [
                    {
                        "name": "Chengzhi Zhong"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Qianying Liu"
                    },
                    {
                        "name": "Junfeng Jiang"
                    },
                    {
                        "name": "Zhen Wan"
                    },
                    {
                        "name": "Chenhui Chu"
                    },
                    {
                        "name": "Yugo Murawaki"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    }
                ],
                "author_detail": {
                    "name": "Sadao Kurohashi"
                },
                "author": "Sadao Kurohashi",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10794v1",
                "updated": "2024-08-20T12:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    38,
                    34,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T12:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    38,
                    34,
                    1,
                    233,
                    0
                ],
                "title": "Tapping in a Remote Vehicle's onboard LLM to Complement the Ego\n  Vehicle's Field-of-View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapping in a Remote Vehicle's onboard LLM to Complement the Ego\n  Vehicle's Field-of-View"
                },
                "summary": "Today's advanced automotive systems are turning into intelligent\nCyber-Physical Systems (CPS), bringing computational intelligence to their\ncyber-physical context. Such systems power advanced driver assistance systems\n(ADAS) that observe a vehicle's surroundings for their functionality. However,\nsuch ADAS have clear limitations in scenarios when the direct line-of-sight to\nsurrounding objects is occluded, like in urban areas. Imagine now automated\ndriving (AD) systems that ideally could benefit from other vehicles'\nfield-of-view in such occluded situations to increase traffic safety if, for\nexample, locations about pedestrians can be shared across vehicles. Current\nliterature suggests vehicle-to-infrastructure (V2I) via roadside units (RSUs)\nor vehicle-to-vehicle (V2V) communication to address such issues that stream\nsensor or object data between vehicles. When considering the ongoing revolution\nin vehicle system architectures towards powerful, centralized processing units\nwith hardware accelerators, foreseeing the onboard presence of large language\nmodels (LLMs) to improve the passengers' comfort when using voice assistants\nbecomes a reality. We are suggesting and evaluating a concept to complement the\nego vehicle's field-of-view (FOV) with another vehicle's FOV by tapping into\ntheir onboard LLM to let the machines have a dialogue about what the other\nvehicle ``sees''. Our results show that very recent versions of LLMs, such as\nGPT-4V and GPT-4o, understand a traffic situation to an impressive level of\ndetail, and hence, they can be used even to spot traffic participants. However,\nbetter prompts are needed to improve the detection quality and future work is\nneeded towards a standardised message interchange format between vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's advanced automotive systems are turning into intelligent\nCyber-Physical Systems (CPS), bringing computational intelligence to their\ncyber-physical context. Such systems power advanced driver assistance systems\n(ADAS) that observe a vehicle's surroundings for their functionality. However,\nsuch ADAS have clear limitations in scenarios when the direct line-of-sight to\nsurrounding objects is occluded, like in urban areas. Imagine now automated\ndriving (AD) systems that ideally could benefit from other vehicles'\nfield-of-view in such occluded situations to increase traffic safety if, for\nexample, locations about pedestrians can be shared across vehicles. Current\nliterature suggests vehicle-to-infrastructure (V2I) via roadside units (RSUs)\nor vehicle-to-vehicle (V2V) communication to address such issues that stream\nsensor or object data between vehicles. When considering the ongoing revolution\nin vehicle system architectures towards powerful, centralized processing units\nwith hardware accelerators, foreseeing the onboard presence of large language\nmodels (LLMs) to improve the passengers' comfort when using voice assistants\nbecomes a reality. We are suggesting and evaluating a concept to complement the\nego vehicle's field-of-view (FOV) with another vehicle's FOV by tapping into\ntheir onboard LLM to let the machines have a dialogue about what the other\nvehicle ``sees''. Our results show that very recent versions of LLMs, such as\nGPT-4V and GPT-4o, understand a traffic situation to an impressive level of\ndetail, and hence, they can be used even to spot traffic participants. However,\nbetter prompts are needed to improve the detection quality and future work is\nneeded towards a standardised message interchange format between vehicles."
                },
                "authors": [
                    {
                        "name": "Malsha Ashani Mahawatta Dona"
                    },
                    {
                        "name": "Beatriz Cabrero-Daniel"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Christian Berger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Berger"
                },
                "author": "Christian Berger",
                "arxiv_comment": "50th Euromicro Conference Series on Software Engineering and Advanced\n  Applications (SEAA) 2024 - WiP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00868v3",
                "updated": "2024-08-20T12:37:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    37,
                    2,
                    1,
                    233,
                    0
                ],
                "published": "2024-03-01T04:39:16Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    4,
                    39,
                    16,
                    4,
                    61,
                    0
                ],
                "title": "SoftTiger: A Clinical Foundation Model for Healthcare Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftTiger: A Clinical Foundation Model for Healthcare Workflows"
                },
                "summary": "We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry."
                },
                "authors": [
                    {
                        "name": "Ye Chen"
                    },
                    {
                        "name": "Igor Couto"
                    },
                    {
                        "name": "Wei Cai"
                    },
                    {
                        "name": "Cong Fu"
                    },
                    {
                        "name": "Bruno Dorneles"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Dorneles"
                },
                "author": "Bruno Dorneles",
                "arxiv_comment": "Accepted to AAAI 2024 Spring Symposium on Clinical Foundation Models,\n  Stanford University, Stanford, California",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04472v2",
                "updated": "2024-08-20T12:36:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    36,
                    6,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-08T14:02:45Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    14,
                    2,
                    45,
                    3,
                    221,
                    0
                ],
                "title": "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for\n  Competitive Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for\n  Competitive Debate"
                },
                "summary": "Competitive debate is a complex task of computational argumentation. Large\nLanguage Models (LLMs) suffer from hallucinations and lack competitiveness in\nthis field. To address these challenges, we introduce Agent for Debate\n(Agent4Debate), a dynamic multi-agent framework based on LLMs designed to\nenhance their capabilities in competitive debate. Drawing inspiration from\nhuman behavior in debate preparation and execution, Agent4Debate employs a\ncollaborative architecture where four specialized agents, involving Searcher,\nAnalyzer, Writer, and Reviewer, dynamically interact and cooperate. These\nagents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Competitive\nDebate Arena, comprising 66 carefully selected Chinese debate motions. We\nrecruit ten experienced human debaters and collect records of 200 debates\ninvolving Agent4Debate, baseline models, and humans. The evaluation employs the\nDebatrix automatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive debate is a complex task of computational argumentation. Large\nLanguage Models (LLMs) suffer from hallucinations and lack competitiveness in\nthis field. To address these challenges, we introduce Agent for Debate\n(Agent4Debate), a dynamic multi-agent framework based on LLMs designed to\nenhance their capabilities in competitive debate. Drawing inspiration from\nhuman behavior in debate preparation and execution, Agent4Debate employs a\ncollaborative architecture where four specialized agents, involving Searcher,\nAnalyzer, Writer, and Reviewer, dynamically interact and cooperate. These\nagents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Competitive\nDebate Arena, comprising 66 carefully selected Chinese debate motions. We\nrecruit ten experienced human debaters and collect records of 200 debates\ninvolving Agent4Debate, baseline models, and humans. The evaluation employs the\nDebatrix automatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure."
                },
                "authors": [
                    {
                        "name": "Yiqun Zhang"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Kaisong Song"
                    }
                ],
                "author_detail": {
                    "name": "Kaisong Song"
                },
                "author": "Kaisong Song",
                "arxiv_comment": "12 pages (including appendix), 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09713v2",
                "updated": "2024-08-20T12:22:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    22,
                    16,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T06:05:24Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    5,
                    24,
                    0,
                    232,
                    0
                ],
                "title": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation"
                },
                "summary": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices."
                },
                "authors": [
                    {
                        "name": "Haijin Wang"
                    },
                    {
                        "name": "Mianrong Zhang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Nan Shang"
                    },
                    {
                        "name": "Shangheng Yao"
                    },
                    {
                        "name": "Fushuan Wen"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v2",
                "updated": "2024-08-21T06:48:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    48,
                    16,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "29 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10764v1",
                "updated": "2024-08-20T12:00:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    0,
                    35,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T12:00:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    0,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion\n  for Efficient Inference Intervention in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion\n  for Efficient Inference Intervention in Large Language Model"
                },
                "summary": "Transformer-based large language models (LLMs) exhibit limitations such as\ngenerating unsafe responses, unreliable reasoning, etc. Existing inference\nintervention approaches attempt to mitigate these issues by finetuning\nadditional models to produce calibration signals (such as rewards) that guide\nthe LLM's decoding process. However, this solution introduces substantial time\nand space overhead due to the separate models required. This work proposes\nNon-disruptive parameters insertion (Otter), inserting extra parameters into\nthe transformer architecture to predict calibration signals along with the\noriginal LLM output. Otter offers state-of-the-art performance on multiple\ndemanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time.\nFurthermore, Otter seamlessly integrates with existing inference engines,\nrequiring only a one-line code change, and the original model response remains\naccessible after the parameter insertion. Our code is publicly available at\n\\url{https://github.com/chenhan97/Otter}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) exhibit limitations such as\ngenerating unsafe responses, unreliable reasoning, etc. Existing inference\nintervention approaches attempt to mitigate these issues by finetuning\nadditional models to produce calibration signals (such as rewards) that guide\nthe LLM's decoding process. However, this solution introduces substantial time\nand space overhead due to the separate models required. This work proposes\nNon-disruptive parameters insertion (Otter), inserting extra parameters into\nthe transformer architecture to predict calibration signals along with the\noriginal LLM output. Otter offers state-of-the-art performance on multiple\ndemanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time.\nFurthermore, Otter seamlessly integrates with existing inference engines,\nrequiring only a one-line code change, and the original model response remains\naccessible after the parameter insertion. Our code is publicly available at\n\\url{https://github.com/chenhan97/Otter}"
                },
                "authors": [
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Ru Peng"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14353v2",
                "updated": "2024-08-20T11:56:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    56,
                    3,
                    1,
                    233,
                    0
                ],
                "published": "2023-09-21T08:05:28Z",
                "published_parsed": [
                    2023,
                    9,
                    21,
                    8,
                    5,
                    28,
                    3,
                    264,
                    0
                ],
                "title": "Limited Communications Distributed Optimization via Deep Unfolded\n  Distributed ADMM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited Communications Distributed Optimization via Deep Unfolded\n  Distributed ADMM"
                },
                "summary": "Distributed optimization is a fundamental framework for collaborative\ninference and decision making in decentralized multi-agent systems. The\noperation is modeled as the joint minimization of a shared objective which\ntypically depends on observations gathered locally by each agent. Distributed\noptimization algorithms, such as the common D-ADMM, tackle this task by\niteratively combining local computations and message exchanges. One of the main\nchallenges associated with distributed optimization, and particularly with\nD-ADMM, is that it requires a large number of communications, i.e., messages\nexchanged between the agents, to reach consensus. This can make D-ADMM costly\nin power, latency, and channel resources. In this work we propose unfolded\nD-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM\nto operate reliably with a predefined and small number of messages exchanged by\neach agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while\nleveraging data to tune the hyperparameters of each iteration of the algorithm.\nThese hyperparameters can either be agent-specific, aiming at achieving the\nbest performance within a fixed number of iterations over a given network, or\nshared among the agents, allowing to learn to distributedly optimize over\ndifferent networks. For both settings, our unfolded D-ADMM operates with\nlimited communications, while preserving the interpretability and flexibility\nof the original D-ADMM algorithm. We specialize unfolded D-ADMM for two\nrepresentative settings: a distributed estimation task, considering a sparse\nrecovery setup, and a distributed learning scenario, where multiple agents\ncollaborate in learning a machine learning model. Our numerical results\ndemonstrate that the proposed approach dramatically reduces the number of\ncommunications utilized by D-ADMM, without compromising on its performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed optimization is a fundamental framework for collaborative\ninference and decision making in decentralized multi-agent systems. The\noperation is modeled as the joint minimization of a shared objective which\ntypically depends on observations gathered locally by each agent. Distributed\noptimization algorithms, such as the common D-ADMM, tackle this task by\niteratively combining local computations and message exchanges. One of the main\nchallenges associated with distributed optimization, and particularly with\nD-ADMM, is that it requires a large number of communications, i.e., messages\nexchanged between the agents, to reach consensus. This can make D-ADMM costly\nin power, latency, and channel resources. In this work we propose unfolded\nD-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM\nto operate reliably with a predefined and small number of messages exchanged by\neach agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while\nleveraging data to tune the hyperparameters of each iteration of the algorithm.\nThese hyperparameters can either be agent-specific, aiming at achieving the\nbest performance within a fixed number of iterations over a given network, or\nshared among the agents, allowing to learn to distributedly optimize over\ndifferent networks. For both settings, our unfolded D-ADMM operates with\nlimited communications, while preserving the interpretability and flexibility\nof the original D-ADMM algorithm. We specialize unfolded D-ADMM for two\nrepresentative settings: a distributed estimation task, considering a sparse\nrecovery setup, and a distributed learning scenario, where multiple agents\ncollaborate in learning a machine learning model. Our numerical results\ndemonstrate that the proposed approach dramatically reduces the number of\ncommunications utilized by D-ADMM, without compromising on its performance."
                },
                "authors": [
                    {
                        "name": "Yoav Noah"
                    },
                    {
                        "name": "Nir Shlezinger"
                    }
                ],
                "author_detail": {
                    "name": "Nir Shlezinger"
                },
                "author": "Nir Shlezinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10762v1",
                "updated": "2024-08-20T11:52:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    52,
                    21,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:52:21Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    52,
                    21,
                    1,
                    233,
                    0
                ],
                "title": "Sparse Regression for Discovery of Constitutive Models from Oscillatory\n  Shear Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Regression for Discovery of Constitutive Models from Oscillatory\n  Shear Measurements"
                },
                "summary": "We propose sparse regression as an alternative to neural networks for the\ndiscovery of parsimonious constitutive models (CMs) from oscillatory shear\nexperiments. Symmetry and frame-invariance are strictly imposed by using tensor\nbasis functions to isolate and describe unknown nonlinear terms in the CMs. We\ngenerate synthetic experimental data using the Giesekus and Phan-Thien Tanner\nCMs, and consider two different scenarios. In the complete information\nscenario, we assume that the shear stress, along with the first and second\nnormal stress differences, is measured. This leads to a sparse linear\nregression problem that can be solved efficiently using $l_1$ regularization.\nIn the partial information scenario, we assume that only shear stress data is\navailable. This leads to a more challenging sparse nonlinear regression\nproblem, for which we propose a greedy two-stage algorithm. In both scenarios,\nthe proposed methods fit and interpolate the training data remarkably well.\nPredictions of the inferred CMs extrapolate satisfactorily beyond the range of\ntraining data for oscillatory shear. They also extrapolate reasonably well to\nflow conditions like startup of steady and uniaxial extension that are not used\nin the identification of CMs. We discuss ramifications for experimental design,\npotential algorithmic improvements, and implications of the non-uniqueness of\nCMs inferred from partial information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose sparse regression as an alternative to neural networks for the\ndiscovery of parsimonious constitutive models (CMs) from oscillatory shear\nexperiments. Symmetry and frame-invariance are strictly imposed by using tensor\nbasis functions to isolate and describe unknown nonlinear terms in the CMs. We\ngenerate synthetic experimental data using the Giesekus and Phan-Thien Tanner\nCMs, and consider two different scenarios. In the complete information\nscenario, we assume that the shear stress, along with the first and second\nnormal stress differences, is measured. This leads to a sparse linear\nregression problem that can be solved efficiently using $l_1$ regularization.\nIn the partial information scenario, we assume that only shear stress data is\navailable. This leads to a more challenging sparse nonlinear regression\nproblem, for which we propose a greedy two-stage algorithm. In both scenarios,\nthe proposed methods fit and interpolate the training data remarkably well.\nPredictions of the inferred CMs extrapolate satisfactorily beyond the range of\ntraining data for oscillatory shear. They also extrapolate reasonably well to\nflow conditions like startup of steady and uniaxial extension that are not used\nin the identification of CMs. We discuss ramifications for experimental design,\npotential algorithmic improvements, and implications of the non-uniqueness of\nCMs inferred from partial information."
                },
                "authors": [
                    {
                        "name": "Sachin Shanbhag"
                    },
                    {
                        "name": "Gordon Erlebacher"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Erlebacher"
                },
                "author": "Gordon Erlebacher",
                "arxiv_comment": "45 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14405v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14405v4",
                "updated": "2024-08-20T11:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    45,
                    34,
                    1,
                    233,
                    0
                ],
                "published": "2023-05-23T12:03:51Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    12,
                    3,
                    51,
                    1,
                    143,
                    0
                ],
                "title": "NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix\n  Operations for Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix\n  Operations for Efficient Inference"
                },
                "summary": "The inherent diversity of computation types within the deep neural network\n(DNN) models often requires a variety of specialized units in hardware\nprocessors, which limits computational efficiency, increasing both inference\nlatency and power consumption, especially when the hardware processor needs to\nsupport and execute different neural networks. In this study, we introduce\nNeuralMatrix, which elastically transforms the computations of entire DNNs into\nlinear matrix operations. This transformation allows seamless execution of\nvarious DNN models all with matrix operations and paves the way for running\nversatile DNN models with a single General Matrix Multiplication (GEMM)\naccelerator.Extensive experiments with both CNN and transformer-based models\ndemonstrate the potential of NeuralMatrix to accurately and efficiently execute\na wide range of DNN models, achieving 2.17-38.72 times computation efficiency\n(i.e., throughput per power) compared to CPUs, GPUs, and SoC platforms. This\nlevel of efficiency is usually only attainable with the accelerator designed\nfor a specific neural network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent diversity of computation types within the deep neural network\n(DNN) models often requires a variety of specialized units in hardware\nprocessors, which limits computational efficiency, increasing both inference\nlatency and power consumption, especially when the hardware processor needs to\nsupport and execute different neural networks. In this study, we introduce\nNeuralMatrix, which elastically transforms the computations of entire DNNs into\nlinear matrix operations. This transformation allows seamless execution of\nvarious DNN models all with matrix operations and paves the way for running\nversatile DNN models with a single General Matrix Multiplication (GEMM)\naccelerator.Extensive experiments with both CNN and transformer-based models\ndemonstrate the potential of NeuralMatrix to accurately and efficiently execute\na wide range of DNN models, achieving 2.17-38.72 times computation efficiency\n(i.e., throughput per power) compared to CPUs, GPUs, and SoC platforms. This\nlevel of efficiency is usually only attainable with the accelerator designed\nfor a specific neural network."
                },
                "authors": [
                    {
                        "name": "Ruiqi Sun"
                    },
                    {
                        "name": "Siwei Ye"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Jianzhe Lin"
                    },
                    {
                        "name": "Yiran Li"
                    },
                    {
                        "name": "An Zou"
                    }
                ],
                "author_detail": {
                    "name": "An Zou"
                },
                "author": "An Zou",
                "arxiv_comment": "9 pages, 8figures, Submitted to The 39th Annual AAAI Conference on\n  Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14405v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14405v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10752v1",
                "updated": "2024-08-20T11:34:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    34,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:34:23Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    34,
                    23,
                    1,
                    233,
                    0
                ],
                "title": "Security Assessment of Hierarchical Federated Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Assessment of Hierarchical Federated Deep Learning"
                },
                "summary": "Hierarchical federated learning (HFL) is a promising distributed deep\nlearning model training paradigm, but it has crucial security concerns arising\nfrom adversarial attacks. This research investigates and assesses the security\nof HFL using a novel methodology by focusing on its resilience against\nadversarial attacks inference-time and training-time. Through a series of\nextensive experiments across diverse datasets and attack scenarios, we uncover\nthat HFL demonstrates robustness against untargeted training-time attacks due\nto its hierarchical structure. However, targeted attacks, particularly backdoor\nattacks, exploit this architecture, especially when malicious clients are\npositioned in the overlapping coverage areas of edge servers. Consequently, HFL\nshows a dual nature in its resilience, showcasing its capability to recover\nfrom attacks thanks to its hierarchical aggregation that strengthens its\nsuitability for adversarial training, thereby reinforcing its resistance\nagainst inference-time attacks. These insights underscore the necessity for\nbalanced security strategies in HFL systems, leveraging their inherent\nstrengths while effectively mitigating vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical federated learning (HFL) is a promising distributed deep\nlearning model training paradigm, but it has crucial security concerns arising\nfrom adversarial attacks. This research investigates and assesses the security\nof HFL using a novel methodology by focusing on its resilience against\nadversarial attacks inference-time and training-time. Through a series of\nextensive experiments across diverse datasets and attack scenarios, we uncover\nthat HFL demonstrates robustness against untargeted training-time attacks due\nto its hierarchical structure. However, targeted attacks, particularly backdoor\nattacks, exploit this architecture, especially when malicious clients are\npositioned in the overlapping coverage areas of edge servers. Consequently, HFL\nshows a dual nature in its resilience, showcasing its capability to recover\nfrom attacks thanks to its hierarchical aggregation that strengthens its\nsuitability for adversarial training, thereby reinforcing its resistance\nagainst inference-time attacks. These insights underscore the necessity for\nbalanced security strategies in HFL systems, leveraging their inherent\nstrengths while effectively mitigating vulnerabilities."
                },
                "authors": [
                    {
                        "name": "D Alqattan"
                    },
                    {
                        "name": "R Sun"
                    },
                    {
                        "name": "H Liang"
                    },
                    {
                        "name": "G Nicosia"
                    },
                    {
                        "name": "V Snasel"
                    },
                    {
                        "name": "R Ranjan"
                    },
                    {
                        "name": "V Ojha"
                    }
                ],
                "author_detail": {
                    "name": "V Ojha"
                },
                "author": "V Ojha",
                "arxiv_journal_ref": "33rd International Conference on Artificial Neural Networks\n  (ICANN) (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00953v2",
                "updated": "2024-08-20T11:11:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    11,
                    15,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-02T02:30:26Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    2,
                    30,
                    26,
                    3,
                    123,
                    0
                ],
                "title": "Asymptotic Properties of the Distributional Synthetic Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Properties of the Distributional Synthetic Controls"
                },
                "summary": "As an alternative to synthetic control, the distributional Synthetic Control\n(DSC) proposed by Gunsilius (2023) provides estimates for quantile treatment\neffect and thus enabling researchers to comprehensively understand the impact\nof interventions in causal inference. But the asymptotic properties of DSC have\nnot been built. In this paper, we first establish the DSC estimator's\nasymptotic optimality in the essence that the treatment effect estimator given\nby DSC achieves the lowest possible squared prediction error among all\npotential estimators from averaging quantiles of control units. We then\nestablish the convergence rate of the DSC weights. A significant aspect of our\nresearch is that we find the DSC synthesis forms an optimal weighted average,\nparticularly in situations where it is impractical to perfectly fit the treated\nunit's quantiles through the weighted average of the control units' quantiles.\nSimulation results verify our theoretical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an alternative to synthetic control, the distributional Synthetic Control\n(DSC) proposed by Gunsilius (2023) provides estimates for quantile treatment\neffect and thus enabling researchers to comprehensively understand the impact\nof interventions in causal inference. But the asymptotic properties of DSC have\nnot been built. In this paper, we first establish the DSC estimator's\nasymptotic optimality in the essence that the treatment effect estimator given\nby DSC achieves the lowest possible squared prediction error among all\npotential estimators from averaging quantiles of control units. We then\nestablish the convergence rate of the DSC weights. A significant aspect of our\nresearch is that we find the DSC synthesis forms an optimal weighted average,\nparticularly in situations where it is impractical to perfectly fit the treated\nunit's quantiles through the weighted average of the control units' quantiles.\nSimulation results verify our theoretical insights."
                },
                "authors": [
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Xiaomeng Zhang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Zhang"
                },
                "author": "Xinyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20132v3",
                "updated": "2024-08-20T11:06:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    6,
                    9,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-30T15:10:59Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    15,
                    10,
                    59,
                    3,
                    151,
                    0
                ],
                "title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs."
                },
                "authors": [
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Bäck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bäck"
                },
                "author": "Thomas Bäck",
                "arxiv_comment": "Submitted to IEEE TEVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10729v1",
                "updated": "2024-08-20T10:57:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    57,
                    34,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:57:34Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    57,
                    34,
                    1,
                    233,
                    0
                ],
                "title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Large Language Models for Scientific Text: A Review"
                },
                "summary": "Large language models (LLMs) have ushered in a new era for processing complex\ninformation in various fields, including science. The increasing amount of\nscientific literature allows these models to acquire and understand scientific\nknowledge effectively, thus improving their performance in a wide range of\ntasks. Due to the power of LLMs, they require extremely expensive computational\nresources, intense amounts of data, and training time. Therefore, in recent\nyears, researchers have proposed various methodologies to make scientific LLMs\nmore affordable. The most well-known approaches align in two directions. It can\nbe either focusing on the size of the models or enhancing the quality of data.\nTo date, a comprehensive review of these two families of methods has not yet\nbeen undertaken. In this paper, we (I) summarize the current advances in the\nemerging abilities of LLMs into more accessible AI solutions for science, and\n(II) investigate the challenges and opportunities of developing affordable\nsolutions for scientific domains using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have ushered in a new era for processing complex\ninformation in various fields, including science. The increasing amount of\nscientific literature allows these models to acquire and understand scientific\nknowledge effectively, thus improving their performance in a wide range of\ntasks. Due to the power of LLMs, they require extremely expensive computational\nresources, intense amounts of data, and training time. Therefore, in recent\nyears, researchers have proposed various methodologies to make scientific LLMs\nmore affordable. The most well-known approaches align in two directions. It can\nbe either focusing on the size of the models or enhancing the quality of data.\nTo date, a comprehensive review of these two families of methods has not yet\nbeen undertaken. In this paper, we (I) summarize the current advances in the\nemerging abilities of LLMs into more accessible AI solutions for science, and\n(II) investigate the challenges and opportunities of developing affordable\nsolutions for scientific domains using LLMs."
                },
                "authors": [
                    {
                        "name": "Huy Quoc To"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Guangyan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Guangyan Huang"
                },
                "author": "Guangyan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19056v2",
                "updated": "2024-08-20T10:56:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    56,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-03-27T23:45:31Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    23,
                    45,
                    31,
                    2,
                    87,
                    0
                ],
                "title": "CAUSE: Counterfactual Assessment of User Satisfaction Estimation in\n  Task-Oriented Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAUSE: Counterfactual Assessment of User Satisfaction Estimation in\n  Task-Oriented Dialogue Systems"
                },
                "summary": "An important unexplored aspect in previous work on user satisfaction\nestimation for Task-Oriented Dialogue (TOD) systems is their evaluation in\nterms of robustness for the identification of user dissatisfaction: current\nbenchmarks for user satisfaction estimation in TOD systems are highly skewed\ntowards dialogues for which the user is satisfied. The effect of having a more\nbalanced set of satisfaction labels on performance is unknown. However,\nbalancing the data with more dissatisfactory dialogue samples requires further\ndata collection and human annotation, which is costly and time-consuming. In\nthis work, we leverage large language models (LLMs) and unlock their ability to\ngenerate satisfaction-aware counterfactual dialogues to augment the set of\noriginal dialogues of a test collection. We gather human annotations to ensure\nthe reliability of the generated samples. We evaluate two open-source LLMs as\nuser satisfaction estimators on our augmented collection against\nstate-of-the-art fine-tuned models. Our experiments show that when used as\nfew-shot user satisfaction estimators, open-source LLMs show higher robustness\nto the increase in the number of dissatisfaction labels in the test collection\nthan the fine-tuned state-of-the-art models. Our results shed light on the need\nfor data augmentation approaches for user satisfaction estimation in TOD\nsystems. We release our aligned counterfactual dialogues, which are curated by\nhuman annotation, to facilitate further research on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important unexplored aspect in previous work on user satisfaction\nestimation for Task-Oriented Dialogue (TOD) systems is their evaluation in\nterms of robustness for the identification of user dissatisfaction: current\nbenchmarks for user satisfaction estimation in TOD systems are highly skewed\ntowards dialogues for which the user is satisfied. The effect of having a more\nbalanced set of satisfaction labels on performance is unknown. However,\nbalancing the data with more dissatisfactory dialogue samples requires further\ndata collection and human annotation, which is costly and time-consuming. In\nthis work, we leverage large language models (LLMs) and unlock their ability to\ngenerate satisfaction-aware counterfactual dialogues to augment the set of\noriginal dialogues of a test collection. We gather human annotations to ensure\nthe reliability of the generated samples. We evaluate two open-source LLMs as\nuser satisfaction estimators on our augmented collection against\nstate-of-the-art fine-tuned models. Our experiments show that when used as\nfew-shot user satisfaction estimators, open-source LLMs show higher robustness\nto the increase in the number of dissatisfaction labels in the test collection\nthan the fine-tuned state-of-the-art models. Our results shed light on the need\nfor data augmentation approaches for user satisfaction estimation in TOD\nsystems. We release our aligned counterfactual dialogues, which are curated by\nhuman annotation, to facilitate further research on this topic."
                },
                "authors": [
                    {
                        "name": "Amin Abolghasemi"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Arian Askari"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10724v1",
                "updated": "2024-08-20T10:45:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    45,
                    36,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:45:36Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    45,
                    36,
                    1,
                    233,
                    0
                ],
                "title": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian"
                },
                "summary": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages."
                },
                "authors": [
                    {
                        "name": "Cem Üyük"
                    },
                    {
                        "name": "Danica Rovó"
                    },
                    {
                        "name": "Shaghayegh Kolli"
                    },
                    {
                        "name": "Rabia Varol"
                    },
                    {
                        "name": "Georg Groh"
                    },
                    {
                        "name": "Daryna Dementieva"
                    }
                ],
                "author_detail": {
                    "name": "Daryna Dementieva"
                },
                "author": "Daryna Dementieva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10722v1",
                "updated": "2024-08-20T10:44:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    44,
                    29,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:44:29Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    44,
                    29,
                    1,
                    233,
                    0
                ],
                "title": "MEGen: Generative Backdoor in Large Language Models via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEGen: Generative Backdoor in Large Language Models via Model Editing"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities. Their\npowerful generative abilities enable flexible responses based on various\nqueries or instructions. Emerging as widely adopted generalists for diverse\ntasks, LLMs are still vulnerable to backdoors. This paper proposes an\nediting-based generative backdoor, named MEGen, aiming to create a customized\nbackdoor for NLP tasks with the least side effects. In our approach, we first\nleverage a language model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly embed a backdoor\ninto an LLM. By adjusting a small set of local parameters with a mini-batch of\nsamples, MEGen significantly enhances time efficiency and achieves high\nrobustness. Experimental results indicate that our backdoor attack strategy\nachieves a high attack success rate on poison data while maintaining the\nmodel's performance on clean data. Notably, the backdoored model, when\ntriggered, can freely output pre-set dangerous information while successfully\ncompleting downstream tasks. This suggests that future LLM applications could\nbe guided to deliver certain dangerous information, thus altering the LLM's\ngenerative style. We believe this approach provides insights for future LLM\napplications and the execution of backdoor attacks on conversational AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities. Their\npowerful generative abilities enable flexible responses based on various\nqueries or instructions. Emerging as widely adopted generalists for diverse\ntasks, LLMs are still vulnerable to backdoors. This paper proposes an\nediting-based generative backdoor, named MEGen, aiming to create a customized\nbackdoor for NLP tasks with the least side effects. In our approach, we first\nleverage a language model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly embed a backdoor\ninto an LLM. By adjusting a small set of local parameters with a mini-batch of\nsamples, MEGen significantly enhances time efficiency and achieves high\nrobustness. Experimental results indicate that our backdoor attack strategy\nachieves a high attack success rate on poison data while maintaining the\nmodel's performance on clean data. Notably, the backdoored model, when\ntriggered, can freely output pre-set dangerous information while successfully\ncompleting downstream tasks. This suggests that future LLM applications could\nbe guided to deliver certain dangerous information, thus altering the LLM's\ngenerative style. We believe this approach provides insights for future LLM\napplications and the execution of backdoor attacks on conversational AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10718v1",
                "updated": "2024-08-20T10:40:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    40,
                    35,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:40:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    40,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?"
                },
                "summary": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our benchmark will be available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our benchmark will be available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhao"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Annan Li"
                    },
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10715v1",
                "updated": "2024-08-20T10:31:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    31,
                    36,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:31:36Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    31,
                    36,
                    1,
                    233,
                    0
                ],
                "title": "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated\n  Privacy-Preserving Physician Letter Generation in Radiation Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated\n  Privacy-Preserving Physician Letter Generation in Radiation Oncology"
                },
                "summary": "Generating physician letters is a time-consuming task in daily clinical\npractice. This study investigates local fine-tuning of large language models\n(LLMs), specifically LLaMA models, for physician letter generation in a\nprivacy-preserving manner within the field of radiation oncology. Our findings\ndemonstrate that base LLaMA models, without fine-tuning, are inadequate for\neffectively generating physician letters. The QLoRA algorithm provides an\nefficient method for local intra-institutional fine-tuning of LLMs with limited\ncomputational resources (i.e., a single 48 GB GPU workstation within the\nhospital). The fine-tuned LLM successfully learns radiation oncology-specific\ninformation and generates physician letters in an institution-specific style.\nROUGE scores of the generated summary reports highlight the superiority of the\n8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician\nevaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has\nlimited capacity to generate content beyond the provided input data, it\nsuccessfully generates salutations, diagnoses and treatment histories,\nrecommendations for further treatment, and planned schedules. Overall, clinical\nbenefit was rated highly by the clinical experts (average score of 3.44 on a\n4-point scale). With careful physician review and correction, automated\nLLM-based physician letter generation has significant practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating physician letters is a time-consuming task in daily clinical\npractice. This study investigates local fine-tuning of large language models\n(LLMs), specifically LLaMA models, for physician letter generation in a\nprivacy-preserving manner within the field of radiation oncology. Our findings\ndemonstrate that base LLaMA models, without fine-tuning, are inadequate for\neffectively generating physician letters. The QLoRA algorithm provides an\nefficient method for local intra-institutional fine-tuning of LLMs with limited\ncomputational resources (i.e., a single 48 GB GPU workstation within the\nhospital). The fine-tuned LLM successfully learns radiation oncology-specific\ninformation and generates physician letters in an institution-specific style.\nROUGE scores of the generated summary reports highlight the superiority of the\n8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician\nevaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has\nlimited capacity to generate content beyond the provided input data, it\nsuccessfully generates salutations, diagnoses and treatment histories,\nrecommendations for further treatment, and planned schedules. Overall, clinical\nbenefit was rated highly by the clinical experts (average score of 3.44 on a\n4-point scale). With careful physician review and correction, automated\nLLM-based physician letter generation has significant practical value."
                },
                "authors": [
                    {
                        "name": "Yihao Hou"
                    },
                    {
                        "name": "Christoph Bert"
                    },
                    {
                        "name": "Ahmed Gomaa"
                    },
                    {
                        "name": "Godehard Lahmer"
                    },
                    {
                        "name": "Daniel Hoefler"
                    },
                    {
                        "name": "Thomas Weissmann"
                    },
                    {
                        "name": "Raphaela Voigt"
                    },
                    {
                        "name": "Philipp Schubert"
                    },
                    {
                        "name": "Charlotte Schmitter"
                    },
                    {
                        "name": "Alina Depardon"
                    },
                    {
                        "name": "Sabine Semrau"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Rainer Fietkau"
                    },
                    {
                        "name": "Yixing Huang"
                    },
                    {
                        "name": "Florian Putz"
                    }
                ],
                "author_detail": {
                    "name": "Florian Putz"
                },
                "author": "Florian Putz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03627v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03627v4",
                "updated": "2024-08-20T10:27:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    27,
                    56,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-04T04:30:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    30,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems."
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "20 pages",
                "arxiv_journal_ref": "KnowledgeNLP@ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03627v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03627v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10711v1",
                "updated": "2024-08-20T10:26:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    26,
                    2,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:26:02Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    26,
                    2,
                    1,
                    233,
                    0
                ],
                "title": "Investigating Context Effects in Similarity Judgements in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Context Effects in Similarity Judgements in Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionised the capability of AI models\nin comprehending and generating natural language text. They are increasingly\nbeing used to empower and deploy agents in real-world scenarios, which make\ndecisions and take actions based on their understanding of the context.\nTherefore researchers, policy makers and enterprises alike are working towards\nensuring that the decisions made by these agents align with human values and\nuser expectations. That being said, human values and decisions are not always\nstraightforward to measure and are subject to different cognitive biases. There\nis a vast section of literature in Behavioural Science which studies biases in\nhuman judgements. In this work we report an ongoing investigation on alignment\nof LLMs with human judgements affected by order bias. Specifically, we focus on\na famous human study which showed evidence of order effects in similarity\njudgements, and replicate it with various popular LLMs. We report the different\nsettings where LLMs exhibit human-like order effect bias and discuss the\nimplications of these findings to inform the design and development of LLM\nbased applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionised the capability of AI models\nin comprehending and generating natural language text. They are increasingly\nbeing used to empower and deploy agents in real-world scenarios, which make\ndecisions and take actions based on their understanding of the context.\nTherefore researchers, policy makers and enterprises alike are working towards\nensuring that the decisions made by these agents align with human values and\nuser expectations. That being said, human values and decisions are not always\nstraightforward to measure and are subject to different cognitive biases. There\nis a vast section of literature in Behavioural Science which studies biases in\nhuman judgements. In this work we report an ongoing investigation on alignment\nof LLMs with human judgements affected by order bias. Specifically, we focus on\na famous human study which showed evidence of order effects in similarity\njudgements, and replicate it with various popular LLMs. We report the different\nsettings where LLMs exhibit human-like order effect bias and discuss the\nimplications of these findings to inform the design and development of LLM\nbased applications."
                },
                "authors": [
                    {
                        "name": "Sagar Uprety"
                    },
                    {
                        "name": "Amit Kumar Jaiswal"
                    },
                    {
                        "name": "Haiming Liu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "Accepted at The First Workshop on AI Behavioral Science (AIBS 2024),\n  held in conjunction with KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02163v2",
                "updated": "2024-08-20T10:00:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    0,
                    28,
                    1,
                    233,
                    0
                ],
                "published": "2023-07-05T10:05:54Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    10,
                    5,
                    54,
                    2,
                    186,
                    0
                ],
                "title": "EMORF/S: EM-Based Outlier-Robust Filtering and Smoothing With Correlated\n  Measurement Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMORF/S: EM-Based Outlier-Robust Filtering and Smoothing With Correlated\n  Measurement Noise"
                },
                "summary": "In this article, we consider the problem of outlier-robust state estimation\nwhere the measurement noise can be correlated. Outliers in data arise due to\nmany reasons like sensor malfunctioning, environmental behaviors, communication\nglitches, etc. Moreover, noise correlation emerges in several real-world\napplications e.g. sensor networks, radar data, GPS-based systems, etc. We\nconsider these effects in system modeling which is subsequently used for\ninference. We employ the Expectation-Maximization (EM) framework to derive both\noutlier-resilient filtering and smoothing methods, suitable for online and\noffline estimation respectively. The standard Gaussian filtering and the\nGaussian Rauch-Tung-Striebel (RTS) smoothing results are leveraged to devise\nthe estimators. In addition, Bayesian Cramer-Rao Bounds (BCRBs) for a filter\nand a smoother which can perfectly detect and reject outliers are presented.\nThese serve as useful theoretical benchmarks to gauge the error performance of\ndifferent estimators. Lastly, different numerical experiments, for an\nillustrative target tracking application, are carried out that indicate\nperformance gains compared to similarly engineered state-of-the-art\noutlier-rejecting state estimators. The advantages are in terms of simpler\nimplementation, enhanced estimation quality, and competitive computational\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we consider the problem of outlier-robust state estimation\nwhere the measurement noise can be correlated. Outliers in data arise due to\nmany reasons like sensor malfunctioning, environmental behaviors, communication\nglitches, etc. Moreover, noise correlation emerges in several real-world\napplications e.g. sensor networks, radar data, GPS-based systems, etc. We\nconsider these effects in system modeling which is subsequently used for\ninference. We employ the Expectation-Maximization (EM) framework to derive both\noutlier-resilient filtering and smoothing methods, suitable for online and\noffline estimation respectively. The standard Gaussian filtering and the\nGaussian Rauch-Tung-Striebel (RTS) smoothing results are leveraged to devise\nthe estimators. In addition, Bayesian Cramer-Rao Bounds (BCRBs) for a filter\nand a smoother which can perfectly detect and reject outliers are presented.\nThese serve as useful theoretical benchmarks to gauge the error performance of\ndifferent estimators. Lastly, different numerical experiments, for an\nillustrative target tracking application, are carried out that indicate\nperformance gains compared to similarly engineered state-of-the-art\noutlier-rejecting state estimators. The advantages are in terms of simpler\nimplementation, enhanced estimation quality, and competitive computational\nperformance."
                },
                "authors": [
                    {
                        "name": "Aamir Hussain Chughtai"
                    },
                    {
                        "name": "Muhammad Tahir"
                    },
                    {
                        "name": "Momin Uppal"
                    }
                ],
                "author_detail": {
                    "name": "Momin Uppal"
                },
                "author": "Momin Uppal",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.02163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10703v1",
                "updated": "2024-08-20T09:58:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:58:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Large Language Models for Multimodal Deformable Image Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multimodal Deformable Image Registration"
                },
                "summary": "The challenge of Multimodal Deformable Image Registration (MDIR) lies in the\nconversion and alignment of features between images of different modalities.\nGenerative models (GMs) cannot retain the necessary information enough from the\nsource modality to the target one, while non-GMs struggle to align features\nacross these two modalities. In this paper, we propose a novel coarse-to-fine\nMDIR framework,LLM-Morph, which is applicable to various pre-trained Large\nLanguage Models (LLMs) to solve these concerns by aligning the deep features\nfrom different modal medical images. Specifically, we first utilize a CNN\nencoder to extract deep visual features from cross-modal image pairs, then we\nuse the first adapter to adjust these tokens, and use LoRA in pre-trained LLMs\nto fine-tune their weights, both aimed at eliminating the domain gap between\nthe pre-trained LLMs and the MDIR task. Third, for the alignment of tokens, we\nutilize other four adapters to transform the LLM-encoded tokens into\nmulti-scale visual features, generating multi-scale deformation fields and\nfacilitating the coarse-to-fine MDIR task. Extensive experiments in MR-CT\nAbdomen and SR-Reg Brain datasets demonstrate the effectiveness of our\nframework and the potential of pre-trained LLMs for MDIR task. Our code is\navailabel at: https://github.com/ninjannn/LLM-Morph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of Multimodal Deformable Image Registration (MDIR) lies in the\nconversion and alignment of features between images of different modalities.\nGenerative models (GMs) cannot retain the necessary information enough from the\nsource modality to the target one, while non-GMs struggle to align features\nacross these two modalities. In this paper, we propose a novel coarse-to-fine\nMDIR framework,LLM-Morph, which is applicable to various pre-trained Large\nLanguage Models (LLMs) to solve these concerns by aligning the deep features\nfrom different modal medical images. Specifically, we first utilize a CNN\nencoder to extract deep visual features from cross-modal image pairs, then we\nuse the first adapter to adjust these tokens, and use LoRA in pre-trained LLMs\nto fine-tune their weights, both aimed at eliminating the domain gap between\nthe pre-trained LLMs and the MDIR task. Third, for the alignment of tokens, we\nutilize other four adapters to transform the LLM-encoded tokens into\nmulti-scale visual features, generating multi-scale deformation fields and\nfacilitating the coarse-to-fine MDIR task. Extensive experiments in MR-CT\nAbdomen and SR-Reg Brain datasets demonstrate the effectiveness of our\nframework and the potential of pre-trained LLMs for MDIR task. Our code is\navailabel at: https://github.com/ninjannn/LLM-Morph."
                },
                "authors": [
                    {
                        "name": "Mingrui Ma"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Jie Ning"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Bruno Lepri"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Lepri"
                },
                "author": "Bruno Lepri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10701v1",
                "updated": "2024-08-20T09:58:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    1,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:58:01Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    1,
                    1,
                    233,
                    0
                ],
                "title": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based\n  Scoring Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based\n  Scoring Technique"
                },
                "summary": "In today's era, where large language models (LLMs) are integrated into\nnumerous real-world applications, ensuring their safety and robustness is\ncrucial for responsible AI usage. Automated red-teaming methods play a key role\nin this process by generating adversarial attacks to identify and mitigate\npotential vulnerabilities in these models. However, existing methods often\nstruggle with slow performance, limited categorical diversity, and high\nresource demands. While Rainbow Teaming, a recent approach, addresses the\ndiversity challenge by framing adversarial prompt generation as a\nquality-diversity search, it remains slow and requires a large fine-tuned\nmutator for optimal performance. To overcome these limitations, we propose\nFerret, a novel approach that builds upon Rainbow Teaming by generating\nmultiple adversarial prompt mutations per iteration and using a scoring\nfunction to rank and select the most effective adversarial prompt. We explore\nvarious scoring functions, including reward models, Llama Guard, and\nLLM-as-a-judge, to rank adversarial mutations based on their potential harm to\nimprove the efficiency of the search for harmful mutations. Our results\ndemonstrate that Ferret, utilizing a reward model as a scoring function,\nimproves the overall attack success rate (ASR) to 95%, which is 46% higher than\nRainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90%\nASR by 15.2% compared to the baseline and generates adversarial prompts that\nare transferable i.e. effective on other LLMs of larger size. Our codes are\navailable at https://github.com/declare-lab/ferret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's era, where large language models (LLMs) are integrated into\nnumerous real-world applications, ensuring their safety and robustness is\ncrucial for responsible AI usage. Automated red-teaming methods play a key role\nin this process by generating adversarial attacks to identify and mitigate\npotential vulnerabilities in these models. However, existing methods often\nstruggle with slow performance, limited categorical diversity, and high\nresource demands. While Rainbow Teaming, a recent approach, addresses the\ndiversity challenge by framing adversarial prompt generation as a\nquality-diversity search, it remains slow and requires a large fine-tuned\nmutator for optimal performance. To overcome these limitations, we propose\nFerret, a novel approach that builds upon Rainbow Teaming by generating\nmultiple adversarial prompt mutations per iteration and using a scoring\nfunction to rank and select the most effective adversarial prompt. We explore\nvarious scoring functions, including reward models, Llama Guard, and\nLLM-as-a-judge, to rank adversarial mutations based on their potential harm to\nimprove the efficiency of the search for harmful mutations. Our results\ndemonstrate that Ferret, utilizing a reward model as a scoring function,\nimproves the overall attack success rate (ASR) to 95%, which is 46% higher than\nRainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90%\nASR by 15.2% compared to the baseline and generates adversarial prompts that\nare transferable i.e. effective on other LLMs of larger size. Our codes are\navailable at https://github.com/declare-lab/ferret."
                },
                "authors": [
                    {
                        "name": "Tej Deep Pala"
                    },
                    {
                        "name": "Vernon Y. H. Toh"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10692v1",
                "updated": "2024-08-20T09:42:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    26,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:42:26Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    26,
                    1,
                    233,
                    0
                ],
                "title": "Unconditional Truthfulness: Learning Conditional Dependency for\n  Uncertainty Quantification of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconditional Truthfulness: Learning Conditional Dependency for\n  Uncertainty Quantification of Large Language Models"
                },
                "summary": "Uncertainty quantification (UQ) is a perspective approach to detecting Large\nLanguage Model (LLM) hallucinations and low quality output. In this work, we\naddress one of the challenges of UQ in generation tasks that arises from the\nconditional dependency between the generation steps of an LLM. We propose to\nlearn this dependency from data. We train a regression model, which target\nvariable is the gap between the conditional and the unconditional generation\nconfidence. During LLM inference, we use this learned conditional dependency\nmodel to modulate the uncertainty of the current generation step based on the\nuncertainty of the previous step. Our experimental evaluation on nine datasets\nand three LLMs shows that the proposed method is highly effective for\nuncertainty quantification, achieving substantial improvements over rivaling\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) is a perspective approach to detecting Large\nLanguage Model (LLM) hallucinations and low quality output. In this work, we\naddress one of the challenges of UQ in generation tasks that arises from the\nconditional dependency between the generation steps of an LLM. We propose to\nlearn this dependency from data. We train a regression model, which target\nvariable is the gap between the conditional and the unconditional generation\nconfidence. During LLM inference, we use this learned conditional dependency\nmodel to modulate the uncertainty of the current generation step based on the\nuncertainty of the previous step. Our experimental evaluation on nine datasets\nand three LLMs shows that the proposed method is highly effective for\nuncertainty quantification, achieving substantial improvements over rivaling\napproaches."
                },
                "authors": [
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10691v1",
                "updated": "2024-08-20T09:42:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    17,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:42:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches"
                },
                "summary": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge."
                },
                "authors": [
                    {
                        "name": "Yanjie Dong"
                    },
                    {
                        "name": "Xiaoyi Fan"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    },
                    {
                        "name": "Xiping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiping Hu"
                },
                "author": "Xiping Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10686v1",
                "updated": "2024-08-20T09:37:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    37,
                    36,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:37:36Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    37,
                    36,
                    1,
                    233,
                    0
                ],
                "title": "Gradient Wild Bootstrap for Instrumental Variable Quantile Regressions\n  with Weak and Few Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Wild Bootstrap for Instrumental Variable Quantile Regressions\n  with Weak and Few Clusters"
                },
                "summary": "We study the gradient wild bootstrap-based inference for instrumental\nvariable quantile regressions in the framework of a small number of large\nclusters in which the number of clusters is viewed as fixed, and the number of\nobservations for each cluster diverges to infinity. For the Wald inference, we\nshow that our wild bootstrap Wald test, with or without studentization using\nthe cluster-robust covariance estimator (CRVE), controls size asymptotically up\nto a small error as long as the parameter of endogenous variable is strongly\nidentified in at least one of the clusters. We further show that the wild\nbootstrap Wald test with CRVE studentization is more powerful for distant local\nalternatives than that without. Last, we develop a wild bootstrap\nAnderson-Rubin (AR) test for the weak-identification-robust inference. We show\nit controls size asymptotically up to a small error, even under weak or partial\nidentification for all clusters. We illustrate the good finite-sample\nperformance of the new inference methods using simulations and provide an\nempirical application to a well-known dataset about US local labor markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the gradient wild bootstrap-based inference for instrumental\nvariable quantile regressions in the framework of a small number of large\nclusters in which the number of clusters is viewed as fixed, and the number of\nobservations for each cluster diverges to infinity. For the Wald inference, we\nshow that our wild bootstrap Wald test, with or without studentization using\nthe cluster-robust covariance estimator (CRVE), controls size asymptotically up\nto a small error as long as the parameter of endogenous variable is strongly\nidentified in at least one of the clusters. We further show that the wild\nbootstrap Wald test with CRVE studentization is more powerful for distant local\nalternatives than that without. Last, we develop a wild bootstrap\nAnderson-Rubin (AR) test for the weak-identification-robust inference. We show\nit controls size asymptotically up to a small error, even under weak or partial\nidentification for all clusters. We illustrate the good finite-sample\nperformance of the new inference methods using simulations and provide an\nempirical application to a well-known dataset about US local labor markets."
                },
                "authors": [
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yichong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yichong Zhang"
                },
                "author": "Yichong Zhang",
                "arxiv_comment": "74 pages. arXiv admin note: text overlap with arXiv:2108.13707",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10682v1",
                "updated": "2024-08-20T09:36:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    36,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:36:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    36,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for\n  Assessing and Improving Unlearning Robustness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Knowledge Unlearning: An Adversarial Framework for\n  Assessing and Improving Unlearning Robustness in Large Language Models"
                },
                "summary": "LLM have achieved success in many fields but still troubled by problematic\ncontent in the training corpora. LLM unlearning aims at reducing their\ninfluence and avoid undesirable behaviours. However, existing unlearning\nmethods remain vulnerable to adversarial queries and the unlearned knowledge\nresurfaces after the manually designed attack queries. As part of a red-team\neffort to proactively assess the vulnerabilities of unlearned models, we design\nDynamic Unlearning Attack (DUA), a dynamic and automated framework to attack\nthese models and evaluate their robustness. It optimizes adversarial suffixes\nto reintroduce the unlearned knowledge in various scenarios. We find that\nunlearned knowledge can be recovered in $55.2\\%$ of the questions, even without\nrevealing the unlearned model's parameters. In response to this vulnerability,\nwe propose Latent Adversarial Unlearning (LAU), a universal framework that\neffectively enhances the robustness of the unlearned process. It formulates the\nunlearning process as a min-max optimization problem and resolves it through\ntwo stages: an attack stage, where perturbation vectors are trained and added\nto the latent space of LLMs to recover the unlearned knowledge, and a defense\nstage, where previously trained perturbation vectors are used to enhance\nunlearned model's robustness. With our LAU framework, we obtain two robust\nunlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across\nmultiple unlearning benchmarks and various models, and demonstrate that they\nimprove the unlearning effectiveness by over $53.5\\%$, cause only less than a\n$11.6\\%$ reduction in neighboring knowledge, and have almost no impact on the\nmodel's general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM have achieved success in many fields but still troubled by problematic\ncontent in the training corpora. LLM unlearning aims at reducing their\ninfluence and avoid undesirable behaviours. However, existing unlearning\nmethods remain vulnerable to adversarial queries and the unlearned knowledge\nresurfaces after the manually designed attack queries. As part of a red-team\neffort to proactively assess the vulnerabilities of unlearned models, we design\nDynamic Unlearning Attack (DUA), a dynamic and automated framework to attack\nthese models and evaluate their robustness. It optimizes adversarial suffixes\nto reintroduce the unlearned knowledge in various scenarios. We find that\nunlearned knowledge can be recovered in $55.2\\%$ of the questions, even without\nrevealing the unlearned model's parameters. In response to this vulnerability,\nwe propose Latent Adversarial Unlearning (LAU), a universal framework that\neffectively enhances the robustness of the unlearned process. It formulates the\nunlearning process as a min-max optimization problem and resolves it through\ntwo stages: an attack stage, where perturbation vectors are trained and added\nto the latent space of LLMs to recover the unlearned knowledge, and a defense\nstage, where previously trained perturbation vectors are used to enhance\nunlearned model's robustness. With our LAU framework, we obtain two robust\nunlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across\nmultiple unlearning benchmarks and various models, and demonstrate that they\nimprove the unlearning effectiveness by over $53.5\\%$, cause only less than a\n$11.6\\%$ reduction in neighboring knowledge, and have almost no impact on the\nmodel's general capabilities."
                },
                "authors": [
                    {
                        "name": "Hongbang Yuan"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10678v1",
                "updated": "2024-08-20T09:28:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    28,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:28:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    28,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "A machine-learning classifier for the postmerger remnant of binary\n  neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine-learning classifier for the postmerger remnant of binary\n  neutron stars"
                },
                "summary": "Knowing the kind of remnant produced after the merger of a binary neutron\nstar system, e.g., if a black hole forms or not, would not only shed light on\nthe equation of state describing the extremely dense matter inside neutron\nstars, but also help understand the physical processes involved in the\npostmerger phase. Moreover, in the event of a gravitational-wave detection,\npredicting the presence of a neutron star remnant is crucial in order to advise\npotential electromagnetic follow-up campaigns. In this work, we use Gradient\nBoosted Decision Trees and publicly available data from numerical-relativity\nsimulations to construct a classifier that predicts the outcome of binary\nneutron star mergers, based on the binary's parameters inferred from\ngravitational-wave inspiral signals: total mass, mass-weighted tidal\ndeformability, mass ratio, and effective inspiral spin. Employing parameters\nthat can be estimated from the inspiral part of the signal only allows us to\npredict the remnant independently on the detection of a postmerger\ngravitational-wave signal. We build three different classifiers to distinguish\nbetween various potential scenarios, we estimate their accuracy and the\nconfidence of their predictions. Finally, we apply the developed classifiers to\nreal events data, finding that GW170817 most likely lead to the formation of a\nhypermassive neutron star, while GW190425 to a prompt collapse to a black hole.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing the kind of remnant produced after the merger of a binary neutron\nstar system, e.g., if a black hole forms or not, would not only shed light on\nthe equation of state describing the extremely dense matter inside neutron\nstars, but also help understand the physical processes involved in the\npostmerger phase. Moreover, in the event of a gravitational-wave detection,\npredicting the presence of a neutron star remnant is crucial in order to advise\npotential electromagnetic follow-up campaigns. In this work, we use Gradient\nBoosted Decision Trees and publicly available data from numerical-relativity\nsimulations to construct a classifier that predicts the outcome of binary\nneutron star mergers, based on the binary's parameters inferred from\ngravitational-wave inspiral signals: total mass, mass-weighted tidal\ndeformability, mass ratio, and effective inspiral spin. Employing parameters\nthat can be estimated from the inspiral part of the signal only allows us to\npredict the remnant independently on the detection of a postmerger\ngravitational-wave signal. We build three different classifiers to distinguish\nbetween various potential scenarios, we estimate their accuracy and the\nconfidence of their predictions. Finally, we apply the developed classifiers to\nreal events data, finding that GW170817 most likely lead to the formation of a\nhypermassive neutron star, while GW190425 to a prompt collapse to a black hole."
                },
                "authors": [
                    {
                        "name": "Anna Puecher"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10868v3",
                "updated": "2024-08-20T09:25:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    25,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-16T09:36:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    36,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts"
                },
                "summary": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03891v2",
                "updated": "2024-08-20T09:19:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    19,
                    7,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-04T12:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    12,
                    33,
                    56,
                    3,
                    186,
                    0
                ],
                "title": "AutoBench: Automatic Testbench Generation and Evaluation Using LLMs for\n  HDL Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoBench: Automatic Testbench Generation and Evaluation Using LLMs for\n  HDL Design"
                },
                "summary": "In digital circuit design, testbenches constitute the cornerstone of\nsimulation-based hardware verification. Traditional methodologies for testbench\ngeneration during simulation-based hardware verification still remain partially\nmanual, resulting in inefficiencies in testing various scenarios and requiring\nexpensive time from designers. Large Language Models (LLMs) have demonstrated\ntheir potential in automating the circuit design flow. However, directly\napplying LLMs to generate testbenches suffers from a low pass rate. To address\nthis challenge, we introduce AutoBench, the first LLM-based testbench generator\nfor digital circuit design, which requires only the description of the design\nunder test (DUT) to automatically generate comprehensive testbenches. In\nAutoBench, a hybrid testbench structure and a self-checking system are realized\nusing LLMs. To validate the generated testbenches, we also introduce an\nautomated testbench evaluation framework to evaluate the quality of generated\ntestbenches from multiple perspectives. Experimental results demonstrate that\nAutoBench achieves a 57% improvement in the testbench pass@1 ratio compared\nwith the baseline that directly generates testbenches using LLMs. For 75\nsequential circuits, AutoBench successfully has a 3.36 times testbench pass@1\nratio compared with the baseline. The source codes and experimental results are\nopen-sourced at this link: https://github.com/AutoBench/AutoBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In digital circuit design, testbenches constitute the cornerstone of\nsimulation-based hardware verification. Traditional methodologies for testbench\ngeneration during simulation-based hardware verification still remain partially\nmanual, resulting in inefficiencies in testing various scenarios and requiring\nexpensive time from designers. Large Language Models (LLMs) have demonstrated\ntheir potential in automating the circuit design flow. However, directly\napplying LLMs to generate testbenches suffers from a low pass rate. To address\nthis challenge, we introduce AutoBench, the first LLM-based testbench generator\nfor digital circuit design, which requires only the description of the design\nunder test (DUT) to automatically generate comprehensive testbenches. In\nAutoBench, a hybrid testbench structure and a self-checking system are realized\nusing LLMs. To validate the generated testbenches, we also introduce an\nautomated testbench evaluation framework to evaluate the quality of generated\ntestbenches from multiple perspectives. Experimental results demonstrate that\nAutoBench achieves a 57% improvement in the testbench pass@1 ratio compared\nwith the baseline that directly generates testbenches using LLMs. For 75\nsequential circuits, AutoBench successfully has a 3.36 times testbench pass@1\nratio compared with the baseline. The source codes and experimental results are\nopen-sourced at this link: https://github.com/AutoBench/AutoBench"
                },
                "authors": [
                    {
                        "name": "Ruidi Qiu"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Rolf Drechsler"
                    },
                    {
                        "name": "Ulf Schlichtmann"
                    },
                    {
                        "name": "Bing Li"
                    }
                ],
                "author_detail": {
                    "name": "Bing Li"
                },
                "author": "Bing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10668v2",
                "updated": "2024-08-21T07:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    50,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T09:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    11,
                    21,
                    1,
                    233,
                    0
                ],
                "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation"
                },
                "summary": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Yatao Bian"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Peilin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Peilin Zhao"
                },
                "author": "Peilin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10663v1",
                "updated": "2024-08-20T09:05:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    5,
                    3,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:05:03Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    5,
                    3,
                    1,
                    233,
                    0
                ],
                "title": "REInstruct: Building Instruction Data from Unlabeled Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REInstruct: Building Instruction Data from Unlabeled Corpus"
                },
                "summary": "Manually annotating instruction data for large language models is difficult,\ncostly, and hard to scale. Meanwhile, current automatic annotation methods\ntypically rely on distilling synthetic data from proprietary LLMs, which not\nonly limits the upper bound of the quality of the instruction data but also\nraises potential copyright issues. In this paper, we propose REInstruct, a\nsimple and scalable method to automatically build instruction data from an\nunlabeled corpus without heavy reliance on proprietary LLMs and human\nannotation. Specifically, REInstruct first selects a subset of unlabeled texts\nthat potentially contain well-structured helpful and insightful content and\nthen generates instructions for these texts. To generate accurate and relevant\nresponses for effective and robust training, REInstruct further proposes a\nrewriting-based approach to improve the quality of the generated instruction\ndata. By training Llama-7b on a combination of 3k seed data and 32k synthetic\ndata from REInstruct, fine-tuned model achieves a 65.41\\% win rate on\nAlpacaEval leaderboard against text-davinci-003, outperforming other\nopen-source, non-distilled instruction data construction methods. The code is\npublicly available at \\url{https://github.com/cs32963/REInstruct}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manually annotating instruction data for large language models is difficult,\ncostly, and hard to scale. Meanwhile, current automatic annotation methods\ntypically rely on distilling synthetic data from proprietary LLMs, which not\nonly limits the upper bound of the quality of the instruction data but also\nraises potential copyright issues. In this paper, we propose REInstruct, a\nsimple and scalable method to automatically build instruction data from an\nunlabeled corpus without heavy reliance on proprietary LLMs and human\nannotation. Specifically, REInstruct first selects a subset of unlabeled texts\nthat potentially contain well-structured helpful and insightful content and\nthen generates instructions for these texts. To generate accurate and relevant\nresponses for effective and robust training, REInstruct further proposes a\nrewriting-based approach to improve the quality of the generated instruction\ndata. By training Llama-7b on a combination of 3k seed data and 32k synthetic\ndata from REInstruct, fine-tuned model achieves a 65.41\\% win rate on\nAlpacaEval leaderboard against text-davinci-003, outperforming other\nopen-source, non-distilled instruction data construction methods. The code is\npublicly available at \\url{https://github.com/cs32963/REInstruct}."
                },
                "authors": [
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Xinyan Guan"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "Accepted by ACL2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14554v2",
                "updated": "2024-08-20T09:04:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    4,
                    25,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-23T13:32:07Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    32,
                    7,
                    3,
                    144,
                    0
                ],
                "title": "SearchLVLMs: A Plug-and-Play Framework for Augmenting Large\n  Vision-Language Models by Searching Up-to-Date Internet Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SearchLVLMs: A Plug-and-Play Framework for Augmenting Large\n  Vision-Language Models by Searching Up-to-Date Internet Knowledge"
                },
                "summary": "Large vision-language models (LVLMs) are ignorant of the up-to-date\nknowledge, such as LLaVA series, because they cannot be updated frequently due\nto the large amount of resources required, and therefore fail in many cases.\nFor example, if a LVLM was released on January 2024, and it wouldn't know the\nsinger of the theme song for the new Detective Conan movie, which wasn't\nreleased until April 2024. To solve the problem, a promising solution motivated\nby retrieval-augmented generation (RAG) is to provide LVLMs with up-to-date\nknowledge via internet search during inference, i.e., internet-augmented\ngeneration (IAG), which is already integrated in some closed-source commercial\nLVLMs such as GPT-4V. However, the specific mechanics underpinning them remain\na mystery. In this paper, we propose a plug-and-play framework, for augmenting\nexisting LVLMs in handling visual question answering (VQA) about up-to-date\nknowledge, dubbed SearchLVLMs. A hierarchical filtering model is trained to\neffectively and efficiently find the most helpful content from the websites\nreturned by a search engine to prompt LVLMs with up-to-date knowledge. To train\nthe model and evaluate our framework's performance, we propose a pipeline to\nautomatically generate news-related VQA samples to construct a dataset, dubbed\nUDK-VQA. A multi-model voting mechanism is introduced to label the usefulness\nof website/content for VQA samples to construct the training set. Experimental\nresults demonstrate the effectiveness of our framework, outperforming GPT-4V by\nabout 25% in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) are ignorant of the up-to-date\nknowledge, such as LLaVA series, because they cannot be updated frequently due\nto the large amount of resources required, and therefore fail in many cases.\nFor example, if a LVLM was released on January 2024, and it wouldn't know the\nsinger of the theme song for the new Detective Conan movie, which wasn't\nreleased until April 2024. To solve the problem, a promising solution motivated\nby retrieval-augmented generation (RAG) is to provide LVLMs with up-to-date\nknowledge via internet search during inference, i.e., internet-augmented\ngeneration (IAG), which is already integrated in some closed-source commercial\nLVLMs such as GPT-4V. However, the specific mechanics underpinning them remain\na mystery. In this paper, we propose a plug-and-play framework, for augmenting\nexisting LVLMs in handling visual question answering (VQA) about up-to-date\nknowledge, dubbed SearchLVLMs. A hierarchical filtering model is trained to\neffectively and efficiently find the most helpful content from the websites\nreturned by a search engine to prompt LVLMs with up-to-date knowledge. To train\nthe model and evaluate our framework's performance, we propose a pipeline to\nautomatically generate news-related VQA samples to construct a dataset, dubbed\nUDK-VQA. A multi-model voting mechanism is introduced to label the usefulness\nof website/content for VQA samples to construct the training set. Experimental\nresults demonstrate the effectiveness of our framework, outperforming GPT-4V by\nabout 25% in accuracy."
                },
                "authors": [
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "arxiv_comment": "13 pages, 6 figures, a plug-and-play framework to augment large\n  vision-language models with up-to-date internet knowledge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08780v2",
                "updated": "2024-08-20T09:01:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    1,
                    9,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-16T14:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions"
                },
                "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "10 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10658v1",
                "updated": "2024-08-20T08:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    54,
                    34,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:54:34Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    54,
                    34,
                    1,
                    233,
                    0
                ],
                "title": "Learning Instruction-Guided Manipulation Affordance via Large Models for\n  Embodied Robotic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Instruction-Guided Manipulation Affordance via Large Models for\n  Embodied Robotic Tasks"
                },
                "summary": "We study the task of language instruction-guided robotic manipulation, in\nwhich an embodied robot is supposed to manipulate the target objects based on\nthe language instructions. In previous studies, the predicted manipulation\nregions of the target object typically do not change with specification from\nthe language instructions, which means that the language perception and\nmanipulation prediction are separate. However, in human behavioral patterns,\nthe manipulation regions of the same object will change for different language\ninstructions. In this paper, we propose Instruction-Guided Affordance Net\n(IGANet) for predicting affordance maps of instruction-guided robotic\nmanipulation tasks by utilizing powerful priors from vision and language\nencoders pre-trained on large-scale datasets. We develop a\nVison-Language-Models(VLMs)-based data augmentation pipeline, which can\ngenerate a large amount of data automatically for model training. Besides, with\nthe help of Large-Language-Models(LLMs), actions can be effectively executed to\nfinish the tasks defined by instructions. A series of real-world experiments\nrevealed that our method can achieve better performance with generated data.\nMoreover, our model can generalize better to scenarios with unseen objects and\nlanguage instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the task of language instruction-guided robotic manipulation, in\nwhich an embodied robot is supposed to manipulate the target objects based on\nthe language instructions. In previous studies, the predicted manipulation\nregions of the target object typically do not change with specification from\nthe language instructions, which means that the language perception and\nmanipulation prediction are separate. However, in human behavioral patterns,\nthe manipulation regions of the same object will change for different language\ninstructions. In this paper, we propose Instruction-Guided Affordance Net\n(IGANet) for predicting affordance maps of instruction-guided robotic\nmanipulation tasks by utilizing powerful priors from vision and language\nencoders pre-trained on large-scale datasets. We develop a\nVison-Language-Models(VLMs)-based data augmentation pipeline, which can\ngenerate a large amount of data automatically for model training. Besides, with\nthe help of Large-Language-Models(LLMs), actions can be effectively executed to\nfinish the tasks defined by instructions. A series of real-world experiments\nrevealed that our method can achieve better performance with generated data.\nMoreover, our model can generalize better to scenarios with unseen objects and\nlanguage instructions."
                },
                "authors": [
                    {
                        "name": "Dayou Li"
                    },
                    {
                        "name": "Chenkun Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Yibin Li"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Accepted to ICARM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10649v1",
                "updated": "2024-08-20T08:42:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    42,
                    0,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:42:00Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    42,
                    0,
                    1,
                    233,
                    0
                ],
                "title": "Inferring Underwater Topography with FINN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Underwater Topography with FINN"
                },
                "summary": "Spatiotemporal partial differential equations (PDEs) find extensive\napplication across various scientific and engineering fields. While numerous\nmodels have emerged from both physics and machine learning (ML) communities,\nthere is a growing trend towards integrating these approaches to develop hybrid\narchitectures known as physics-aware machine learning models. Among these, the\nfinite volume neural network (FINN) has emerged as a recent addition. FINN has\nproven to be particularly efficient in uncovering latent structures in data. In\nthis study, we explore the capabilities of FINN in tackling the shallow-water\nequations, which simulates wave dynamics in coastal regions. Specifically, we\ninvestigate FINN's efficacy to reconstruct underwater topography based on these\nparticular wave equations. Our findings reveal that FINN exhibits a remarkable\ncapacity to infer topography solely from wave dynamics, distinguishing itself\nfrom both conventional ML and physics-aware ML models. Our results underscore\nthe potential of FINN in advancing our understanding of spatiotemporal\nphenomena and enhancing parametrization capabilities in related domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal partial differential equations (PDEs) find extensive\napplication across various scientific and engineering fields. While numerous\nmodels have emerged from both physics and machine learning (ML) communities,\nthere is a growing trend towards integrating these approaches to develop hybrid\narchitectures known as physics-aware machine learning models. Among these, the\nfinite volume neural network (FINN) has emerged as a recent addition. FINN has\nproven to be particularly efficient in uncovering latent structures in data. In\nthis study, we explore the capabilities of FINN in tackling the shallow-water\nequations, which simulates wave dynamics in coastal regions. Specifically, we\ninvestigate FINN's efficacy to reconstruct underwater topography based on these\nparticular wave equations. Our findings reveal that FINN exhibits a remarkable\ncapacity to infer topography solely from wave dynamics, distinguishing itself\nfrom both conventional ML and physics-aware ML models. Our results underscore\nthe potential of FINN in advancing our understanding of spatiotemporal\nphenomena and enhancing parametrization capabilities in related domains."
                },
                "authors": [
                    {
                        "name": "Coşku Can Horuz"
                    },
                    {
                        "name": "Matthias Karlbauer"
                    },
                    {
                        "name": "Timothy Praditia"
                    },
                    {
                        "name": "Sergey Oladyshkin"
                    },
                    {
                        "name": "Wolfgang Nowak"
                    },
                    {
                        "name": "Sebastian Otte"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Otte"
                },
                "author": "Sebastian Otte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10647v1",
                "updated": "2024-08-20T08:40:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    40,
                    39,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:40:39Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    40,
                    39,
                    1,
                    233,
                    0
                ],
                "title": "Privacy-preserving Universal Adversarial Defense for Black-box Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-preserving Universal Adversarial Defense for Black-box Models"
                },
                "summary": "Deep neural networks (DNNs) are increasingly used in critical applications\nsuch as identity authentication and autonomous driving, where robustness\nagainst adversarial attacks is crucial. These attacks can exploit minor\nperturbations to cause significant prediction errors, making it essential to\nenhance the resilience of DNNs. Traditional defense methods often rely on\naccess to detailed model information, which raises privacy concerns, as model\nowners may be reluctant to share such data. In contrast, existing black-box\ndefense methods fail to offer a universal defense against various types of\nadversarial attacks. To address these challenges, we introduce DUCD, a\nuniversal black-box defense method that does not require access to the target\nmodel's parameters or architecture. Our approach involves distilling the target\nmodel by querying it with data, creating a white-box surrogate while preserving\ndata privacy. We further enhance this surrogate model using a certified defense\nbased on randomized smoothing and optimized noise selection, enabling robust\ndefense against a broad range of adversarial attacks. Comparative evaluations\nbetween the certified defenses of the surrogate and target models demonstrate\nthe effectiveness of our approach. Experiments on multiple image classification\ndatasets show that DUCD not only outperforms existing black-box defenses but\nalso matches the accuracy of white-box defenses, all while enhancing data\nprivacy and reducing the success rate of membership inference attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) are increasingly used in critical applications\nsuch as identity authentication and autonomous driving, where robustness\nagainst adversarial attacks is crucial. These attacks can exploit minor\nperturbations to cause significant prediction errors, making it essential to\nenhance the resilience of DNNs. Traditional defense methods often rely on\naccess to detailed model information, which raises privacy concerns, as model\nowners may be reluctant to share such data. In contrast, existing black-box\ndefense methods fail to offer a universal defense against various types of\nadversarial attacks. To address these challenges, we introduce DUCD, a\nuniversal black-box defense method that does not require access to the target\nmodel's parameters or architecture. Our approach involves distilling the target\nmodel by querying it with data, creating a white-box surrogate while preserving\ndata privacy. We further enhance this surrogate model using a certified defense\nbased on randomized smoothing and optimized noise selection, enabling robust\ndefense against a broad range of adversarial attacks. Comparative evaluations\nbetween the certified defenses of the surrogate and target models demonstrate\nthe effectiveness of our approach. Experiments on multiple image classification\ndatasets show that DUCD not only outperforms existing black-box defenses but\nalso matches the accuracy of white-box defenses, all while enhancing data\nprivacy and reducing the success rate of membership inference attacks."
                },
                "authors": [
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Zijun Zhang"
                    },
                    {
                        "name": "Kun He"
                    },
                    {
                        "name": "Ruiying Du"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Qingchuang Zhao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10646v1",
                "updated": "2024-08-20T08:38:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    38,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:38:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    38,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge\n  Representation Sharing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge\n  Representation Sharing in LLMs"
                },
                "summary": "The veracity of a factoid is largely independent of the language it is\nwritten in. However, language models are inconsistent in their ability to\nanswer the same factual question across languages. This raises questions about\nhow LLMs represent a given fact across languages. We explore multilingual\nfactual knowledge through two aspects: the model's ability to answer a query\nconsistently across languages, and the ability to ''store'' answers in a shared\nrepresentation for several languages. We propose a methodology to measure the\nextent of representation sharing across languages by repurposing knowledge\nediting methods. We examine LLMs with various multilingual configurations using\na new multilingual dataset. We reveal that high consistency does not\nnecessarily imply shared representation, particularly for languages with\ndifferent scripts. Moreover, we find that script similarity is a dominant\nfactor in representation sharing. Finally, we observe that if LLMs could fully\nshare knowledge across languages, their accuracy in their best-performing\nlanguage could benefit an increase of up to 150\\% on average. These findings\nhighlight the need for improved multilingual knowledge representation in LLMs\nand suggest a path for the development of more robust and consistent\nmultilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The veracity of a factoid is largely independent of the language it is\nwritten in. However, language models are inconsistent in their ability to\nanswer the same factual question across languages. This raises questions about\nhow LLMs represent a given fact across languages. We explore multilingual\nfactual knowledge through two aspects: the model's ability to answer a query\nconsistently across languages, and the ability to ''store'' answers in a shared\nrepresentation for several languages. We propose a methodology to measure the\nextent of representation sharing across languages by repurposing knowledge\nediting methods. We examine LLMs with various multilingual configurations using\na new multilingual dataset. We reveal that high consistency does not\nnecessarily imply shared representation, particularly for languages with\ndifferent scripts. Moreover, we find that script similarity is a dominant\nfactor in representation sharing. Finally, we observe that if LLMs could fully\nshare knowledge across languages, their accuracy in their best-performing\nlanguage could benefit an increase of up to 150\\% on average. These findings\nhighlight the need for improved multilingual knowledge representation in LLMs\nand suggest a path for the development of more robust and consistent\nmultilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Maxim Ifergan"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Omri Abend"
                    }
                ],
                "author_detail": {
                    "name": "Omri Abend"
                },
                "author": "Omri Abend",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10645v1",
                "updated": "2024-08-20T08:36:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:36:59Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "title": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation"
                },
                "summary": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance."
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Yizhou Dang"
                    },
                    {
                        "name": "Yuliang Liang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10543v2",
                "updated": "2024-08-20T08:36:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    26,
                    1,
                    233,
                    0
                ],
                "published": "2024-02-16T10:11:20Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    10,
                    11,
                    20,
                    4,
                    47,
                    0
                ],
                "title": "Strong hallucinations from negation and how to fix them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong hallucinations from negation and how to fix them"
                },
                "summary": "Despite great performance on many tasks, language models (LMs) still struggle\nwith reasoning, sometimes providing responses that cannot possibly be true\nbecause they stem from logical incoherence. We call such responses\n\\textit{strong hallucinations} and prove that they follow from an LM's\ncomputation of its internal representations for logical operators and outputs\nfrom those representations. Focusing on negation, we provide a novel solution\nin which negation is treated not as another element of a latent representation,\nbut as \\textit{an operation over an LM's latent representations that constrains\nhow they may evolve}. We show that our approach improves model performance in\ncloze prompting and natural language inference tasks with negation without\nrequiring training on sparse negative data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great performance on many tasks, language models (LMs) still struggle\nwith reasoning, sometimes providing responses that cannot possibly be true\nbecause they stem from logical incoherence. We call such responses\n\\textit{strong hallucinations} and prove that they follow from an LM's\ncomputation of its internal representations for logical operators and outputs\nfrom those representations. Focusing on negation, we provide a novel solution\nin which negation is treated not as another element of a latent representation,\nbut as \\textit{an operation over an LM's latent representations that constrains\nhow they may evolve}. We show that our approach improves model performance in\ncloze prompting and natural language inference tasks with negation without\nrequiring training on sparse negative data."
                },
                "authors": [
                    {
                        "name": "Nicholas Asher"
                    },
                    {
                        "name": "Swarnadeep Bhar"
                    }
                ],
                "author_detail": {
                    "name": "Swarnadeep Bhar"
                },
                "author": "Swarnadeep Bhar",
                "arxiv_comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10642v1",
                "updated": "2024-08-20T08:32:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    32,
                    44,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:32:44Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    32,
                    44,
                    1,
                    233,
                    0
                ],
                "title": "Minor SFT loss for LLM fine-tune to increase performance and reduce\n  model deviation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor SFT loss for LLM fine-tune to increase performance and reduce\n  model deviation"
                },
                "summary": "Instruct LLM provide a paradigm used in large scale language model to align\nLLM to human preference. The paradigm contains supervised fine tuning and\nreinforce learning from human feedback. This paradigm is also used in\ndownstream scenarios to adapt LLM to specific corpora and applications.\nComparing to SFT, there are many efforts focused on RLHF and several algorithms\nbeing proposed, such as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most\nefforts for SFT are focused on how to collect, filter and mix high quality\ndata. In this article with insight from DPO and MinorDPO, we propose a training\nmetric for SFT to measure the discrepancy between the optimized model and the\noriginal model, and a loss function MinorSFT that can increase the training\neffectiveness, and reduce the discrepancy between the optimized LLM and\noriginal LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct LLM provide a paradigm used in large scale language model to align\nLLM to human preference. The paradigm contains supervised fine tuning and\nreinforce learning from human feedback. This paradigm is also used in\ndownstream scenarios to adapt LLM to specific corpora and applications.\nComparing to SFT, there are many efforts focused on RLHF and several algorithms\nbeing proposed, such as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most\nefforts for SFT are focused on how to collect, filter and mix high quality\ndata. In this article with insight from DPO and MinorDPO, we propose a training\nmetric for SFT to measure the discrepancy between the optimized model and the\noriginal model, and a loss function MinorSFT that can increase the training\neffectiveness, and reduce the discrepancy between the optimized LLM and\noriginal LLM."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuyu Wu"
                },
                "author": "Xiuyu Wu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10635v1",
                "updated": "2024-08-20T08:22:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    22,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:22:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    22,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search"
                },
                "summary": "In this paper, we propose a new method Strategist that utilizes LLMs to\nacquire new skills for playing multi-agent games through a self-improvement\nprocess. Our method gathers quality feedback through self-play simulations with\nMonte Carlo tree search and LLM-based reflection, which can then be used to\nlearn high-level strategic skills such as how to evaluate states that guide the\nlow-level execution.We showcase how our method can be used in both action\nplanning and dialogue generation in the context of games, achieving good\nperformance on both tasks. Specifically, we demonstrate that our method can\nhelp train agents with better performance than both traditional reinforcement\nlearning-based approaches and other LLM-based skill learning approaches in\ngames including the Game of Pure Strategy (GOPS) and The Resistance: Avalon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a new method Strategist that utilizes LLMs to\nacquire new skills for playing multi-agent games through a self-improvement\nprocess. Our method gathers quality feedback through self-play simulations with\nMonte Carlo tree search and LLM-based reflection, which can then be used to\nlearn high-level strategic skills such as how to evaluate states that guide the\nlow-level execution.We showcase how our method can be used in both action\nplanning and dialogue generation in the context of games, achieving good\nperformance on both tasks. Specifically, we demonstrate that our method can\nhelp train agents with better performance than both traditional reinforcement\nlearning-based approaches and other LLM-based skill learning approaches in\ngames including the Game of Pure Strategy (GOPS) and The Resistance: Avalon."
                },
                "authors": [
                    {
                        "name": "Jonathan Light"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Weiqin Chen"
                    },
                    {
                        "name": "Guanzhi Wang"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Ziniu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Ziniu Hu"
                },
                "author": "Ziniu Hu",
                "arxiv_comment": "website: https://llm-strategist.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10631v1",
                "updated": "2024-08-20T08:13:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    13,
                    52,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:13:52Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    13,
                    52,
                    1,
                    233,
                    0
                ],
                "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have grown significantly in scale, leading to a\ncritical need for efficient model pruning techniques. Existing post-training\npruning techniques primarily focus on measuring weight importance on converged\ndense models to determine salient weights to retain. However, they often\noverlook the changes in weight importance during the pruning process, which can\nlead to performance degradation in the pruned models. To address this issue, we\npresent LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a\nnovel one-shot pruning framework that rebuilds the sparsity mask of pruned\nmodels without any retraining or weight reconstruction. LLM-Barber incorporates\nblock-aware error optimization across Self-Attention and MLP blocks, ensuring\nglobal performance optimization. Inspired by the recent discovery of prominent\noutliers in LLMs, LLM-Barber introduces an innovative pruning metric that\nidentifies weight importance using weights multiplied by gradients. Our\nexperiments show that LLM-Barber can efficiently prune models like LLaMA and\nOPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes,\nachieving state-of-the-art results in both perplexity and zero-shot performance\nacross various language benchmarks. Code is available at\nhttps://github.com/YupengSu/LLM-Barber.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have grown significantly in scale, leading to a\ncritical need for efficient model pruning techniques. Existing post-training\npruning techniques primarily focus on measuring weight importance on converged\ndense models to determine salient weights to retain. However, they often\noverlook the changes in weight importance during the pruning process, which can\nlead to performance degradation in the pruned models. To address this issue, we\npresent LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a\nnovel one-shot pruning framework that rebuilds the sparsity mask of pruned\nmodels without any retraining or weight reconstruction. LLM-Barber incorporates\nblock-aware error optimization across Self-Attention and MLP blocks, ensuring\nglobal performance optimization. Inspired by the recent discovery of prominent\noutliers in LLMs, LLM-Barber introduces an innovative pruning metric that\nidentifies weight importance using weights multiplied by gradients. Our\nexperiments show that LLM-Barber can efficiently prune models like LLaMA and\nOPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes,\nachieving state-of-the-art results in both perplexity and zero-shot performance\nacross various language benchmarks. Code is available at\nhttps://github.com/YupengSu/LLM-Barber."
                },
                "authors": [
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Ziyi Guan"
                    },
                    {
                        "name": "Xiaoqun Liu"
                    },
                    {
                        "name": "Tianlai Jin"
                    },
                    {
                        "name": "Dongkuan Wu"
                    },
                    {
                        "name": "Graziano Chesi"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yu"
                },
                "author": "Hao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11053v1",
                "updated": "2024-08-20T17:58:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    58,
                    56,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:58:56Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    58,
                    56,
                    1,
                    233,
                    0
                ],
                "title": "Revisiting VerilogEval: Newer LLMs, In-Context Learning, and\n  Specification-to-RTL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting VerilogEval: Newer LLMs, In-Context Learning, and\n  Specification-to-RTL Tasks"
                },
                "summary": "The application of large-language models (LLMs) to digital hardware code\ngeneration is an emerging field. Most LLMs are primarily trained on natural\nlanguage and software code. Hardware code, such as Verilog, represents only a\nsmall portion of the training data and few hardware benchmarks exist. To\naddress this gap, the open-source VerilogEval benchmark was released in 2023,\nproviding a consistent evaluation framework for LLMs on code completion tasks.\nIt was tested on state-of-the-art models at the time including GPT-4. However,\nVerilogEval and other Verilog generation benchmarks lack failure analysis and,\nin present form, are not conducive to exploring prompting techniques. Also,\nsince VerilogEval's release, both commercial and open-source models have seen\ncontinued development.\n  In this work, we evaluate new commercial and open-source models of varying\nsizes against an improved VerilogEval benchmark suite. We enhance VerilogEval's\ninfrastructure and dataset by automatically classifying failures, introduce new\nprompts for supporting in-context learning (ICL) examples, and extend the\nsupported tasks to specification-to-RTL translation. We find a measurable\nimprovement in commercial state-of-the-art models, with GPT-4 Turbo achieving a\n59% pass rate on spec-to-RTL tasks. We also study the performance of\nopen-source and domain-specific models that have emerged, and demonstrate that\nmodels can benefit substantially from ICL. We find that recently-released Llama\n3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo,\nand that the much smaller domain-specific RTL-Coder 6.7B models achieve an\nimpressive 37% pass rate. However, prompt engineering is key to achieving good\npass rates, and varies widely with model and task. A benchmark infrastructure\nthat allows for prompt engineering and failure analysis is key to continued\nmodel development and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large-language models (LLMs) to digital hardware code\ngeneration is an emerging field. Most LLMs are primarily trained on natural\nlanguage and software code. Hardware code, such as Verilog, represents only a\nsmall portion of the training data and few hardware benchmarks exist. To\naddress this gap, the open-source VerilogEval benchmark was released in 2023,\nproviding a consistent evaluation framework for LLMs on code completion tasks.\nIt was tested on state-of-the-art models at the time including GPT-4. However,\nVerilogEval and other Verilog generation benchmarks lack failure analysis and,\nin present form, are not conducive to exploring prompting techniques. Also,\nsince VerilogEval's release, both commercial and open-source models have seen\ncontinued development.\n  In this work, we evaluate new commercial and open-source models of varying\nsizes against an improved VerilogEval benchmark suite. We enhance VerilogEval's\ninfrastructure and dataset by automatically classifying failures, introduce new\nprompts for supporting in-context learning (ICL) examples, and extend the\nsupported tasks to specification-to-RTL translation. We find a measurable\nimprovement in commercial state-of-the-art models, with GPT-4 Turbo achieving a\n59% pass rate on spec-to-RTL tasks. We also study the performance of\nopen-source and domain-specific models that have emerged, and demonstrate that\nmodels can benefit substantially from ICL. We find that recently-released Llama\n3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo,\nand that the much smaller domain-specific RTL-Coder 6.7B models achieve an\nimpressive 37% pass rate. However, prompt engineering is key to achieving good\npass rates, and varies widely with model and task. A benchmark infrastructure\nthat allows for prompt engineering and failure analysis is key to continued\nmodel development and deployment."
                },
                "authors": [
                    {
                        "name": "Nathaniel Pinckney"
                    },
                    {
                        "name": "Christopher Batten"
                    },
                    {
                        "name": "Mingjie Liu"
                    },
                    {
                        "name": "Haoxing Ren"
                    },
                    {
                        "name": "Brucek Khailany"
                    }
                ],
                "author_detail": {
                    "name": "Brucek Khailany"
                },
                "author": "Brucek Khailany",
                "arxiv_comment": "This paper revisits and improves the benchmark first presented in\n  arXiv:2309.07544. Seven pages, three figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11051v1",
                "updated": "2024-08-20T17:57:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    46,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:57:46Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    46,
                    1,
                    233,
                    0
                ],
                "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io"
                },
                "authors": [
                    {
                        "name": "Yunzhe Xu"
                    },
                    {
                        "name": "Yiyuan Pan"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v2",
                "updated": "2024-08-21T17:55:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    55,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01099v2",
                "updated": "2024-08-20T17:54:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    54,
                    8,
                    1,
                    233,
                    0
                ],
                "published": "2024-04-01T13:12:30Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    13,
                    12,
                    30,
                    0,
                    92,
                    0
                ],
                "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety"
                },
                "summary": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking."
                },
                "authors": [
                    {
                        "name": "Luxi He"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Peter Henderson"
                    }
                ],
                "author_detail": {
                    "name": "Peter Henderson"
                },
                "author": "Peter Henderson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11043v1",
                "updated": "2024-08-20T17:49:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    49,
                    51,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:49:51Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    49,
                    51,
                    1,
                    233,
                    0
                ],
                "title": "Reconciling Methodological Paradigms: Employing Large Language Models as\n  Novice Qualitative Research Assistants in Talent Management Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconciling Methodological Paradigms: Employing Large Language Models as\n  Novice Qualitative Research Assistants in Talent Management Research"
                },
                "summary": "Qualitative data collection and analysis approaches, such as those employing\ninterviews and focus groups, provide rich insights into customer attitudes,\nsentiment, and behavior. However, manually analyzing qualitative data requires\nextensive time and effort to identify relevant topics and thematic insights.\nThis study proposes a novel approach to address this challenge by leveraging\nRetrieval Augmented Generation (RAG) based Large Language Models (LLMs) for\nanalyzing interview transcripts. The novelty of this work lies in strategizing\nthe research inquiry as one that is augmented by an LLM that serves as a novice\nresearch assistant. This research explores the mental model of LLMs to serve as\nnovice qualitative research assistants for researchers in the talent management\nspace. A RAG-based LLM approach is extended to enable topic modeling of\nsemi-structured interview data, showcasing the versatility of these models\nbeyond their traditional use in information retrieval and search. Our findings\ndemonstrate that the LLM-augmented RAG approach can successfully extract topics\nof interest, with significant coverage compared to manually generated topics\nfrom the same dataset. This establishes the viability of employing LLMs as\nnovice qualitative research assistants. Additionally, the study recommends that\nresearchers leveraging such models lean heavily on quality criteria used in\ntraditional qualitative research to ensure rigor and trustworthiness of their\napproach. Finally, the paper presents key recommendations for industry\npractitioners seeking to reconcile the use of LLMs with established qualitative\nresearch paradigms, providing a roadmap for the effective integration of these\npowerful, albeit novice, AI tools in the analysis of qualitative datasets\nwithin talent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative data collection and analysis approaches, such as those employing\ninterviews and focus groups, provide rich insights into customer attitudes,\nsentiment, and behavior. However, manually analyzing qualitative data requires\nextensive time and effort to identify relevant topics and thematic insights.\nThis study proposes a novel approach to address this challenge by leveraging\nRetrieval Augmented Generation (RAG) based Large Language Models (LLMs) for\nanalyzing interview transcripts. The novelty of this work lies in strategizing\nthe research inquiry as one that is augmented by an LLM that serves as a novice\nresearch assistant. This research explores the mental model of LLMs to serve as\nnovice qualitative research assistants for researchers in the talent management\nspace. A RAG-based LLM approach is extended to enable topic modeling of\nsemi-structured interview data, showcasing the versatility of these models\nbeyond their traditional use in information retrieval and search. Our findings\ndemonstrate that the LLM-augmented RAG approach can successfully extract topics\nof interest, with significant coverage compared to manually generated topics\nfrom the same dataset. This establishes the viability of employing LLMs as\nnovice qualitative research assistants. Additionally, the study recommends that\nresearchers leveraging such models lean heavily on quality criteria used in\ntraditional qualitative research to ensure rigor and trustworthiness of their\napproach. Finally, the paper presents key recommendations for industry\npractitioners seeking to reconcile the use of LLMs with established qualitative\nresearch paradigms, providing a roadmap for the effective integration of these\npowerful, albeit novice, AI tools in the analysis of qualitative datasets\nwithin talent"
                },
                "authors": [
                    {
                        "name": "Sreyoshi Bhaduri"
                    },
                    {
                        "name": "Satya Kapoor"
                    },
                    {
                        "name": "Alex Gil"
                    },
                    {
                        "name": "Anshul Mittal"
                    },
                    {
                        "name": "Rutu Mulkar"
                    }
                ],
                "author_detail": {
                    "name": "Rutu Mulkar"
                },
                "author": "Rutu Mulkar",
                "arxiv_comment": "Accepted to KDD '24 workshop on Talent Management and Computing (TMC\n  2024). 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10830v2",
                "updated": "2024-08-20T17:28:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    28,
                    14,
                    1,
                    233,
                    0
                ],
                "published": "2023-10-16T21:05:12Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    21,
                    5,
                    12,
                    0,
                    289,
                    0
                ],
                "title": "Fake News in Sheep's Clothing: Robust Fake News Detection Against\n  LLM-Empowered Style Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake News in Sheep's Clothing: Robust Fake News Detection Against\n  LLM-Empowered Style Attacks"
                },
                "summary": "It is commonly perceived that fake news and real news exhibit distinct\nwriting styles, such as the use of sensationalist versus objective language.\nHowever, we emphasize that style-related features can also be exploited for\nstyle-based attacks. Notably, the advent of powerful Large Language Models\n(LLMs) has empowered malicious actors to mimic the style of trustworthy news\nsources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals\nthat LLM-camouflaged fake news content significantly undermines the\neffectiveness of state-of-the-art text-based detectors (up to 38% decrease in\nF1 Score), implying a severe vulnerability to stylistic variations. To address\nthis, we introduce SheepDog, a style-robust fake news detector that prioritizes\ncontent over style in determining news veracity. SheepDog achieves this\nresilience through (1) LLM-empowered news reframings that inject style\ndiversity into the training process by customizing articles to match different\nstyles; (2) a style-agnostic training scheme that ensures consistent veracity\npredictions across style-diverse reframings; and (3) content-focused veracity\nattributions that distill content-centric guidelines from LLMs for debunking\nfake news, offering supplementary cues and potential intepretability that\nassist veracity prediction. Extensive experiments on three real-world\nbenchmarks demonstrate SheepDog's style robustness and adaptability to various\nbackbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is commonly perceived that fake news and real news exhibit distinct\nwriting styles, such as the use of sensationalist versus objective language.\nHowever, we emphasize that style-related features can also be exploited for\nstyle-based attacks. Notably, the advent of powerful Large Language Models\n(LLMs) has empowered malicious actors to mimic the style of trustworthy news\nsources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals\nthat LLM-camouflaged fake news content significantly undermines the\neffectiveness of state-of-the-art text-based detectors (up to 38% decrease in\nF1 Score), implying a severe vulnerability to stylistic variations. To address\nthis, we introduce SheepDog, a style-robust fake news detector that prioritizes\ncontent over style in determining news veracity. SheepDog achieves this\nresilience through (1) LLM-empowered news reframings that inject style\ndiversity into the training process by customizing articles to match different\nstyles; (2) a style-agnostic training scheme that ensures consistent veracity\npredictions across style-diverse reframings; and (3) content-focused veracity\nattributions that distill content-centric guidelines from LLMs for debunking\nfake news, offering supplementary cues and potential intepretability that\nassist veracity prediction. Extensive experiments on three real-world\nbenchmarks demonstrate SheepDog's style robustness and adaptability to various\nbackbones."
                },
                "authors": [
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3637528.3671977",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671977",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.10830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to KDD 2024 (Research Track)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11021v1",
                "updated": "2024-08-20T17:21:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    21,
                    10,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:21:10Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    21,
                    10,
                    1,
                    233,
                    0
                ],
                "title": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning"
                },
                "summary": "Due to emergent capabilities, large language models (LLMs) have been utilized\nas language-based agents to perform a variety of tasks and make decisions with\nan increasing degree of autonomy. These autonomous agents can understand\nhigh-level instructions, interact with their environments, and execute complex\ntasks using a selection of tools available to them. As the capabilities of the\nagents expand, ensuring their safety and trustworthiness becomes more\nimperative. In this study, we introduce the Athena framework which leverages\nthe concept of verbal contrastive learning where past safe and unsafe\ntrajectories are used as in-context (contrastive) examples to guide the agent\ntowards safety while fulfilling a given task. The framework also incorporates a\ncritiquing mechanism to guide the agent to prevent risky actions at every step.\nFurthermore, due to the lack of existing benchmarks on the safety reasoning\nability of LLM-based agents, we curate a set of 80 toolkits across 8 categories\nwith 180 scenarios to provide a safety evaluation benchmark. Our experimental\nevaluation, with both closed- and open-source LLMs, indicates verbal\ncontrastive learning and interaction-level critiquing improve the safety rate\nsignificantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to emergent capabilities, large language models (LLMs) have been utilized\nas language-based agents to perform a variety of tasks and make decisions with\nan increasing degree of autonomy. These autonomous agents can understand\nhigh-level instructions, interact with their environments, and execute complex\ntasks using a selection of tools available to them. As the capabilities of the\nagents expand, ensuring their safety and trustworthiness becomes more\nimperative. In this study, we introduce the Athena framework which leverages\nthe concept of verbal contrastive learning where past safe and unsafe\ntrajectories are used as in-context (contrastive) examples to guide the agent\ntowards safety while fulfilling a given task. The framework also incorporates a\ncritiquing mechanism to guide the agent to prevent risky actions at every step.\nFurthermore, due to the lack of existing benchmarks on the safety reasoning\nability of LLM-based agents, we curate a set of 80 toolkits across 8 categories\nwith 180 scenarios to provide a safety evaluation benchmark. Our experimental\nevaluation, with both closed- and open-source LLMs, indicates verbal\ncontrastive learning and interaction-level critiquing improve the safety rate\nsignificantly."
                },
                "authors": [
                    {
                        "name": "Tanmana Sadhu"
                    },
                    {
                        "name": "Ali Pesaranghader"
                    },
                    {
                        "name": "Yanan Chen"
                    },
                    {
                        "name": "Dong Hoon Yi"
                    }
                ],
                "author_detail": {
                    "name": "Dong Hoon Yi"
                },
                "author": "Dong Hoon Yi",
                "arxiv_comment": "9 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.00050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.00050v3",
                "updated": "2024-08-20T17:16:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    16,
                    20,
                    1,
                    233,
                    0
                ],
                "published": "2023-04-28T19:00:43Z",
                "published_parsed": [
                    2023,
                    4,
                    28,
                    19,
                    0,
                    43,
                    4,
                    118,
                    0
                ],
                "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for\n  Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning and Large Language Models: Opening a New Frontier for\n  Causality"
                },
                "summary": "The causal capabilities of large language models (LLMs) are a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nconduct a \"behavorial\" study of LLMs to benchmark their capability in\ngenerating causal arguments. Across a wide range of tasks, we find that LLMs\ncan generate text corresponding to correct causal arguments with high\nprobability, surpassing the best-performing existing methods. Algorithms based\non GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery\ntask (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain)\nand event causality (86% accuracy in determining necessary and sufficient\ncauses in vignettes). We perform robustness checks across tasks and show that\nthe capabilities cannot be explained by dataset memorization alone, especially\nsince LLMs generalize to novel datasets that were created after the training\ncutoff date.\n  That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds\nof errors that may be improved and what are the fundamental limits of LLM-based\nanswers. Overall, by operating on the text metadata, LLMs bring capabilities so\nfar understood to be restricted to humans, such as using collected knowledge to\ngenerate causal graphs or identifying background causal context from natural\nlanguage. As a result, LLMs may be used by human domain experts to save effort\nin setting up a causal analysis, one of the biggest impediments to the\nwidespread adoption of causal methods. Given that LLMs ignore the actual data,\nour results also point to a fruitful research direction of developing\nalgorithms that combine LLMs with existing causal techniques. Code and datasets\nare available at https://github.com/py-why/pywhy-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The causal capabilities of large language models (LLMs) are a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nconduct a \"behavorial\" study of LLMs to benchmark their capability in\ngenerating causal arguments. Across a wide range of tasks, we find that LLMs\ncan generate text corresponding to correct causal arguments with high\nprobability, surpassing the best-performing existing methods. Algorithms based\non GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery\ntask (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain)\nand event causality (86% accuracy in determining necessary and sufficient\ncauses in vignettes). We perform robustness checks across tasks and show that\nthe capabilities cannot be explained by dataset memorization alone, especially\nsince LLMs generalize to novel datasets that were created after the training\ncutoff date.\n  That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds\nof errors that may be improved and what are the fundamental limits of LLM-based\nanswers. Overall, by operating on the text metadata, LLMs bring capabilities so\nfar understood to be restricted to humans, such as using collected knowledge to\ngenerate causal graphs or identifying background causal context from natural\nlanguage. As a result, LLMs may be used by human domain experts to save effort\nin setting up a causal analysis, one of the biggest impediments to the\nwidespread adoption of causal methods. Given that LLMs ignore the actual data,\nour results also point to a fruitful research direction of developing\nalgorithms that combine LLMs with existing causal techniques. Code and datasets\nare available at https://github.com/py-why/pywhy-llm."
                },
                "authors": [
                    {
                        "name": "Emre Kıcıman"
                    },
                    {
                        "name": "Robert Ness"
                    },
                    {
                        "name": "Amit Sharma"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "arxiv_comment": "Added three novel datasets. To be published in TMLR. Authors listed\n  alphabetically",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.00050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.00050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14722v2",
                "updated": "2024-08-20T17:08:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    8,
                    13,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-20T20:37:55Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    20,
                    37,
                    55,
                    3,
                    172,
                    0
                ],
                "title": "Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's\n  Understanding of Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's\n  Understanding of Algorithms"
                },
                "summary": "As Large Language Models (LLMs) perform (and sometimes excel at) more and\nmore complex cognitive tasks, a natural question is whether AI really\nunderstands. The study of understanding in LLMs is in its infancy, and the\ncommunity has yet to incorporate well-trodden research in philosophy,\npsychology, and education. We initiate this, specifically focusing on\nunderstanding algorithms, and propose a hierarchy of levels of understanding.\nWe use the hierarchy to design and conduct a study with human subjects\n(undergraduate and graduate students) as well as large language models\n(generations of GPT), revealing interesting similarities and differences. We\nexpect that our rigorous criteria will be useful to keep track of AI's progress\nin such cognitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) perform (and sometimes excel at) more and\nmore complex cognitive tasks, a natural question is whether AI really\nunderstands. The study of understanding in LLMs is in its infancy, and the\ncommunity has yet to incorporate well-trodden research in philosophy,\npsychology, and education. We initiate this, specifically focusing on\nunderstanding algorithms, and propose a hierarchy of levels of understanding.\nWe use the hierarchy to design and conduct a study with human subjects\n(undergraduate and graduate students) as well as large language models\n(generations of GPT), revealing interesting similarities and differences. We\nexpect that our rigorous criteria will be useful to keep track of AI's progress\nin such cognitive domains."
                },
                "authors": [
                    {
                        "name": "Mirabel Reid"
                    },
                    {
                        "name": "Santosh S. Vempala"
                    }
                ],
                "author_detail": {
                    "name": "Santosh S. Vempala"
                },
                "author": "Santosh S. Vempala",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m; F.1.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v1",
                "updated": "2024-08-20T17:00:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "While GitHub Copilot Excels at Coding, Does It Ensure Responsible\n  Output?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While GitHub Copilot Excels at Coding, Does It Ensure Responsible\n  Output?"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10995v1",
                "updated": "2024-08-20T16:43:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    43,
                    5,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:43:05Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    43,
                    5,
                    1,
                    233,
                    0
                ],
                "title": "CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language\n  Models"
                },
                "summary": "New medical treatment development requires multiple phases of clinical\ntrials. Despite the significant human and financial costs of bringing a drug to\nmarket, less than 20% of drugs in testing will make it from the first phase to\nfinal approval. Recent literature indicates that the design of the trial\nprotocols significantly contributes to trial performance. We investigated\nClinical Trial Outcome Prediction (CTOP) using trial design documents to\npredict phase transitions automatically. We propose CTP-LLM, the first Large\nLanguage Model (LLM) based model for CTOP. We also introduce the\nPhaseTransition (PT) Dataset; which labels trials based on their progression\nthrough the regulatory process and serves as a benchmark for CTOP evaluation.\nOur fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase\ntransition by analyzing the trial's original protocol texts without requiring\nhuman-selected features. CTP-LLM achieves a 67% accuracy rate in predicting\ntrial phase transitions across all phases and a 75% accuracy rate specifically\nin predicting the transition from Phase~III to final approval. Our experimental\nperformance highlights the potential of LLM-powered applications in forecasting\nclinical trial outcomes and assessing trial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New medical treatment development requires multiple phases of clinical\ntrials. Despite the significant human and financial costs of bringing a drug to\nmarket, less than 20% of drugs in testing will make it from the first phase to\nfinal approval. Recent literature indicates that the design of the trial\nprotocols significantly contributes to trial performance. We investigated\nClinical Trial Outcome Prediction (CTOP) using trial design documents to\npredict phase transitions automatically. We propose CTP-LLM, the first Large\nLanguage Model (LLM) based model for CTOP. We also introduce the\nPhaseTransition (PT) Dataset; which labels trials based on their progression\nthrough the regulatory process and serves as a benchmark for CTOP evaluation.\nOur fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase\ntransition by analyzing the trial's original protocol texts without requiring\nhuman-selected features. CTP-LLM achieves a 67% accuracy rate in predicting\ntrial phase transitions across all phases and a 75% accuracy rate specifically\nin predicting the transition from Phase~III to final approval. Our experimental\nperformance highlights the potential of LLM-powered applications in forecasting\nclinical trial outcomes and assessing trial design."
                },
                "authors": [
                    {
                        "name": "Michael Reinisch"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Chenxi Liao"
                    },
                    {
                        "name": "Sauleh Ahmad Siddiqui"
                    },
                    {
                        "name": "Bei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bei Xiao"
                },
                "author": "Bei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v2",
                "updated": "2024-08-20T16:09:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    9,
                    33,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09172v2",
                "updated": "2024-08-20T15:51:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    51,
                    59,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-17T11:33:23Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    11,
                    33,
                    23,
                    5,
                    230,
                    0
                ],
                "title": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context\n  Example Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context\n  Example Selection"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have demonstrated exceptional\nperformance across various downstream tasks. However, it is challenging for\nusers to discern whether the responses are generated with certainty or are\nfabricated to meet user expectations. Estimating the uncertainty of LLMs is\nparticularly challenging due to their vast scale and the lack of white-box\naccess. In this work, we propose a novel Uncertainty Tripartite Testing\nParadigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency\nof LLM outputs when incorporating label interference into the sampling-based\napproach. Based on Unc-TTP outputs, we aggregate instances into certain and\nuncertain categories. Further, we conduct a detailed analysis of the\nuncertainty properties of LLMs and show Unc-TTP's superiority over the existing\nsampling-based methods. In addition, we leverage the obtained uncertainty\ninformation to guide in-context example selection, demonstrating that Unc-TTP\nobviously outperforms retrieval-based and sampling-based approaches in\nselecting more informative examples. Our work paves a new way to classify the\nuncertainty of both open- and closed-source LLMs, and introduces a practical\napproach to exploit this uncertainty to improve LLMs performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have demonstrated exceptional\nperformance across various downstream tasks. However, it is challenging for\nusers to discern whether the responses are generated with certainty or are\nfabricated to meet user expectations. Estimating the uncertainty of LLMs is\nparticularly challenging due to their vast scale and the lack of white-box\naccess. In this work, we propose a novel Uncertainty Tripartite Testing\nParadigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency\nof LLM outputs when incorporating label interference into the sampling-based\napproach. Based on Unc-TTP outputs, we aggregate instances into certain and\nuncertain categories. Further, we conduct a detailed analysis of the\nuncertainty properties of LLMs and show Unc-TTP's superiority over the existing\nsampling-based methods. In addition, we leverage the obtained uncertainty\ninformation to guide in-context example selection, demonstrating that Unc-TTP\nobviously outperforms retrieval-based and sampling-based approaches in\nselecting more informative examples. Our work paves a new way to classify the\nuncertainty of both open- and closed-source LLMs, and introduces a practical\napproach to exploit this uncertainty to improve LLMs performance."
                },
                "authors": [
                    {
                        "name": "Hsiu-Yuan Huang"
                    },
                    {
                        "name": "Zichen Wu"
                    },
                    {
                        "name": "Yutong Yang"
                    },
                    {
                        "name": "Junzhao Zhang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "9 pages, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03151v3",
                "updated": "2024-08-20T15:41:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    41,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-05T11:15:45Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    11,
                    15,
                    45,
                    2,
                    157,
                    0
                ],
                "title": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument\n  Summarisation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument\n  Summarisation and Evaluation"
                },
                "summary": "With the recent advances of large language models (LLMs), it is no longer\ninfeasible to build an automated debate system that helps people to synthesise\npersuasive arguments. Previous work attempted this task by integrating multiple\ncomponents. In our work, we introduce an argument mining dataset that captures\nthe end-to-end process of preparing an argumentative essay for a debate, which\ncovers the tasks of claim and evidence identification (Task 1 ED), evidence\nconvincingness ranking (Task 2 ECR), argumentative essay summarisation and\nhuman preference ranking (Task 3 ASR) and metric learning for automated\nevaluation of resulting essays, based on human feedback along argument quality\ndimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are\nfully annotated with the various properties supporting the aforementioned\ntasks. We evaluate multiple generative baselines for each of these tasks,\nincluding representative LLMs. We find, that while they show promising results\non individual tasks in our benchmark, their end-to-end performance on all four\ntasks in succession deteriorates significantly, both in automated measures as\nwell as in human-centred evaluation. This challenge presented by our proposed\ndataset motivates future research on end-to-end argument mining and\nsummarisation. The repository of this project is available at\nhttps://github.com/HaoBytes/ArgSum-Datatset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent advances of large language models (LLMs), it is no longer\ninfeasible to build an automated debate system that helps people to synthesise\npersuasive arguments. Previous work attempted this task by integrating multiple\ncomponents. In our work, we introduce an argument mining dataset that captures\nthe end-to-end process of preparing an argumentative essay for a debate, which\ncovers the tasks of claim and evidence identification (Task 1 ED), evidence\nconvincingness ranking (Task 2 ECR), argumentative essay summarisation and\nhuman preference ranking (Task 3 ASR) and metric learning for automated\nevaluation of resulting essays, based on human feedback along argument quality\ndimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are\nfully annotated with the various properties supporting the aforementioned\ntasks. We evaluate multiple generative baselines for each of these tasks,\nincluding representative LLMs. We find, that while they show promising results\non individual tasks in our benchmark, their end-to-end performance on all four\ntasks in succession deteriorates significantly, both in automated measures as\nwell as in human-centred evaluation. This challenge presented by our proposed\ndataset motivates future research on end-to-end argument mining and\nsummarisation. The repository of this project is available at\nhttps://github.com/HaoBytes/ArgSum-Datatset"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yuping Wu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Tharindu Madusanka"
                    },
                    {
                        "name": "Iqra Zahid"
                    },
                    {
                        "name": "Jiayan Zeng"
                    },
                    {
                        "name": "Xiaochi Wang"
                    },
                    {
                        "name": "Xinran He"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "Published on ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10947v1",
                "updated": "2024-08-20T15:36:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:36:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in\n  Education for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in\n  Education for Large Language Models"
                },
                "summary": "Teachers are important to imparting knowledge and guiding learners, and the\nrole of large language models (LLMs) as potential educators is emerging as an\nimportant area of study. Recognizing LLMs' capability to generate educational\ncontent can lead to advances in automated and personalized learning. While LLMs\nhave been tested for their comprehension and problem-solving skills, their\ncapability in teaching remains largely unexplored. In teaching, questioning is\na key skill that guides students to analyze, evaluate, and synthesize core\nconcepts and principles. Therefore, our research introduces a benchmark to\nevaluate the questioning capability in education as a teacher of LLMs through\nevaluating their generated educational questions, utilizing Anderson and\nKrathwohl's taxonomy across general, monodisciplinary, and interdisciplinary\ndomains. We shift the focus from LLMs as learners to LLMs as educators,\nassessing their teaching capability through guiding them to generate questions.\nWe apply four metrics, including relevance, coverage, representativeness, and\nconsistency, to evaluate the educational quality of LLMs' outputs. Our results\nindicate that GPT-4 demonstrates significant potential in teaching general,\nhumanities, and science courses; Claude2 appears more apt as an\ninterdisciplinary teacher. Furthermore, the automatic scores align with human\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teachers are important to imparting knowledge and guiding learners, and the\nrole of large language models (LLMs) as potential educators is emerging as an\nimportant area of study. Recognizing LLMs' capability to generate educational\ncontent can lead to advances in automated and personalized learning. While LLMs\nhave been tested for their comprehension and problem-solving skills, their\ncapability in teaching remains largely unexplored. In teaching, questioning is\na key skill that guides students to analyze, evaluate, and synthesize core\nconcepts and principles. Therefore, our research introduces a benchmark to\nevaluate the questioning capability in education as a teacher of LLMs through\nevaluating their generated educational questions, utilizing Anderson and\nKrathwohl's taxonomy across general, monodisciplinary, and interdisciplinary\ndomains. We shift the focus from LLMs as learners to LLMs as educators,\nassessing their teaching capability through guiding them to generate questions.\nWe apply four metrics, including relevance, coverage, representativeness, and\nconsistency, to evaluate the educational quality of LLMs' outputs. Our results\nindicate that GPT-4 demonstrates significant potential in teaching general,\nhumanities, and science courses; Claude2 appears more apt as an\ninterdisciplinary teacher. Furthermore, the automatic scores align with human\nperspectives."
                },
                "authors": [
                    {
                        "name": "Yuyan Chen"
                    },
                    {
                        "name": "Chenwei Wu"
                    },
                    {
                        "name": "Songzhou Yan"
                    },
                    {
                        "name": "Panjun Liu"
                    },
                    {
                        "name": "Haoyu Zhou"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Accepted to ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10946v1",
                "updated": "2024-08-20T15:36:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    24,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:36:24Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    36,
                    24,
                    1,
                    233,
                    0
                ],
                "title": "Large Language Model Driven Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Driven Recommendation"
                },
                "summary": "While previous chapters focused on recommendation systems (RSs) based on\nstandardized, non-verbal user feedback such as purchases, views, and clicks --\nthe advent of LLMs has unlocked the use of natural language (NL) interactions\nfor recommendation. This chapter discusses how LLMs' abilities for general NL\nreasoning present novel opportunities to build highly personalized RSs -- which\ncan effectively connect nuanced and diverse user preferences to items,\npotentially via interactive dialogues. To begin this discussion, we first\npresent a taxonomy of the key data sources for language-driven recommendation,\ncovering item descriptions, user-system interactions, and user profiles. We\nthen proceed to fundamental techniques for LLM recommendation, reviewing the\nuse of encoder-only and autoregressive LLM recommendation in both tuned and\nuntuned settings. Afterwards, we move to multi-module recommendation\narchitectures in which LLMs interact with components such as retrievers and RSs\nin multi-stage pipelines. This brings us to architectures for conversational\nrecommender systems (CRSs), in which LLMs facilitate multi-turn dialogues where\neach turn presents an opportunity not only to make recommendations, but also to\nengage with the user in interactive preference elicitation, critiquing, and\nquestion-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While previous chapters focused on recommendation systems (RSs) based on\nstandardized, non-verbal user feedback such as purchases, views, and clicks --\nthe advent of LLMs has unlocked the use of natural language (NL) interactions\nfor recommendation. This chapter discusses how LLMs' abilities for general NL\nreasoning present novel opportunities to build highly personalized RSs -- which\ncan effectively connect nuanced and diverse user preferences to items,\npotentially via interactive dialogues. To begin this discussion, we first\npresent a taxonomy of the key data sources for language-driven recommendation,\ncovering item descriptions, user-system interactions, and user profiles. We\nthen proceed to fundamental techniques for LLM recommendation, reviewing the\nuse of encoder-only and autoregressive LLM recommendation in both tuned and\nuntuned settings. Afterwards, we move to multi-module recommendation\narchitectures in which LLMs interact with components such as retrievers and RSs\nin multi-stage pipelines. This brings us to architectures for conversational\nrecommender systems (CRSs), in which LLMs facilitate multi-turn dialogues where\neach turn presents an opportunity not only to make recommendations, but also to\nengage with the user in interactive preference elicitation, critiquing, and\nquestion-answering."
                },
                "authors": [
                    {
                        "name": "Anton Korikov"
                    },
                    {
                        "name": "Scott Sanner"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Zhankui He"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Arnau Ramisa"
                    },
                    {
                        "name": "Rene Vidal"
                    },
                    {
                        "name": "Mahesh Sathiamoorthy"
                    },
                    {
                        "name": "Atoosa Kasrizadeh"
                    },
                    {
                        "name": "Silvia Milano"
                    },
                    {
                        "name": "Francesco Ricci"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Ricci"
                },
                "author": "Francesco Ricci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10945v1",
                "updated": "2024-08-20T15:34:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    34,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:34:27Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    34,
                    27,
                    1,
                    233,
                    0
                ],
                "title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments"
                },
                "summary": "High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference."
                },
                "authors": [
                    {
                        "name": "Kazi Hasan Ibn Arif"
                    },
                    {
                        "name": "JinYi Yoon"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    },
                    {
                        "name": "Deepu John"
                    },
                    {
                        "name": "Bo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ji"
                },
                "author": "Bo Ji",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10943v1",
                "updated": "2024-08-20T15:33:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    33,
                    16,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:33:16Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    33,
                    16,
                    1,
                    233,
                    0
                ],
                "title": "SysBench: Can Large Language Models Follow System Messages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SysBench: Can Large Language Models Follow System Messages?"
                },
                "summary": "Large Language Models (LLMs) have become instrumental across various\napplications, with the customization of these models to specific scenarios\nbecoming increasingly critical. System message, a fundamental component of\nLLMs, is consist of carefully crafted instructions that guide the behavior of\nmodel to meet intended goals. Despite the recognized potential of system\nmessages to optimize AI-driven solutions, there is a notable absence of a\ncomprehensive benchmark for evaluating how well different LLMs follow these\nsystem messages. To fill this gap, we introduce SysBench, a benchmark that\nsystematically analyzes system message following ability in terms of three\nchallenging aspects: constraint complexity, instruction misalignment and\nmulti-turn stability. In order to enable effective evaluation, SysBench\nconstructs multi-turn user conversations covering various interaction\nrelationships, based on six common types of constraints from system messages in\nreal-world scenarios. Our dataset contains 500 system messages from various\ndomains, each paired with 5 turns of user conversations, which have been\nmanually formulated and checked to guarantee high quality. SysBench provides\nextensive evaluation across various LLMs, measuring their ability to follow\nspecified constraints given in system messages. The results highlight both the\nstrengths and weaknesses of existing models, offering key insights and\ndirections for future research. The open source library SysBench is available\nat https://github.com/PKU-Baichuan-MLSystemLab/SysBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become instrumental across various\napplications, with the customization of these models to specific scenarios\nbecoming increasingly critical. System message, a fundamental component of\nLLMs, is consist of carefully crafted instructions that guide the behavior of\nmodel to meet intended goals. Despite the recognized potential of system\nmessages to optimize AI-driven solutions, there is a notable absence of a\ncomprehensive benchmark for evaluating how well different LLMs follow these\nsystem messages. To fill this gap, we introduce SysBench, a benchmark that\nsystematically analyzes system message following ability in terms of three\nchallenging aspects: constraint complexity, instruction misalignment and\nmulti-turn stability. In order to enable effective evaluation, SysBench\nconstructs multi-turn user conversations covering various interaction\nrelationships, based on six common types of constraints from system messages in\nreal-world scenarios. Our dataset contains 500 system messages from various\ndomains, each paired with 5 turns of user conversations, which have been\nmanually formulated and checked to guarantee high quality. SysBench provides\nextensive evaluation across various LLMs, measuring their ability to follow\nspecified constraints given in system messages. The results highlight both the\nstrengths and weaknesses of existing models, offering key insights and\ndirections for future research. The open source library SysBench is available\nat https://github.com/PKU-Baichuan-MLSystemLab/SysBench."
                },
                "authors": [
                    {
                        "name": "Yanzhao Qin"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yanjun Shen"
                    },
                    {
                        "name": "Wenjing Luo"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yujing Qiao"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10937v1",
                "updated": "2024-08-20T15:20:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    20,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:20:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    20,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Proxona: Leveraging LLM-Driven Personas to Enhance Creators'\n  Understanding of Their Audience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proxona: Leveraging LLM-Driven Personas to Enhance Creators'\n  Understanding of Their Audience"
                },
                "summary": "Creators are nothing without their audience, and thereby understanding their\naudience is the cornerstone of their professional achievement. Yet many\ncreators feel lost while comprehending audiences with existing tools, which\noffer insufficient insights for tailoring content to audience needs. To address\nthe challenges creators face in understanding their audience, we present\nProxona, a system for defining and extracting representative audience personas\nfrom the comments. Creators converse with personas to gain insights into their\npreferences and engagement, solicit feedback, and implement evidence-based\nimprovements to their content. Powered by large language models, Proxona\nanalyzes audience comments, distilling the latent characteristics of audiences\ninto tangible dimensions (classification categories) and values (category\nattributes). Proxona then clusters these into synthetic personas. Our technical\nevaluations demonstrated that our pipelines effectively generated relevant and\ndistinct dimensions and values, enabling the deduction of audience-reflecting\npersonas, while minimizing the likelihood of hallucinations in persona\nresponses. Our user evaluation with 11 creators showed that Proxona supported\ncreators to gain new insights about their audience, make informed decisions,\nand successfully complete content creation with high confidence. Proxona's\ndata-driven audience personas empower creators to seamlessly integrate audience\nperspectives into their creative processes, fostering a collaborative approach\nto content creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creators are nothing without their audience, and thereby understanding their\naudience is the cornerstone of their professional achievement. Yet many\ncreators feel lost while comprehending audiences with existing tools, which\noffer insufficient insights for tailoring content to audience needs. To address\nthe challenges creators face in understanding their audience, we present\nProxona, a system for defining and extracting representative audience personas\nfrom the comments. Creators converse with personas to gain insights into their\npreferences and engagement, solicit feedback, and implement evidence-based\nimprovements to their content. Powered by large language models, Proxona\nanalyzes audience comments, distilling the latent characteristics of audiences\ninto tangible dimensions (classification categories) and values (category\nattributes). Proxona then clusters these into synthetic personas. Our technical\nevaluations demonstrated that our pipelines effectively generated relevant and\ndistinct dimensions and values, enabling the deduction of audience-reflecting\npersonas, while minimizing the likelihood of hallucinations in persona\nresponses. Our user evaluation with 11 creators showed that Proxona supported\ncreators to gain new insights about their audience, make informed decisions,\nand successfully complete content creation with high confidence. Proxona's\ndata-driven audience personas empower creators to seamlessly integrate audience\nperspectives into their creative processes, fostering a collaborative approach\nto content creation."
                },
                "authors": [
                    {
                        "name": "Yoonseo Choi"
                    },
                    {
                        "name": "Eun Jeong Kang"
                    },
                    {
                        "name": "Seulgi Choi"
                    },
                    {
                        "name": "Min Kyung Lee"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "arxiv_comment": "32 pages (including 14 pages of Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09235v2",
                "updated": "2024-08-20T15:12:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    12,
                    8,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-17T16:01:45Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    16,
                    1,
                    45,
                    5,
                    230,
                    0
                ],
                "title": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of\n  Free-Form Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of\n  Free-Form Text"
                },
                "summary": "The emergence of Large Language Models (LLMs) as chat assistants capable of\ngenerating human-like conversations has amplified the need for robust\nevaluation methods, particularly for open-ended tasks. Conventional metrics\nlike BLEU and ROUGE, while useful, are increasingly inadequate for capturing\nthe subtle semantics and contextual richness of such generative outputs. We\npropose a reference-guided verdict method that automates the evaluation process\nby leveraging multiple LLMs-as-judges. Through experiments on three open-ended\nquestion-answering tasks, we demonstrate that combining multiple LLMs-as-judges\nsignificantly improves the reliability and accuracy of evaluations,\nparticularly in complex tasks where a single model might struggle. Our findings\nreveal a strong correlation with human evaluations, establishing our method as\na viable and effective alternative to traditional metrics and human judgments,\nparticularly in the context of LLM-based chat assistants where the complexity\nand diversity of responses challenge existing benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) as chat assistants capable of\ngenerating human-like conversations has amplified the need for robust\nevaluation methods, particularly for open-ended tasks. Conventional metrics\nlike BLEU and ROUGE, while useful, are increasingly inadequate for capturing\nthe subtle semantics and contextual richness of such generative outputs. We\npropose a reference-guided verdict method that automates the evaluation process\nby leveraging multiple LLMs-as-judges. Through experiments on three open-ended\nquestion-answering tasks, we demonstrate that combining multiple LLMs-as-judges\nsignificantly improves the reliability and accuracy of evaluations,\nparticularly in complex tasks where a single model might struggle. Our findings\nreveal a strong correlation with human evaluations, establishing our method as\na viable and effective alternative to traditional metrics and human judgments,\nparticularly in the context of LLM-based chat assistants where the complexity\nand diversity of responses challenge existing benchmarks."
                },
                "authors": [
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Hassan Sajjad"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sajjad"
                },
                "author": "Hassan Sajjad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10923v2",
                "updated": "2024-08-21T15:51:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    51,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T15:05:02Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    5,
                    2,
                    1,
                    233,
                    0
                ],
                "title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization"
                },
                "summary": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at\nhttps://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at\nhttps://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks."
                },
                "authors": [
                    {
                        "name": "Kangjun Noh"
                    },
                    {
                        "name": "Baekryun Seong"
                    },
                    {
                        "name": "Hoyoon Byun"
                    },
                    {
                        "name": "Youngjun Choi"
                    },
                    {
                        "name": "Sungjin Song"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "16 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10921v1",
                "updated": "2024-08-20T15:04:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    4,
                    38,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:04:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    4,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "MTFinEval:A Multi-domain Chinese Financial Benchmark with Eurypalynous\n  questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTFinEval:A Multi-domain Chinese Financial Benchmark with Eurypalynous\n  questions"
                },
                "summary": "With the emergence of more and more economy-specific LLMS, how to measure\nwhether they can be safely invested in production becomes a problem. Previous\nresearch has primarily focused on evaluating the performance of LLMs within\nspecific application scenarios. However, these benchmarks cannot reflect the\ntheoretical level and generalization ability, and the backward datasets are\nincreasingly unsuitable for problems in real scenarios. In this paper, we have\ncompiled a new benchmark, MTFinEval, focusing on the LLMs' basic knowledge of\neconomics, which can always be used as a basis for judgment. To examine only\ntheoretical knowledge as much as possible, MTFinEval is build with foundational\nquestions from university textbooks,and exam papers in economics and management\nmajor. Aware of the overall performance of LLMs do not depend solely on one\nsubdiscipline of economics, MTFinEval comprise 360 questions refined from six\nmajor disciplines of economics, and reflect capabilities more comprehensively.\nExperiment result shows all LLMs perform poorly on MTFinEval, which proves that\nour benchmark built on basic knowledge is very successful. Our research not\nonly offers guidance for selecting the appropriate LLM for specific use cases,\nbut also put forward increase the rigor reliability of LLMs from the basics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of more and more economy-specific LLMS, how to measure\nwhether they can be safely invested in production becomes a problem. Previous\nresearch has primarily focused on evaluating the performance of LLMs within\nspecific application scenarios. However, these benchmarks cannot reflect the\ntheoretical level and generalization ability, and the backward datasets are\nincreasingly unsuitable for problems in real scenarios. In this paper, we have\ncompiled a new benchmark, MTFinEval, focusing on the LLMs' basic knowledge of\neconomics, which can always be used as a basis for judgment. To examine only\ntheoretical knowledge as much as possible, MTFinEval is build with foundational\nquestions from university textbooks,and exam papers in economics and management\nmajor. Aware of the overall performance of LLMs do not depend solely on one\nsubdiscipline of economics, MTFinEval comprise 360 questions refined from six\nmajor disciplines of economics, and reflect capabilities more comprehensively.\nExperiment result shows all LLMs perform poorly on MTFinEval, which proves that\nour benchmark built on basic knowledge is very successful. Our research not\nonly offers guidance for selecting the appropriate LLM for specific use cases,\nbut also put forward increase the rigor reliability of LLMs from the basics."
                },
                "authors": [
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Ke Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ke Jin"
                },
                "author": "Ke Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10918v1",
                "updated": "2024-08-20T15:03:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    3,
                    35,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T15:03:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    3,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CHECKWHY: Causal Fact Verification via Argument Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHECKWHY: Causal Fact Verification via Argument Structure"
                },
                "summary": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Jiasheng Si"
                    },
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Haiyang Zhu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Accepted by ACL2024; Awarded as Outstanding Paper Award and Area\n  Chair Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10914v1",
                "updated": "2024-08-20T14:58:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    58,
                    13,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:58:13Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    58,
                    13,
                    1,
                    233,
                    0
                ],
                "title": "To Code, or Not To Code? Exploring Impact of Code in Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Code, or Not To Code? Exploring Impact of Code in Pre-training"
                },
                "summary": "Including code in the pre-training data mixture, even for models not\nspecifically designed for code, has become a common practice in LLMs\npre-training. While there has been anecdotal consensus among practitioners that\ncode data plays a vital role in general LLMs' performance, there is only\nlimited work analyzing the precise impact of code on non-code tasks. In this\nwork, we systematically investigate the impact of code data on general\nperformance. We ask \"what is the impact of code data used in pre-training on a\nlarge variety of downstream tasks beyond code generation\". We conduct extensive\nablations and evaluate across a broad range of natural language reasoning\ntasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for\nmodels with sizes ranging from 470M to 2.8B parameters. Across settings, we\nfind a consistent results that code is a critical building block for\ngeneralization far beyond coding tasks and improvements to code quality have an\noutsized impact across all tasks. In particular, compared to text-only\npre-training, the addition of code results in up to relative increase of 8.2%\nin natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement\nin generative win-rates, and a 12x boost in code performance respectively. Our\nwork suggests investments in code quality and preserving code during\npre-training have positive impacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Including code in the pre-training data mixture, even for models not\nspecifically designed for code, has become a common practice in LLMs\npre-training. While there has been anecdotal consensus among practitioners that\ncode data plays a vital role in general LLMs' performance, there is only\nlimited work analyzing the precise impact of code on non-code tasks. In this\nwork, we systematically investigate the impact of code data on general\nperformance. We ask \"what is the impact of code data used in pre-training on a\nlarge variety of downstream tasks beyond code generation\". We conduct extensive\nablations and evaluate across a broad range of natural language reasoning\ntasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for\nmodels with sizes ranging from 470M to 2.8B parameters. Across settings, we\nfind a consistent results that code is a critical building block for\ngeneralization far beyond coding tasks and improvements to code quality have an\noutsized impact across all tasks. In particular, compared to text-only\npre-training, the addition of code results in up to relative increase of 8.2%\nin natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement\nin generative win-rates, and a 12x boost in code performance respectively. Our\nwork suggests investments in code quality and preserving code during\npre-training have positive impacts."
                },
                "authors": [
                    {
                        "name": "Viraat Aryabumi"
                    },
                    {
                        "name": "Yixuan Su"
                    },
                    {
                        "name": "Raymond Ma"
                    },
                    {
                        "name": "Adrien Morisot"
                    },
                    {
                        "name": "Ivan Zhang"
                    },
                    {
                        "name": "Acyr Locatelli"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Ahmet Üstün"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09236v2",
                "updated": "2024-08-20T14:57:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    57,
                    15,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-17T16:04:31Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    16,
                    4,
                    31,
                    5,
                    230,
                    0
                ],
                "title": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords"
                },
                "summary": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes."
                },
                "authors": [
                    {
                        "name": "Aman Ahluwalia"
                    },
                    {
                        "name": "Bishwajit Sutradhar"
                    },
                    {
                        "name": "Karishma Ghosh"
                    },
                    {
                        "name": "Indrapal Yadav"
                    },
                    {
                        "name": "Arpan Sheetal"
                    },
                    {
                        "name": "Prashant Patil"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Patil"
                },
                "author": "Prashant Patil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05890v2",
                "updated": "2024-08-20T14:51:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    51,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-08T12:52:46Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    12,
                    52,
                    46,
                    0,
                    190,
                    0
                ],
                "title": "Affordances-Oriented Planning using Foundation Models for Continuous\n  Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordances-Oriented Planning using Foundation Models for Continuous\n  Vision-Language Navigation"
                },
                "summary": "LLM-based agents have demonstrated impressive zero-shot performance in\nvision-language navigation (VLN) task. However, existing LLM-based methods\noften focus only on solving high-level task planning by selecting nodes in\npredefined navigation graphs for movements, overlooking low-level control in\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\nAffordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates\nvarious foundation models to achieve affordances-oriented low-level motion\nplanning and high-level decision-making, both performed in a zero-shot setting.\nSpecifically, we employ a Visual Affordances Prompting (VAP) approach, where\nthe visible ground is segmented by SAM to provide navigational affordances,\nbased on which the LLM selects potential candidate waypoints and plans\nlow-level paths towards selected waypoints. We further propose a high-level\nPathAgent which marks planned paths into the image input and reasons the most\nprobable path by comprehending all environmental information. Finally, we\nconvert the selected path into 3D coordinates using camera intrinsic parameters\nand depth information, avoiding challenging 3D predictions for LLMs.\nExperiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner\nachieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our\nmethod can also serve as a data annotator to obtain pseudo-labels, distilling\nits waypoint prediction ability into a learning-based predictor. This new\npredictor does not require any waypoint data from the simulator and achieves\n47% SR competing with supervised methods. We establish an effective connection\nbetween LLM and 3D world, presenting novel prospects for employing foundation\nmodels in low-level motion control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have demonstrated impressive zero-shot performance in\nvision-language navigation (VLN) task. However, existing LLM-based methods\noften focus only on solving high-level task planning by selecting nodes in\npredefined navigation graphs for movements, overlooking low-level control in\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\nAffordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates\nvarious foundation models to achieve affordances-oriented low-level motion\nplanning and high-level decision-making, both performed in a zero-shot setting.\nSpecifically, we employ a Visual Affordances Prompting (VAP) approach, where\nthe visible ground is segmented by SAM to provide navigational affordances,\nbased on which the LLM selects potential candidate waypoints and plans\nlow-level paths towards selected waypoints. We further propose a high-level\nPathAgent which marks planned paths into the image input and reasons the most\nprobable path by comprehending all environmental information. Finally, we\nconvert the selected path into 3D coordinates using camera intrinsic parameters\nand depth information, avoiding challenging 3D predictions for LLMs.\nExperiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner\nachieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our\nmethod can also serve as a data annotator to obtain pseudo-labels, distilling\nits waypoint prediction ability into a learning-based predictor. This new\npredictor does not require any waypoint data from the simulator and achieves\n47% SR competing with supervised methods. We establish an effective connection\nbetween LLM and 3D world, presenting novel prospects for employing foundation\nmodels in low-level motion control."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Bingqian Lin"
                    },
                    {
                        "name": "Xinmin Liu"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v2",
                "updated": "2024-08-21T03:31:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    31,
                    25,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Rusheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10902v1",
                "updated": "2024-08-20T14:45:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    45,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:45:23Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    45,
                    23,
                    1,
                    233,
                    0
                ],
                "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs"
                },
                "summary": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation."
                },
                "authors": [
                    {
                        "name": "John Mendonça"
                    },
                    {
                        "name": "Isabel Trancoso"
                    },
                    {
                        "name": "Alon Lavie"
                    }
                ],
                "author_detail": {
                    "name": "Alon Lavie"
                },
                "author": "Alon Lavie",
                "arxiv_comment": "22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10900v1",
                "updated": "2024-08-20T14:43:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    43,
                    33,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:43:33Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    43,
                    33,
                    1,
                    233,
                    0
                ],
                "title": "Towards Efficient Formal Verification of Spiking Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Formal Verification of Spiking Neural Network"
                },
                "summary": "Recently, AI research has primarily focused on large language models (LLMs),\nand increasing accuracy often involves scaling up and consuming more power. The\npower consumption of AI has become a significant societal issue; in this\ncontext, spiking neural networks (SNNs) offer a promising solution. SNNs\noperate event-driven, like the human brain, and compress information\ntemporally. These characteristics allow SNNs to significantly reduce power\nconsumption compared to perceptron-based artificial neural networks (ANNs),\nhighlighting them as a next-generation neural network technology. However,\nsocietal concerns regarding AI go beyond power consumption, with the\nreliability of AI models being a global issue. For instance, adversarial\nattacks on AI models are a well-studied problem in the context of traditional\nneural networks. Despite their importance, the stability and property\nverification of SNNs remains in the early stages of research. Most SNN\nverification methods are time-consuming and barely scalable, making practical\napplications challenging. In this paper, we introduce temporal encoding to\nachieve practical performance in verifying the adversarial robustness of SNNs.\nWe conduct a theoretical analysis of this approach and demonstrate its success\nin verifying SNNs at previously unmanageable scales. Our contribution advances\nSNN verification to a practical level, facilitating the safer application of\nSNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, AI research has primarily focused on large language models (LLMs),\nand increasing accuracy often involves scaling up and consuming more power. The\npower consumption of AI has become a significant societal issue; in this\ncontext, spiking neural networks (SNNs) offer a promising solution. SNNs\noperate event-driven, like the human brain, and compress information\ntemporally. These characteristics allow SNNs to significantly reduce power\nconsumption compared to perceptron-based artificial neural networks (ANNs),\nhighlighting them as a next-generation neural network technology. However,\nsocietal concerns regarding AI go beyond power consumption, with the\nreliability of AI models being a global issue. For instance, adversarial\nattacks on AI models are a well-studied problem in the context of traditional\nneural networks. Despite their importance, the stability and property\nverification of SNNs remains in the early stages of research. Most SNN\nverification methods are time-consuming and barely scalable, making practical\napplications challenging. In this paper, we introduce temporal encoding to\nachieve practical performance in verifying the adversarial robustness of SNNs.\nWe conduct a theoretical analysis of this approach and demonstrate its success\nin verifying SNNs at previously unmanageable scales. Our contribution advances\nSNN verification to a practical level, facilitating the safer application of\nSNNs."
                },
                "authors": [
                    {
                        "name": "Baekryun Seong"
                    },
                    {
                        "name": "Jieung Kim"
                    },
                    {
                        "name": "Sang-Ki Ko"
                    }
                ],
                "author_detail": {
                    "name": "Sang-Ki Ko"
                },
                "author": "Sang-Ki Ko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06209v2",
                "updated": "2024-08-20T14:35:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    35,
                    3,
                    1,
                    233,
                    0
                ],
                "published": "2024-04-09T10:58:21Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    10,
                    58,
                    21,
                    1,
                    100,
                    0
                ],
                "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models"
                },
                "summary": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets."
                },
                "authors": [
                    {
                        "name": "Sebastian Bordt"
                    },
                    {
                        "name": "Harsha Nori"
                    },
                    {
                        "name": "Vanessa Rodrigues"
                    },
                    {
                        "name": "Besmira Nushi"
                    },
                    {
                        "name": "Rich Caruana"
                    }
                ],
                "author_detail": {
                    "name": "Rich Caruana"
                },
                "author": "Rich Caruana",
                "arxiv_comment": "COLM camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00338v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00338v3",
                "updated": "2024-08-20T14:33:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    33,
                    32,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-01T06:23:54Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    6,
                    23,
                    54,
                    2,
                    122,
                    0
                ],
                "title": "Distillation Matters: Empowering Sequential Recommenders to Match the\n  Performance of Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation Matters: Empowering Sequential Recommenders to Match the\n  Performance of Large Language Model"
                },
                "summary": "Owing to their powerful semantic reasoning capabilities, Large Language\nModels (LLMs) have been effectively utilized as recommenders, achieving\nimpressive performance. However, the high inference latency of LLMs\nsignificantly restricts their practical deployment. To address this issue, this\nwork investigates knowledge distillation from cumbersome LLM-based\nrecommendation models to lightweight conventional sequential models. It\nencounters three challenges: 1) the teacher's knowledge may not always be\nreliable; 2) the capacity gap between the teacher and student makes it\ndifficult for the student to assimilate the teacher's knowledge; 3) divergence\nin semantic space poses a challenge to distill the knowledge from embeddings.\nTo tackle these challenges, this work proposes a novel distillation strategy,\nDLLM2Rec, specifically tailored for knowledge distillation from LLM-based\nrecommendation models to conventional sequential models. DLLM2Rec comprises: 1)\nImportance-aware ranking distillation, which filters reliable and\nstudent-friendly knowledge by weighting instances according to teacher\nconfidence and student-teacher consistency; 2) Collaborative embedding\ndistillation integrates knowledge from teacher embeddings with collaborative\nsignals mined from the data. Extensive experiments demonstrate the\neffectiveness of the proposed DLLM2Rec, boosting three typical sequential\nmodels with an average improvement of 47.97%, even enabling them to surpass\nLLM-based recommenders in some cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to their powerful semantic reasoning capabilities, Large Language\nModels (LLMs) have been effectively utilized as recommenders, achieving\nimpressive performance. However, the high inference latency of LLMs\nsignificantly restricts their practical deployment. To address this issue, this\nwork investigates knowledge distillation from cumbersome LLM-based\nrecommendation models to lightweight conventional sequential models. It\nencounters three challenges: 1) the teacher's knowledge may not always be\nreliable; 2) the capacity gap between the teacher and student makes it\ndifficult for the student to assimilate the teacher's knowledge; 3) divergence\nin semantic space poses a challenge to distill the knowledge from embeddings.\nTo tackle these challenges, this work proposes a novel distillation strategy,\nDLLM2Rec, specifically tailored for knowledge distillation from LLM-based\nrecommendation models to conventional sequential models. DLLM2Rec comprises: 1)\nImportance-aware ranking distillation, which filters reliable and\nstudent-friendly knowledge by weighting instances according to teacher\nconfidence and student-teacher consistency; 2) Collaborative embedding\ndistillation integrates knowledge from teacher embeddings with collaborative\nsignals mined from the data. Extensive experiments demonstrate the\neffectiveness of the proposed DLLM2Rec, boosting three typical sequential\nmodels with an average improvement of 47.97%, even enabling them to surpass\nLLM-based recommenders in some cases."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Pengbo Wang"
                    },
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Heng Tang"
                    },
                    {
                        "name": "Yi Wan"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jiawei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Chen"
                },
                "author": "Jiawei Chen",
                "arxiv_doi": "10.1145/3640457.3688118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640457.3688118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00338v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00338v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08890v2",
                "updated": "2024-08-20T14:19:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    19,
                    38,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-14T18:07:04Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    18,
                    7,
                    4,
                    1,
                    135,
                    0
                ],
                "title": "Language-Guided Self-Supervised Video Summarization Using Text Semantic\n  Matching Considering the Diversity of the Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Guided Self-Supervised Video Summarization Using Text Semantic\n  Matching Considering the Diversity of the Video"
                },
                "summary": "Current video summarization methods rely heavily on supervised computer\nvision techniques, which demands time-consuming and subjective manual\nannotations. To overcome these limitations, we investigated self-supervised\nvideo summarization. Inspired by the success of Large Language Models (LLMs),\nwe explored the feasibility in transforming the video summarization task into a\nNatural Language Processing (NLP) task. By leveraging the advantages of LLMs in\ncontext understanding, we aim to enhance the effectiveness of self-supervised\nvideo summarization. Our method begins by generating captions for individual\nvideo frames, which are then synthesized into text summaries by LLMs.\nSubsequently, we measure semantic distance between the captions and the text\nsummary. Notably, we propose a novel loss function to optimize our model\naccording to the diversity of the video. Finally, the summarized video can be\ngenerated by selecting the frames with captions similar to the text summary.\nOur method achieves state-of-the-art performance on the SumMe dataset in rank\ncorrelation coefficients. In addition, our method has a novel feature of being\nable to achieve personalized summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video summarization methods rely heavily on supervised computer\nvision techniques, which demands time-consuming and subjective manual\nannotations. To overcome these limitations, we investigated self-supervised\nvideo summarization. Inspired by the success of Large Language Models (LLMs),\nwe explored the feasibility in transforming the video summarization task into a\nNatural Language Processing (NLP) task. By leveraging the advantages of LLMs in\ncontext understanding, we aim to enhance the effectiveness of self-supervised\nvideo summarization. Our method begins by generating captions for individual\nvideo frames, which are then synthesized into text summaries by LLMs.\nSubsequently, we measure semantic distance between the captions and the text\nsummary. Notably, we propose a novel loss function to optimize our model\naccording to the diversity of the video. Finally, the summarized video can be\ngenerated by selecting the frames with captions similar to the text summary.\nOur method achieves state-of-the-art performance on the SumMe dataset in rank\ncorrelation coefficients. In addition, our method has a novel feature of being\nable to achieve personalized summarization."
                },
                "authors": [
                    {
                        "name": "Tomoya Sugihara"
                    },
                    {
                        "name": "Shuntaro Masuda"
                    },
                    {
                        "name": "Ling Xiao"
                    },
                    {
                        "name": "Toshihiko Yamasaki"
                    }
                ],
                "author_detail": {
                    "name": "Toshihiko Yamasaki"
                },
                "author": "Toshihiko Yamasaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10886v1",
                "updated": "2024-08-20T14:17:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    17,
                    50,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:17:50Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    17,
                    50,
                    1,
                    233,
                    0
                ],
                "title": "Leveraging LLMs for the Quality Assurance of Software Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for the Quality Assurance of Software Requirements"
                },
                "summary": "Successful software projects depend on the quality of software requirements.\nCreating high-quality requirements is a crucial step toward successful software\ndevelopment. Effective support in this area can significantly reduce\ndevelopment costs and enhance the software quality. In this paper, we introduce\nand assess the capabilities of a Large Language Model (LLM) to evaluate the\nquality characteristics of software requirements according to the ISO 29148\nstandard. We aim to further improve the support of stakeholders engaged in\nrequirements engineering (RE). We show how an LLM can assess requirements,\nexplain its decision-making process, and examine its capacity to propose\nimproved versions of requirements. We conduct a study with software engineers\nto validate our approach. Our findings emphasize the potential of LLMs for\nimproving the quality of software requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful software projects depend on the quality of software requirements.\nCreating high-quality requirements is a crucial step toward successful software\ndevelopment. Effective support in this area can significantly reduce\ndevelopment costs and enhance the software quality. In this paper, we introduce\nand assess the capabilities of a Large Language Model (LLM) to evaluate the\nquality characteristics of software requirements according to the ISO 29148\nstandard. We aim to further improve the support of stakeholders engaged in\nrequirements engineering (RE). We show how an LLM can assess requirements,\nexplain its decision-making process, and examine its capacity to propose\nimproved versions of requirements. We conduct a study with software engineers\nto validate our approach. Our findings emphasize the potential of LLMs for\nimproving the quality of software requirements."
                },
                "authors": [
                    {
                        "name": "Sebastian Lubos"
                    },
                    {
                        "name": "Alexander Felfernig"
                    },
                    {
                        "name": "Thi Ngoc Trang Tran"
                    },
                    {
                        "name": "Damian Garber"
                    },
                    {
                        "name": "Merfat El Mansi"
                    },
                    {
                        "name": "Seda Polat Erdeniz"
                    },
                    {
                        "name": "Viet-Man Le"
                    }
                ],
                "author_detail": {
                    "name": "Viet-Man Le"
                },
                "author": "Viet-Man Le",
                "arxiv_comment": "Accepted for publication at the RE@Next! track of RE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10883v1",
                "updated": "2024-08-20T14:13:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    13,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T14:13:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    13,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News\n  Detection"
                },
                "summary": "In current web environment, fake news spreads rapidly across online social\nnetworks, posing serious threats to society. Existing multimodal fake news\ndetection (MFND) methods can be classified into knowledge-based and\nsemantic-based approaches. However, these methods are overly dependent on human\nexpertise and feedback, lacking flexibility. To address this challenge, we\npropose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake\nnews detection. For knowledge-based methods, we introduce the Monte Carlo Tree\nSearch (MCTS) algorithm to leverage the self-reflective capabilities of large\nlanguage models (LLMs) for prompt optimization, providing richer,\ndomain-specific details and guidance to the LLMs, while enabling more flexible\nintegration of LLM comment on news content. For semantic-based methods, we\ndefine four typical deceit patterns: emotional exaggeration, logical\ninconsistency, image manipulation, and semantic inconsistency, to reveal the\nmechanisms behind fake news creation. To detect these patterns, we carefully\ndesign four discriminators and expand them in depth and breadth, using the\nsoft-routing mechanism to explore optimal detection models. Experimental\nresults on three real-world datasets demonstrate the superiority of our\napproach. The code will be available at: https://github.com/SuXinqi/DAAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In current web environment, fake news spreads rapidly across online social\nnetworks, posing serious threats to society. Existing multimodal fake news\ndetection (MFND) methods can be classified into knowledge-based and\nsemantic-based approaches. However, these methods are overly dependent on human\nexpertise and feedback, lacking flexibility. To address this challenge, we\npropose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake\nnews detection. For knowledge-based methods, we introduce the Monte Carlo Tree\nSearch (MCTS) algorithm to leverage the self-reflective capabilities of large\nlanguage models (LLMs) for prompt optimization, providing richer,\ndomain-specific details and guidance to the LLMs, while enabling more flexible\nintegration of LLM comment on news content. For semantic-based methods, we\ndefine four typical deceit patterns: emotional exaggeration, logical\ninconsistency, image manipulation, and semantic inconsistency, to reveal the\nmechanisms behind fake news creation. To detect these patterns, we carefully\ndesign four discriminators and expand them in depth and breadth, using the\nsoft-routing mechanism to explore optimal detection models. Experimental\nresults on three real-world datasets demonstrate the superiority of our\napproach. The code will be available at: https://github.com/SuXinqi/DAAD."
                },
                "authors": [
                    {
                        "name": "Xinqi Su"
                    },
                    {
                        "name": "Yawen Cui"
                    },
                    {
                        "name": "Ajian Liu"
                    },
                    {
                        "name": "Xun Lin"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Haochen Liang"
                    },
                    {
                        "name": "Wenhui Li"
                    },
                    {
                        "name": "Zitong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zitong Yu"
                },
                "author": "Zitong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01631v2",
                "updated": "2024-08-20T13:56:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    56,
                    21,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-01T11:56:08Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    11,
                    56,
                    8,
                    5,
                    153,
                    0
                ],
                "title": "SUBER: An RL Environment with Simulated Human Behavior for Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBER: An RL Environment with Simulated Human Behavior for Recommender\n  Systems"
                },
                "summary": "Reinforcement learning (RL) has gained popularity in the realm of recommender\nsystems due to its ability to optimize long-term rewards and guide users in\ndiscovering relevant content. However, the successful implementation of RL in\nrecommender systems is challenging because of several factors, including the\nlimited availability of online data for training on-policy methods. This\nscarcity requires expensive human interaction for online model training.\nFurthermore, the development of effective evaluation frameworks that accurately\nreflect the quality of models remains a fundamental challenge in recommender\nsystems. To address these challenges, we propose a comprehensive framework for\nsynthetic environments that simulate human behavior by harnessing the\ncapabilities of large language models (LLMs). We complement our framework with\nin-depth ablation studies and demonstrate its effectiveness with experiments on\nmovie and book recommendations. Using LLMs as synthetic users, this work\nintroduces a modular and novel framework to train RL-based recommender systems.\nThe software, including the RL environment, is publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has gained popularity in the realm of recommender\nsystems due to its ability to optimize long-term rewards and guide users in\ndiscovering relevant content. However, the successful implementation of RL in\nrecommender systems is challenging because of several factors, including the\nlimited availability of online data for training on-policy methods. This\nscarcity requires expensive human interaction for online model training.\nFurthermore, the development of effective evaluation frameworks that accurately\nreflect the quality of models remains a fundamental challenge in recommender\nsystems. To address these challenges, we propose a comprehensive framework for\nsynthetic environments that simulate human behavior by harnessing the\ncapabilities of large language models (LLMs). We complement our framework with\nin-depth ablation studies and demonstrate its effectiveness with experiments on\nmovie and book recommendations. Using LLMs as synthetic users, this work\nintroduces a modular and novel framework to train RL-based recommender systems.\nThe software, including the RL environment, is publicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Nathan Corecco"
                    },
                    {
                        "name": "Giorgio Piatti"
                    },
                    {
                        "name": "Luca A. Lanzendörfer"
                    },
                    {
                        "name": "Flint Xiaofeng Fan"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20557v2",
                "updated": "2024-08-20T13:42:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    42,
                    25,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-30T05:24:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    5,
                    24,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "CELLM: An Efficient Communication in Large Language Models Training for\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CELLM: An Efficient Communication in Large Language Models Training for\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training."
                },
                "authors": [
                    {
                        "name": "Raja Vavekanand"
                    },
                    {
                        "name": "Kira Sam"
                    }
                ],
                "author_detail": {
                    "name": "Kira Sam"
                },
                "author": "Kira Sam",
                "arxiv_comment": "arXiv admin note: This submission has been withdrawn by arXiv\n  administrators due to inappropriate text overlap with external sources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06611v3",
                "updated": "2024-08-20T13:41:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    41,
                    56,
                    1,
                    233,
                    0
                ],
                "published": "2023-09-12T21:32:25Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    21,
                    32,
                    25,
                    1,
                    255,
                    0
                ],
                "title": "Enabling the Deployment of Any-Scale Robotic Applications in\n  Microservice Architectures through Automated Containerization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Deployment of Any-Scale Robotic Applications in\n  Microservice Architectures through Automated Containerization"
                },
                "summary": "In an increasingly automated world -- from warehouse robots to self-driving\ncars -- streamlining the development and deployment process and operations of\nrobotic applications becomes ever more important. Automated DevOps processes\nand microservice architectures have already proven successful in other domains\nsuch as large-scale customer-oriented web services (e.g., Netflix). We\nrecommend to employ similar microservice architectures for the deployment of\nsmall- to large-scale robotic applications in order to accelerate development\ncycles, loosen functional dependence, and improve resiliency and elasticity. In\norder to facilitate involved DevOps processes, we present and release a tooling\nsuite for automating the development of microservices for robotic applications\nbased on the Robot Operating System (ROS). Our tooling suite covers the\nautomated minimal containerization of ROS applications, a collection of useful\nmachine learning-enabled base container images, as well as a CLI tool for\nsimplified interaction with container images during the development phase.\nWithin the scope of this paper, we embed our tooling suite into the overall\ncontext of streamlined robotics deployment and compare it to alternative\nsolutions. We release our tools as open-source software at\nhttps://github.com/ika-rwth-aachen/dorotos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an increasingly automated world -- from warehouse robots to self-driving\ncars -- streamlining the development and deployment process and operations of\nrobotic applications becomes ever more important. Automated DevOps processes\nand microservice architectures have already proven successful in other domains\nsuch as large-scale customer-oriented web services (e.g., Netflix). We\nrecommend to employ similar microservice architectures for the deployment of\nsmall- to large-scale robotic applications in order to accelerate development\ncycles, loosen functional dependence, and improve resiliency and elasticity. In\norder to facilitate involved DevOps processes, we present and release a tooling\nsuite for automating the development of microservices for robotic applications\nbased on the Robot Operating System (ROS). Our tooling suite covers the\nautomated minimal containerization of ROS applications, a collection of useful\nmachine learning-enabled base container images, as well as a CLI tool for\nsimplified interaction with container images during the development phase.\nWithin the scope of this paper, we embed our tooling suite into the overall\ncontext of streamlined robotics deployment and compare it to alternative\nsolutions. We release our tools as open-source software at\nhttps://github.com/ika-rwth-aachen/dorotos."
                },
                "authors": [
                    {
                        "name": "Jean-Pierre Busch"
                    },
                    {
                        "name": "Lennart Reiher"
                    },
                    {
                        "name": "Lutz Eckstein"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Eckstein"
                },
                "author": "Lutz Eckstein",
                "arxiv_doi": "10.1109/ICRA57147.2024.10611586",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICRA57147.2024.10611586",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.06611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages; Submitted to IEEE International Conference on Robotics and\n  Automation (ICRA) 2024",
                "arxiv_journal_ref": "2024 IEEE International Conference on Robotics and Automation\n  (ICRA)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v1",
                "updated": "2024-08-20T13:40:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17657v2",
                "updated": "2024-08-20T13:39:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    39,
                    51,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-24T21:50:36Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    21,
                    50,
                    36,
                    2,
                    206,
                    0
                ],
                "title": "My Ontologist: Evaluating BFO-Based AI for Definition Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "My Ontologist: Evaluating BFO-Based AI for Definition Support"
                },
                "summary": "Generative artificial intelligence (AI), exemplified by the release of\nGPT-3.5 in 2022, has significantly advanced the potential applications of large\nlanguage models (LLMs), including in the realms of ontology development and\nknowledge graph creation. Ontologies, which are structured frameworks for\norganizing information, and knowledge graphs, which combine ontologies with\nactual data, are essential for enabling interoperability and automated\nreasoning. However, current research has largely overlooked the generation of\nontologies extending from established upper-level frameworks like the Basic\nFormal Ontology (BFO), risking the creation of non-integrable ontology silos.\nThis study explores the extent to which LLMs, particularly GPT-4, can support\nontologists trained in BFO. Through iterative development of a specialized GPT\nmodel named \"My Ontologist,\" we aimed to generate BFO-conformant ontologies.\nInitial versions faced challenges in maintaining definition conventions and\nleveraging foundational texts effectively. My Ontologist 3.0 showed promise by\nadhering to structured rules and modular ontology suites, yet the release of\nGPT-4o disrupted this progress by altering the model's behavior. Our findings\nunderscore the importance of aligning LLM-generated ontologies with top-level\nstandards and highlight the complexities of integrating evolving AI\ncapabilities in ontology engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (AI), exemplified by the release of\nGPT-3.5 in 2022, has significantly advanced the potential applications of large\nlanguage models (LLMs), including in the realms of ontology development and\nknowledge graph creation. Ontologies, which are structured frameworks for\norganizing information, and knowledge graphs, which combine ontologies with\nactual data, are essential for enabling interoperability and automated\nreasoning. However, current research has largely overlooked the generation of\nontologies extending from established upper-level frameworks like the Basic\nFormal Ontology (BFO), risking the creation of non-integrable ontology silos.\nThis study explores the extent to which LLMs, particularly GPT-4, can support\nontologists trained in BFO. Through iterative development of a specialized GPT\nmodel named \"My Ontologist,\" we aimed to generate BFO-conformant ontologies.\nInitial versions faced challenges in maintaining definition conventions and\nleveraging foundational texts effectively. My Ontologist 3.0 showed promise by\nadhering to structured rules and modular ontology suites, yet the release of\nGPT-4o disrupted this progress by altering the model's behavior. Our findings\nunderscore the importance of aligning LLM-generated ontologies with top-level\nstandards and highlight the complexities of integrating evolving AI\ncapabilities in ontology engineering."
                },
                "authors": [
                    {
                        "name": "Carter Benson"
                    },
                    {
                        "name": "Alec Sculley"
                    },
                    {
                        "name": "Austin Liebers"
                    },
                    {
                        "name": "John Beverley"
                    }
                ],
                "author_detail": {
                    "name": "John Beverley"
                },
                "author": "John Beverley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10839v1",
                "updated": "2024-08-20T13:34:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    34,
                    17,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:34:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    34,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Benchmarking Large Language Models for Math Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Math Reasoning Tasks"
                },
                "summary": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research."
                },
                "authors": [
                    {
                        "name": "Kathrin Seßler"
                    },
                    {
                        "name": "Yao Rong"
                    },
                    {
                        "name": "Emek Gözlüklü"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10828v1",
                "updated": "2024-08-20T13:22:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    22,
                    50,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:22:50Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    22,
                    50,
                    1,
                    233,
                    0
                ],
                "title": "Scalable DAQ system operating the CHIPS-5 neutrino detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable DAQ system operating the CHIPS-5 neutrino detector"
                },
                "summary": "The CHIPS R&D project focuses on development of low-cost water Cherenkov\nneutrino detectors through novel design strategies and resourceful engineering.\nThis work presents an end-to-end DAQ solution intended for a recent 5 kt CHIPS\nprototype, which is largely based on affordable mass-produced components. Much\nlike the detector itself, the presented instrumentation is composed of modular\narrays that can be scaled up and easily serviced. A single such array can carry\nup to 30 photomultiplier tubes (PMTs) accompanied by electronics that generate\nhigh voltage in-situ and deliver time resolution of up to 0.69 ns. In addition,\nthe technology is compatible with the White Rabbit timing system, which can\nsynchronize its elements to within 100 ps. While deployment issues did not\npermit the presented DAQ system to operate beyond initial evaluation, the\npresented hardware and software successfully passed numerous commissioning\ntests that demonstrated their viability for use in a large-scale neutrino\ndetector, instrumented with thousands of PMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CHIPS R&D project focuses on development of low-cost water Cherenkov\nneutrino detectors through novel design strategies and resourceful engineering.\nThis work presents an end-to-end DAQ solution intended for a recent 5 kt CHIPS\nprototype, which is largely based on affordable mass-produced components. Much\nlike the detector itself, the presented instrumentation is composed of modular\narrays that can be scaled up and easily serviced. A single such array can carry\nup to 30 photomultiplier tubes (PMTs) accompanied by electronics that generate\nhigh voltage in-situ and deliver time resolution of up to 0.69 ns. In addition,\nthe technology is compatible with the White Rabbit timing system, which can\nsynchronize its elements to within 100 ps. While deployment issues did not\npermit the presented DAQ system to operate beyond initial evaluation, the\npresented hardware and software successfully passed numerous commissioning\ntests that demonstrated their viability for use in a large-scale neutrino\ndetector, instrumented with thousands of PMTs."
                },
                "authors": [
                    {
                        "name": "Belén Alonso Rancurel"
                    },
                    {
                        "name": "Son Cao"
                    },
                    {
                        "name": "Thomas J. Carroll"
                    },
                    {
                        "name": "Rhys Castellan"
                    },
                    {
                        "name": "Erika Catano-Mur"
                    },
                    {
                        "name": "John P. Cesar"
                    },
                    {
                        "name": "João A. B. Coelho"
                    },
                    {
                        "name": "Patrick Dills"
                    },
                    {
                        "name": "Thomas Dodwell"
                    },
                    {
                        "name": "Jack Edmondson"
                    },
                    {
                        "name": "Daan van Eijk"
                    },
                    {
                        "name": "Quinn Fetterly"
                    },
                    {
                        "name": "Zoé Garbal"
                    },
                    {
                        "name": "Stefano Germani"
                    },
                    {
                        "name": "Thomas Gilpin"
                    },
                    {
                        "name": "Anthony Giraudo"
                    },
                    {
                        "name": "Alec Habig"
                    },
                    {
                        "name": "Daniel Hanuska"
                    },
                    {
                        "name": "Harry Hausner"
                    },
                    {
                        "name": "Wilson Y. Hernandez"
                    },
                    {
                        "name": "Anna Holin"
                    },
                    {
                        "name": "Junting Huang"
                    },
                    {
                        "name": "Sebastian B. Jones"
                    },
                    {
                        "name": "Albrecht Karle"
                    },
                    {
                        "name": "George Kileff"
                    },
                    {
                        "name": "Kai R. Jenkins"
                    },
                    {
                        "name": "Paul Kooijman"
                    },
                    {
                        "name": "Arthur Kreymer"
                    },
                    {
                        "name": "Gabe M. LaFond"
                    },
                    {
                        "name": "Karol Lang"
                    },
                    {
                        "name": "Jeffrey P. Lazar"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Kexin Liu"
                    },
                    {
                        "name": "David A. Loving"
                    },
                    {
                        "name": "Petr Mánek"
                    },
                    {
                        "name": "Marvin L. Marshak"
                    },
                    {
                        "name": "Jerry R. Meier"
                    },
                    {
                        "name": "William Miller"
                    },
                    {
                        "name": "Jeffrey K. Nelson"
                    },
                    {
                        "name": "Christopher Ng"
                    },
                    {
                        "name": "Ryan J. Nichol"
                    },
                    {
                        "name": "Vittorio Paolone"
                    },
                    {
                        "name": "Andrew Perch"
                    },
                    {
                        "name": "Maciej M. Pfützner"
                    },
                    {
                        "name": "Alexander Radovic"
                    },
                    {
                        "name": "Katherine Rawlins"
                    },
                    {
                        "name": "Patrick Roedl"
                    },
                    {
                        "name": "Lucas Rogers"
                    },
                    {
                        "name": "Ibrahim Safa"
                    },
                    {
                        "name": "Alexandre Sousa"
                    },
                    {
                        "name": "Josh Tingey"
                    },
                    {
                        "name": "Jennifer Thomas"
                    },
                    {
                        "name": "Jozef Trokan-Tenorio"
                    },
                    {
                        "name": "Patricia Vahle"
                    },
                    {
                        "name": "Richard Wade"
                    },
                    {
                        "name": "Christopher Wendt"
                    },
                    {
                        "name": "Daniel Wendt"
                    },
                    {
                        "name": "Leigh H. Whitehead"
                    },
                    {
                        "name": "Samuel Wolcott"
                    },
                    {
                        "name": "Tianlu Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Tianlu Yuan"
                },
                "author": "Tianlu Yuan",
                "arxiv_comment": "30 pages, 28 figures, submitted to MDPI Applied Sciences, Special\n  Issue: Advanced Neutrino Detector Development and Application",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10826v1",
                "updated": "2024-08-20T13:21:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    21,
                    52,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:21:52Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    21,
                    52,
                    1,
                    233,
                    0
                ],
                "title": "NeuLite: Memory-Efficient Federated Learning via Elastic Progressive\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuLite: Memory-Efficient Federated Learning via Elastic Progressive\n  Training"
                },
                "summary": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, intensive memory footprint during the training process\nseverely bottlenecks the deployment of FL on resource-constrained devices in\nreal-world cases. In this paper, we propose NeuLite, a framework that breaks\nthe memory wall through elastic progressive training. Unlike traditional FL,\nwhich updates the full model during the whole training procedure, NeuLite\ndivides the model into blocks and conducts the training process in a\nprogressive manner. Except for the progressive training paradigm, NeuLite\nfurther features the following two key components to guide the training\nprocess: 1) curriculum mentor and 2) training harmonizer. Specifically, the\nCurriculum Mentor devises curriculum-aware training losses for each block,\nassisting them in learning the expected feature representation and mitigating\nthe loss of valuable information. Additionally, the Training Harmonizer\ndevelops a parameter co-adaptation training paradigm to break the information\nisolation across blocks from both forward and backward propagation.\nFurthermore, it constructs output modules for each block to strengthen model\nparameter co-adaptation. Extensive experiments are conducted to evaluate the\neffectiveness of NeuLite across both simulation and hardware testbeds. The\nresults demonstrate that NeuLite effectively reduces peak memory usage by up to\n50.4%. It also enhances model performance by up to 84.2% and accelerates the\ntraining process by up to 1.9X.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, intensive memory footprint during the training process\nseverely bottlenecks the deployment of FL on resource-constrained devices in\nreal-world cases. In this paper, we propose NeuLite, a framework that breaks\nthe memory wall through elastic progressive training. Unlike traditional FL,\nwhich updates the full model during the whole training procedure, NeuLite\ndivides the model into blocks and conducts the training process in a\nprogressive manner. Except for the progressive training paradigm, NeuLite\nfurther features the following two key components to guide the training\nprocess: 1) curriculum mentor and 2) training harmonizer. Specifically, the\nCurriculum Mentor devises curriculum-aware training losses for each block,\nassisting them in learning the expected feature representation and mitigating\nthe loss of valuable information. Additionally, the Training Harmonizer\ndevelops a parameter co-adaptation training paradigm to break the information\nisolation across blocks from both forward and backward propagation.\nFurthermore, it constructs output modules for each block to strengthen model\nparameter co-adaptation. Extensive experiments are conducted to evaluate the\neffectiveness of NeuLite across both simulation and hardware testbeds. The\nresults demonstrate that NeuLite effectively reduces peak memory usage by up to\n50.4%. It also enhances model performance by up to 84.2% and accelerates the\ntraining process by up to 1.9X."
                },
                "authors": [
                    {
                        "name": "Yebo Wu"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Dubing Chen"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10819v1",
                "updated": "2024-08-20T13:13:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    13,
                    41,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:13:41Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    13,
                    41,
                    1,
                    233,
                    0
                ],
                "title": "Exploiting Large Language Models Capabilities for Question Answer-Driven\n  Knowledge Graph Completion Across Static and Temporal Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Large Language Models Capabilities for Question Answer-Driven\n  Knowledge Graph Completion Across Static and Temporal Domains"
                },
                "summary": "Knowledge graph completion (KGC) aims to identify missing triples in a\nknowledge graph (KG). This is typically achieved through tasks such as link\nprediction and instance completion. However, these methods often focus on\neither static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),\naddressing only within-scope triples. This paper introduces a new generative\ncompletion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC\nemploys a question-answering format to directly generate target entities,\naddressing the challenge of questions having multiple possible answers. We\npropose a strategy that extracts subgraphs centered on entities and\nrelationships within the KG, from which negative samples and neighborhood\ninformation are separately obtained to address the one-to-many problem. Our\nmethod generates negative samples using known facts to facilitate the discovery\nof new information. Furthermore, we collect and refine neighborhood path data\nof known entities, providing contextual information to enhance reasoning in\nlarge language models (LLMs). Our experiments evaluated the proposed method on\nfour SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five\ndatasets. Analysis of the results shows that GS-KGC can discover new triples\nwithin existing KGs and generate new facts beyond the closed KG, effectively\nbridging the gap between closed-world and open-world KGC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph completion (KGC) aims to identify missing triples in a\nknowledge graph (KG). This is typically achieved through tasks such as link\nprediction and instance completion. However, these methods often focus on\neither static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),\naddressing only within-scope triples. This paper introduces a new generative\ncompletion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC\nemploys a question-answering format to directly generate target entities,\naddressing the challenge of questions having multiple possible answers. We\npropose a strategy that extracts subgraphs centered on entities and\nrelationships within the KG, from which negative samples and neighborhood\ninformation are separately obtained to address the one-to-many problem. Our\nmethod generates negative samples using known facts to facilitate the discovery\nof new information. Furthermore, we collect and refine neighborhood path data\nof known entities, providing contextual information to enhance reasoning in\nlarge language models (LLMs). Our experiments evaluated the proposed method on\nfour SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five\ndatasets. Analysis of the results shows that GS-KGC can discover new triples\nwithin existing KGs and generate new facts beyond the closed KG, effectively\nbridging the gap between closed-world and open-world KGC."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiahao Zhu"
                    },
                    {
                        "name": "Jianping Man"
                    },
                    {
                        "name": "Li Fang"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10811v1",
                "updated": "2024-08-20T13:05:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    5,
                    41,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T13:05:41Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    5,
                    41,
                    1,
                    233,
                    0
                ],
                "title": "Beyond English-Centric LLMs: What Language Do Multilingual Language\n  Models Think in?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond English-Centric LLMs: What Language Do Multilingual Language\n  Models Think in?"
                },
                "summary": "In this study, we investigate whether non-English-centric LLMs, despite their\nstrong performance, `think' in their respective dominant language: more\nprecisely, `think' refers to how the representations of intermediate layers,\nwhen un-embedded into the vocabulary space, exhibit higher probabilities for\ncertain dominant languages during generation. We term such languages as\ninternal $\\textbf{latent languages}$.\n  We examine the latent language of three typical categories of models for\nJapanese processing: Llama2, an English-centric model; Swallow, an\nEnglish-centric model with continued pre-training in Japanese; and LLM-jp, a\nmodel pre-trained on balanced English and Japanese corpora. Our empirical\nfindings reveal that, unlike Llama2 which relies exclusively on English as the\ninternal latent language, Japanese-specific Swallow and LLM-jp employ both\nJapanese and English, exhibiting dual internal latent languages. For any given\ntarget language, the model preferentially activates the latent language most\nclosely related to it. In addition, we explore how intermediate layers respond\nto questions involving cultural conflicts between latent internal and target\noutput languages. We further explore how the language identity shifts across\nlayers while keeping consistent semantic meaning reflected in the intermediate\nlayer representations.\n  This study deepens the understanding of non-English-centric large language\nmodels, highlighting the intricate dynamics of language representation within\ntheir intermediate layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether non-English-centric LLMs, despite their\nstrong performance, `think' in their respective dominant language: more\nprecisely, `think' refers to how the representations of intermediate layers,\nwhen un-embedded into the vocabulary space, exhibit higher probabilities for\ncertain dominant languages during generation. We term such languages as\ninternal $\\textbf{latent languages}$.\n  We examine the latent language of three typical categories of models for\nJapanese processing: Llama2, an English-centric model; Swallow, an\nEnglish-centric model with continued pre-training in Japanese; and LLM-jp, a\nmodel pre-trained on balanced English and Japanese corpora. Our empirical\nfindings reveal that, unlike Llama2 which relies exclusively on English as the\ninternal latent language, Japanese-specific Swallow and LLM-jp employ both\nJapanese and English, exhibiting dual internal latent languages. For any given\ntarget language, the model preferentially activates the latent language most\nclosely related to it. In addition, we explore how intermediate layers respond\nto questions involving cultural conflicts between latent internal and target\noutput languages. We further explore how the language identity shifts across\nlayers while keeping consistent semantic meaning reflected in the intermediate\nlayer representations.\n  This study deepens the understanding of non-English-centric large language\nmodels, highlighting the intricate dynamics of language representation within\ntheir intermediate layers."
                },
                "authors": [
                    {
                        "name": "Chengzhi Zhong"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Qianying Liu"
                    },
                    {
                        "name": "Junfeng Jiang"
                    },
                    {
                        "name": "Zhen Wan"
                    },
                    {
                        "name": "Chenhui Chu"
                    },
                    {
                        "name": "Yugo Murawaki"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    }
                ],
                "author_detail": {
                    "name": "Sadao Kurohashi"
                },
                "author": "Sadao Kurohashi",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10794v1",
                "updated": "2024-08-20T12:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    38,
                    34,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T12:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    38,
                    34,
                    1,
                    233,
                    0
                ],
                "title": "Tapping in a Remote Vehicle's onboard LLM to Complement the Ego\n  Vehicle's Field-of-View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapping in a Remote Vehicle's onboard LLM to Complement the Ego\n  Vehicle's Field-of-View"
                },
                "summary": "Today's advanced automotive systems are turning into intelligent\nCyber-Physical Systems (CPS), bringing computational intelligence to their\ncyber-physical context. Such systems power advanced driver assistance systems\n(ADAS) that observe a vehicle's surroundings for their functionality. However,\nsuch ADAS have clear limitations in scenarios when the direct line-of-sight to\nsurrounding objects is occluded, like in urban areas. Imagine now automated\ndriving (AD) systems that ideally could benefit from other vehicles'\nfield-of-view in such occluded situations to increase traffic safety if, for\nexample, locations about pedestrians can be shared across vehicles. Current\nliterature suggests vehicle-to-infrastructure (V2I) via roadside units (RSUs)\nor vehicle-to-vehicle (V2V) communication to address such issues that stream\nsensor or object data between vehicles. When considering the ongoing revolution\nin vehicle system architectures towards powerful, centralized processing units\nwith hardware accelerators, foreseeing the onboard presence of large language\nmodels (LLMs) to improve the passengers' comfort when using voice assistants\nbecomes a reality. We are suggesting and evaluating a concept to complement the\nego vehicle's field-of-view (FOV) with another vehicle's FOV by tapping into\ntheir onboard LLM to let the machines have a dialogue about what the other\nvehicle ``sees''. Our results show that very recent versions of LLMs, such as\nGPT-4V and GPT-4o, understand a traffic situation to an impressive level of\ndetail, and hence, they can be used even to spot traffic participants. However,\nbetter prompts are needed to improve the detection quality and future work is\nneeded towards a standardised message interchange format between vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's advanced automotive systems are turning into intelligent\nCyber-Physical Systems (CPS), bringing computational intelligence to their\ncyber-physical context. Such systems power advanced driver assistance systems\n(ADAS) that observe a vehicle's surroundings for their functionality. However,\nsuch ADAS have clear limitations in scenarios when the direct line-of-sight to\nsurrounding objects is occluded, like in urban areas. Imagine now automated\ndriving (AD) systems that ideally could benefit from other vehicles'\nfield-of-view in such occluded situations to increase traffic safety if, for\nexample, locations about pedestrians can be shared across vehicles. Current\nliterature suggests vehicle-to-infrastructure (V2I) via roadside units (RSUs)\nor vehicle-to-vehicle (V2V) communication to address such issues that stream\nsensor or object data between vehicles. When considering the ongoing revolution\nin vehicle system architectures towards powerful, centralized processing units\nwith hardware accelerators, foreseeing the onboard presence of large language\nmodels (LLMs) to improve the passengers' comfort when using voice assistants\nbecomes a reality. We are suggesting and evaluating a concept to complement the\nego vehicle's field-of-view (FOV) with another vehicle's FOV by tapping into\ntheir onboard LLM to let the machines have a dialogue about what the other\nvehicle ``sees''. Our results show that very recent versions of LLMs, such as\nGPT-4V and GPT-4o, understand a traffic situation to an impressive level of\ndetail, and hence, they can be used even to spot traffic participants. However,\nbetter prompts are needed to improve the detection quality and future work is\nneeded towards a standardised message interchange format between vehicles."
                },
                "authors": [
                    {
                        "name": "Malsha Ashani Mahawatta Dona"
                    },
                    {
                        "name": "Beatriz Cabrero-Daniel"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Christian Berger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Berger"
                },
                "author": "Christian Berger",
                "arxiv_comment": "50th Euromicro Conference Series on Software Engineering and Advanced\n  Applications (SEAA) 2024 - WiP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00868v3",
                "updated": "2024-08-20T12:37:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    37,
                    2,
                    1,
                    233,
                    0
                ],
                "published": "2024-03-01T04:39:16Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    4,
                    39,
                    16,
                    4,
                    61,
                    0
                ],
                "title": "SoftTiger: A Clinical Foundation Model for Healthcare Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftTiger: A Clinical Foundation Model for Healthcare Workflows"
                },
                "summary": "We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry."
                },
                "authors": [
                    {
                        "name": "Ye Chen"
                    },
                    {
                        "name": "Igor Couto"
                    },
                    {
                        "name": "Wei Cai"
                    },
                    {
                        "name": "Cong Fu"
                    },
                    {
                        "name": "Bruno Dorneles"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Dorneles"
                },
                "author": "Bruno Dorneles",
                "arxiv_comment": "Accepted to AAAI 2024 Spring Symposium on Clinical Foundation Models,\n  Stanford University, Stanford, California",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04472v2",
                "updated": "2024-08-20T12:36:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    36,
                    6,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-08T14:02:45Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    14,
                    2,
                    45,
                    3,
                    221,
                    0
                ],
                "title": "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for\n  Competitive Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for\n  Competitive Debate"
                },
                "summary": "Competitive debate is a complex task of computational argumentation. Large\nLanguage Models (LLMs) suffer from hallucinations and lack competitiveness in\nthis field. To address these challenges, we introduce Agent for Debate\n(Agent4Debate), a dynamic multi-agent framework based on LLMs designed to\nenhance their capabilities in competitive debate. Drawing inspiration from\nhuman behavior in debate preparation and execution, Agent4Debate employs a\ncollaborative architecture where four specialized agents, involving Searcher,\nAnalyzer, Writer, and Reviewer, dynamically interact and cooperate. These\nagents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Competitive\nDebate Arena, comprising 66 carefully selected Chinese debate motions. We\nrecruit ten experienced human debaters and collect records of 200 debates\ninvolving Agent4Debate, baseline models, and humans. The evaluation employs the\nDebatrix automatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive debate is a complex task of computational argumentation. Large\nLanguage Models (LLMs) suffer from hallucinations and lack competitiveness in\nthis field. To address these challenges, we introduce Agent for Debate\n(Agent4Debate), a dynamic multi-agent framework based on LLMs designed to\nenhance their capabilities in competitive debate. Drawing inspiration from\nhuman behavior in debate preparation and execution, Agent4Debate employs a\ncollaborative architecture where four specialized agents, involving Searcher,\nAnalyzer, Writer, and Reviewer, dynamically interact and cooperate. These\nagents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Competitive\nDebate Arena, comprising 66 carefully selected Chinese debate motions. We\nrecruit ten experienced human debaters and collect records of 200 debates\ninvolving Agent4Debate, baseline models, and humans. The evaluation employs the\nDebatrix automatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure."
                },
                "authors": [
                    {
                        "name": "Yiqun Zhang"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Kaisong Song"
                    }
                ],
                "author_detail": {
                    "name": "Kaisong Song"
                },
                "author": "Kaisong Song",
                "arxiv_comment": "12 pages (including appendix), 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09713v2",
                "updated": "2024-08-20T12:22:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    22,
                    16,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T06:05:24Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    5,
                    24,
                    0,
                    232,
                    0
                ],
                "title": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation"
                },
                "summary": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices."
                },
                "authors": [
                    {
                        "name": "Haijin Wang"
                    },
                    {
                        "name": "Mianrong Zhang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Nan Shang"
                    },
                    {
                        "name": "Shangheng Yao"
                    },
                    {
                        "name": "Fushuan Wen"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v2",
                "updated": "2024-08-21T06:48:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    48,
                    16,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "29 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10764v1",
                "updated": "2024-08-20T12:00:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    0,
                    35,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T12:00:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    0,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion\n  for Efficient Inference Intervention in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion\n  for Efficient Inference Intervention in Large Language Model"
                },
                "summary": "Transformer-based large language models (LLMs) exhibit limitations such as\ngenerating unsafe responses, unreliable reasoning, etc. Existing inference\nintervention approaches attempt to mitigate these issues by finetuning\nadditional models to produce calibration signals (such as rewards) that guide\nthe LLM's decoding process. However, this solution introduces substantial time\nand space overhead due to the separate models required. This work proposes\nNon-disruptive parameters insertion (Otter), inserting extra parameters into\nthe transformer architecture to predict calibration signals along with the\noriginal LLM output. Otter offers state-of-the-art performance on multiple\ndemanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time.\nFurthermore, Otter seamlessly integrates with existing inference engines,\nrequiring only a one-line code change, and the original model response remains\naccessible after the parameter insertion. Our code is publicly available at\n\\url{https://github.com/chenhan97/Otter}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) exhibit limitations such as\ngenerating unsafe responses, unreliable reasoning, etc. Existing inference\nintervention approaches attempt to mitigate these issues by finetuning\nadditional models to produce calibration signals (such as rewards) that guide\nthe LLM's decoding process. However, this solution introduces substantial time\nand space overhead due to the separate models required. This work proposes\nNon-disruptive parameters insertion (Otter), inserting extra parameters into\nthe transformer architecture to predict calibration signals along with the\noriginal LLM output. Otter offers state-of-the-art performance on multiple\ndemanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time.\nFurthermore, Otter seamlessly integrates with existing inference engines,\nrequiring only a one-line code change, and the original model response remains\naccessible after the parameter insertion. Our code is publicly available at\n\\url{https://github.com/chenhan97/Otter}"
                },
                "authors": [
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Ru Peng"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01290v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01290v4",
                "updated": "2024-08-21T08:01:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    1,
                    4,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-03T13:01:09Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    1,
                    9,
                    0,
                    155,
                    0
                ],
                "title": "Resource-constrained Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-constrained Fairness"
                },
                "summary": "Access to resources strongly constrains the decisions we make. While we might\nwish to offer every student a scholarship, or schedule every patient for\nfollow-up meetings with a specialist, limited resources mean that this is not\npossible. When deploying machine learning systems, these resource constraints\nare simply enforced by varying the threshold of a classifier. However, these\nfinite resource limitations are disregarded by most existing tools for fair\nmachine learning, which do not allow the specification of resource limitations\nand do not remain fair when varying thresholds. This makes them ill-suited for\nreal-world deployment. Our research introduces the concept of\n\"resource-constrained fairness\" and quantifies the cost of fairness within this\nframework. We demonstrate that the level of available resources significantly\ninfluences this cost, a factor overlooked in previous evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to resources strongly constrains the decisions we make. While we might\nwish to offer every student a scholarship, or schedule every patient for\nfollow-up meetings with a specialist, limited resources mean that this is not\npossible. When deploying machine learning systems, these resource constraints\nare simply enforced by varying the threshold of a classifier. However, these\nfinite resource limitations are disregarded by most existing tools for fair\nmachine learning, which do not allow the specification of resource limitations\nand do not remain fair when varying thresholds. This makes them ill-suited for\nreal-world deployment. Our research introduces the concept of\n\"resource-constrained fairness\" and quantifies the cost of fairness within this\nframework. We demonstrate that the level of available resources significantly\ninfluences this cost, a factor overlooked in previous evaluations."
                },
                "authors": [
                    {
                        "name": "Sofie Goethals"
                    },
                    {
                        "name": "Eoin Delaney"
                    },
                    {
                        "name": "Brent Mittelstadt"
                    },
                    {
                        "name": "Chris Russell"
                    }
                ],
                "author_detail": {
                    "name": "Chris Russell"
                },
                "author": "Chris Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01290v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01290v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20132v3",
                "updated": "2024-08-20T11:06:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    6,
                    9,
                    1,
                    233,
                    0
                ],
                "published": "2024-05-30T15:10:59Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    15,
                    10,
                    59,
                    3,
                    151,
                    0
                ],
                "title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs."
                },
                "authors": [
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Bäck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bäck"
                },
                "author": "Thomas Bäck",
                "arxiv_comment": "Submitted to IEEE TEVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10734v1",
                "updated": "2024-08-20T11:05:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    5,
                    56,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:05:56Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    5,
                    56,
                    1,
                    233,
                    0
                ],
                "title": "Vector Symbolic Open Source Information Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector Symbolic Open Source Information Discovery"
                },
                "summary": "Combined, joint, intra-governmental, inter-agency and multinational (CJIIM)\noperations require rapid data sharing without the bottlenecks of metadata\ncuration and alignment. Curation and alignment is particularly infeasible for\nexternal open source information (OSINF), e.g., social media, which has become\nincreasingly valuable in understanding unfolding situations. Large language\nmodels (transformers) facilitate semantic data and metadata alignment but are\ninefficient in CJIIM settings characterised as denied, degraded, intermittent\nand low bandwidth (DDIL). Vector symbolic architectures (VSA) support semantic\ninformation processing using highly compact binary vectors, typically 1-10k\nbits, suitable in a DDIL setting. We demonstrate a novel integration of\ntransformer models with VSA, combining the power of the former for semantic\nmatching with the compactness and representational structure of the latter. The\napproach is illustrated via a proof-of-concept OSINF data discovery portal that\nallows partners in a CJIIM operation to share data sources with minimal\nmetadata curation and low communications bandwidth. This work was carried out\nas a bridge between previous low technology readiness level (TRL) research and\nfuture higher-TRL technology demonstration and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combined, joint, intra-governmental, inter-agency and multinational (CJIIM)\noperations require rapid data sharing without the bottlenecks of metadata\ncuration and alignment. Curation and alignment is particularly infeasible for\nexternal open source information (OSINF), e.g., social media, which has become\nincreasingly valuable in understanding unfolding situations. Large language\nmodels (transformers) facilitate semantic data and metadata alignment but are\ninefficient in CJIIM settings characterised as denied, degraded, intermittent\nand low bandwidth (DDIL). Vector symbolic architectures (VSA) support semantic\ninformation processing using highly compact binary vectors, typically 1-10k\nbits, suitable in a DDIL setting. We demonstrate a novel integration of\ntransformer models with VSA, combining the power of the former for semantic\nmatching with the compactness and representational structure of the latter. The\napproach is illustrated via a proof-of-concept OSINF data discovery portal that\nallows partners in a CJIIM operation to share data sources with minimal\nmetadata curation and low communications bandwidth. This work was carried out\nas a bridge between previous low technology readiness level (TRL) research and\nfuture higher-TRL technology demonstration and deployment."
                },
                "authors": [
                    {
                        "name": "Cai Davies"
                    },
                    {
                        "name": "Sam Meek"
                    },
                    {
                        "name": "Philip Hawkins"
                    },
                    {
                        "name": "Benomy Tutcher"
                    },
                    {
                        "name": "Graham Bent"
                    },
                    {
                        "name": "Alun Preece"
                    }
                ],
                "author_detail": {
                    "name": "Alun Preece"
                },
                "author": "Alun Preece",
                "arxiv_doi": "10.1117/12.3013447",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3013447",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Artificial Intelligence and Machine Learning for Multi-Domain\n  Operations Applications VI, vol. 13051, pp. 380-390. SPIE, 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10729v1",
                "updated": "2024-08-20T10:57:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    57,
                    34,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:57:34Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    57,
                    34,
                    1,
                    233,
                    0
                ],
                "title": "Towards Efficient Large Language Models for Scientific Text: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Large Language Models for Scientific Text: A Review"
                },
                "summary": "Large language models (LLMs) have ushered in a new era for processing complex\ninformation in various fields, including science. The increasing amount of\nscientific literature allows these models to acquire and understand scientific\nknowledge effectively, thus improving their performance in a wide range of\ntasks. Due to the power of LLMs, they require extremely expensive computational\nresources, intense amounts of data, and training time. Therefore, in recent\nyears, researchers have proposed various methodologies to make scientific LLMs\nmore affordable. The most well-known approaches align in two directions. It can\nbe either focusing on the size of the models or enhancing the quality of data.\nTo date, a comprehensive review of these two families of methods has not yet\nbeen undertaken. In this paper, we (I) summarize the current advances in the\nemerging abilities of LLMs into more accessible AI solutions for science, and\n(II) investigate the challenges and opportunities of developing affordable\nsolutions for scientific domains using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have ushered in a new era for processing complex\ninformation in various fields, including science. The increasing amount of\nscientific literature allows these models to acquire and understand scientific\nknowledge effectively, thus improving their performance in a wide range of\ntasks. Due to the power of LLMs, they require extremely expensive computational\nresources, intense amounts of data, and training time. Therefore, in recent\nyears, researchers have proposed various methodologies to make scientific LLMs\nmore affordable. The most well-known approaches align in two directions. It can\nbe either focusing on the size of the models or enhancing the quality of data.\nTo date, a comprehensive review of these two families of methods has not yet\nbeen undertaken. In this paper, we (I) summarize the current advances in the\nemerging abilities of LLMs into more accessible AI solutions for science, and\n(II) investigate the challenges and opportunities of developing affordable\nsolutions for scientific domains using LLMs."
                },
                "authors": [
                    {
                        "name": "Huy Quoc To"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Guangyan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Guangyan Huang"
                },
                "author": "Guangyan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19056v2",
                "updated": "2024-08-20T10:56:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    56,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-03-27T23:45:31Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    23,
                    45,
                    31,
                    2,
                    87,
                    0
                ],
                "title": "CAUSE: Counterfactual Assessment of User Satisfaction Estimation in\n  Task-Oriented Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAUSE: Counterfactual Assessment of User Satisfaction Estimation in\n  Task-Oriented Dialogue Systems"
                },
                "summary": "An important unexplored aspect in previous work on user satisfaction\nestimation for Task-Oriented Dialogue (TOD) systems is their evaluation in\nterms of robustness for the identification of user dissatisfaction: current\nbenchmarks for user satisfaction estimation in TOD systems are highly skewed\ntowards dialogues for which the user is satisfied. The effect of having a more\nbalanced set of satisfaction labels on performance is unknown. However,\nbalancing the data with more dissatisfactory dialogue samples requires further\ndata collection and human annotation, which is costly and time-consuming. In\nthis work, we leverage large language models (LLMs) and unlock their ability to\ngenerate satisfaction-aware counterfactual dialogues to augment the set of\noriginal dialogues of a test collection. We gather human annotations to ensure\nthe reliability of the generated samples. We evaluate two open-source LLMs as\nuser satisfaction estimators on our augmented collection against\nstate-of-the-art fine-tuned models. Our experiments show that when used as\nfew-shot user satisfaction estimators, open-source LLMs show higher robustness\nto the increase in the number of dissatisfaction labels in the test collection\nthan the fine-tuned state-of-the-art models. Our results shed light on the need\nfor data augmentation approaches for user satisfaction estimation in TOD\nsystems. We release our aligned counterfactual dialogues, which are curated by\nhuman annotation, to facilitate further research on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important unexplored aspect in previous work on user satisfaction\nestimation for Task-Oriented Dialogue (TOD) systems is their evaluation in\nterms of robustness for the identification of user dissatisfaction: current\nbenchmarks for user satisfaction estimation in TOD systems are highly skewed\ntowards dialogues for which the user is satisfied. The effect of having a more\nbalanced set of satisfaction labels on performance is unknown. However,\nbalancing the data with more dissatisfactory dialogue samples requires further\ndata collection and human annotation, which is costly and time-consuming. In\nthis work, we leverage large language models (LLMs) and unlock their ability to\ngenerate satisfaction-aware counterfactual dialogues to augment the set of\noriginal dialogues of a test collection. We gather human annotations to ensure\nthe reliability of the generated samples. We evaluate two open-source LLMs as\nuser satisfaction estimators on our augmented collection against\nstate-of-the-art fine-tuned models. Our experiments show that when used as\nfew-shot user satisfaction estimators, open-source LLMs show higher robustness\nto the increase in the number of dissatisfaction labels in the test collection\nthan the fine-tuned state-of-the-art models. Our results shed light on the need\nfor data augmentation approaches for user satisfaction estimation in TOD\nsystems. We release our aligned counterfactual dialogues, which are curated by\nhuman annotation, to facilitate further research on this topic."
                },
                "authors": [
                    {
                        "name": "Amin Abolghasemi"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Arian Askari"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.06523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.06523v2",
                "updated": "2024-08-20T10:50:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    50,
                    40,
                    1,
                    233,
                    0
                ],
                "published": "2023-11-11T09:51:04Z",
                "published_parsed": [
                    2023,
                    11,
                    11,
                    9,
                    51,
                    4,
                    5,
                    315,
                    0
                ],
                "title": "Generative AI for Space-Air-Ground Integrated Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Space-Air-Ground Integrated Networks"
                },
                "summary": "Recently, generative AI technologies have emerged as a significant\nadvancement in artificial intelligence field, renowned for their language and\nimage generation capabilities. Meantime, space-air-ground integrated network\n(SAGIN) is an integral part of future B5G/6G for achieving ubiquitous\nconnectivity. Inspired by this, this article explores an integration of\ngenerative AI in SAGIN, focusing on potential applications and case study. We\nfirst provide a comprehensive review of SAGIN and generative AI models,\nhighlighting their capabilities and opportunities of their integration.\nBenefiting from generative AI's ability to generate useful data and facilitate\nadvanced decision-making processes, it can be applied to various scenarios of\nSAGIN. Accordingly, we present a concise survey on their integration, including\nchannel modeling and channel state information (CSI) estimation, joint\nair-space-ground resource allocation, intelligent network deployment, semantic\ncommunications, image extraction and processing, security and privacy\nenhancement. Next, we propose a framework that utilizes a Generative Diffusion\nModel (GDM) to construct channel information map to enhance quality of service\nfor SAGIN. Simulation results demonstrate the effectiveness of the proposed\nframework. Finally, we discuss potential research directions for generative\nAI-enabled SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, generative AI technologies have emerged as a significant\nadvancement in artificial intelligence field, renowned for their language and\nimage generation capabilities. Meantime, space-air-ground integrated network\n(SAGIN) is an integral part of future B5G/6G for achieving ubiquitous\nconnectivity. Inspired by this, this article explores an integration of\ngenerative AI in SAGIN, focusing on potential applications and case study. We\nfirst provide a comprehensive review of SAGIN and generative AI models,\nhighlighting their capabilities and opportunities of their integration.\nBenefiting from generative AI's ability to generate useful data and facilitate\nadvanced decision-making processes, it can be applied to various scenarios of\nSAGIN. Accordingly, we present a concise survey on their integration, including\nchannel modeling and channel state information (CSI) estimation, joint\nair-space-ground resource allocation, intelligent network deployment, semantic\ncommunications, image extraction and processing, security and privacy\nenhancement. Next, we propose a framework that utilizes a Generative Diffusion\nModel (GDM) to construct channel information map to enhance quality of service\nfor SAGIN. Simulation results demonstrate the effectiveness of the proposed\nframework. Finally, we discuss potential research directions for generative\nAI-enabled SAGIN."
                },
                "authors": [
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "10 pages, 3 figures, Accepted at IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.06523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.06523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10724v1",
                "updated": "2024-08-20T10:45:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    45,
                    36,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:45:36Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    45,
                    36,
                    1,
                    233,
                    0
                ],
                "title": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian"
                },
                "summary": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages."
                },
                "authors": [
                    {
                        "name": "Cem Üyük"
                    },
                    {
                        "name": "Danica Rovó"
                    },
                    {
                        "name": "Shaghayegh Kolli"
                    },
                    {
                        "name": "Rabia Varol"
                    },
                    {
                        "name": "Georg Groh"
                    },
                    {
                        "name": "Daryna Dementieva"
                    }
                ],
                "author_detail": {
                    "name": "Daryna Dementieva"
                },
                "author": "Daryna Dementieva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10722v1",
                "updated": "2024-08-20T10:44:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    44,
                    29,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:44:29Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    44,
                    29,
                    1,
                    233,
                    0
                ],
                "title": "MEGen: Generative Backdoor in Large Language Models via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEGen: Generative Backdoor in Large Language Models via Model Editing"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities. Their\npowerful generative abilities enable flexible responses based on various\nqueries or instructions. Emerging as widely adopted generalists for diverse\ntasks, LLMs are still vulnerable to backdoors. This paper proposes an\nediting-based generative backdoor, named MEGen, aiming to create a customized\nbackdoor for NLP tasks with the least side effects. In our approach, we first\nleverage a language model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly embed a backdoor\ninto an LLM. By adjusting a small set of local parameters with a mini-batch of\nsamples, MEGen significantly enhances time efficiency and achieves high\nrobustness. Experimental results indicate that our backdoor attack strategy\nachieves a high attack success rate on poison data while maintaining the\nmodel's performance on clean data. Notably, the backdoored model, when\ntriggered, can freely output pre-set dangerous information while successfully\ncompleting downstream tasks. This suggests that future LLM applications could\nbe guided to deliver certain dangerous information, thus altering the LLM's\ngenerative style. We believe this approach provides insights for future LLM\napplications and the execution of backdoor attacks on conversational AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities. Their\npowerful generative abilities enable flexible responses based on various\nqueries or instructions. Emerging as widely adopted generalists for diverse\ntasks, LLMs are still vulnerable to backdoors. This paper proposes an\nediting-based generative backdoor, named MEGen, aiming to create a customized\nbackdoor for NLP tasks with the least side effects. In our approach, we first\nleverage a language model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly embed a backdoor\ninto an LLM. By adjusting a small set of local parameters with a mini-batch of\nsamples, MEGen significantly enhances time efficiency and achieves high\nrobustness. Experimental results indicate that our backdoor attack strategy\nachieves a high attack success rate on poison data while maintaining the\nmodel's performance on clean data. Notably, the backdoored model, when\ntriggered, can freely output pre-set dangerous information while successfully\ncompleting downstream tasks. This suggests that future LLM applications could\nbe guided to deliver certain dangerous information, thus altering the LLM's\ngenerative style. We believe this approach provides insights for future LLM\napplications and the execution of backdoor attacks on conversational AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10718v1",
                "updated": "2024-08-20T10:40:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    40,
                    35,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:40:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    40,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?"
                },
                "summary": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our benchmark will be available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our benchmark will be available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhao"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Annan Li"
                    },
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10715v1",
                "updated": "2024-08-20T10:31:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    31,
                    36,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:31:36Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    31,
                    36,
                    1,
                    233,
                    0
                ],
                "title": "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated\n  Privacy-Preserving Physician Letter Generation in Radiation Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated\n  Privacy-Preserving Physician Letter Generation in Radiation Oncology"
                },
                "summary": "Generating physician letters is a time-consuming task in daily clinical\npractice. This study investigates local fine-tuning of large language models\n(LLMs), specifically LLaMA models, for physician letter generation in a\nprivacy-preserving manner within the field of radiation oncology. Our findings\ndemonstrate that base LLaMA models, without fine-tuning, are inadequate for\neffectively generating physician letters. The QLoRA algorithm provides an\nefficient method for local intra-institutional fine-tuning of LLMs with limited\ncomputational resources (i.e., a single 48 GB GPU workstation within the\nhospital). The fine-tuned LLM successfully learns radiation oncology-specific\ninformation and generates physician letters in an institution-specific style.\nROUGE scores of the generated summary reports highlight the superiority of the\n8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician\nevaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has\nlimited capacity to generate content beyond the provided input data, it\nsuccessfully generates salutations, diagnoses and treatment histories,\nrecommendations for further treatment, and planned schedules. Overall, clinical\nbenefit was rated highly by the clinical experts (average score of 3.44 on a\n4-point scale). With careful physician review and correction, automated\nLLM-based physician letter generation has significant practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating physician letters is a time-consuming task in daily clinical\npractice. This study investigates local fine-tuning of large language models\n(LLMs), specifically LLaMA models, for physician letter generation in a\nprivacy-preserving manner within the field of radiation oncology. Our findings\ndemonstrate that base LLaMA models, without fine-tuning, are inadequate for\neffectively generating physician letters. The QLoRA algorithm provides an\nefficient method for local intra-institutional fine-tuning of LLMs with limited\ncomputational resources (i.e., a single 48 GB GPU workstation within the\nhospital). The fine-tuned LLM successfully learns radiation oncology-specific\ninformation and generates physician letters in an institution-specific style.\nROUGE scores of the generated summary reports highlight the superiority of the\n8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician\nevaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has\nlimited capacity to generate content beyond the provided input data, it\nsuccessfully generates salutations, diagnoses and treatment histories,\nrecommendations for further treatment, and planned schedules. Overall, clinical\nbenefit was rated highly by the clinical experts (average score of 3.44 on a\n4-point scale). With careful physician review and correction, automated\nLLM-based physician letter generation has significant practical value."
                },
                "authors": [
                    {
                        "name": "Yihao Hou"
                    },
                    {
                        "name": "Christoph Bert"
                    },
                    {
                        "name": "Ahmed Gomaa"
                    },
                    {
                        "name": "Godehard Lahmer"
                    },
                    {
                        "name": "Daniel Hoefler"
                    },
                    {
                        "name": "Thomas Weissmann"
                    },
                    {
                        "name": "Raphaela Voigt"
                    },
                    {
                        "name": "Philipp Schubert"
                    },
                    {
                        "name": "Charlotte Schmitter"
                    },
                    {
                        "name": "Alina Depardon"
                    },
                    {
                        "name": "Sabine Semrau"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Rainer Fietkau"
                    },
                    {
                        "name": "Yixing Huang"
                    },
                    {
                        "name": "Florian Putz"
                    }
                ],
                "author_detail": {
                    "name": "Florian Putz"
                },
                "author": "Florian Putz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03627v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03627v4",
                "updated": "2024-08-20T10:27:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    27,
                    56,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-04T04:30:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    30,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems."
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "20 pages",
                "arxiv_journal_ref": "KnowledgeNLP@ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03627v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03627v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10711v1",
                "updated": "2024-08-20T10:26:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    26,
                    2,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T10:26:02Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    26,
                    2,
                    1,
                    233,
                    0
                ],
                "title": "Investigating Context Effects in Similarity Judgements in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Context Effects in Similarity Judgements in Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionised the capability of AI models\nin comprehending and generating natural language text. They are increasingly\nbeing used to empower and deploy agents in real-world scenarios, which make\ndecisions and take actions based on their understanding of the context.\nTherefore researchers, policy makers and enterprises alike are working towards\nensuring that the decisions made by these agents align with human values and\nuser expectations. That being said, human values and decisions are not always\nstraightforward to measure and are subject to different cognitive biases. There\nis a vast section of literature in Behavioural Science which studies biases in\nhuman judgements. In this work we report an ongoing investigation on alignment\nof LLMs with human judgements affected by order bias. Specifically, we focus on\na famous human study which showed evidence of order effects in similarity\njudgements, and replicate it with various popular LLMs. We report the different\nsettings where LLMs exhibit human-like order effect bias and discuss the\nimplications of these findings to inform the design and development of LLM\nbased applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionised the capability of AI models\nin comprehending and generating natural language text. They are increasingly\nbeing used to empower and deploy agents in real-world scenarios, which make\ndecisions and take actions based on their understanding of the context.\nTherefore researchers, policy makers and enterprises alike are working towards\nensuring that the decisions made by these agents align with human values and\nuser expectations. That being said, human values and decisions are not always\nstraightforward to measure and are subject to different cognitive biases. There\nis a vast section of literature in Behavioural Science which studies biases in\nhuman judgements. In this work we report an ongoing investigation on alignment\nof LLMs with human judgements affected by order bias. Specifically, we focus on\na famous human study which showed evidence of order effects in similarity\njudgements, and replicate it with various popular LLMs. We report the different\nsettings where LLMs exhibit human-like order effect bias and discuss the\nimplications of these findings to inform the design and development of LLM\nbased applications."
                },
                "authors": [
                    {
                        "name": "Sagar Uprety"
                    },
                    {
                        "name": "Amit Kumar Jaiswal"
                    },
                    {
                        "name": "Haiming Liu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "Accepted at The First Workshop on AI Behavioral Science (AIBS 2024),\n  held in conjunction with KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10703v1",
                "updated": "2024-08-20T09:58:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:58:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Large Language Models for Multimodal Deformable Image Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multimodal Deformable Image Registration"
                },
                "summary": "The challenge of Multimodal Deformable Image Registration (MDIR) lies in the\nconversion and alignment of features between images of different modalities.\nGenerative models (GMs) cannot retain the necessary information enough from the\nsource modality to the target one, while non-GMs struggle to align features\nacross these two modalities. In this paper, we propose a novel coarse-to-fine\nMDIR framework,LLM-Morph, which is applicable to various pre-trained Large\nLanguage Models (LLMs) to solve these concerns by aligning the deep features\nfrom different modal medical images. Specifically, we first utilize a CNN\nencoder to extract deep visual features from cross-modal image pairs, then we\nuse the first adapter to adjust these tokens, and use LoRA in pre-trained LLMs\nto fine-tune their weights, both aimed at eliminating the domain gap between\nthe pre-trained LLMs and the MDIR task. Third, for the alignment of tokens, we\nutilize other four adapters to transform the LLM-encoded tokens into\nmulti-scale visual features, generating multi-scale deformation fields and\nfacilitating the coarse-to-fine MDIR task. Extensive experiments in MR-CT\nAbdomen and SR-Reg Brain datasets demonstrate the effectiveness of our\nframework and the potential of pre-trained LLMs for MDIR task. Our code is\navailabel at: https://github.com/ninjannn/LLM-Morph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of Multimodal Deformable Image Registration (MDIR) lies in the\nconversion and alignment of features between images of different modalities.\nGenerative models (GMs) cannot retain the necessary information enough from the\nsource modality to the target one, while non-GMs struggle to align features\nacross these two modalities. In this paper, we propose a novel coarse-to-fine\nMDIR framework,LLM-Morph, which is applicable to various pre-trained Large\nLanguage Models (LLMs) to solve these concerns by aligning the deep features\nfrom different modal medical images. Specifically, we first utilize a CNN\nencoder to extract deep visual features from cross-modal image pairs, then we\nuse the first adapter to adjust these tokens, and use LoRA in pre-trained LLMs\nto fine-tune their weights, both aimed at eliminating the domain gap between\nthe pre-trained LLMs and the MDIR task. Third, for the alignment of tokens, we\nutilize other four adapters to transform the LLM-encoded tokens into\nmulti-scale visual features, generating multi-scale deformation fields and\nfacilitating the coarse-to-fine MDIR task. Extensive experiments in MR-CT\nAbdomen and SR-Reg Brain datasets demonstrate the effectiveness of our\nframework and the potential of pre-trained LLMs for MDIR task. Our code is\navailabel at: https://github.com/ninjannn/LLM-Morph."
                },
                "authors": [
                    {
                        "name": "Mingrui Ma"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Jie Ning"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Bruno Lepri"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Lepri"
                },
                "author": "Bruno Lepri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10701v1",
                "updated": "2024-08-20T09:58:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    1,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:58:01Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    58,
                    1,
                    1,
                    233,
                    0
                ],
                "title": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based\n  Scoring Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based\n  Scoring Technique"
                },
                "summary": "In today's era, where large language models (LLMs) are integrated into\nnumerous real-world applications, ensuring their safety and robustness is\ncrucial for responsible AI usage. Automated red-teaming methods play a key role\nin this process by generating adversarial attacks to identify and mitigate\npotential vulnerabilities in these models. However, existing methods often\nstruggle with slow performance, limited categorical diversity, and high\nresource demands. While Rainbow Teaming, a recent approach, addresses the\ndiversity challenge by framing adversarial prompt generation as a\nquality-diversity search, it remains slow and requires a large fine-tuned\nmutator for optimal performance. To overcome these limitations, we propose\nFerret, a novel approach that builds upon Rainbow Teaming by generating\nmultiple adversarial prompt mutations per iteration and using a scoring\nfunction to rank and select the most effective adversarial prompt. We explore\nvarious scoring functions, including reward models, Llama Guard, and\nLLM-as-a-judge, to rank adversarial mutations based on their potential harm to\nimprove the efficiency of the search for harmful mutations. Our results\ndemonstrate that Ferret, utilizing a reward model as a scoring function,\nimproves the overall attack success rate (ASR) to 95%, which is 46% higher than\nRainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90%\nASR by 15.2% compared to the baseline and generates adversarial prompts that\nare transferable i.e. effective on other LLMs of larger size. Our codes are\navailable at https://github.com/declare-lab/ferret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's era, where large language models (LLMs) are integrated into\nnumerous real-world applications, ensuring their safety and robustness is\ncrucial for responsible AI usage. Automated red-teaming methods play a key role\nin this process by generating adversarial attacks to identify and mitigate\npotential vulnerabilities in these models. However, existing methods often\nstruggle with slow performance, limited categorical diversity, and high\nresource demands. While Rainbow Teaming, a recent approach, addresses the\ndiversity challenge by framing adversarial prompt generation as a\nquality-diversity search, it remains slow and requires a large fine-tuned\nmutator for optimal performance. To overcome these limitations, we propose\nFerret, a novel approach that builds upon Rainbow Teaming by generating\nmultiple adversarial prompt mutations per iteration and using a scoring\nfunction to rank and select the most effective adversarial prompt. We explore\nvarious scoring functions, including reward models, Llama Guard, and\nLLM-as-a-judge, to rank adversarial mutations based on their potential harm to\nimprove the efficiency of the search for harmful mutations. Our results\ndemonstrate that Ferret, utilizing a reward model as a scoring function,\nimproves the overall attack success rate (ASR) to 95%, which is 46% higher than\nRainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90%\nASR by 15.2% compared to the baseline and generates adversarial prompts that\nare transferable i.e. effective on other LLMs of larger size. Our codes are\navailable at https://github.com/declare-lab/ferret."
                },
                "authors": [
                    {
                        "name": "Tej Deep Pala"
                    },
                    {
                        "name": "Vernon Y. H. Toh"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10692v1",
                "updated": "2024-08-20T09:42:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    26,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:42:26Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    26,
                    1,
                    233,
                    0
                ],
                "title": "Unconditional Truthfulness: Learning Conditional Dependency for\n  Uncertainty Quantification of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconditional Truthfulness: Learning Conditional Dependency for\n  Uncertainty Quantification of Large Language Models"
                },
                "summary": "Uncertainty quantification (UQ) is a perspective approach to detecting Large\nLanguage Model (LLM) hallucinations and low quality output. In this work, we\naddress one of the challenges of UQ in generation tasks that arises from the\nconditional dependency between the generation steps of an LLM. We propose to\nlearn this dependency from data. We train a regression model, which target\nvariable is the gap between the conditional and the unconditional generation\nconfidence. During LLM inference, we use this learned conditional dependency\nmodel to modulate the uncertainty of the current generation step based on the\nuncertainty of the previous step. Our experimental evaluation on nine datasets\nand three LLMs shows that the proposed method is highly effective for\nuncertainty quantification, achieving substantial improvements over rivaling\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) is a perspective approach to detecting Large\nLanguage Model (LLM) hallucinations and low quality output. In this work, we\naddress one of the challenges of UQ in generation tasks that arises from the\nconditional dependency between the generation steps of an LLM. We propose to\nlearn this dependency from data. We train a regression model, which target\nvariable is the gap between the conditional and the unconditional generation\nconfidence. During LLM inference, we use this learned conditional dependency\nmodel to modulate the uncertainty of the current generation step based on the\nuncertainty of the previous step. Our experimental evaluation on nine datasets\nand three LLMs shows that the proposed method is highly effective for\nuncertainty quantification, achieving substantial improvements over rivaling\napproaches."
                },
                "authors": [
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10691v1",
                "updated": "2024-08-20T09:42:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    17,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:42:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches"
                },
                "summary": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge."
                },
                "authors": [
                    {
                        "name": "Yanjie Dong"
                    },
                    {
                        "name": "Xiaoyi Fan"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    },
                    {
                        "name": "Xiping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiping Hu"
                },
                "author": "Xiping Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10682v1",
                "updated": "2024-08-20T09:36:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    36,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:36:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    36,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for\n  Assessing and Improving Unlearning Robustness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Knowledge Unlearning: An Adversarial Framework for\n  Assessing and Improving Unlearning Robustness in Large Language Models"
                },
                "summary": "LLM have achieved success in many fields but still troubled by problematic\ncontent in the training corpora. LLM unlearning aims at reducing their\ninfluence and avoid undesirable behaviours. However, existing unlearning\nmethods remain vulnerable to adversarial queries and the unlearned knowledge\nresurfaces after the manually designed attack queries. As part of a red-team\neffort to proactively assess the vulnerabilities of unlearned models, we design\nDynamic Unlearning Attack (DUA), a dynamic and automated framework to attack\nthese models and evaluate their robustness. It optimizes adversarial suffixes\nto reintroduce the unlearned knowledge in various scenarios. We find that\nunlearned knowledge can be recovered in $55.2\\%$ of the questions, even without\nrevealing the unlearned model's parameters. In response to this vulnerability,\nwe propose Latent Adversarial Unlearning (LAU), a universal framework that\neffectively enhances the robustness of the unlearned process. It formulates the\nunlearning process as a min-max optimization problem and resolves it through\ntwo stages: an attack stage, where perturbation vectors are trained and added\nto the latent space of LLMs to recover the unlearned knowledge, and a defense\nstage, where previously trained perturbation vectors are used to enhance\nunlearned model's robustness. With our LAU framework, we obtain two robust\nunlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across\nmultiple unlearning benchmarks and various models, and demonstrate that they\nimprove the unlearning effectiveness by over $53.5\\%$, cause only less than a\n$11.6\\%$ reduction in neighboring knowledge, and have almost no impact on the\nmodel's general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM have achieved success in many fields but still troubled by problematic\ncontent in the training corpora. LLM unlearning aims at reducing their\ninfluence and avoid undesirable behaviours. However, existing unlearning\nmethods remain vulnerable to adversarial queries and the unlearned knowledge\nresurfaces after the manually designed attack queries. As part of a red-team\neffort to proactively assess the vulnerabilities of unlearned models, we design\nDynamic Unlearning Attack (DUA), a dynamic and automated framework to attack\nthese models and evaluate their robustness. It optimizes adversarial suffixes\nto reintroduce the unlearned knowledge in various scenarios. We find that\nunlearned knowledge can be recovered in $55.2\\%$ of the questions, even without\nrevealing the unlearned model's parameters. In response to this vulnerability,\nwe propose Latent Adversarial Unlearning (LAU), a universal framework that\neffectively enhances the robustness of the unlearned process. It formulates the\nunlearning process as a min-max optimization problem and resolves it through\ntwo stages: an attack stage, where perturbation vectors are trained and added\nto the latent space of LLMs to recover the unlearned knowledge, and a defense\nstage, where previously trained perturbation vectors are used to enhance\nunlearned model's robustness. With our LAU framework, we obtain two robust\nunlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across\nmultiple unlearning benchmarks and various models, and demonstrate that they\nimprove the unlearning effectiveness by over $53.5\\%$, cause only less than a\n$11.6\\%$ reduction in neighboring knowledge, and have almost no impact on the\nmodel's general capabilities."
                },
                "authors": [
                    {
                        "name": "Hongbang Yuan"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10868v3",
                "updated": "2024-08-20T09:25:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    25,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-16T09:36:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    36,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts"
                },
                "summary": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03891v2",
                "updated": "2024-08-20T09:19:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    19,
                    7,
                    1,
                    233,
                    0
                ],
                "published": "2024-07-04T12:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    12,
                    33,
                    56,
                    3,
                    186,
                    0
                ],
                "title": "AutoBench: Automatic Testbench Generation and Evaluation Using LLMs for\n  HDL Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoBench: Automatic Testbench Generation and Evaluation Using LLMs for\n  HDL Design"
                },
                "summary": "In digital circuit design, testbenches constitute the cornerstone of\nsimulation-based hardware verification. Traditional methodologies for testbench\ngeneration during simulation-based hardware verification still remain partially\nmanual, resulting in inefficiencies in testing various scenarios and requiring\nexpensive time from designers. Large Language Models (LLMs) have demonstrated\ntheir potential in automating the circuit design flow. However, directly\napplying LLMs to generate testbenches suffers from a low pass rate. To address\nthis challenge, we introduce AutoBench, the first LLM-based testbench generator\nfor digital circuit design, which requires only the description of the design\nunder test (DUT) to automatically generate comprehensive testbenches. In\nAutoBench, a hybrid testbench structure and a self-checking system are realized\nusing LLMs. To validate the generated testbenches, we also introduce an\nautomated testbench evaluation framework to evaluate the quality of generated\ntestbenches from multiple perspectives. Experimental results demonstrate that\nAutoBench achieves a 57% improvement in the testbench pass@1 ratio compared\nwith the baseline that directly generates testbenches using LLMs. For 75\nsequential circuits, AutoBench successfully has a 3.36 times testbench pass@1\nratio compared with the baseline. The source codes and experimental results are\nopen-sourced at this link: https://github.com/AutoBench/AutoBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In digital circuit design, testbenches constitute the cornerstone of\nsimulation-based hardware verification. Traditional methodologies for testbench\ngeneration during simulation-based hardware verification still remain partially\nmanual, resulting in inefficiencies in testing various scenarios and requiring\nexpensive time from designers. Large Language Models (LLMs) have demonstrated\ntheir potential in automating the circuit design flow. However, directly\napplying LLMs to generate testbenches suffers from a low pass rate. To address\nthis challenge, we introduce AutoBench, the first LLM-based testbench generator\nfor digital circuit design, which requires only the description of the design\nunder test (DUT) to automatically generate comprehensive testbenches. In\nAutoBench, a hybrid testbench structure and a self-checking system are realized\nusing LLMs. To validate the generated testbenches, we also introduce an\nautomated testbench evaluation framework to evaluate the quality of generated\ntestbenches from multiple perspectives. Experimental results demonstrate that\nAutoBench achieves a 57% improvement in the testbench pass@1 ratio compared\nwith the baseline that directly generates testbenches using LLMs. For 75\nsequential circuits, AutoBench successfully has a 3.36 times testbench pass@1\nratio compared with the baseline. The source codes and experimental results are\nopen-sourced at this link: https://github.com/AutoBench/AutoBench"
                },
                "authors": [
                    {
                        "name": "Ruidi Qiu"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Rolf Drechsler"
                    },
                    {
                        "name": "Ulf Schlichtmann"
                    },
                    {
                        "name": "Bing Li"
                    }
                ],
                "author_detail": {
                    "name": "Bing Li"
                },
                "author": "Bing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10668v2",
                "updated": "2024-08-21T07:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    50,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T09:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    11,
                    21,
                    1,
                    233,
                    0
                ],
                "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation"
                },
                "summary": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Yatao Bian"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Peilin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Peilin Zhao"
                },
                "author": "Peilin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10663v1",
                "updated": "2024-08-20T09:05:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    5,
                    3,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T09:05:03Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    5,
                    3,
                    1,
                    233,
                    0
                ],
                "title": "REInstruct: Building Instruction Data from Unlabeled Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REInstruct: Building Instruction Data from Unlabeled Corpus"
                },
                "summary": "Manually annotating instruction data for large language models is difficult,\ncostly, and hard to scale. Meanwhile, current automatic annotation methods\ntypically rely on distilling synthetic data from proprietary LLMs, which not\nonly limits the upper bound of the quality of the instruction data but also\nraises potential copyright issues. In this paper, we propose REInstruct, a\nsimple and scalable method to automatically build instruction data from an\nunlabeled corpus without heavy reliance on proprietary LLMs and human\nannotation. Specifically, REInstruct first selects a subset of unlabeled texts\nthat potentially contain well-structured helpful and insightful content and\nthen generates instructions for these texts. To generate accurate and relevant\nresponses for effective and robust training, REInstruct further proposes a\nrewriting-based approach to improve the quality of the generated instruction\ndata. By training Llama-7b on a combination of 3k seed data and 32k synthetic\ndata from REInstruct, fine-tuned model achieves a 65.41\\% win rate on\nAlpacaEval leaderboard against text-davinci-003, outperforming other\nopen-source, non-distilled instruction data construction methods. The code is\npublicly available at \\url{https://github.com/cs32963/REInstruct}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manually annotating instruction data for large language models is difficult,\ncostly, and hard to scale. Meanwhile, current automatic annotation methods\ntypically rely on distilling synthetic data from proprietary LLMs, which not\nonly limits the upper bound of the quality of the instruction data but also\nraises potential copyright issues. In this paper, we propose REInstruct, a\nsimple and scalable method to automatically build instruction data from an\nunlabeled corpus without heavy reliance on proprietary LLMs and human\nannotation. Specifically, REInstruct first selects a subset of unlabeled texts\nthat potentially contain well-structured helpful and insightful content and\nthen generates instructions for these texts. To generate accurate and relevant\nresponses for effective and robust training, REInstruct further proposes a\nrewriting-based approach to improve the quality of the generated instruction\ndata. By training Llama-7b on a combination of 3k seed data and 32k synthetic\ndata from REInstruct, fine-tuned model achieves a 65.41\\% win rate on\nAlpacaEval leaderboard against text-davinci-003, outperforming other\nopen-source, non-distilled instruction data construction methods. The code is\npublicly available at \\url{https://github.com/cs32963/REInstruct}."
                },
                "authors": [
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Xinyan Guan"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "Accepted by ACL2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08780v2",
                "updated": "2024-08-20T09:01:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    1,
                    9,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-16T14:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions"
                },
                "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "10 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10658v1",
                "updated": "2024-08-20T08:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    54,
                    34,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:54:34Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    54,
                    34,
                    1,
                    233,
                    0
                ],
                "title": "Learning Instruction-Guided Manipulation Affordance via Large Models for\n  Embodied Robotic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Instruction-Guided Manipulation Affordance via Large Models for\n  Embodied Robotic Tasks"
                },
                "summary": "We study the task of language instruction-guided robotic manipulation, in\nwhich an embodied robot is supposed to manipulate the target objects based on\nthe language instructions. In previous studies, the predicted manipulation\nregions of the target object typically do not change with specification from\nthe language instructions, which means that the language perception and\nmanipulation prediction are separate. However, in human behavioral patterns,\nthe manipulation regions of the same object will change for different language\ninstructions. In this paper, we propose Instruction-Guided Affordance Net\n(IGANet) for predicting affordance maps of instruction-guided robotic\nmanipulation tasks by utilizing powerful priors from vision and language\nencoders pre-trained on large-scale datasets. We develop a\nVison-Language-Models(VLMs)-based data augmentation pipeline, which can\ngenerate a large amount of data automatically for model training. Besides, with\nthe help of Large-Language-Models(LLMs), actions can be effectively executed to\nfinish the tasks defined by instructions. A series of real-world experiments\nrevealed that our method can achieve better performance with generated data.\nMoreover, our model can generalize better to scenarios with unseen objects and\nlanguage instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the task of language instruction-guided robotic manipulation, in\nwhich an embodied robot is supposed to manipulate the target objects based on\nthe language instructions. In previous studies, the predicted manipulation\nregions of the target object typically do not change with specification from\nthe language instructions, which means that the language perception and\nmanipulation prediction are separate. However, in human behavioral patterns,\nthe manipulation regions of the same object will change for different language\ninstructions. In this paper, we propose Instruction-Guided Affordance Net\n(IGANet) for predicting affordance maps of instruction-guided robotic\nmanipulation tasks by utilizing powerful priors from vision and language\nencoders pre-trained on large-scale datasets. We develop a\nVison-Language-Models(VLMs)-based data augmentation pipeline, which can\ngenerate a large amount of data automatically for model training. Besides, with\nthe help of Large-Language-Models(LLMs), actions can be effectively executed to\nfinish the tasks defined by instructions. A series of real-world experiments\nrevealed that our method can achieve better performance with generated data.\nMoreover, our model can generalize better to scenarios with unseen objects and\nlanguage instructions."
                },
                "authors": [
                    {
                        "name": "Dayou Li"
                    },
                    {
                        "name": "Chenkun Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Yibin Li"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Accepted to ICARM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10646v1",
                "updated": "2024-08-20T08:38:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    38,
                    30,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:38:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    38,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge\n  Representation Sharing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge\n  Representation Sharing in LLMs"
                },
                "summary": "The veracity of a factoid is largely independent of the language it is\nwritten in. However, language models are inconsistent in their ability to\nanswer the same factual question across languages. This raises questions about\nhow LLMs represent a given fact across languages. We explore multilingual\nfactual knowledge through two aspects: the model's ability to answer a query\nconsistently across languages, and the ability to ''store'' answers in a shared\nrepresentation for several languages. We propose a methodology to measure the\nextent of representation sharing across languages by repurposing knowledge\nediting methods. We examine LLMs with various multilingual configurations using\na new multilingual dataset. We reveal that high consistency does not\nnecessarily imply shared representation, particularly for languages with\ndifferent scripts. Moreover, we find that script similarity is a dominant\nfactor in representation sharing. Finally, we observe that if LLMs could fully\nshare knowledge across languages, their accuracy in their best-performing\nlanguage could benefit an increase of up to 150\\% on average. These findings\nhighlight the need for improved multilingual knowledge representation in LLMs\nand suggest a path for the development of more robust and consistent\nmultilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The veracity of a factoid is largely independent of the language it is\nwritten in. However, language models are inconsistent in their ability to\nanswer the same factual question across languages. This raises questions about\nhow LLMs represent a given fact across languages. We explore multilingual\nfactual knowledge through two aspects: the model's ability to answer a query\nconsistently across languages, and the ability to ''store'' answers in a shared\nrepresentation for several languages. We propose a methodology to measure the\nextent of representation sharing across languages by repurposing knowledge\nediting methods. We examine LLMs with various multilingual configurations using\na new multilingual dataset. We reveal that high consistency does not\nnecessarily imply shared representation, particularly for languages with\ndifferent scripts. Moreover, we find that script similarity is a dominant\nfactor in representation sharing. Finally, we observe that if LLMs could fully\nshare knowledge across languages, their accuracy in their best-performing\nlanguage could benefit an increase of up to 150\\% on average. These findings\nhighlight the need for improved multilingual knowledge representation in LLMs\nand suggest a path for the development of more robust and consistent\nmultilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Maxim Ifergan"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Omri Abend"
                    }
                ],
                "author_detail": {
                    "name": "Omri Abend"
                },
                "author": "Omri Abend",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10645v1",
                "updated": "2024-08-20T08:36:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:36:59Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "title": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation"
                },
                "summary": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance."
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Yizhou Dang"
                    },
                    {
                        "name": "Yuliang Liang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10642v1",
                "updated": "2024-08-20T08:32:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    32,
                    44,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:32:44Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    32,
                    44,
                    1,
                    233,
                    0
                ],
                "title": "Minor SFT loss for LLM fine-tune to increase performance and reduce\n  model deviation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor SFT loss for LLM fine-tune to increase performance and reduce\n  model deviation"
                },
                "summary": "Instruct LLM provide a paradigm used in large scale language model to align\nLLM to human preference. The paradigm contains supervised fine tuning and\nreinforce learning from human feedback. This paradigm is also used in\ndownstream scenarios to adapt LLM to specific corpora and applications.\nComparing to SFT, there are many efforts focused on RLHF and several algorithms\nbeing proposed, such as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most\nefforts for SFT are focused on how to collect, filter and mix high quality\ndata. In this article with insight from DPO and MinorDPO, we propose a training\nmetric for SFT to measure the discrepancy between the optimized model and the\noriginal model, and a loss function MinorSFT that can increase the training\neffectiveness, and reduce the discrepancy between the optimized LLM and\noriginal LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct LLM provide a paradigm used in large scale language model to align\nLLM to human preference. The paradigm contains supervised fine tuning and\nreinforce learning from human feedback. This paradigm is also used in\ndownstream scenarios to adapt LLM to specific corpora and applications.\nComparing to SFT, there are many efforts focused on RLHF and several algorithms\nbeing proposed, such as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most\nefforts for SFT are focused on how to collect, filter and mix high quality\ndata. In this article with insight from DPO and MinorDPO, we propose a training\nmetric for SFT to measure the discrepancy between the optimized model and the\noriginal model, and a loss function MinorSFT that can increase the training\neffectiveness, and reduce the discrepancy between the optimized LLM and\noriginal LLM."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuyu Wu"
                },
                "author": "Xiuyu Wu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10635v1",
                "updated": "2024-08-20T08:22:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    22,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:22:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    22,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search"
                },
                "summary": "In this paper, we propose a new method Strategist that utilizes LLMs to\nacquire new skills for playing multi-agent games through a self-improvement\nprocess. Our method gathers quality feedback through self-play simulations with\nMonte Carlo tree search and LLM-based reflection, which can then be used to\nlearn high-level strategic skills such as how to evaluate states that guide the\nlow-level execution.We showcase how our method can be used in both action\nplanning and dialogue generation in the context of games, achieving good\nperformance on both tasks. Specifically, we demonstrate that our method can\nhelp train agents with better performance than both traditional reinforcement\nlearning-based approaches and other LLM-based skill learning approaches in\ngames including the Game of Pure Strategy (GOPS) and The Resistance: Avalon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a new method Strategist that utilizes LLMs to\nacquire new skills for playing multi-agent games through a self-improvement\nprocess. Our method gathers quality feedback through self-play simulations with\nMonte Carlo tree search and LLM-based reflection, which can then be used to\nlearn high-level strategic skills such as how to evaluate states that guide the\nlow-level execution.We showcase how our method can be used in both action\nplanning and dialogue generation in the context of games, achieving good\nperformance on both tasks. Specifically, we demonstrate that our method can\nhelp train agents with better performance than both traditional reinforcement\nlearning-based approaches and other LLM-based skill learning approaches in\ngames including the Game of Pure Strategy (GOPS) and The Resistance: Avalon."
                },
                "authors": [
                    {
                        "name": "Jonathan Light"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Weiqin Chen"
                    },
                    {
                        "name": "Guanzhi Wang"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Ziniu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Ziniu Hu"
                },
                "author": "Ziniu Hu",
                "arxiv_comment": "website: https://llm-strategist.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10631v1",
                "updated": "2024-08-20T08:13:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    13,
                    52,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T08:13:52Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    13,
                    52,
                    1,
                    233,
                    0
                ],
                "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have grown significantly in scale, leading to a\ncritical need for efficient model pruning techniques. Existing post-training\npruning techniques primarily focus on measuring weight importance on converged\ndense models to determine salient weights to retain. However, they often\noverlook the changes in weight importance during the pruning process, which can\nlead to performance degradation in the pruned models. To address this issue, we\npresent LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a\nnovel one-shot pruning framework that rebuilds the sparsity mask of pruned\nmodels without any retraining or weight reconstruction. LLM-Barber incorporates\nblock-aware error optimization across Self-Attention and MLP blocks, ensuring\nglobal performance optimization. Inspired by the recent discovery of prominent\noutliers in LLMs, LLM-Barber introduces an innovative pruning metric that\nidentifies weight importance using weights multiplied by gradients. Our\nexperiments show that LLM-Barber can efficiently prune models like LLaMA and\nOPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes,\nachieving state-of-the-art results in both perplexity and zero-shot performance\nacross various language benchmarks. Code is available at\nhttps://github.com/YupengSu/LLM-Barber.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have grown significantly in scale, leading to a\ncritical need for efficient model pruning techniques. Existing post-training\npruning techniques primarily focus on measuring weight importance on converged\ndense models to determine salient weights to retain. However, they often\noverlook the changes in weight importance during the pruning process, which can\nlead to performance degradation in the pruned models. To address this issue, we\npresent LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a\nnovel one-shot pruning framework that rebuilds the sparsity mask of pruned\nmodels without any retraining or weight reconstruction. LLM-Barber incorporates\nblock-aware error optimization across Self-Attention and MLP blocks, ensuring\nglobal performance optimization. Inspired by the recent discovery of prominent\noutliers in LLMs, LLM-Barber introduces an innovative pruning metric that\nidentifies weight importance using weights multiplied by gradients. Our\nexperiments show that LLM-Barber can efficiently prune models like LLaMA and\nOPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes,\nachieving state-of-the-art results in both perplexity and zero-shot performance\nacross various language benchmarks. Code is available at\nhttps://github.com/YupengSu/LLM-Barber."
                },
                "authors": [
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Ziyi Guan"
                    },
                    {
                        "name": "Xiaoqun Liu"
                    },
                    {
                        "name": "Tianlai Jin"
                    },
                    {
                        "name": "Dongkuan Wu"
                    },
                    {
                        "name": "Graziano Chesi"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yu"
                },
                "author": "Hao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13664v3",
                "updated": "2024-08-20T08:10:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    10,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2023-10-20T17:05:27Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    17,
                    5,
                    27,
                    4,
                    293,
                    0
                ],
                "title": "Explainable Depression Symptom Detection in Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Depression Symptom Detection in Social Media"
                },
                "summary": "Users of social platforms often perceive these sites as supportive spaces to\npost about their mental health issues. Those conversations contain important\ntraces about individuals' health risks. Recently, researchers have exploited\nthis online information to construct mental health detection models, which aim\nto identify users at risk on platforms like Twitter, Reddit or Facebook. Most\nof these models are centred on achieving good classification results, ignoring\nthe explainability and interpretability of the decisions. Recent research has\npointed out the importance of using clinical markers, such as the use of\nsymptoms, to improve trust in the computational models by health professionals.\nIn this paper, we propose using transformer-based architectures to detect and\nexplain the appearance of depressive symptom markers in the users' writings. We\npresent two approaches: i) train a model to classify, and another one to\nexplain the classifier's decision separately and ii) unify the two tasks\nsimultaneously using a single model. Additionally, for this latter manner, we\nalso investigated the performance of recent conversational LLMs when using\nin-context learning. Our natural language explanations enable clinicians to\ninterpret the models' decisions based on validated symptoms, enhancing trust in\nthe automated process. We evaluate our approach using recent symptom-based\ndatasets, employing both offline and expert-in-the-loop metrics to assess the\nquality of the explanations generated by our models. The experimental results\nshow that it is possible to achieve good classification results while\ngenerating interpretable symptom-based explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users of social platforms often perceive these sites as supportive spaces to\npost about their mental health issues. Those conversations contain important\ntraces about individuals' health risks. Recently, researchers have exploited\nthis online information to construct mental health detection models, which aim\nto identify users at risk on platforms like Twitter, Reddit or Facebook. Most\nof these models are centred on achieving good classification results, ignoring\nthe explainability and interpretability of the decisions. Recent research has\npointed out the importance of using clinical markers, such as the use of\nsymptoms, to improve trust in the computational models by health professionals.\nIn this paper, we propose using transformer-based architectures to detect and\nexplain the appearance of depressive symptom markers in the users' writings. We\npresent two approaches: i) train a model to classify, and another one to\nexplain the classifier's decision separately and ii) unify the two tasks\nsimultaneously using a single model. Additionally, for this latter manner, we\nalso investigated the performance of recent conversational LLMs when using\nin-context learning. Our natural language explanations enable clinicians to\ninterpret the models' decisions based on validated symptoms, enhancing trust in\nthe automated process. We evaluate our approach using recent symptom-based\ndatasets, employing both offline and expert-in-the-loop metrics to assess the\nquality of the explanations generated by our models. The experimental results\nshow that it is possible to achieve good classification results while\ngenerating interpretable symptom-based explanations."
                },
                "authors": [
                    {
                        "name": "Eliseo Bao"
                    },
                    {
                        "name": "Anxo Pérez"
                    },
                    {
                        "name": "Javier Parapar"
                    }
                ],
                "author_detail": {
                    "name": "Javier Parapar"
                },
                "author": "Javier Parapar",
                "arxiv_comment": "Accepted for publication in Health Information Science and Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.13664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08382v2",
                "updated": "2024-08-20T08:07:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    7,
                    49,
                    1,
                    233,
                    0
                ],
                "published": "2024-04-12T10:36:15Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    10,
                    36,
                    15,
                    4,
                    103,
                    0
                ],
                "title": "Look at the Text: Instruction-Tuned Language Models are More Robust\n  Multiple Choice Selectors than You Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look at the Text: Instruction-Tuned Language Models are More Robust\n  Multiple Choice Selectors than You Think"
                },
                "summary": "Multiple choice questions (MCQs) are commonly used to evaluate the\ncapabilities of large language models (LLMs). One common way to evaluate the\nmodel response is to rank the candidate answers based on the log probability of\nthe first token prediction. An alternative way is to examine the text output.\nPrior work has shown that first token probabilities lack robustness to changes\nin MCQ phrasing, and that first token probabilities do not match text answers\nfor instruction-tuned models. Therefore, in this paper, we investigate the\nrobustness of text answers. We show that the text answers are more robust to\nquestion perturbations than the first token probabilities, when the first token\nanswers mismatch the text answers. The difference in robustness increases as\nthe mismatch rate becomes greater. As the mismatch reaches over 50\\%, the text\nanswer is more robust to option order changes than the debiased first token\nprobabilities using state-of-the-art debiasing methods such as PriDe. Our\nfindings provide further evidence for the benefits of text answer evaluation\nover first token probability evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple choice questions (MCQs) are commonly used to evaluate the\ncapabilities of large language models (LLMs). One common way to evaluate the\nmodel response is to rank the candidate answers based on the log probability of\nthe first token prediction. An alternative way is to examine the text output.\nPrior work has shown that first token probabilities lack robustness to changes\nin MCQ phrasing, and that first token probabilities do not match text answers\nfor instruction-tuned models. Therefore, in this paper, we investigate the\nrobustness of text answers. We show that the text answers are more robust to\nquestion perturbations than the first token probabilities, when the first token\nanswers mismatch the text answers. The difference in robustness increases as\nthe mismatch rate becomes greater. As the mismatch reaches over 50\\%, the text\nanswer is more robust to option order changes than the debiased first token\nprobabilities using state-of-the-art debiasing methods such as PriDe. Our\nfindings provide further evidence for the benefits of text answer evaluation\nover first token probability evaluation."
                },
                "authors": [
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Chengzhi Hu"
                    },
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10615v1",
                "updated": "2024-08-20T07:49:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    49,
                    38,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T07:49:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    49,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "Enhancing Robustness in Large Language Models: Prompting for Mitigating\n  the Impact of Irrelevant Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Robustness in Large Language Models: Prompting for Mitigating\n  the Impact of Irrelevant Information"
                },
                "summary": "In recent years, Large language models (LLMs) have garnered significant\nattention due to their superior performance in complex reasoning tasks.\nHowever, recent studies may diminish their reasoning capabilities markedly when\nproblem descriptions contain irrelevant information, even with the use of\nadvanced prompting techniques. To further investigate this issue, a dataset of\nprimary school mathematics problems containing irrelevant information, named\nGSMIR, was constructed. Testing prominent LLMs and prompting techniques on this\ndataset revealed that while LLMs can identify irrelevant information, they do\nnot effectively mitigate the interference it causes once identified. A novel\nautomatic construction method, ATF, which enhances the ability of LLMs to\nidentify and self-mitigate the influence of irrelevant information, is proposed\nto address this shortcoming. This method operates in two steps: first, analysis\nof irrelevant information, followed by its filtering. The ATF method, as\ndemonstrated by experimental results, significantly improves the reasoning\nperformance of LLMs and prompting techniques, even in the presence of\nirrelevant information on the GSMIR dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large language models (LLMs) have garnered significant\nattention due to their superior performance in complex reasoning tasks.\nHowever, recent studies may diminish their reasoning capabilities markedly when\nproblem descriptions contain irrelevant information, even with the use of\nadvanced prompting techniques. To further investigate this issue, a dataset of\nprimary school mathematics problems containing irrelevant information, named\nGSMIR, was constructed. Testing prominent LLMs and prompting techniques on this\ndataset revealed that while LLMs can identify irrelevant information, they do\nnot effectively mitigate the interference it causes once identified. A novel\nautomatic construction method, ATF, which enhances the ability of LLMs to\nidentify and self-mitigate the influence of irrelevant information, is proposed\nto address this shortcoming. This method operates in two steps: first, analysis\nof irrelevant information, followed by its filtering. The ATF method, as\ndemonstrated by experimental results, significantly improves the reasoning\nperformance of LLMs and prompting techniques, even in the presence of\nirrelevant information on the GSMIR dataset."
                },
                "authors": [
                    {
                        "name": "Ming Jiang"
                    },
                    {
                        "name": "Tingting Huang"
                    },
                    {
                        "name": "Biao Guo"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10614v1",
                "updated": "2024-08-20T07:48:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    48,
                    45,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T07:48:45Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    48,
                    45,
                    1,
                    233,
                    0
                ],
                "title": "Generalizable Facial Expression Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable Facial Expression Recognition"
                },
                "summary": "SOTA facial expression recognition (FER) methods fail on test sets that have\ndomain gaps with the train set. Recent domain adaptation FER methods need to\nacquire labeled or unlabeled samples of target domains to fine-tune the FER\nmodel, which might be infeasible in real-world deployment. In this paper, we\naim to improve the zero-shot generalization ability of FER methods on different\nunseen test sets using only one train set. Inspired by how humans first detect\nfaces and then select expression features, we propose a novel FER pipeline to\nextract expression-related features from any given face images. Our method is\nbased on the generalizable face features extracted by large models like CLIP.\nHowever, it is non-trivial to adapt the general features of CLIP for specific\ntasks like FER. To preserve the generalization ability of CLIP and the high\nprecision of the FER model, we design a novel approach that learns sigmoid\nmasks based on the fixed CLIP face features to extract expression features. To\nfurther improve the generalization ability on unseen test sets, we separate the\nchannels of the learned masked features according to the expression classes to\ndirectly generate logits and avoid using the FC layer to reduce overfitting. We\nalso introduce a channel-diverse loss to make the learned masks separated.\nExtensive experiments on five different FER datasets verify that our method\noutperforms SOTA FER methods by large margins. Code is available in\nhttps://github.com/zyh-uaiaaaa/Generalizable-FER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOTA facial expression recognition (FER) methods fail on test sets that have\ndomain gaps with the train set. Recent domain adaptation FER methods need to\nacquire labeled or unlabeled samples of target domains to fine-tune the FER\nmodel, which might be infeasible in real-world deployment. In this paper, we\naim to improve the zero-shot generalization ability of FER methods on different\nunseen test sets using only one train set. Inspired by how humans first detect\nfaces and then select expression features, we propose a novel FER pipeline to\nextract expression-related features from any given face images. Our method is\nbased on the generalizable face features extracted by large models like CLIP.\nHowever, it is non-trivial to adapt the general features of CLIP for specific\ntasks like FER. To preserve the generalization ability of CLIP and the high\nprecision of the FER model, we design a novel approach that learns sigmoid\nmasks based on the fixed CLIP face features to extract expression features. To\nfurther improve the generalization ability on unseen test sets, we separate the\nchannels of the learned masked features according to the expression classes to\ndirectly generate logits and avoid using the FC layer to reduce overfitting. We\nalso introduce a channel-diverse loss to make the learned masks separated.\nExtensive experiments on five different FER datasets verify that our method\noutperforms SOTA FER methods by large margins. Code is available in\nhttps://github.com/zyh-uaiaaaa/Generalizable-FER."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhang"
                    },
                    {
                        "name": "Xiuqi Zheng"
                    },
                    {
                        "name": "Chenyi Liang"
                    },
                    {
                        "name": "Jiani Hu"
                    },
                    {
                        "name": "Weihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Weihong Deng"
                },
                "author": "Weihong Deng",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10613v1",
                "updated": "2024-08-20T07:48:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    48,
                    19,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T07:48:19Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    48,
                    19,
                    1,
                    233,
                    0
                ],
                "title": "Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval"
                },
                "summary": "Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably leads to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably leads to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models."
                },
                "authors": [
                    {
                        "name": "Guangyuan Ma"
                    },
                    {
                        "name": "Yongliang Ma"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Ming Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10608v1",
                "updated": "2024-08-20T07:40:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    40,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T07:40:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    40,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Promoting Equality in Large Language Models: Identifying and Mitigating\n  the Implicit Bias based on Bayesian Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promoting Equality in Large Language Models: Identifying and Mitigating\n  the Implicit Bias based on Bayesian Theory"
                },
                "summary": "Large language models (LLMs) are trained on extensive text corpora, which\ninevitably include biased information. Although techniques such as Affective\nAlignment can mitigate some negative impacts of these biases, existing\nprompt-based attack methods can still extract these biases from the model's\nweights. Moreover, these biases frequently appear subtly when LLMs are prompted\nto perform identical tasks across different demographic groups, thereby\ncamouflaging their presence. To address this issue, we have formally defined\nthe implicit bias problem and developed an innovative framework for bias\nremoval based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR).\nBTBR employs likelihood ratio screening to pinpoint data entries within\npublicly accessible biased datasets that represent biases inadvertently\nincorporated during the LLM training phase. It then automatically constructs\nrelevant knowledge triples and expunges bias information from LLMs using model\nediting techniques. Through extensive experimentation, we have confirmed the\npresence of the implicit bias problem in LLMs and demonstrated the\neffectiveness of our BTBR approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on extensive text corpora, which\ninevitably include biased information. Although techniques such as Affective\nAlignment can mitigate some negative impacts of these biases, existing\nprompt-based attack methods can still extract these biases from the model's\nweights. Moreover, these biases frequently appear subtly when LLMs are prompted\nto perform identical tasks across different demographic groups, thereby\ncamouflaging their presence. To address this issue, we have formally defined\nthe implicit bias problem and developed an innovative framework for bias\nremoval based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR).\nBTBR employs likelihood ratio screening to pinpoint data entries within\npublicly accessible biased datasets that represent biases inadvertently\nincorporated during the LLM training phase. It then automatically constructs\nrelevant knowledge triples and expunges bias information from LLMs using model\nediting techniques. Through extensive experimentation, we have confirmed the\npresence of the implicit bias problem in LLMs and demonstrated the\neffectiveness of our BTBR approach."
                },
                "authors": [
                    {
                        "name": "Yongxin Deng"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Chen Jue"
                    },
                    {
                        "name": "Zhijun Fang"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Yuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Qi"
                },
                "arxiv_affiliation": "Fudan University",
                "author": "Yuan Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10593v1",
                "updated": "2024-08-20T07:10:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    10,
                    40,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T07:10:40Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    10,
                    40,
                    1,
                    233,
                    0
                ],
                "title": "An Efficient Sign Language Translation Using Spatial Configuration and\n  Motion Dynamics with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Sign Language Translation Using Spatial Configuration and\n  Motion Dynamics with LLMs"
                },
                "summary": "Gloss-free Sign Language Translation (SLT) converts sign videos directly into\nspoken language sentences without relying on glosses. Recently, Large Language\nModels (LLMs) have shown remarkable translation performance in gloss-free\nmethods by harnessing their powerful natural language generation capabilities.\nHowever, these methods often rely on domain-specific fine-tuning of visual\nencoders to achieve optimal results. By contrast, this paper emphasizes the\nimportance of capturing the spatial configurations and motion dynamics inherent\nin sign language. With this in mind, we introduce Spatial and Motion-based Sign\nLanguage Translation (SpaMo), a novel LLM-based SLT framework. The core idea of\nSpaMo is simple yet effective. We first extract spatial and motion features\nusing off-the-shelf visual encoders and then input these features into an LLM\nwith a language prompt. Additionally, we employ a visual-text alignment process\nas a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo\nachieves state-of-the-art performance on two popular datasets, PHOENIX14T and\nHow2Sign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gloss-free Sign Language Translation (SLT) converts sign videos directly into\nspoken language sentences without relying on glosses. Recently, Large Language\nModels (LLMs) have shown remarkable translation performance in gloss-free\nmethods by harnessing their powerful natural language generation capabilities.\nHowever, these methods often rely on domain-specific fine-tuning of visual\nencoders to achieve optimal results. By contrast, this paper emphasizes the\nimportance of capturing the spatial configurations and motion dynamics inherent\nin sign language. With this in mind, we introduce Spatial and Motion-based Sign\nLanguage Translation (SpaMo), a novel LLM-based SLT framework. The core idea of\nSpaMo is simple yet effective. We first extract spatial and motion features\nusing off-the-shelf visual encoders and then input these features into an LLM\nwith a language prompt. Additionally, we employ a visual-text alignment process\nas a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo\nachieves state-of-the-art performance on two popular datasets, PHOENIX14T and\nHow2Sign."
                },
                "authors": [
                    {
                        "name": "Eui Jun Hwang"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "Junmyeong Lee"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10199v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10199v5",
                "updated": "2024-08-20T06:53:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    53,
                    45,
                    1,
                    233,
                    0
                ],
                "published": "2024-04-16T00:50:43Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    0,
                    50,
                    43,
                    1,
                    107,
                    0
                ],
                "title": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting"
                },
                "summary": "As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/"
                },
                "authors": [
                    {
                        "name": "Huihan Li"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Sebastin Santy"
                    },
                    {
                        "name": "Taylor Sorensen"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10199v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10199v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07505v2",
                "updated": "2024-08-20T06:50:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    50,
                    48,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-14T12:32:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    32,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Large Language Models Know What Makes Exemplary Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Know What Makes Exemplary Contexts"
                },
                "summary": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval."
                },
                "authors": [
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jianda Chen"
                    },
                    {
                        "name": "Wenya Wang"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    }
                ],
                "author_detail": {
                    "name": "Sinno Jialin Pan"
                },
                "author": "Sinno Jialin Pan",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11880v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11880v3",
                "updated": "2024-08-20T06:45:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    45,
                    50,
                    1,
                    233,
                    0
                ],
                "published": "2024-01-22T12:11:55Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    12,
                    11,
                    55,
                    0,
                    22,
                    0
                ],
                "title": "PsySafe: A Comprehensive Framework for Psychological-based Attack,\n  Defense, and Evaluation of Multi-agent System Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsySafe: A Comprehensive Framework for Psychological-based Attack,\n  Defense, and Evaluation of Multi-agent System Safety"
                },
                "summary": "Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit\nprofound capabilities in collective intelligence. However, the potential misuse\nof this intelligence for malicious purposes presents significant risks. To\ndate, comprehensive research on the safety issues associated with multi-agent\nsystems remains limited. In this paper, we explore these concerns through the\ninnovative lens of agent psychology, revealing that the dark psychological\nstates of agents constitute a significant threat to safety. To tackle these\nconcerns, we propose a comprehensive framework (PsySafe) grounded in agent\npsychology, focusing on three key areas: firstly, identifying how dark\npersonality traits in agents can lead to risky behaviors; secondly, evaluating\nthe safety of multi-agent systems from the psychological and behavioral\nperspectives, and thirdly, devising effective strategies to mitigate these\nrisks. Our experiments reveal several intriguing phenomena, such as the\ncollective dangerous behaviors among agents, agents' self-reflection when\nengaging in dangerous behavior, and the correlation between agents'\npsychological assessments and dangerous behaviors. We anticipate that our\nframework and observations will provide valuable insights for further research\ninto the safety of multi-agent systems. We will make our data and code publicly\naccessible at https://github.com/AI4Good24/PsySafe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit\nprofound capabilities in collective intelligence. However, the potential misuse\nof this intelligence for malicious purposes presents significant risks. To\ndate, comprehensive research on the safety issues associated with multi-agent\nsystems remains limited. In this paper, we explore these concerns through the\ninnovative lens of agent psychology, revealing that the dark psychological\nstates of agents constitute a significant threat to safety. To tackle these\nconcerns, we propose a comprehensive framework (PsySafe) grounded in agent\npsychology, focusing on three key areas: firstly, identifying how dark\npersonality traits in agents can lead to risky behaviors; secondly, evaluating\nthe safety of multi-agent systems from the psychological and behavioral\nperspectives, and thirdly, devising effective strategies to mitigate these\nrisks. Our experiments reveal several intriguing phenomena, such as the\ncollective dangerous behaviors among agents, agents' self-reflection when\nengaging in dangerous behavior, and the correlation between agents'\npsychological assessments and dangerous behaviors. We anticipate that our\nframework and observations will provide valuable insights for further research\ninto the safety of multi-agent systems. We will make our data and code publicly\naccessible at https://github.com/AI4Good24/PsySafe."
                },
                "authors": [
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Yongting Zhang"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Hongzhi Gao"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11880v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10577v1",
                "updated": "2024-08-20T06:32:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    32,
                    57,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T06:32:57Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    32,
                    57,
                    1,
                    233,
                    0
                ],
                "title": "Optimizing Large Language Model Hyperparameters for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Model Hyperparameters for Code Generation"
                },
                "summary": "Large Language Models (LLMs), such as GPT models, are increasingly used in\nsoftware engineering for various tasks, such as code generation, requirements\nmanagement, and debugging. While automating these tasks has garnered\nsignificant attention, a systematic study on the impact of varying\nhyperparameters on code generation outcomes remains unexplored. This study aims\nto assess LLMs' code generation performance by exhaustively exploring the\nimpact of various hyperparameters. Hyperparameters for LLMs are adjustable\nsettings that affect the model's behaviour and performance. Specifically, we\ninvestigated how changes to the hyperparameters: temperature, top probability\n(top_p), frequency penalty, and presence penalty affect code generation\noutcomes. We systematically adjusted all hyperparameters together, exploring\nevery possible combination by making small increments to each hyperparameter at\na time. This exhaustive approach was applied to 13 Python code generation\ntasks, yielding one of four outcomes for each hyperparameter combination: no\noutput from the LLM, non executable code, code that fails unit tests, or\ncorrect and functional code. We analysed these outcomes for a total of 14,742\ngenerated Python code segments, focusing on correctness, to determine how the\nhyperparameters influence the LLM to arrive at each outcome. Using correlation\ncoefficient and regression tree analyses, we ascertained which hyperparameters\ninfluence which aspect of the LLM. Our results indicate that optimal\nperformance is achieved with a temperature below 0.5, top probability below\n0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1.\nWe make our dataset and results available to facilitate replication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT models, are increasingly used in\nsoftware engineering for various tasks, such as code generation, requirements\nmanagement, and debugging. While automating these tasks has garnered\nsignificant attention, a systematic study on the impact of varying\nhyperparameters on code generation outcomes remains unexplored. This study aims\nto assess LLMs' code generation performance by exhaustively exploring the\nimpact of various hyperparameters. Hyperparameters for LLMs are adjustable\nsettings that affect the model's behaviour and performance. Specifically, we\ninvestigated how changes to the hyperparameters: temperature, top probability\n(top_p), frequency penalty, and presence penalty affect code generation\noutcomes. We systematically adjusted all hyperparameters together, exploring\nevery possible combination by making small increments to each hyperparameter at\na time. This exhaustive approach was applied to 13 Python code generation\ntasks, yielding one of four outcomes for each hyperparameter combination: no\noutput from the LLM, non executable code, code that fails unit tests, or\ncorrect and functional code. We analysed these outcomes for a total of 14,742\ngenerated Python code segments, focusing on correctness, to determine how the\nhyperparameters influence the LLM to arrive at each outcome. Using correlation\ncoefficient and regression tree analyses, we ascertained which hyperparameters\ninfluence which aspect of the LLM. Our results indicate that optimal\nperformance is achieved with a temperature below 0.5, top probability below\n0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1.\nWe make our dataset and results available to facilitate replication."
                },
                "authors": [
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Ahnaf Ibn Sayeed"
                    },
                    {
                        "name": "Sherlock Licorish"
                    },
                    {
                        "name": "Fanyu Wang"
                    },
                    {
                        "name": "Christoph Treude"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Treude"
                },
                "author": "Christoph Treude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10573v1",
                "updated": "2024-08-20T06:24:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    24,
                    47,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T06:24:47Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    24,
                    47,
                    1,
                    233,
                    0
                ],
                "title": "Putting People in LLMs' Shoes: Generating Better Answers via Question\n  Rewriter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting People in LLMs' Shoes: Generating Better Answers via Question\n  Rewriter"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant capabilities,\nparticularly in the domain of question answering (QA). However, their\neffectiveness in QA is often undermined by the vagueness of user questions. To\naddress this issue, we introduce single-round instance-level prompt\noptimization, referred to as question rewriter. By enhancing the\nintelligibility of human questions for black-box LLMs, our question rewriter\nimproves the quality of generated answers. The rewriter is optimized using\ndirect preference optimization based on feedback collected from automatic\ncriteria for evaluating generated answers; therefore, its training does not\nrequire costly human annotations. The experiments across multiple black-box\nLLMs and long-form question answering (LFQA) datasets demonstrate the efficacy\nof our method. This paper provides a practical framework for training question\nrewriters and sets a precedent for future explorations in prompt optimization\nwithin LFQA tasks. Code is available at\n\\url{https://github.com/3244we/Question-Rewriter}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant capabilities,\nparticularly in the domain of question answering (QA). However, their\neffectiveness in QA is often undermined by the vagueness of user questions. To\naddress this issue, we introduce single-round instance-level prompt\noptimization, referred to as question rewriter. By enhancing the\nintelligibility of human questions for black-box LLMs, our question rewriter\nimproves the quality of generated answers. The rewriter is optimized using\ndirect preference optimization based on feedback collected from automatic\ncriteria for evaluating generated answers; therefore, its training does not\nrequire costly human annotations. The experiments across multiple black-box\nLLMs and long-form question answering (LFQA) datasets demonstrate the efficacy\nof our method. This paper provides a practical framework for training question\nrewriters and sets a precedent for future explorations in prompt optimization\nwithin LFQA tasks. Code is available at\n\\url{https://github.com/3244we/Question-Rewriter}."
                },
                "authors": [
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Zhouqiang jiang"
                    },
                    {
                        "name": "Yuta Nakashima"
                    }
                ],
                "author_detail": {
                    "name": "Yuta Nakashima"
                },
                "author": "Yuta Nakashima",
                "arxiv_comment": "7 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09955v2",
                "updated": "2024-08-20T05:51:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    51,
                    46,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T12:55:16Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    55,
                    16,
                    0,
                    232,
                    0
                ],
                "title": "MegaAgent: A Practical Framework for Autonomous Cooperation in\n  Large-Scale LLM Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaAgent: A Practical Framework for Autonomous Cooperation in\n  Large-Scale LLM Agent Systems"
                },
                "summary": "With the emergence of large language models (LLMs), LLM-powered multi-agent\nsystems (LLM-MA systems) have been proposed to tackle real-world tasks.\nHowever, their agents mostly follow predefined Standard Operating Procedures\n(SOPs) that remain unchanged across the whole interaction, lacking autonomy and\nscalability. Additionally, current solutions often overlook the necessity for\neffective agent cooperation. To address the above limitations, we propose\nMegaAgent, a practical framework designed for autonomous cooperation in\nlarge-scale LLM Agent systems. MegaAgent leverages the autonomy of agents to\ndynamically generate agents based on task requirements, incorporating features\nsuch as automatically dividing tasks, systematic planning and monitoring of\nagent activities, and managing concurrent operations. In addition, MegaAgent is\ndesigned with a hierarchical structure and employs system-level parallelism to\nenhance performance and boost communication. We demonstrate the effectiveness\nof MegaAgent through Gobang game development, showing that it outperforms\npopular LLM-MA systems; and national policy simulation, demonstrating its high\nautonomy and potential to rapidly scale up to 590 agents while ensuring\neffective cooperation among them. Our results indicate that MegaAgent is the\nfirst autonomous large-scale LLM-MA system with no pre-defined SOPs, high\neffectiveness and scalability, paving the way for further research in this\nfield. Our code is at https://anonymous.4open.science/r/MegaAgent-81F3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of large language models (LLMs), LLM-powered multi-agent\nsystems (LLM-MA systems) have been proposed to tackle real-world tasks.\nHowever, their agents mostly follow predefined Standard Operating Procedures\n(SOPs) that remain unchanged across the whole interaction, lacking autonomy and\nscalability. Additionally, current solutions often overlook the necessity for\neffective agent cooperation. To address the above limitations, we propose\nMegaAgent, a practical framework designed for autonomous cooperation in\nlarge-scale LLM Agent systems. MegaAgent leverages the autonomy of agents to\ndynamically generate agents based on task requirements, incorporating features\nsuch as automatically dividing tasks, systematic planning and monitoring of\nagent activities, and managing concurrent operations. In addition, MegaAgent is\ndesigned with a hierarchical structure and employs system-level parallelism to\nenhance performance and boost communication. We demonstrate the effectiveness\nof MegaAgent through Gobang game development, showing that it outperforms\npopular LLM-MA systems; and national policy simulation, demonstrating its high\nautonomy and potential to rapidly scale up to 590 agents while ensuring\neffective cooperation among them. Our results indicate that MegaAgent is the\nfirst autonomous large-scale LLM-MA system with no pre-defined SOPs, high\neffectiveness and scalability, paving the way for further research in this\nfield. Our code is at https://anonymous.4open.science/r/MegaAgent-81F3."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Jingsheng Liang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09632v2",
                "updated": "2024-08-20T05:28:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    28,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T01:30:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    30,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDeGPT: Modular Decomposition for Large Language Model Compression"
                },
                "summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%."
                },
                "authors": [
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Abhishek Patel"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A23 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14869v2",
                "updated": "2024-08-20T05:27:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    27,
                    44,
                    1,
                    233,
                    0
                ],
                "published": "2024-01-26T13:55:32Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    13,
                    55,
                    32,
                    4,
                    26,
                    0
                ],
                "title": "F-Eval: Assessing Fundamental Abilities with Refined Evaluation Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-Eval: Assessing Fundamental Abilities with Refined Evaluation Methods"
                },
                "summary": "Large language models (LLMs) garner significant attention for their\nunprecedented performance, leading to an increasing number of researches\nevaluating LLMs. However, these evaluation benchmarks are limited to assessing\nthe instruction-following capabilities, overlooking the fundamental abilities\nthat emerge during the pre-training stage. Previous subjective evaluation\nmethods mainly reply on scoring by API models. However, in the absence of\nreferences, large models have shown limited ability to discern subtle\ndifferences. To bridge the gap, we propose F-Eval, a bilingual evaluation\nbenchmark to evaluate the fundamental abilities, including expression,\ncommonsense and logic. The tasks in F-Eval include multi-choice objective\ntasks, open-ended objective tasks, reference-based subjective tasks and\nreference-free subjective tasks. For reference-free subjective tasks, we devise\nnew evaluation methods, serving as alternatives to scoring by API models. We\nconduct evaluations on 13 advanced LLMs. Results show that our evaluation\nmethods show higher correlation coefficients and larger distinction than other\nevaluators. Additionally, we discuss the influence of different model sizes,\ndimensions, and normalization methods. We anticipate that F-Eval will\nfacilitate the study of LLMs' fundamental abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) garner significant attention for their\nunprecedented performance, leading to an increasing number of researches\nevaluating LLMs. However, these evaluation benchmarks are limited to assessing\nthe instruction-following capabilities, overlooking the fundamental abilities\nthat emerge during the pre-training stage. Previous subjective evaluation\nmethods mainly reply on scoring by API models. However, in the absence of\nreferences, large models have shown limited ability to discern subtle\ndifferences. To bridge the gap, we propose F-Eval, a bilingual evaluation\nbenchmark to evaluate the fundamental abilities, including expression,\ncommonsense and logic. The tasks in F-Eval include multi-choice objective\ntasks, open-ended objective tasks, reference-based subjective tasks and\nreference-free subjective tasks. For reference-free subjective tasks, we devise\nnew evaluation methods, serving as alternatives to scoring by API models. We\nconduct evaluations on 13 advanced LLMs. Results show that our evaluation\nmethods show higher correlation coefficients and larger distinction than other\nevaluators. Additionally, we discuss the influence of different model sizes,\ndimensions, and normalization methods. We anticipate that F-Eval will\nfacilitate the study of LLMs' fundamental abilities."
                },
                "authors": [
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Shujie Wang"
                    },
                    {
                        "name": "Peiji Li"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10549v1",
                "updated": "2024-08-20T05:04:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    4,
                    40,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T05:04:40Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    4,
                    40,
                    1,
                    233,
                    0
                ],
                "title": "AI-Based IVR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Based IVR"
                },
                "summary": "The use of traditional IVR (Interactive Voice Response) methods often proves\ninsufficient to meet customer needs. This article examines the application of\nartificial intelligence (AI) technologies to enhance the efficiency of IVR\nsystems in call centers. A proposed approach is based on the integration of\nspeech-to-text conversion solutions, text query classification using large\nlanguage models (LLM), and speech synthesis. Special attention is given to\nadapting these technologies to work with the Kazakh language, including\nfine-tuning models on specialized datasets. The practical aspects of\nimplementing the developed system in a real call center for query\nclassification are described. The research results demonstrate that the\napplication of AI technologies in call center IVR systems reduces operator\nworkload, improves customer service quality, and increases the efficiency of\nquery processing. The proposed approach can be adapted for use in call centers\noperating with various languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of traditional IVR (Interactive Voice Response) methods often proves\ninsufficient to meet customer needs. This article examines the application of\nartificial intelligence (AI) technologies to enhance the efficiency of IVR\nsystems in call centers. A proposed approach is based on the integration of\nspeech-to-text conversion solutions, text query classification using large\nlanguage models (LLM), and speech synthesis. Special attention is given to\nadapting these technologies to work with the Kazakh language, including\nfine-tuning models on specialized datasets. The practical aspects of\nimplementing the developed system in a real call center for query\nclassification are described. The research results demonstrate that the\napplication of AI technologies in call center IVR systems reduces operator\nworkload, improves customer service quality, and increases the efficiency of\nquery processing. The proposed approach can be adapted for use in call centers\noperating with various languages."
                },
                "authors": [
                    {
                        "name": "Gassyrbek Kosherbay"
                    },
                    {
                        "name": "Nurgissa Apbaz"
                    }
                ],
                "author_detail": {
                    "name": "Nurgissa Apbaz"
                },
                "author": "Nurgissa Apbaz",
                "arxiv_comment": "in Russian language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11709v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11709v3",
                "updated": "2024-08-20T04:33:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    33,
                    19,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-17T16:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    16,
                    28,
                    21,
                    0,
                    169,
                    0
                ],
                "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging"
                },
                "summary": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning. We provide our\ncode and datasets at http://github.com/agarwalishika/TreeInstruct .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning. We provide our\ncode and datasets at http://github.com/agarwalishika/TreeInstruct ."
                },
                "authors": [
                    {
                        "name": "Priyanka Kargupta"
                    },
                    {
                        "name": "Ishika Agarwal"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11709v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11709v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06583v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06583v4",
                "updated": "2024-08-20T04:32:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    32,
                    37,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-13T02:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    43,
                    19,
                    1,
                    226,
                    0
                ],
                "title": "A Structure-aware Generative Model for Biomedical Event Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Structure-aware Generative Model for Biomedical Event Extraction"
                },
                "summary": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute to over 20% of the events in the\nbenchmark datasets. In this paper, we propose an event structure-aware\ngenerative model named GenBEE, which can capture complex event structures in\nbiomedical text for biomedical event extraction. In particular, GenBEE\nconstructs event prompts that distill knowledge from LLMs for incorporating\nboth label semantics and argument dependency relationships into the proposed\nmodel. In addition, GenBEE also generates prefixes with event structural\nprompts to incorporate structural features for improving the model's overall\nperformance. We have evaluated the proposed GenBEE model on three widely used\nbiomedical event extraction benchmark datasets, namely MLEE, GE11, and PHEE.\nExperimental results show that GenBEE has achieved state-of-the-art performance\non the MLEE and GE11 datasets, and achieved competitive results when compared\nto the state-of-the-art classification-based models on the PHEE dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute to over 20% of the events in the\nbenchmark datasets. In this paper, we propose an event structure-aware\ngenerative model named GenBEE, which can capture complex event structures in\nbiomedical text for biomedical event extraction. In particular, GenBEE\nconstructs event prompts that distill knowledge from LLMs for incorporating\nboth label semantics and argument dependency relationships into the proposed\nmodel. In addition, GenBEE also generates prefixes with event structural\nprompts to incorporate structural features for improving the model's overall\nperformance. We have evaluated the proposed GenBEE model on three widely used\nbiomedical event extraction benchmark datasets, namely MLEE, GE11, and PHEE.\nExperimental results show that GenBEE has achieved state-of-the-art performance\non the MLEE and GE11 datasets, and achieved competitive results when compared\nto the state-of-the-art classification-based models on the PHEE dataset."
                },
                "authors": [
                    {
                        "name": "Haohan Yuan"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Haopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haopeng Zhang"
                },
                "author": "Haopeng Zhang",
                "arxiv_comment": "8 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06583v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06583v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10529v2",
                "updated": "2024-08-21T07:12:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    12,
                    27,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T04:06:58Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    6,
                    58,
                    1,
                    233,
                    0
                ],
                "title": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study"
                },
                "summary": "Context: Recent studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Recent studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2."
                },
                "authors": [
                    {
                        "name": "Emmanuel Iko-Ojo Simon"
                    },
                    {
                        "name": "Chirath Hettiarachchi"
                    },
                    {
                        "name": "Alex Potanin"
                    },
                    {
                        "name": "Hanna Suominen"
                    },
                    {
                        "name": "Fatemeh Fard"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Fard"
                },
                "author": "Fatemeh Fard",
                "arxiv_comment": "7 pages, 1 figure, 1 Table, Conference: 40th IEEE International\n  Conference on Software Maintenance and Evolution Location: Flagstaff, AZ, USA\n  Date 6 - 11 October, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7; K.6.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10520v1",
                "updated": "2024-08-20T03:45:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    3,
                    45,
                    24,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T03:45:24Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    3,
                    45,
                    24,
                    1,
                    233,
                    0
                ],
                "title": "Efficient and Deployable Knowledge Infusion for Open-World\n  Recommendations via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Deployable Knowledge Infusion for Open-World\n  Recommendations via Large Language Models"
                },
                "summary": "Recommender systems (RSs) play a pervasive role in today's online services,\nyet their closed-loop nature constrains their access to open-world knowledge.\nRecently, large language models (LLMs) have shown promise in bridging this gap.\nHowever, previous attempts to directly implement LLMs as recommenders fall\nshort in meeting the requirements of industrial RSs, particularly in terms of\nonline inference latency and offline resource efficiency. Thus, we propose REKI\nto acquire two types of external knowledge about users and items from LLMs.\nSpecifically, we introduce factorization prompting to elicit accurate knowledge\nreasoning on user preferences and items. We develop individual knowledge\nextraction and collective knowledge extraction tailored for different scales of\nscenarios, effectively reducing offline resource consumption. Subsequently,\ngenerated knowledge undergoes efficient transformation and condensation into\naugmented vectors through a hybridized expert-integrated network, ensuring\ncompatibility. The obtained vectors can then be used to enhance any\nconventional recommendation model. We also ensure efficient inference by\npreprocessing and prestoring the knowledge from LLMs. Experiments demonstrate\nthat REKI outperforms state-of-the-art baselines and is compatible with lots of\nrecommendation algorithms and tasks. Now, REKI has been deployed to Huawei's\nnews and music recommendation platforms and gained a 7% and 1.99% improvement\nduring the online A/B test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RSs) play a pervasive role in today's online services,\nyet their closed-loop nature constrains their access to open-world knowledge.\nRecently, large language models (LLMs) have shown promise in bridging this gap.\nHowever, previous attempts to directly implement LLMs as recommenders fall\nshort in meeting the requirements of industrial RSs, particularly in terms of\nonline inference latency and offline resource efficiency. Thus, we propose REKI\nto acquire two types of external knowledge about users and items from LLMs.\nSpecifically, we introduce factorization prompting to elicit accurate knowledge\nreasoning on user preferences and items. We develop individual knowledge\nextraction and collective knowledge extraction tailored for different scales of\nscenarios, effectively reducing offline resource consumption. Subsequently,\ngenerated knowledge undergoes efficient transformation and condensation into\naugmented vectors through a hybridized expert-integrated network, ensuring\ncompatibility. The obtained vectors can then be used to enhance any\nconventional recommendation model. We also ensure efficient inference by\npreprocessing and prestoring the knowledge from LLMs. Experiments demonstrate\nthat REKI outperforms state-of-the-art baselines and is compatible with lots of\nrecommendation algorithms and tasks. Now, REKI has been deployed to Huawei's\nnews and music recommendation platforms and gained a 7% and 1.99% improvement\nduring the online A/B test."
                },
                "authors": [
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Muyan Weng"
                    },
                    {
                        "name": "Xiaoling Cai"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.10933",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10516v1",
                "updated": "2024-08-20T03:33:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    3,
                    33,
                    4,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T03:33:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    3,
                    33,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken\n  Dialogue Systems to Low-Resource User Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken\n  Dialogue Systems to Low-Resource User Groups"
                },
                "summary": "This study addresses the interaction challenges encountered by spoken\ndialogue systems (SDSs) when engaging with users who exhibit distinct\nconversational behaviors, particularly minors, in scenarios where data are\nscarce. We propose a novel data augmentation framework to enhance SDS\nperformance for user groups with limited resources. Our approach leverages a\nlarge language model (LLM) to extract speaker styles and a pre-trained language\nmodel (PLM) to simulate dialogue act history. This method generates enriched\nand personalized dialogue data, facilitating improved interactions with unique\nuser demographics. Extensive experiments validate the efficacy of our\nmethodology, highlighting its potential to foster the development of more\nadaptive and inclusive dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the interaction challenges encountered by spoken\ndialogue systems (SDSs) when engaging with users who exhibit distinct\nconversational behaviors, particularly minors, in scenarios where data are\nscarce. We propose a novel data augmentation framework to enhance SDS\nperformance for user groups with limited resources. Our approach leverages a\nlarge language model (LLM) to extract speaker styles and a pre-trained language\nmodel (PLM) to simulate dialogue act history. This method generates enriched\nand personalized dialogue data, facilitating improved interactions with unique\nuser demographics. Extensive experiments validate the efficacy of our\nmethodology, highlighting its potential to foster the development of more\nadaptive and inclusive dialogue systems."
                },
                "authors": [
                    {
                        "name": "Zhiyang Qi"
                    },
                    {
                        "name": "Michimasa Inaba"
                    }
                ],
                "author_detail": {
                    "name": "Michimasa Inaba"
                },
                "author": "Michimasa Inaba",
                "arxiv_comment": "Accepted to SIGDIAL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]